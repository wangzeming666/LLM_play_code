scikit-learn user guide
Release 0.18.2
scikit-learn developers
Jun 28, 2017
CONTENTS
1
Welcome to scikit-learn
1
1.1
Installing scikit-learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Frequently Asked Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4
Related Projects
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.5
About us . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.6
Who is using scikit-learn? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.7
Release history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2
scikit-learn Tutorials
91
2.1
An introduction to machine learning with scikit-learn . . . . . . . . . . . . . . . . . . . . . . . . . .
91
2.2
A tutorial on statistical-learning for scientiﬁc data processing
. . . . . . . . . . . . . . . . . . . . .
97
2.3
Working With Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
2.4
Choosing the right estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
2.5
External Resources, Videos and Talks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
3
User Guide
133
3.1
Supervised learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
3.2
Unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
3.3
Model selection and evaluation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
3.4
Dataset transformations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
3.5
Dataset loading utilities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
3.6
Strategies to scale computationally: bigger data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542
3.7
Computational Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
4
General examples
553
4.1
Plotting Cross-Validated Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
4.2
Isotonic Regression
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554
4.3
Concatenating multiple feature extraction methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 556
4.4
Pipelining: chaining a PCA and a logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . 557
4.5
Selecting dimensionality reduction with Pipeline and GridSearchCV . . . . . . . . . . . . . . . . . . 559
4.6
Imputing missing values before building an estimator . . . . . . . . . . . . . . . . . . . . . . . . . . 561
4.7
Face completion with a multi-output estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
4.8
Multilabel classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565
4.9
The Johnson-Lindenstrauss bound for embedding with random projections . . . . . . . . . . . . . . 568
4.10
Comparison of kernel ridge regression and SVR
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 573
4.11
Feature Union with Heterogeneous Data Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577
4.12
Explicit feature map approximation for RBF kernels . . . . . . . . . . . . . . . . . . . . . . . . . . 580
5
Examples based on real world datasets
585
5.1
Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation . . . . . . . 585
i
5.2
Outlier detection on a real data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587
5.3
Compressive sensing: tomography reconstruction with L1 prior (Lasso) . . . . . . . . . . . . . . . . 589
5.4
Faces recognition example using eigenfaces and SVMs . . . . . . . . . . . . . . . . . . . . . . . . . 592
5.5
Model Complexity Inﬂuence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595
5.6
Species distribution modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
5.7
Visualizing the stock market structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 605
5.8
Wikipedia principal eigenvector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 610
5.9
Libsvm GUI
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614
5.10
Prediction Latency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620
5.11
Out-of-core classiﬁcation of text documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626
6
Biclustering
637
6.1
A demo of the Spectral Co-Clustering algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 637
6.2
A demo of the Spectral Biclustering algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639
6.3
Biclustering documents with the Spectral Co-clustering algorithm . . . . . . . . . . . . . . . . . . . 642
7
Calibration
647
7.1
Comparison of Calibration of Classiﬁers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647
7.2
Probability Calibration curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650
7.3
Probability calibration of classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
7.4
Probability Calibration for 3-class classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656
8
Classiﬁcation
661
8.1
Recognizing hand-written digits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661
8.2
Normal and Shrinkage Linear Discriminant Analysis for classiﬁcation . . . . . . . . . . . . . . . . . 663
8.3
Plot classiﬁcation probability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 665
8.4
Classiﬁer comparison
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 668
8.5
Linear and Quadratic Discriminant Analysis with conﬁdence ellipsoid . . . . . . . . . . . . . . . . . 671
9
Clustering
675
9.1
A demo of the mean-shift clustering algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675
9.2
Feature agglomeration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677
9.3
Demonstration of k-means assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 678
9.4
A demo of structured Ward hierarchical clustering on a raccoon face image . . . . . . . . . . . . . . 680
9.5
Online learning of a dictionary of parts of faces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 682
9.6
Demo of afﬁnity propagation clustering algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . 685
9.7
Hierarchical clustering: structured vs unstructured ward
. . . . . . . . . . . . . . . . . . . . . . . . 687
9.8
Agglomerative clustering with and without structure . . . . . . . . . . . . . . . . . . . . . . . . . . 690
9.9
K-means Clustering
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 692
9.10
Segmenting the picture of a raccoon face in regions . . . . . . . . . . . . . . . . . . . . . . . . . . . 694
9.11
Demo of DBSCAN clustering algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 697
9.12
Spectral clustering for image segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 699
9.13
Vector Quantization Example
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 702
9.14
Various Agglomerative Clustering on a 2D embedding of digits
. . . . . . . . . . . . . . . . . . . . 704
9.15
Color Quantization using K-Means
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 706
9.16
Agglomerative clustering with different metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 709
9.17
Comparison of the K-Means and MiniBatchKMeans clustering algorithms
. . . . . . . . . . . . . . 713
9.18
Feature agglomeration vs. univariate selection
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 715
9.19
Compare BIRCH and MiniBatchKMeans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718
9.20
Empirical evaluation of the impact of k-means initialization . . . . . . . . . . . . . . . . . . . . . . 720
9.21
Adjustment for chance in clustering performance evaluation . . . . . . . . . . . . . . . . . . . . . . 723
9.22
A demo of K-Means clustering on the handwritten digits data
. . . . . . . . . . . . . . . . . . . . . 726
9.23
Comparing different clustering algorithms on toy datasets
. . . . . . . . . . . . . . . . . . . . . . . 729
9.24
Selecting the number of clusters with silhouette analysis on KMeans clustering . . . . . . . . . . . . 732
ii
10 Covariance estimation
737
10.1
Ledoit-Wolf vs OAS estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 737
10.2
Sparse inverse covariance estimation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 739
10.3
Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood . . . . . . . . . . . . . . . 742
10.4
Outlier detection with several methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 745
10.5
Robust covariance estimation and Mahalanobis distances relevance
. . . . . . . . . . . . . . . . . . 747
10.6
Robust vs Empirical covariance estimate
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 750
11 Cross decomposition
755
11.1
Compare cross decomposition methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 755
12 Dataset examples
761
12.1
The Digit Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 761
12.2
The Iris Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 762
12.3
Plot randomly generated classiﬁcation dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763
12.4
Plot randomly generated multilabel dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 765
13 Decomposition
769
13.1
PCA example with Iris Data-set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 769
13.2
Incremental PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770
13.3
Comparison of LDA and PCA 2D projection of Iris dataset . . . . . . . . . . . . . . . . . . . . . . . 772
13.4
Blind source separation using FastICA
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 774
13.5
FastICA on 2D point clouds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776
13.6
Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 778
13.7
Principal components analysis (PCA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 780
13.8
Model selection with Probabilistic PCA and Factor Analysis (FA) . . . . . . . . . . . . . . . . . . . 782
13.9
Sparse coding with a precomputed dictionary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785
13.10 Faces dataset decompositions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787
13.11 Image denoising using dictionary learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 793
14 Ensemble methods
799
14.1
Decision Tree Regression with AdaBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 799
14.2
Pixel importances with a parallel forest of trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 800
14.3
IsolationForest example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 802
14.4
Feature importances with forests of trees
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 804
14.5
Plot the decision boundaries of a VotingClassiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 806
14.6
Comparing random forests and the multi-output meta estimator
. . . . . . . . . . . . . . . . . . . . 808
14.7
Gradient Boosting regression
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 810
14.8
Prediction Intervals for Gradient Boosting Regression
. . . . . . . . . . . . . . . . . . . . . . . . . 813
14.9
Plot class probabilities calculated by the VotingClassiﬁer . . . . . . . . . . . . . . . . . . . . . . . . 815
14.10 Gradient Boosting regularization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817
14.11 OOB Errors for Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 819
14.12 Two-class AdaBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 821
14.13 Hashing feature transformation using Totally Random Trees . . . . . . . . . . . . . . . . . . . . . . 824
14.14 Partial Dependence Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 826
14.15 Discrete versus Real AdaBoost
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 829
14.16 Multi-class AdaBoosted Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 831
14.17 Feature transformations with ensembles of trees
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 833
14.18 Gradient Boosting Out-of-Bag estimates
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836
14.19 Single estimator versus bagging: bias-variance decomposition . . . . . . . . . . . . . . . . . . . . . 839
14.20 Plot the decision surfaces of ensembles of trees on the iris dataset . . . . . . . . . . . . . . . . . . . 844
15 Tutorial exercises
849
15.1
Digits Classiﬁcation Exercise
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 849
15.2
Cross-validation on Digits Dataset Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 849
iii
15.3
SVM Exercise
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 851
15.4
Cross-validation on diabetes Dataset Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 853
16 Feature Selection
857
16.1
Pipeline Anova SVM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 857
16.2
Recursive feature elimination
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 857
16.3
Comparison of F-test and mutual information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 859
16.4
Recursive feature elimination with cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . 860
16.5
Feature selection using SelectFromModel and LassoCV . . . . . . . . . . . . . . . . . . . . . . . . 862
16.6
Univariate Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 863
16.7
Test with permutations the signiﬁcance of a classiﬁcation score
. . . . . . . . . . . . . . . . . . . . 867
17 Gaussian Process for Machine Learning
871
17.1
Illustration of Gaussian process classiﬁcation (GPC) on the XOR dataset
. . . . . . . . . . . . . . . 871
17.2
Gaussian process classiﬁcation (GPC) on iris dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 872
17.3
Comparison of kernel ridge and Gaussian process regression . . . . . . . . . . . . . . . . . . . . . . 874
17.4
Gaussian process regression (GPR) on Mauna Loa CO2 data.
. . . . . . . . . . . . . . . . . . . . . 877
17.5
Illustration of prior and posterior Gaussian process for different kernels . . . . . . . . . . . . . . . . 880
17.6
Iso-probability lines for Gaussian Processes classiﬁcation (GPC) . . . . . . . . . . . . . . . . . . . . 883
17.7
Probabilistic predictions with Gaussian process classiﬁcation (GPC) . . . . . . . . . . . . . . . . . . 886
17.8
Gaussian process regression (GPR) with noise-level estimation
. . . . . . . . . . . . . . . . . . . . 889
17.9
Gaussian Processes regression: basic introductory example . . . . . . . . . . . . . . . . . . . . . . . 891
18 Generalized Linear Models
895
18.1
Lasso path using LARS
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 895
18.2
Plot Ridge coefﬁcients as a function of the regularization . . . . . . . . . . . . . . . . . . . . . . . . 896
18.3
Path with L1- Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 898
18.4
SGD: Maximum margin separating hyperplane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 900
18.5
SGD: convex loss functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 902
18.6
Plot Ridge coefﬁcients as a function of the L2 regularization . . . . . . . . . . . . . . . . . . . . . . 903
18.7
Ordinary Least Squares and Ridge Regression Variance
. . . . . . . . . . . . . . . . . . . . . . . . 905
18.8
Logistic function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 906
18.9
Polynomial interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 908
18.10 Linear Regression Example
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 910
18.11 Logistic Regression 3-class Classiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912
18.12 SGD: Weighted samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 914
18.13 Lasso on dense and sparse data
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 915
18.14 Lasso and Elastic Net for Sparse Signals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 916
18.15 Sparsity Example: Fitting only features 1 and 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 919
18.16 Joint feature selection with multi-task Lasso
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 920
18.17 Comparing various online solvers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 922
18.18 Robust linear model estimation using RANSAC
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 924
18.19 HuberRegressor vs Ridge on dataset with strong outliers . . . . . . . . . . . . . . . . . . . . . . . . 926
18.20 SGD: Penalties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 928
18.21 Bayesian Ridge Regression
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 930
18.22 Automatic Relevance Determination Regression (ARD)
. . . . . . . . . . . . . . . . . . . . . . . . 933
18.23 Orthogonal Matching Pursuit
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 935
18.24 Plot multi-class SGD on the iris dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 939
18.25 Theil-Sen Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 941
18.26 L1 Penalty and Sparsity in Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 944
18.27 Plot multinomial and One-vs-Rest Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . 947
18.28 Robust linear estimator ﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 949
18.29 Lasso and Elastic Net
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 951
18.30 Lasso model selection: Cross-Validation / AIC / BIC . . . . . . . . . . . . . . . . . . . . . . . . . . 954
iv
18.31 Sparse recovery: feature selection for sparse linear models . . . . . . . . . . . . . . . . . . . . . . . 958
19 Manifold learning
963
19.1
Swiss Roll reduction with LLE
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 963
19.2
Multi-dimensional scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 964
19.3
Comparison of Manifold Learning methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 967
19.4
Manifold Learning methods on a severed sphere
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 969
19.5
Manifold learning on handwritten digits: Locally Linear Embedding, Isomap... . . . . . . . . . . . . 973
20 Gaussian Mixture Models
981
20.1
Density Estimation for a Gaussian mixture
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 981
20.2
Gaussian Mixture Model Ellipsoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 982
20.3
Gaussian Mixture Model Selection
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 984
20.4
GMM covariances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 987
20.5
Gaussian Mixture Model Sine Curve
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 990
20.6
Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture . . . . . . . . . . . . . . 994
21 Model Selection
997
21.1
Plotting Validation Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 997
21.2
Underﬁtting vs. Overﬁtting
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 998
21.3
Train error vs Test error
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1000
21.4
Receiver Operating Characteristic (ROC) with cross validation . . . . . . . . . . . . . . . . . . . . . 1002
21.5
Parameter estimation using grid search with cross-validation . . . . . . . . . . . . . . . . . . . . . . 1004
21.6
Confusion matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1006
21.7
Comparing randomized search and grid search for hyperparameter estimation . . . . . . . . . . . . . 1009
21.8
Nested versus non-nested cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1010
21.9
Sample pipeline for text feature extraction and evaluation
. . . . . . . . . . . . . . . . . . . . . . . 1013
21.10 Precision-Recall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1015
21.11 Receiver Operating Characteristic (ROC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1018
21.12 Plotting Learning Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1022
22 Nearest Neighbors
1027
22.1
Nearest Neighbors regression
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1027
22.2
Nearest Neighbors Classiﬁcation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1028
22.3
Nearest Centroid Classiﬁcation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1030
22.4
Kernel Density Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1032
22.5
Kernel Density Estimate of Species Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1034
22.6
Simple 1D Kernel Density Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1037
22.7
Hyper-parameters of Approximate Nearest Neighbors
. . . . . . . . . . . . . . . . . . . . . . . . . 1040
22.8
Scalability of Approximate Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1043
23 Neural Networks
1047
23.1
Visualization of MLP weights on MNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1047
23.2
Restricted Boltzmann Machine features for digit classiﬁcation . . . . . . . . . . . . . . . . . . . . . 1049
23.3
Compare Stochastic learning strategies for MLPClassiﬁer
. . . . . . . . . . . . . . . . . . . . . . . 1053
23.4
Varying regularization in Multi-layer Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1057
24 Preprocessing
1061
24.1
Using FunctionTransformer to select columns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1061
24.2
Robust Scaling on Toy Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1063
25 Semi Supervised Classiﬁcation
1065
25.1
Label Propagation learning a complex structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1065
25.2
Label Propagation digits: Demonstrating performance . . . . . . . . . . . . . . . . . . . . . . . . . 1066
25.3
Decision boundary of label propagation versus SVM on the Iris dataset . . . . . . . . . . . . . . . . 1069
v
25.4
Label Propagation digits active learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1071
26 Support Vector Machines
1077
26.1
Support Vector Regression (SVR) using linear and non-linear kernels . . . . . . . . . . . . . . . . . 1077
26.2
Non-linear SVM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1078
26.3
SVM: Maximum margin separating hyperplane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1080
26.4
SVM: Separating hyperplane for unbalanced classes . . . . . . . . . . . . . . . . . . . . . . . . . . 1081
26.5
SVM-Anova: SVM with univariate feature selection . . . . . . . . . . . . . . . . . . . . . . . . . . 1083
26.6
SVM with custom kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1085
26.7
SVM: Weighted samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1086
26.8
SVM-Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1088
26.9
SVM Margins Example
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1090
26.10 Plot different SVM classiﬁers in the iris dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1092
26.11 One-class SVM with non-linear kernel (RBF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1094
26.12 Scaling the regularization parameter for SVCs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1096
26.13 RBF SVM parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1099
27 Working with text documents
1105
27.1
FeatureHasher and DictVectorizer Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1105
27.2
Classiﬁcation of text documents: using a MLComp dataset . . . . . . . . . . . . . . . . . . . . . . . 1107
27.3
Clustering text documents using k-means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1109
27.4
Classiﬁcation of text documents using sparse features
. . . . . . . . . . . . . . . . . . . . . . . . . 1113
28 Decision Trees
1119
28.1
Decision Tree Regression
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1119
28.2
Multi-output Decision Tree Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1121
28.3
Plot the decision surface of a decision tree on the iris dataset . . . . . . . . . . . . . . . . . . . . . . 1122
28.4
Understanding the decision tree structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1124
29 API Reference
1129
29.1
sklearn.base: Base classes and utility functions . . . . . . . . . . . . . . . . . . . . . . . . . . 1129
29.2
sklearn.cluster: Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1133
29.3
sklearn.cluster.bicluster: Biclustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 1169
29.4
sklearn.covariance: Covariance Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 1174
29.5
sklearn.model_selection: Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 1203
29.6
sklearn.datasets: Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1249
29.7
sklearn.decomposition: Matrix Decomposition
. . . . . . . . . . . . . . . . . . . . . . . . 1295
29.8
sklearn.dummy: Dummy estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1349
29.9
sklearn.ensemble: Ensemble Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1354
29.10 sklearn.exceptions: Exceptions and warnings
. . . . . . . . . . . . . . . . . . . . . . . . . 1383
29.11 sklearn.feature_extraction: Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . 1386
29.12 sklearn.feature_selection: Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . 1413
29.13 sklearn.gaussian_process: Gaussian Processes . . . . . . . . . . . . . . . . . . . . . . . . 1444
29.14 sklearn.isotonic: Isotonic regression
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1476
29.15 sklearn.kernel_approximation Kernel Approximation . . . . . . . . . . . . . . . . . . . 1481
29.16 sklearn.kernel_ridge Kernel Ridge Regression
. . . . . . . . . . . . . . . . . . . . . . . . 1489
29.17 sklearn.discriminant_analysis: Discriminant Analysis . . . . . . . . . . . . . . . . . . 1492
29.18 sklearn.linear_model: Generalized Linear Models . . . . . . . . . . . . . . . . . . . . . . . 1501
29.19 sklearn.manifold: Manifold Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1606
29.20 sklearn.metrics: Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1622
29.21 sklearn.mixture: Gaussian Mixture Models
. . . . . . . . . . . . . . . . . . . . . . . . . . . 1686
29.22 sklearn.multiclass: Multiclass and multilabel classiﬁcation . . . . . . . . . . . . . . . . . . 1697
29.23 sklearn.multioutput: Multioutput regression and classiﬁcation
. . . . . . . . . . . . . . . . 1705
29.24 sklearn.naive_bayes: Naive Bayes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1709
29.25 sklearn.neighbors: Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1719
vi
29.26 sklearn.neural_network: Neural network models . . . . . . . . . . . . . . . . . . . . . . . 1770
29.27 sklearn.calibration: Probability Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . 1783
29.28 sklearn.cross_decomposition: Cross decomposition
. . . . . . . . . . . . . . . . . . . . 1787
29.29 sklearn.pipeline: Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1801
29.30 sklearn.preprocessing: Preprocessing and Normalization . . . . . . . . . . . . . . . . . . . 1808
29.31 sklearn.random_projection: Random projection . . . . . . . . . . . . . . . . . . . . . . . 1845
29.32 sklearn.semi_supervised Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . 1851
29.33 sklearn.svm: Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1857
29.34 sklearn.tree: Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1890
29.35 sklearn.utils: Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1912
29.36 Recently deprecated . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1915
30 Developer’s Guide
1973
30.1
Contributing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1973
30.2
Developers’ Tips for Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1987
30.3
Utilities for Developers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1988
30.4
How to optimize for speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1992
30.5
Advanced installation instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1999
30.6
Maintainer / core-developer information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2005
Bibliography
2007
Index
2015
vii
viii
CHAPTER
ONE
WELCOME TO SCIKIT-LEARN
1.1 Installing scikit-learn
Note: If you wish to contribute to the project, it’s recommended you install the latest development version.
1.1.1 Installing the latest release
Scikit-learn requires:
• Python (>= 2.6 or >= 3.3),
• NumPy (>= 1.6.1),
• SciPy (>= 0.9).
If you already have a working installation of numpy and scipy, the easiest way to install scikit-learn is using pip
pip install -U scikit-learn
or conda:
conda install scikit-learn
If you have not installed NumPy or SciPy yet, you can also install these using conda or pip. When using pip, please
ensure that binary wheels are used, and NumPy and SciPy are not recompiled from source, which can happen when
using particular conﬁgurations of operating system and hardware (such as Linux on a Raspberry Pi). Building numpy
and scipy from source can be complex (especially on Windows) and requires careful conﬁguration to ensure that they
link against an optimized implementation of linear algebra routines. Instead, use a third-party distribution as described
below.
If you must install scikit-learn and its dependencies with pip, you can install it as scikit-learn[alldeps]. The
most common use case for this is in a requirements.txt ﬁle used as part of an automated build process for a
PaaS application or a Docker image. This option is not intended for manual installation from the command line.
1.1.2 Third-party Distributions
If you don’t already have a python installation with numpy and scipy, we recommend to install either via your package
manager or via a python bundle. These come with numpy, scipy, scikit-learn, matplotlib and many other helpful
1
scikit-learn user guide, Release 0.18.2
scientiﬁc and data processing libraries.
Available options are:
Canopy and Anaconda for all supported platforms
Canopy and Anaconda both ship a recent version of scikit-learn, in addition to a large set of scientiﬁc python library
for Windows, Mac OSX and Linux.
Anaconda offers scikit-learn as part of its free distribution.
Warning:
To upgrade or uninstall scikit-learn installed with Anaconda or conda you should not use the pip
command. Instead:
To upgrade scikit-learn:
conda update scikit-learn
To uninstall scikit-learn:
conda remove scikit-learn
Upgrading with pip install -U scikit-learn or uninstalling pip uninstall scikit-learn is
likely fail to properly remove ﬁles installed by the conda command.
pip upgrade and uninstall operations only work on packages installed via pip install.
WinPython for Windows
The WinPython project distributes scikit-learn as an additional plugin.
For installation instructions for particular operating systems or for compiling the bleeding edge version, see the Ad-
vanced installation instructions.
1.2 Frequently Asked Questions
Here we try to give some answers to questions that regularly pop up on the mailing list.
1.2.1 What is the project name (a lot of people get it wrong)?
scikit-learn, but not scikit or SciKit nor sci-kit learn. Also not scikits.learn or scikits-learn, which were previously
used.
1.2.2 How do you pronounce the project name?
sy-kit learn. sci stands for science!
1.2.3 Why scikit?
There are multiple scikits, which are scientiﬁc toolboxes build around SciPy. You can ﬁnd a list at https://scikits.
appspot.com/scikits. Apart from scikit-learn, another popular one is scikit-image.
2
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
1.2.4 How can I contribute to scikit-learn?
See Contributing. Before wanting to add a new algorithm, which is usually a major and lengthy undertaking, it is
recommended to start with known issues.
1.2.5 What’s the best way to get help on scikit-learn usage?
For general machine learning questions, please use Cross Validated with the [machine-learning] tag.
For scikit-learn usage questions, please use Stack Overﬂow with the [scikit-learn] and [python] tags. You
can alternatively use the mailing list.
Please make sure to include a minimal reproduction code snippet (ideally shorter than 10 lines) that highlights
your problem on a toy dataset (for instance from sklearn.datasets or randomly generated with functions of
numpy.random with a ﬁxed random seed). Please remove any line of code that is not necessary to reproduce your
problem.
The problem should be reproducible by simply copy-pasting your code snippet in a Python shell with scikit-learn
installed. Do not forget to include the import statements.
More guidance to write good reproduction code snippets can be found at:
http://stackoverﬂow.com/help/mcve
If your problem raises an exception that you do not understand (even after googling it), please make sure to include
the full traceback that you obtain when running the reproduction script.
For bug reports or feature requests, please make use of the issue tracker on Github.
There is also a scikit-learn Gitter channel where some users and developers might be found.
Please do not email any authors directly to ask for assistance, report bugs, or for any other issue related to
scikit-learn.
1.2.6 How can I create a bunch object?
Don’t make a bunch object! They are not part of the scikit-learn API. Bunch objects are just a way to package some
numpy arrays. As a scikit-learn user you only ever need numpy arrays to feed your model with data.
For instance to train a classiﬁer, all you need is a 2D array X for the input variables and a 1D array y for the target
variables. The array X holds the features as columns and samples as rows . The array y contains integer values to
encode the class membership of each sample in X.
1.2.7 How can I load my own datasets into a format usable by scikit-learn?
Generally, scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that
are convertible to numeric arrays such as pandas DataFrame are also acceptable.
For more information on loading your data ﬁles into these usable data structures, please refer to loading external
datasets.
1.2.8 What are the inclusion criteria for new algorithms ?
We only consider well-established algorithms for inclusion. A rule of thumb is at least 3 years since publication, 200+
citations and wide use and usefulness. A technique that provides a clear-cut improvement (e.g. an enhanced data
structure or a more efﬁcient approximation technique) on a widely-used method will also be considered for inclusion.
1.2. Frequently Asked Questions
3
scikit-learn user guide, Release 0.18.2
From the algorithms or techniques that meet the above criteria, only those which ﬁt well within the current API of
scikit-learn, that is a fit, predict/transform interface and ordinarily having input/output that is a numpy array
or sparse matrix, are accepted.
The contributor should support the importance of the proposed addition with research papers and/or implementations
in other similar packages, demonstrate its usefulness via common use-cases/applications and corroborate performance
improvements, if any, with benchmarks and/or plots. It is expected that the proposed algorithm should outperform the
methods that are already implemented in scikit-learn at least in some areas.
Also note that your implementation need not be in scikit-learn to be used together with scikit-learn tools. You can
implement your favorite algorithm in a scikit-learn compatible way, upload it to github and let us know. We will list it
under Related Projects.
1.2.9 Why are you so selective on what algorithms you include in scikit-learn?
Code is maintenance cost, and we need to balance the amount of code we have with the size of the team (and add to
this the fact that complexity scales non linearly with the number of features). The package relies on core developers
using their free time to ﬁx bugs, maintain code and review contributions. Any algorithm that is added needs future
attention by the developers, at which point the original author might long have lost interest. Also see this thread on the
mailing list.
1.2.10 Why did you remove HMMs from scikit-learn?
See Will you add graphical models or sequence prediction to scikit-learn?.
1.2.11 Will you add graphical models or sequence prediction to scikit-learn?
Not in the foreseeable future. scikit-learn tries to provide a uniﬁed API for the basic tasks in machine learning, with
pipelines and meta-algorithms like grid search to tie everything together. The required concepts, APIs, algorithms
and expertise required for structured learning are different from what scikit-learn has to offer. If we started doing
arbitrary structured learning, we’d need to redesign the whole package and the project would likely collapse under its
own weight.
There are two project with API similar to scikit-learn that do structured prediction:
• pystruct handles general structured learning (focuses on SSVMs on arbitrary graph structures with approximate
inference; deﬁnes the notion of sample as an instance of the graph structure)
• seqlearn handles sequences only (focuses on exact inference; has HMMs, but mostly for the sake of complete-
ness; treats a feature vector as a sample and uses an offset encoding for the dependencies between feature
vectors)
1.2.12 Will you add GPU support?
No, or at least not in the near future. The main reason is that GPU support will introduce many software dependencies
and introduce platform speciﬁc issues. scikit-learn is designed to be easy to install on a wide variety of platforms.
Outside of neural networks, GPUs don’t play a large role in machine learning today, and much larger gains in speed
can often be achieved by a careful choice of algorithms.
4
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
1.2.13 Do you support PyPy?
In case you didn’t know, PyPy is the new, fast, just-in-time compiling Python implementation. We don’t support it.
When the NumPy support in PyPy is complete or near-complete, and SciPy is ported over as well, we can start thinking
of a port. We use too much of NumPy to work with a partial implementation.
1.2.14 How do I deal with string data (or trees, graphs...)?
scikit-learn estimators assume you’ll feed them real-valued feature vectors. This assumption is hard-coded in pretty
much all of the library. However, you can feed non-numerical inputs to estimators in several ways.
If you have text documents, you can use a term frequency features; see Text feature extraction for the built-in text
vectorizers. For more general feature extraction from any kind of data, see Loading features from dicts and Feature
hashing.
Another common case is when you have non-numerical data and a custom distance (or similarity) metric on these data.
Examples include strings with edit distance (aka. Levenshtein distance; e.g., DNA or RNA sequences). These can be
encoded as numbers, but doing so is painful and error-prone. Working with distance metrics on arbitrary data can be
done in two ways.
Firstly, many estimators take precomputed distance/similarity matrices, so if the dataset is not too large, you can
compute distances for all pairs of inputs. If the dataset is large, you can use feature vectors with only one “feature”,
which is an index into a separate data structure, and supply a custom metric function that looks up the actual data in
this data structure. E.g., to use DBSCAN with Levenshtein distances:
>>> from leven import levenshtein
>>> import numpy as np
>>> from sklearn.cluster import dbscan
>>> data = ["ACCTCCTAGAAG", "ACCTACTAGAAGTT", "GAATATTAGGCCGA"]
>>> def lev_metric(x, y):
...
i, j = int(x[0]), int(y[0])
# extract indices
...
return levenshtein(data[i], data[j])
...
>>> X = np.arange(len(data)).reshape(-1, 1)
>>> X
array([[0],
[1],
[2]])
>>> dbscan(X, metric=lev_metric, eps=5, min_samples=2)
([0, 1], array([ 0,
0, -1]))
(This uses the third-party edit distance package leven.)
Similar tricks can be used, with some care, for tree kernels, graph kernels, etc.
1.2.15 Why do I sometime get a crash/freeze with n_jobs > 1 under OSX or Linux?
Several scikit-learn tools such as GridSearchCV and cross_val_score rely internally on Python’s multipro-
cessing module to parallelize execution onto several Python processes by passing n_jobs > 1 as argument.
The problem is that Python multiprocessing does a fork system call without following it with an exec system
call for performance reasons. Many libraries like (some versions of) Accelerate / vecLib under OSX, (some versions
of) MKL, the OpenMP runtime of GCC, nvidia’s Cuda (and probably many others), manage their own internal thread
pool. Upon a call to fork, the thread pool state in the child process is corrupted: the thread pool believes it has many
threads while only the main thread state has been forked. It is possible to change the libraries to make them detect
1.2. Frequently Asked Questions
5
scikit-learn user guide, Release 0.18.2
when a fork happens and reinitialize the thread pool in that case: we did that for OpenBLAS (merged upstream in
master since 0.2.10) and we contributed a patch to GCC’s OpenMP runtime (not yet reviewed).
But in the end the real culprit is Python’s multiprocessing that does fork without exec to reduce the overhead
of starting and using new Python processes for parallel computing. Unfortunately this is a violation of the POSIX
standard and therefore some software editors like Apple refuse to consider the lack of fork-safety in Accelerate /
vecLib as a bug.
In Python 3.4+ it is now possible to conﬁgure multiprocessing to use the ‘forkserver’ or ‘spawn’ start methods
(instead of the default ‘fork’) to manage the process pools. To work around this issue when using scikit-learn, you
can set the JOBLIB_START_METHOD environment variable to ‘forkserver’. However the user should be aware that
using the ‘forkserver’ method prevents joblib.Parallel to call function interactively deﬁned in a shell session.
If you have custom code that uses multiprocessing directly instead of using it via joblib you can enable the
‘forkserver’ mode globally for your program: Insert the following instructions in your main script:
import multiprocessing
# other imports, custom code, load data, define model...
if __name__ == '__main__':
multiprocessing.set_start_method('forkserver')
# call scikit-learn utils with n_jobs > 1 here
You can ﬁnd more default on the new start methods in the multiprocessing documentation.
1.2.16 Why is there no support for deep or reinforcement learning / Will there be
support for deep or reinforcement learning in scikit-learn?
Deep learning and reinforcement learning both require a rich vocabulary to deﬁne an architecture, with deep learning
additionally requiring GPUs for efﬁcient computing. However, neither of these ﬁt within the design constraints of
scikit-learn; as a result, deep learning and reinforcement learning are currently out of scope for what scikit-learn seeks
to achieve.
1.2.17 Why is my pull request not getting any attention?
The scikit-learn review process takes a signiﬁcant amount of time, and contributors should not be discouraged by a
lack of activity or review on their pull request. We care a lot about getting things right the ﬁrst time, as maintenance
and later change comes at a high cost. We rarely release any “experimental” code, so all of our contributions will be
subject to high use immediately and should be of the highest quality possible initially.
Beyond that, scikit-learn is limited in its reviewing bandwidth; many of the reviewers and core developers are working
on scikit-learn on their own time. If a review of your pull request comes slowly, it is likely because the reviewers are
busy. We ask for your understanding and request that you not close your pull request or discontinue your work solely
because of this reason.
1.2.18 How do I set a random_state for an entire execution?
For testing and replicability, it is often important to have the entire execution controlled by a single seed for the pseudo-
random number generator used in algorithms that have a randomized component. Scikit-learn does not use its own
global random state; whenever a RandomState instance or an integer random seed is not provided as an argument, it
relies on the numpy global random state, which can be set using numpy.random.seed. For example, to set an
execution’s numpy global random state to 42, one could execute the following in his or her script:
6
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
import numpy as np
np.random.seed(42)
However, a global random state is prone to modiﬁcation by other code during execution. Thus, the only way to ensure
replicability is to pass RandomState instances everywhere and ensure that both estimators and cross-validation
splitters have their random_state parameter set.
1.3 Support
There are several ways to get in touch with the developers.
1.3.1 Mailing List
• The main mailing list is scikit-learn.
• There is also a commit list scikit-learn-commits, where updates to the main repository and test failures get
notiﬁed.
1.3.2 User questions
• Some scikit-learn developers support users on StackOverﬂow using the [scikit-learn] tag.
• For general theoretical or methodological Machine Learning questions stack exchange is probably a more suit-
able venue.
In both cases please use a descriptive question in the title ﬁeld (e.g. no “Please help with scikit-learn!” as this is not a
question) and put details on what you tried to achieve, what were the expected results and what you observed instead
in the details ﬁeld.
Code and data snippets are welcome. Minimalistic (up to ~20 lines long) reproduction script very helpful.
Please describe the nature of your data and the how you preprocessed it: what is the number of samples, what is the
number and type of features (i.d. categorical or numerical) and for supervised learning tasks, what target are your
trying to predict: binary, multiclass (1 out of n_classes) or multilabel (k out of n_classes) classiﬁcation or
continuous variable regression.
1.3.3 Bug tracker
If you think you’ve encountered a bug, please report it to the issue tracker:
https://github.com/scikit-learn/scikit-learn/issues
Don’t forget to include:
• steps (or better script) to reproduce,
• expected outcome,
• observed outcome or python (or gdb) tracebacks
To help developers ﬁx your bug faster, please link to a https://gist.github.com holding a standalone minimalistic python
script that reproduces your bug and optionally a minimalistic subsample of your dataset (for instance exported as CSV
ﬁles using numpy.savetxt).
Note: gists are git cloneable repositories and thus you can use git to push dataﬁles to them.
1.3. Support
7
scikit-learn user guide, Release 0.18.2
1.3.4 IRC
Some developers like to hang out on channel #scikit-learn on irc.freenode.net.
If you do not have an IRC client or are behind a ﬁrewall this web client works ﬁne: http://webchat.freenode.net
1.3.5 Documentation resources
This documentation is relative to 0.18.2. Documentation for other versions can be found here:
• 0.17
• 0.16
• 0.15
Printable pdf documentation for all versions can be found here.
1.4 Related Projects
Below is a list of sister-projects, extensions and domain speciﬁc packages.
1.4.1 Interoperability and framework enhancements
These tools adapt scikit-learn for use with other technologies or otherwise enhance the functionality of scikit-learn’s
estimators.
• ML Frontend provides dataset management and SVM ﬁtting/prediction through web-based and programmatic
interfaces.
• sklearn_pandas bridge for scikit-learn pipelines and pandas data frame with dedicated transformers.
• Scikit-Learn Laboratory A command-line wrapper around scikit-learn that makes it easy to run machine learning
experiments with multiple learners and large feature sets.
• auto-sklearn An automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator
• TPOT An automated machine learning toolkit that optimizes a series of scikit-learn operators to design a ma-
chine learning pipeline, including data and feature preprocessors as well as the estimators. Works as a drop-in
replacement for a scikit-learn estimator.
• sklearn-pmml Serialization of (some) scikit-learn estimators into PMML.
• sklearn2pmml Serialization of a wide variety of scikit-learn estimators and transformers into PMML with the
help of JPMML-SkLearn library.
1.4.2 Other estimators and tasks
Not everything belongs or is mature enough for the central scikit-learn project. The following are projects providing
interfaces similar to scikit-learn for additional learning algorithms, infrastructures and tasks.
• pylearn2 A deep learning and neural network library build on theano with scikit-learn like interface.
• sklearn_theano scikit-learn compatible estimators, transformers, and datasets which use Theano internally
• lightning Fast state-of-the-art linear model solvers (SDCA, AdaGrad, SVRG, SAG, etc...).
• Seqlearn Sequence classiﬁcation using HMMs or structured perceptron.
8
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• HMMLearn Implementation of hidden markov models that was previously part of scikit-learn.
• PyStruct General conditional random ﬁelds and structured prediction.
• pomegranate Probabilistic modelling for Python, with an emphasis on hidden Markov models.
• py-earth Multivariate adaptive regression splines
• sklearn-compiledtrees Generate a C++ implementation of the predict function for decision trees (and ensembles)
trained by sklearn. Useful for latency-sensitive production environments.
• lda: Fast implementation of Latent Dirichlet Allocation in Cython.
• Sparse Filtering Unsupervised feature learning based on sparse-ﬁltering
• Kernel Regression Implementation of Nadaraya-Watson kernel regression with automatic bandwidth selection
• gplearn Genetic Programming for symbolic regression tasks.
• nolearn A number of wrappers and abstractions around existing neural network libraries
• sparkit-learn Scikit-learn functionality and API on PySpark.
• keras Theano-based Deep Learning library.
• mlxtend Includes a number of additional estimators as well as model visualization utilities.
• kmodes k-modes clustering algorithm for categorical data, and several of its variations.
• hdbscan HDBSCAN and Robust Single Linkage clustering algorithms for robust variable density clustering.
• lasagne A lightweight library to build and train neural networks in Theano.
• multiisotonic Isotonic regression on multidimensional features.
• spherecluster Spherical K-means and mixture of von Mises Fisher clustering routines for data on the unit hyper-
sphere.
1.4.3 Statistical learning with Python
Other packages useful for data analysis and machine learning.
• Pandas Tools for working with heterogeneous and columnar data, relational queries, time series and basic statis-
tics.
• theano A CPU/GPU array processing framework geared towards deep learning research.
• statsmodels Estimating and analysing statistical models. More focused on statistical tests and less on prediction
than scikit-learn.
• PyMC Bayesian statistical models and ﬁtting algorithms.
• REP Environment for conducting data-driven research in a consistent and reproducible way
• Sacred Tool to help you conﬁgure, organize, log and reproduce experiments
• gensim A library for topic modelling, document indexing and similarity retrieval
• Seaborn Visualization library based on matplotlib. It provides a high-level interface for drawing attractive
statistical graphics.
• Deep Learning A curated list of deep learning software libraries.
1.4. Related Projects
9
scikit-learn user guide, Release 0.18.2
Domain speciﬁc packages
• scikit-image Image processing and computer vision in python.
• Natural language toolkit (nltk) Natural language processing and some machine learning.
• NiLearn Machine learning for neuro-imaging.
• AstroML Machine learning for astronomy.
• MSMBuilder Machine learning for protein conformational dynamics time series.
1.4.4 Snippets and tidbits
The wiki has more!
1.5 About us
This is a community effort, and as such many people have contributed to it over the years.
1.5.1 History
This project was started in 2007 as a Google Summer of Code project by David Cournapeau. Later that year, Matthieu
Brucher started work on this project as part of his thesis.
In 2010 Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort and Vincent Michel of INRIA took leadership of the
project and made the ﬁrst public release, February the 1st 2010. Since then, several releases have appeared following
a ~3 month cycle, and a thriving international community has been leading the development.
1.5.2 People
The following people have been core contributors to scikit-learn’s development and maintenance:
• Mathieu Blondel
• Matthieu Brucher
• Lars Buitinck
• David Cournapeau
• Noel Dawe
• Vincent Dubourg
• Edouard Duchesnay
• Tom Dupré la Tour
• Alexander Fabisch
• Virgile Fritsch
• Satra Ghosh
• Angel Soler Gollonet
• Chris Filo Gorgolewski
• Alexandre Gramfort
• Olivier Grisel
• Jaques Grobler
• Yaroslav Halchenko
• Brian Holt
• Arnaud Joly
• Thouis (Ray) Jones
10
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Kyle Kastner
• Manoj Kumar
• Robert Layton
• Wei Li
• Paolo Losi
• Gilles Louppe
• Jan Hendrik Metzen
• Vincent Michel
• Jarrod Millman
• Andreas Müller (release manager)
• Vlad Niculae
• Joel Nothman
• Alexandre Passos
• Fabian Pedregosa
• Peter Prettenhofer
• Bertrand Thirion
• Jake VanderPlas
• Nelle Varoquaux
• Gael Varoquaux
• Ron Weiss
Please do not email the authors directly to ask for assistance or report issues. Instead, please see What’s the best way
to ask questions about scikit-learn in the FAQ.
See also:
How you can contribute to the project
1.5.3 Citing scikit-learn
If you use scikit-learn in a scientiﬁc publication, we would appreciate citations to the following paper:
Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.
Bibtex entry:
@article{scikit-learn,
title={Scikit-learn: Machine Learning in {P}ython},
author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
journal={Journal of Machine Learning Research},
volume={12},
pages={2825--2830},
year={2011}
}
If you want to cite scikit-learn for its API or design, you may also want to consider the following paper:
API design for machine learning software: experiences from the scikit-learn project, Buitinck et al., 2013.
Bibtex entry:
@inproceedings{sklearn_api,
author
= {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
1.5. About us
11
scikit-learn user guide, Release 0.18.2
and Jaques Grobler and Robert Layton and Jake VanderPlas and
Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
title
= {{API} design for machine learning software: experiences from
˓→the scikit-learn
project},
booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine
˓→Learning},
year
= {2013},
pages = {108--122},
}
1.5.4 Artwork
High quality PNG and SVG logos are available in the doc/logos/ source directory.
1.5.5 Funding
INRIA actively supports this project.
It has provided funding for Fabian Pedregosa (2010-2012), Jaques Grob-
ler (2012-2013) and Olivier Grisel (2013-2015) to work on this project full-time. It also hosts coding sprints and
other events.
Paris-Saclay Center for Data Science funded one
year for a developer to work on the project full-time (2014-2015).
NYU Moore-Sloan Data Science Environment funded Andreas Mueller (2014-2016) to work on this project.
The Moore-Sloan Data Science Environment also funds several students to work on the project part-time.
Télécom Paristech funds Manoj Kumar (2014), Tom Dupré la Tour
12
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
(2015), Raghav RV (2015-2016) and Thierry Guillemot (2016) to work on scikit-learn.
Columbia University funds Andreas Mueller since 2016.
The following students were spon-
sored by Google to work on scikit-learn through the Google Summer of Code program.
• 2007 - David Cournapeau
• 2011 - Vlad Niculae
• 2012 - Vlad Niculae, Immanuel Bayer.
• 2013 - Kemal Eren, Nicolas Trésegnie
• 2014 - Hamzeh Alsalhi, Issam Laradji, Maheshakya Wijewardena, Manoj Kumar.
• 2015 - Raghav RV, Wei Xue
• 2016 - Nelson Liu, YenChen Lin
It also provided funding for sprints and events around scikit-learn. If you would like to participate in the next Google
Summer of code program, please see this page.
The NeuroDebian project providing Debian packaging and contributions is supported by Dr. James V. Haxby (Dart-
mouth College).
The PSF helped ﬁnd and manage funding for our 2011 Granada sprint. More information can be found here
tinyclues funded the 2011 international Granada sprint.
Donating to the project
If you are interested in donating to the project or to one of our code-sprints, you can use the Paypal button below or the
NumFOCUS Donations Page (if you use the latter, please indicate that you are donating for the scikit-learn project).
All donations will be handled by NumFOCUS, a non-proﬁt-organization which is managed by a board of Scipy
community members. NumFOCUS’s mission is to foster scientiﬁc computing software, in particular in Python. As
a ﬁscal home of scikit-learn, it ensures that money is available when needed to keep the project funded and available
while in compliance with tax regulations.
The received donations for the scikit-learn project mostly will go towards covering travel-expenses for code sprints, as
well as towards the organization budget of the project 1.
1 Regarding the organization budget in particular, we might use some of the donated funds to pay for other project expenses such as DNS,
hosting or continuous integration services.
1.5. About us
13
scikit-learn user guide, Release 0.18.2
Notes
The 2013 Paris international sprint
Fig. 1.1: IAP VII/19 - DYSCO
For more information on this sprint, see here
1.5.6 Infrastructure support
• We would like to thank Rackspace for providing us with a free Rackspace Cloud account to automatically build
the documentation and the example gallery from for the development version of scikit-learn using this tool.
• We would also like to thank Shining Panda for free CPU time on their Continuous Integration server.
1.6 Who is using scikit-learn?
1.6.1 Spotify
Scikit-learn provides a toolbox with solid implementations of a bunch of state-of-the-
art models and makes it easy to plug them into existing applications. We’ve been using it quite a lot for music
recommendations at Spotify and I think it’s the most well-designed ML package I’ve seen so far.
Erik Bernhardsson, Engineering Manager Music Discovery & Machine Learning, Spotify
14
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
1.6.2 Inria
At INRIA, we use scikit-learn to support leading-edge basic research in many teams:
Parietal for neuroimaging, Lear for computer vision, Visages for medical image analysis, Privatics for security. The
project is a fantastic tool to address difﬁcult applications of machine learning in an academic environment as it is
performant and versatile, but all easy-to-use and well documented, which makes it well suited to grad students.
Gaël Varoquaux, research at Parietal
1.6.3 betaworks
Betaworks is a NYC-based startup studio that builds new products, grows companies,
and invests in others. Over the past 8 years we’ve launched a handful of social data analytics-driven services, such as
Bitly, Chartbeat, digg and Scale Model. Consistently the betaworks data science team uses Scikit-learn for a variety
of tasks. From exploratory analysis, to product development, it is an essential part of our toolkit. Recent uses are
included in digg’s new video recommender system, and Poncho’s dynamic heuristic subspace clustering.
Gilad Lotan, Chief Data Scientist
1.6.4 Evernote
Building a classiﬁer is typically an iterative process of exploring the data, selecting the
features (the attributes of the data believed to be predictive in some way), training the models, and ﬁnally evaluating
them. For many of these tasks, we relied on the excellent scikit-learn package for Python.
Read more
Mark Ayzenshtat, VP, Augmented Intelligence
1.6.5 Télécom ParisTech
At Telecom ParisTech, scikit-learn is used for hands-on sessions and home assignments
in introductory and advanced machine learning courses. The classes are for undergrads and masters students. The
1.6. Who is using scikit-learn?
15
scikit-learn user guide, Release 0.18.2
great beneﬁt of scikit-learn is its fast learning curve that allows students to quickly start working on interesting and
motivating problems.
Alexandre Gramfort, Assistant Professor
1.6.6 Booking.com
At Booking.com, we use machine learning algorithms for many different applications,
such as recommending hotels and destinations to our customers, detecting fraudulent reservations, or scheduling our
customer service agents. Scikit-learn is one of the tools we use when implementing standard algorithms for prediction
tasks. Its API and documentations are excellent and make it easy to use. The scikit-learn developers do a great job
of incorporating state of the art implementations and new algorithms into the package. Thus, scikit-learn provides
convenient access to a wide spectrum of algorithms, and allows us to readily ﬁnd the right tool for the right job.
Melanie Mueller, Data Scientist
1.6.7 AWeber
The scikit-learn toolkit is indispensable for the Data Analysis and Management team
at AWeber. It allows us to do AWesome stuff we would not otherwise have the time or resources to accomplish. The
documentation is excellent, allowing new engineers to quickly evaluate and apply many different algorithms to our
data. The text feature extraction utilities are useful when working with the large volume of email content we have
at AWeber. The RandomizedPCA implementation, along with Pipelining and FeatureUnions, allows us to develop
complex machine learning algorithms efﬁciently and reliably.
Anyone interested in learning more about how AWeber deploys scikit-learn in a production environment should check
out talks from PyData Boston by AWeber’s Michael Becker available at https://github.com/mdbecker/pydata_2013
Michael Becker, Software Engineer, Data Analysis and Management Ninjas
1.6.8 Yhat
The combination of consistent APIs, thorough documentation, and top notch implemen-
tation make scikit-learn our favorite machine learning package in Python. scikit-learn makes doing advanced analysis
in Python accessible to anyone. At Yhat, we make it easy to integrate these models into your production applications.
Thus eliminating the unnecessary dev time encountered productionizing analytical work.
Greg Lamp, Co-founder Yhat
16
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
1.6.9 Rangespan
The Python scikit-learn toolkit is a core tool in the data science group at Rangespan.
Its large collection of well documented models and algorithms allow our team of data scientists to prototype fast and
quickly iterate to ﬁnd the right solution to our learning problems. We ﬁnd that scikit-learn is not only the right tool
for prototyping, but its careful and well tested implementation give us the conﬁdence to run scikit-learn models in
production.
Jurgen Van Gael, Data Science Director at Rangespan Ltd
1.6.10 Birchbox
At Birchbox, we face a range of machine learning problems typical to E-commerce:
product recommendation, user clustering, inventory prediction, trends detection, etc. Scikit-learn lets us experiment
with many models, especially in the exploration phase of a new project: the data can be passed around in a consistent
way; models are easy to save and reuse; updates keep us informed of new developments from the pattern discovery
research community. Scikit-learn is an important tool for our team, built the right way in the right language.
Thierry Bertin-Mahieux, Birchbox, Data Scientist
1.6.11 Bestofmedia Group
Scikit-learn is our #1 toolkit for all things machine learning at Bestofmedia. We use
it for a variety of tasks (e.g. spam ﬁghting, ad click prediction, various ranking models) thanks to the varied, state-
of-the-art algorithm implementations packaged into it. In the lab it accelerates prototyping of complex pipelines. In
production I can say it has proven to be robust and efﬁcient enough to be deployed for business critical components.
Eustache Diemert, Lead Scientist Bestofmedia Group
1.6. Who is using scikit-learn?
17
scikit-learn user guide, Release 0.18.2
1.6.12 Change.org
At change.org we automate the use of scikit-learn’s RandomForestClassiﬁer in our pro-
duction systems to drive email targeting that reaches millions of users across the world each week. In the lab, scikit-
learn’s ease-of-use, performance, and overall variety of algorithms implemented has proved invaluable in giving us a
single reliable source to turn to for our machine-learning needs.
Vijay Ramesh, Software Engineer in Data/science at Change.org
1.6.13 PHIMECA Engineering
At PHIMECA Engineering, we use scikit-learn estimators as surrogates for expensive-
to-evaluate numerical models (mostly but not exclusively ﬁnite-element mechanical models) for speeding up the inten-
sive post-processing operations involved in our simulation-based decision making framework. Scikit-learn’s ﬁt/predict
API together with its efﬁcient cross-validation tools considerably eases the task of selecting the best-ﬁt estimator. We
are also using scikit-learn for illustrating concepts in our training sessions. Trainees are always impressed by the
ease-of-use of scikit-learn despite the apparent theoretical complexity of machine learning.
Vincent Dubourg, PHIMECA Engineering, PhD Engineer
1.6.14 HowAboutWe
At HowAboutWe, scikit-learn lets us implement a wide array of machine learning tech-
niques in analysis and in production, despite having a small team. We use scikit-learn’s classiﬁcation algorithms to
predict user behavior, enabling us to (for example) estimate the value of leads from a given trafﬁc source early in
the lead’s tenure on our site. Also, our users’ proﬁles consist of primarily unstructured data (answers to open-ended
questions), so we use scikit-learn’s feature extraction and dimensionality reduction tools to translate these unstructured
data into inputs for our matchmaking system.
Daniel Weitzenfeld, Senior Data Scientist at HowAboutWe
18
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
1.6.15 PeerIndex
At PeerIndex we use scientiﬁc methodology to build the Inﬂuence Graph - a unique
dataset that allows us to identify who’s really inﬂuential and in which context. To do this, we have to tackle a range
of machine learning and predictive modeling problems. Scikit-learn has emerged as our primary tool for developing
prototypes and making quick progress. From predicting missing data and classifying tweets to clustering communities
of social media users, scikit- learn proved useful in a variety of applications. Its very intuitive interface and excellent
compatibility with other python tools makes it and indispensable tool in our daily research efforts.
Ferenc Huszar - Senior Data Scientist at Peerindex
1.6.16 DataRobot
DataRobot is building next generation predictive analytics software to make data scien-
tists more productive, and scikit-learn is an integral part of our system. The variety of machine learning techniques in
combination with the solid implementations that scikit-learn offers makes it a one-stop-shopping library for machine
learning in Python. Moreover, its consistent API, well-tested code and permissive licensing allow us to use it in a
production environment. Scikit-learn has literally saved us years of work we would have had to do ourselves to bring
our product to market.
Jeremy Achin, CEO & Co-founder DataRobot Inc.
1.6.17 OkCupid
We’re using scikit-learn at OkCupid to evaluate and improve our matchmaking system.
The range of features it has, especially preprocessing utilities, means we can use it for a wide variety of projects,
and it’s performant enough to handle the volume of data that we need to sort through. The documentation is really
thorough, as well, which makes the library quite easy to use.
David Koh - Senior Data Scientist at OkCupid
1.6.18 Lovely
At Lovely, we strive to deliver the best apartment marketplace, with respect to our users
and our listings. From understanding user behavior, improving data quality, and detecting fraud, scikit-learn is a
1.6. Who is using scikit-learn?
19
scikit-learn user guide, Release 0.18.2
regular tool for gathering insights, predictive modeling and improving our product. The easy-to-read documentation
and intuitive architecture of the API makes machine learning both explorable and accessible to a wide range of python
developers. I’m constantly recommending that more developers and scientists try scikit-learn.
Simon Frid - Data Scientist, Lead at Lovely
1.6.19 Data Publica
Data Publica builds a new predictive sales tool for commercial and marketing teams
called C-Radar. We extensively use scikit-learn to build segmentations of customers through clustering, and to predict
future customers based on past partnerships success or failure. We also categorize companies using their website com-
munication thanks to scikit-learn and its machine learning algorithm implementations. Eventually, machine learning
makes it possible to detect weak signals that traditional tools cannot see. All these complex tasks are performed in an
easy and straightforward way thanks to the great quality of the scikit-learn framework.
Guillaume Lebourgeois & Samuel Charron - Data Scientists at Data Publica
1.6.20 Machinalis
Scikit-learn is the cornerstone of all the machine learning projects carried at Machinalis.
It has a consistent API, a wide selection of algorithms and lots of auxiliary tools to deal with the boilerplate. We
have used it in production environments on a variety of projects including click-through rate prediction, information
extraction, and even counting sheep!
In fact, we use it so much that we’ve started to freeze our common use cases into Python packages, some of them
open-sourced, like FeatureForge . Scikit-learn in one word: Awesome.
Rafael Carrascosa, Lead developer
1.6.21 solido
Scikit-learn is helping to drive Moore’s Law, via Solido. Solido creates computer-aided
design tools used by the majority of top-20 semiconductor companies and fabs, to design the bleeding-edge chips
inside smartphones, automobiles, and more. Scikit-learn helps to power Solido’s algorithms for rare-event estimation,
worst-case veriﬁcation, optimization, and more. At Solido, we are particularly fond of scikit-learn’s libraries for
Gaussian Process models, large-scale regularized linear regression, and classiﬁcation. Scikit-learn has increased our
productivity, because for many ML problems we no longer need to “roll our own” code. This PyData 2014 talk has
details.
Trent McConaghy, founder, Solido Design Automation Inc.
20
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
1.6.22 INFONEA
We employ scikit-learn for rapid prototyping and custom-made Data Science solutions
within our in-memory based Business Intelligence Software INFONEA®. As a well-documented and comprehensive
collection of state-of-the-art algorithms and pipelining methods, scikit-learn enables us to provide ﬂexible and scalable
scientiﬁc analysis solutions. Thus, scikit-learn is immensely valuable in realizing a powerful integration of Data
Science technology within self-service business analytics.
Thorsten Kranz, Data Scientist, Coma Soft AG.
1.6.23 Dataiku
Our software, Data Science Studio (DSS), enables users to create data services that
combine ETL with Machine Learning. Our Machine Learning module integrates many scikit-learn algorithms. The
scikit-learn library is a perfect integration with DSS because it offers algorithms for virtually all business cases. Our
goal is to offer a transparent and ﬂexible tool that makes it easier to optimize time consuming aspects of building a
data service, preparing data, and training machine learning algorithms on all types of data.
Florian Douetteau, CEO, Dataiku
1.6.24 Otto Group
Here at Otto Group, one of global Big Five B2C online retailers, we are using scikit-
learn in all aspects of our daily work from data exploration to development of machine learning application to the
productive deployment of those services. It helps us to tackle machine learning problems ranging from e-commerce
to logistics. It consistent APIs enabled us to build the Palladium REST-API framework around it and continuously
deliver scikit-learn based services.
Christian Rammig, Head of Data Science, Otto Group
1.7 Release history
1.7.1 Version 0.18.2
June 20, 2017
Last release with Python 2.6 support
Scikit-learn 0.18 is the last major release of scikit-learn to support Python 2.6. Later versions of scikit-learn will
require Python 2.7 or above.
Changelog
• Fixes for compatibility with NumPy 1.13.0: #7946 #8355 by Loic Esteve.
1.7. Release history
21
scikit-learn user guide, Release 0.18.2
• Minor compatibility changes in the examples #9010 #8040 #9149.
Code Contributors
Aman Dalmia, Loic Esteve, Nate Guerin, Sergei Lebedev
1.7.2 Version 0.18.1
November 11, 2016
Last release with Python 2.6 support
Scikit-learn 0.18 is the last major release of scikit-learn to support Python 2.6. Later versions of scikit-learn will
require Python 2.7 or above.
Changelog
Enhancements
• Improved sample_without_replacement speed by utilizing numpy.random.permutation for most cases.
As a result, samples may differ in this release for a ﬁxed random state. Affected estimators:
– ensemble.BaggingClassifier
– ensemble.BaggingRegressor
– linear_model.RANSACRegressor
– model_selection.RandomizedSearchCV
– random_projection.SparseRandomProjection
This also affects the datasets.make_classification method.
Bug ﬁxes
• Fix issue where min_grad_norm and n_iter_without_progress parameters were not being utilised
by manifold.TSNE. #6497 by Sebastian Säger
• Fix bug for svm’s decision values when decision_function_shape is ovr in svm.SVC. svm.SVC‘s
decision_function was incorrect from versions 0.17.0 through 0.18.0. #7724 by Bing Tian Dai
• Attribute explained_variance_ratio of discriminant_analysis.LinearDiscriminantAnalysis
calculated with SVD and Eigen solver are now of the same length. #7632 by JPFrancoia
• Fixes issue in Univariate feature selection where score functions were not accepting multi-label targets. #7676
by ‘Mohammed Affan‘_
• Fixed setting parameters when calling fit multiple times on feature_selection.SelectFromModel.
#7756 by Andreas Müller
• Fixes issue in partial_fit method of multiclass.OneVsRestClassifier when number of classes
used in partial_fit was less than the total number of classes in the data. #7786 by Srivatsan Ramesh
22
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Fixes issue in calibration.CalibratedClassifierCV where the sum of probabilities of each class
for a data was not 1, and CalibratedClassifierCV now handles the case where the training set has less
number of classes than the total data. #7799 by Srivatsan Ramesh
• Fix a bug where sklearn.feature_selection.SelectFdr did not exactly implement Benjamini-
Hochberg procedure. It formerly may have selected fewer features than it should. #7490 by Peng Meng.
• sklearn.manifold.LocallyLinearEmbedding now correctly handles integer inputs. #6282 by Jake
Vanderplas.
• The min_weight_fraction_leaf parameter of tree-based classiﬁers and regressors now assumes uniform
sample weights by default if the sample_weight argument is not passed to the fit function. Previously, the
parameter was silently ignored. #7301 by Nelson Liu.
• Numerical issue with linear_model.RidgeCV on centered data when n_features > n_samples. #6178 by
Bertrand Thirion
• Tree splitting criterion classes’ cloning/pickling is now memory safe #7680 by Ibraim Ganiev.
• Fixed a bug where decomposition.NMF sets its n_iters_ attribute in transform(). #7553 by Ekaterina
Krivich.
• sklearn.linear_model.LogisticRegressionCV now correctly handles string labels. #5874 by
Raghav RV.
• Fixed
a bug
where
sklearn.model_selection.train_test_split
raised
an
error
when
stratify is a list of string labels. #7593 by Raghav RV.
• Fixed
a
bug
where
sklearn.model_selection.GridSearchCV
and
sklearn.model_selection.RandomizedSearchCV were not pickleable because of a pickling
bug in np.ma.MaskedArray. #7594 by Raghav RV.
• All cross-validation utilities in sklearn.model_selection now permit one time cross-validation splitters
for the cv parameter. Also non-deterministic cross-validation splitters (where multiple calls to split produce
dissimilar splits) can be used as cv parameter. The sklearn.model_selection.GridSearchCV will
cross-validate each parameter setting on the split produced by the ﬁrst split call to the cross-validation splitter.
#7660 by Raghav RV.
API changes summary
Trees and forests
• The min_weight_fraction_leaf parameter of tree-based classiﬁers and regressors now assumes uniform
sample weights by default if the sample_weight argument is not passed to the fit function. Previously, the
parameter was silently ignored. (#7301) by ‘Nelson Liu‘_.
• Tree splitting criterion classes’ cloning/pickling is now memory safe (#7680). By ‘Ibraim Ganiev‘_.
Linear, kernelized and related models
• Length of explained_variance_ratio of discriminant_analysis.LinearDiscriminantAnalysis
changed for both Eigen and SVD solvers. The attribute has now a length of min(n_components, n_classes - 1).
#7632 by JPFrancoia
• Numerical issue with linear_model.RidgeCV on centered data when n_features > n_samples. (#6178)
by Bertrand Thirion
1.7. Release history
23
scikit-learn user guide, Release 0.18.2
Code Contributors
Aashi, affanv14, Alexander Junge, Alexandre Gramfort, Aman Dalmia, Andreas Mueller, Andrew Jackson, Andrew
Smith, Angus Williams, Artem Golubin, Arthur Douillard, Artsiom, Bertrand Thirion, Bing Tian Dai, Brian Burns, CJ
Carey, Charlton Austin, chkoar, Dave Elliott, David Kirkby, Deborah Gertrude Digges, ditenberg, E. Lynch-Klarup,
Ekaterina Krivich, Fabian Egli, ferria, fukatani, Gael Varoquaux, Giorgio Patrini, Grzegorz Szpak, He Chen, guoci,
Ibraim Ganiev, Iván Vallés, JPFrancoia, Jake VanderPlas, Joel Nothman, Jon Crall, Jonathan Rahn, Jonathan Striebel,
Josh Karnofsky, Julien Aubert, Kathy Chen, Kaushik Lakshmikanth, Kevin Yap, Kyle Gilliam, ljwolf, Loic Esteve,
Mainak Jas, Maniteja Nandana, Mathieu Blondel, Mehul Ahuja, Michele Lacchia, Mikhail Korobov, Nelle Varo-
quaux, Nelson Liu, Nicole Vavrova, nuffe, Olivier Grisel, Om Prakash, Patrick Carlson, Pieter Arthur de Jong, pol-
mauri, Rafael Possas, Raghav R V, Ruifeng Zheng, Sam Shleifer, Sebastian Saeger, Sourav Singh, Srivatsan, Thierry
Guillemot, toastedcornﬂakes, Tom Dupré la Tour, vibrantabhi19, waterponey
1.7.3 Version 0.18
September 28, 2016
Last release with Python 2.6 support
Scikit-learn 0.18 will be the last version of scikit-learn to support Python 2.6. Later versions of scikit-learn will
require Python 2.7 or above.
Model Selection Enhancements and API Changes
• The model_selection module
The new module sklearn.model_selection, which groups together the functionalities of formerly
sklearn.cross_validation, sklearn.grid_search and sklearn.learning_curve, intro-
duces new possibilities such as nested cross-validation and better manipulation of parameter searches with Pan-
das.
Many things will stay the same but there are some key differences. Read below to know more about the changes.
• Data-independent CV splitters enabling nested cross-validation
The new cross-validation splitters, deﬁned in the sklearn.model_selection, are no longer initialized
with any data-dependent parameters such as y. Instead they expose a split method that takes in the data and
yields a generator for the different splits.
This change makes it possible to use the cross-validation splitters to perform nested cross-validation, facilitated
by model_selection.GridSearchCV and model_selection.RandomizedSearchCV utilities.
• The enhanced cv_results_ attribute
The
new
cv_results_
attribute
(of
model_selection.GridSearchCV
and
model_selection.RandomizedSearchCV) introduced in lieu of the grid_scores_ attribute
is a dict of 1D arrays with elements in each array corresponding to the parameter settings (i.e.
search
candidates).
The cv_results_ dict can be easily imported into pandas as a DataFrame for exploring the search results.
The
cv_results_
arrays
include
scores
for
each
cross-validation
split
(with
keys
such
as
'split0_test_score'), as well as their mean ('mean_test_score') and standard deviation
('std_test_score').
24
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
The ranks for the search candidates (based on their mean cross-validation score) is available at
cv_results_['rank_test_score'].
The parameter values for each parameter is stored separately as numpy masked object arrays. The value, for
that search candidate, is masked if the corresponding parameter is not applicable. Additionally a list of all the
parameter dicts are stored at cv_results_['params'].
• Parameters n_folds and n_iter renamed to n_splits
Some parameter names have changed: The n_folds parameter in new model_selection.KFold,
model_selection.GroupKFold
(see
below
for
the
name
change),
and
model_selection.StratifiedKFold is now renamed to n_splits.
The n_iter parameter
in model_selection.ShuffleSplit, the new class model_selection.GroupShuffleSplit
and model_selection.StratifiedShuffleSplit is now renamed to n_splits.
• Rename of splitter classes which accepts group labels along with data
The
cross-validation
splitters
LabelKFold,
LabelShuffleSplit,
LeaveOneLabelOut
and
LeavePLabelOut
have
been
renamed
to
model_selection.GroupKFold,
model_selection.GroupShuffleSplit,
model_selection.LeaveOneGroupOut
and
model_selection.LeavePGroupsOut respectively.
Note the change from singular to plural form in model_selection.LeavePGroupsOut.
• Fit parameter labels renamed to groups
The
labels
parameter
in
the
split
method
of
the
newly
renamed
split-
ters
model_selection.GroupKFold,
model_selection.LeaveOneGroupOut,
model_selection.LeavePGroupsOut, model_selection.GroupShuffleSplit is renamed to
groups following the new nomenclature of their class names.
• Parameter n_labels renamed to n_groups
The parameter n_labels in the newly renamed model_selection.LeavePGroupsOut is changed to
n_groups.
• Training scores and Timing information
cv_results_ also includes the training scores for each cross-validation split (with keys such
as
'split0_train_score'),
as
well
as
their
mean
('mean_train_score')
and
stan-
dard
deviation
('std_train_score').
To
avoid
the
cost
of
evaluating
training
score,
set
return_train_score=False.
Additionally the mean and standard deviation of the times taken to split, train and score the model across all the
cross-validation splits is available at the key 'mean_time' and 'std_time' respectively.
Changelog
New features
Classiﬁers and Regressors
• The
Gaussian
Process
module
has
been
reimplemented
and
now
offers
classiﬁcation
and
re-
gression
estimators
through
gaussian_process.GaussianProcessClassifier
and
gaussian_process.GaussianProcessRegressor.
Among other things, the new implementa-
tion supports kernel engineering, gradient-based hyperparameter optimization or sampling of functions from
GP prior and GP posterior. Extensive documentation and examples are provided. By Jan Hendrik Metzen.
• Added new supervised learning algorithm: Multi-layer Perceptron #3204 by Issam H. Laradji
• Added linear_model.HuberRegressor, a linear model robust to outliers. #5291 by Manoj Kumar.
1.7. Release history
25
scikit-learn user guide, Release 0.18.2
• Added the multioutput.MultiOutputRegressor meta-estimator. It converts single output regressors
to multi-ouput regressors by ﬁtting one regressor per output. By Tim Head.
Other estimators
• New mixture.GaussianMixture and mixture.BayesianGaussianMixture replace former mix-
ture models, employing faster inference for sounder results. #7295 by Wei Xue and Thierry Guillemot.
• Class decomposition.RandomizedPCA is now factored into decomposition.PCA and it is avail-
able calling with parameter svd_solver='randomized'.
The default number of n_iter for
'randomized' has changed to 4. The old behavior of PCA is recovered by svd_solver='full'. An
additional solver calls arpack and performs truncated (non-randomized) SVD. By default, the best solver is
selected depending on the size of the input and the number of components requested. #5299 by Giorgio Patrini.
• Added two functions for mutual information estimation: feature_selection.mutual_info_classif
and
feature_selection.mutual_info_regression.
These
functions
can
be
used
in
feature_selection.SelectKBest and feature_selection.SelectPercentile as score
functions. By Andrea Bravi and Nikolay Mayorov.
• Added the ensemble.IsolationForest class for anomaly detection based on random forests. By Nicolas
Goix.
• Added algorithm="elkan" to cluster.KMeans implementing Elkan’s fast K-Means algorithm. By
Andreas Müller.
Model selection and evaluation
• Added metrics.cluster.fowlkes_mallows_score, the Fowlkes Mallows Index which measures the
similarity of two clusterings of a set of points By Arnaud Fouchet and Thierry Guillemot.
• Added metrics.calinski_harabaz_score, which computes the Calinski and Harabaz score to evalu-
ate the resulting clustering of a set of points. By Arnaud Fouchet and Thierry Guillemot.
• Added new cross-validation splitter model_selection.TimeSeriesSplit to handle time series data.
#6586 by YenChen Lin
• The
cross-validation
iterators
are
replaced
by
cross-validation
splitters
available
from
sklearn.model_selection, allowing for nested cross-validation.
See Model Selection Enhance-
ments and API Changes for more information. #4294 by Raghav RV.
Enhancements
Trees and ensembles
• Added
a
new
splitting
criterion
for
tree.DecisionTreeRegressor,
the
mean
ab-
solute
error.
This
criterion
can
also
be
used
in
ensemble.ExtraTreesRegressor,
ensemble.RandomForestRegressor, and the gradient boosting estimators.
#6667 by Nelson
Liu.
• Added weighted impurity-based early stopping criterion for decision tree growth. #6954 by Nelson Liu
• The random forest, extra tree and decision tree estimators now has a method decision_path which returns
the decision path of samples in the tree. By Arnaud Joly.
• A new example has been added unveiling the decision tree structure. By Arnaud Joly.
• Random forest,
extra trees,
decision trees and gradient boosting estimator accept the parameter
min_samples_split and min_samples_leaf provided as a percentage of the training samples. By
yelite and Arnaud Joly.
26
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Gradient boosting estimators accept the parameter criterion to specify to splitting criterion used in built
decision trees. #6667 by Nelson Liu.
• The memory footprint is reduced (sometimes greatly) for ensemble.bagging.BaseBagging and classes
that inherit from it, i.e, ensemble.BaggingClassifier, ensemble.BaggingRegressor, and
ensemble.IsolationForest, by dynamically generating attribute estimators_samples_ only
when it is needed. By David Staub.
• Added n_jobs and sample_weight parameters for ensemble.VotingClassifier to ﬁt underlying
estimators in parallel. #5805 by Ibraim Ganiev.
Linear, kernelized and related models
• In linear_model.LogisticRegression, the SAG solver is now available in the multinomial case.
#5251 by Tom Dupre la Tour.
• linear_model.RANSACRegressor,
svm.LinearSVC
and
svm.LinearSVR
now
support
sample_weight. By Imaculate.
• Add parameter loss to linear_model.RANSACRegressor to measure the error on the samples for every
trial. By Manoj Kumar.
• Prediction of out-of-sample events with Isotonic Regression (isotonic.IsotonicRegression) is now
much faster (over 1000x in tests with synthetic data). By Jonathan Arfa.
• Isotonic regression (isotonic.IsotonicRegression) now uses a better algorithm to avoid O(n^2) be-
havior in pathological cases, and is also generally faster (##6691). By Antony Lee.
• naive_bayes.GaussianNB now accepts data-independent class-priors through the parameter priors.
By Guillaume Lemaitre.
• linear_model.ElasticNet and linear_model.Lasso now works with np.float32 input data
without converting it into np.float64. This allows to reduce the memory consumption. #6913 by YenChen
Lin.
• semi_supervised.LabelPropagation and semi_supervised.LabelSpreading now accept
arbitrary kernel functions in addition to strings knn and rbf. #5762 by Utkarsh Upadhyay.
Decomposition, manifold learning and clustering
• Added inverse_transform function to decomposition.NMF to compute data matrix of original shape.
By Anish Shah.
• cluster.KMeans
and
cluster.MiniBatchKMeans
now
works
with
np.float32
and
np.float64 input data without converting it.
This allows to reduce the memory consumption by
using np.float32. #6846 by Sebastian Säger and YenChen Lin.
Preprocessing and feature selection
• preprocessing.RobustScaler now accepts quantile_range parameter. #5929 by Konstantin Pod-
shumok.
• feature_extraction.FeatureHasher now accepts string values.
#6173 by Ryad Zenine and
Devashish Deshpande.
• Keyword arguments can now be supplied to func in preprocessing.FunctionTransformer by
means of the kw_args parameter. By Brian McFee.
• feature_selection.SelectKBest and feature_selection.SelectPercentile now accept
score functions that take X, y as input and return only the scores. By Nikolay Mayorov.
Model evaluation and meta-estimators
1.7. Release history
27
scikit-learn user guide, Release 0.18.2
• multiclass.OneVsOneClassifier and multiclass.OneVsRestClassifier now support
partial_fit. By Asish Panda and Philipp Dowling.
• Added support for substituting or disabling pipeline.Pipeline and pipeline.FeatureUnion
components
using
the
set_params
interface
that
powers
sklearn.grid_search.
See
sphx_glr_plot_compare_reduction.py. By Joel Nothman and Robert McGibbon.
• The
new
cv_results_
attribute
of
model_selection.GridSearchCV
(and
model_selection.RandomizedSearchCV) can be easily imported into pandas as a DataFrame. Ref
Model Selection Enhancements and API Changes for more information. #6697 by Raghav RV.
• Generalization of model_selection.cross_val_predict. One can pass method names such as pre-
dict_proba to be used in the cross validation framework instead of the default predict. By Ori Ziv and Sears
Merritt.
• The training scores and time taken for training followed by scoring for each search candidate are now available
at the cv_results_ dict. See Model Selection Enhancements and API Changes for more information. #7325
by Eugene Chen and Raghav RV.
Metrics
• Added labels ﬂag to metrics.log_loss to to explicitly provide the labels when the number of classes
in y_true and y_pred differ. #7239 by Hong Guangguo with help from Mads Jensen and Nelson Liu.
• Support sparse contingency matrices in cluster evaluation (metrics.cluster.supervised) to scale to a
large number of clusters. #7419 by Gregory Stupp and Joel Nothman.
• Add sample_weight parameter to metrics.matthews_corrcoef. By Jatin Shah and Raghav RV.
• Speed up metrics.silhouette_score by using vectorized operations. By Manoj Kumar.
• Add sample_weight parameter to metrics.confusion_matrix. By Bernardo Stein.
Miscellaneous
• Added n_jobs parameter to feature_selection.RFECV to compute the score on the test folds in par-
allel. By Manoj Kumar
• Codebase does not contain C/C++ cython generated ﬁles: they are generated during build. Distribution packages
will still contain generated C/C++ ﬁles. By Arthur Mensch.
• Reduce the memory usage for 32-bit ﬂoat input arrays of utils.sparse_func.mean_variance_axis
and utils.sparse_func.incr_mean_variance_axis by supporting cython fused types.
By
YenChen Lin.
• The ignore_warnings now accept a category argument to ignore only the warnings of a speciﬁed type. By
Thierry Guillemot.
• Added parameter return_X_y and return type (data,target) :
tuple option to load_iris
dataset #7049, load_breast_cancer dataset #7152, load_digits dataset, load_diabetes dataset,
load_linnerud dataset, load_boston dataset #7154 by Manvendra Singh.
• Simpliﬁcation of the clone function, deprecate support for estimators that modify parameters in __init__.
#5540 by Andreas Müller.
• When unpickling a scikit-learn estimator in a different version than the one the estimator was trained with, a
UserWarning is raised, see the documentation on model persistence for more details. (#7248) By Andreas
Müller.
Bug ﬁxes
Trees and ensembles
28
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Random
forest,
extra
trees,
decision
trees
and
gradient
boosting
won’t
accept
anymore
min_samples_split=1 as at least 2 samples are required to split a decision tree node.
By Arnaud
Joly
• ensemble.VotingClassifier
now
raises
NotFittedError
if
predict,
transform
or
predict_proba are called on the non-ﬁtted estimator. by Sebastian Raschka.
• Fix bug where ensemble.AdaBoostClassifier and ensemble.AdaBoostRegressor would per-
form poorly if the random_state was ﬁxed (#7411). By Joel Nothman.
• Fix
bug
in
ensembles
with
randomization
where
the
ensemble
would
not
set
random_state
on
base
estimators
in
a
pipeline
or
similar
nesting.
(#7411).
Note,
results
for
ensemble.BaggingClassifier
ensemble.BaggingRegressor,
ensemble.AdaBoostClassifier and ensemble.AdaBoostRegressor will now differ from
previous versions. By Joel Nothman.
Linear, kernelized and related models
• Fixed
incorrect
gradient
computation
for
loss='squared_epsilon_insensitive'
in
linear_model.SGDClassifier and linear_model.SGDRegressor (#6764).
By Wenhua
Yang.
• Fix bug in linear_model.LogisticRegressionCV where solver='liblinear' did not accept
class_weights='balanced. (#6817). By Tom Dupre la Tour.
• Fix bug in neighbors.RadiusNeighborsClassifier where an error occurred when there were out-
liers being labelled and a weight function speciﬁed (#6902). By LeonieBorne.
• Fix linear_model.ElasticNet sparse decision function to match output with dense in the multioutput
case.
Decomposition, manifold learning and clustering
• decomposition.RandomizedPCA default number of iterated_power is 4 instead of 3. #5141 by Giorgio
Patrini.
• utils.extmath.randomized_svd performs 4 power iterations by default, instead or 0. In practice this
is enough for obtaining a good approximation of the true eigenvalues/vectors in the presence of noise. When
n_components is small (< .1 * min(X.shape)) n_iter is set to 7, unless the user speciﬁes a higher number.
This improves precision with few components. #5299 by Giorgio Patrini.
• Whiten/non-whiten
inconsistency
between
components
of
decomposition.PCA
and
decomposition.RandomizedPCA (now factored into PCA, see the New features) is ﬁxed.
compo-
nents_ are stored with no whitening. #5299 by Giorgio Patrini.
• Fixed bug in manifold.spectral_embedding where diagonal of unnormalized Laplacian matrix was
incorrectly set to 1. #4995 by Peter Fischer.
• Fixed
incorrect
initialization
of
utils.arpack.eigsh
on
all
occurrences.
Affects
cluster.bicluster.SpectralBiclustering,
decomposition.KernelPCA,
manifold.LocallyLinearEmbedding,
and manifold.SpectralEmbedding (#5012).
By
Peter Fischer.
• Attribute
explained_variance_ratio_
calculated
with
the
SVD
solver
of
discriminant_analysis.LinearDiscriminantAnalysis now returns correct results.
By
JPFrancoia
Preprocessing and feature selection
• preprocessing.data._transform_selected now always passes a copy of X to transform function
when copy=True (#7194). By Caio Oliveira.
Model evaluation and meta-estimators
1.7. Release history
29
scikit-learn user guide, Release 0.18.2
• model_selection.StratifiedKFold now raises error if all n_labels for individual classes is less than
n_folds. #6182 by Devashish Deshpande.
• Fixed bug in model_selection.StratifiedShuffleSplit where train and test sample could overlap
in some edge cases, see #6121 for more details. By Loic Esteve.
• Fix
in
sklearn.model_selection.StratifiedShuffleSplit
to
return
splits
of
size
train_size and test_size in all cases (#6472). By Andreas Müller.
• Cross-validation of OneVsOneClassifier and OneVsRestClassifier now works with precomputed
kernels. #7350 by Russell Smith.
• Fix incomplete predict_proba method delegation from model_selection.GridSearchCV to
linear_model.SGDClassifier (#7159) by Yichuan Liu.
Metrics
• Fix bug in metrics.silhouette_score in which clusters of size 1 were incorrectly scored. They should
get a score of 0. By Joel Nothman.
• Fix bug in metrics.silhouette_samples so that it now works with arbitrary labels, not just those
ranging from 0 to n_clusters - 1.
• Fix bug where expected and adjusted mutual information were incorrect if cluster contingency cells exceeded
2**16. By Joel Nothman.
• metrics.pairwise.pairwise_distances now converts arrays to boolean arrays when required in
scipy.spatial.distance. #5460 by Tom Dupre la Tour.
• Fix
sparse
input
support
in
metrics.silhouette_score
as
well
as
example
exam-
ples/text/document_clustering.py. By YenChen Lin.
• metrics.roc_curve and metrics.precision_recall_curve no longer round y_score values
when creating ROC curves; this was causing problems for users with very small differences in scores (#7353).
Miscellaneous
• model_selection.tests._search._check_param_grid now works correctly with all types that
extends/implements Sequence (except string), including range (Python 3.x) and xrange (Python 2.x). #7323 by
Viacheslav Kovalevskyi.
• utils.extmath.randomized_range_finder is more numerically stable when many power iterations
are requested, since it applies LU normalization by default. If n_iter<2 numerical issues are unlikely, thus
no normalization is applied. Other normalization options are available: 'none','LU' and 'QR'. #5141 by
Giorgio Patrini.
• Fix a bug where some formats of scipy.sparse matrix, and estimators with them as parameters, could not
be passed to base.clone. By Loic Esteve.
• datasets.load_svmlight_file now is able to read long int QID values. #7101 by Ibraim Ganiev.
API changes summary
Linear, kernelized and related models
• residual_metric has been deprecated in linear_model.RANSACRegressor. Use loss instead.
By Manoj Kumar.
• Access to public attributes .X_ and .y_ has been deprecated in isotonic.IsotonicRegression. By
Jonathan Arfa.
Decomposition, manifold learning and clustering
30
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• The old mixture.DPGMM is deprecated in favor of the new mixture.BayesianGaussianMixture
(with the parameter weight_concentration_prior_type='dirichlet_process').
The new
class solves the computational problems of the old class and computes the Gaussian mixture with a Dirich-
let process prior faster than before. #7295 by Wei Xue and Thierry Guillemot.
• The old mixture.VBGMM is deprecated in favor of the new mixture.BayesianGaussianMixture
(with the parameter weight_concentration_prior_type='dirichlet_distribution'). The
new class solves the computational problems of the old class and computes the Variational Bayesian Gaussian
mixture faster than before. #6651 by Wei Xue and Thierry Guillemot.
• The old mixture.GMM is deprecated in favor of the new mixture.GaussianMixture. The new class
computes the Gaussian mixture faster than before and some of computational problems have been solved. #6666
by Wei Xue and Thierry Guillemot.
Model evaluation and meta-estimators
• The sklearn.cross_validation, sklearn.grid_search and sklearn.learning_curve
have
been
deprecated
and
the
classes
and
functions
have
been
reorganized
into
the
sklearn.model_selection module.
Ref Model Selection Enhancements and API Changes for
more information. #4294 by Raghav RV.
• The
grid_scores_
attribute
of
model_selection.GridSearchCV
and
model_selection.RandomizedSearchCV is deprecated in favor of the attribute cv_results_. Ref
Model Selection Enhancements and API Changes for more information. #6697 by Raghav RV.
• The parameters n_iter or n_folds in old CV splitters are replaced by the new parameter n_splits since
it can provide a consistent and unambiguous interface to represent the number of train-test splits. #7187 by
YenChen Lin.
• classes parameter was renamed to labels in metrics.hamming_loss. #7260 by Sebastián Vanrell.
• The
splitter
classes
LabelKFold,
LabelShuffleSplit,
LeaveOneLabelOut
and
LeavePLabelsOut
are
renamed
to
model_selection.GroupKFold,
model_selection.GroupShuffleSplit,
model_selection.LeaveOneGroupOut
and
model_selection.LeavePGroupsOut
respectively.
Also
the
parameter
labels
in
the
split
method
of
the
newly
renamed
splitters
model_selection.LeaveOneGroupOut
and
model_selection.LeavePGroupsOut
is
renamed
to
groups.
Additionally
in
model_selection.LeavePGroupsOut,
the parameter n_labels is renamed to n_groups.
#6660 by Raghav RV.
Code Contributors
Aditya Joshi, Alejandro, Alexander Fabisch, Alexander Loginov, Alexander Minyushkin, Alexander Rudy, Alexan-
dre Abadie, Alexandre Abraham, Alexandre Gramfort, Alexandre Saint, alexﬁelds, Alvaro Ulloa, alyssaq, Amlan
Kar, Andreas Mueller, andrew giessel, Andrew Jackson, Andrew McCulloh, Andrew Murray, Anish Shah, Arafat,
Archit Sharma, Ariel Rokem, Arnaud Joly, Arnaud Rachez, Arthur Mensch, Ash Hoover, asnt, b0noI, Behzad Tabib-
ian, Bernardo, Bernhard Kratzwald, Bhargav Mangipudi, blakeﬂei, Boyuan Deng, Brandon Carter, Brett Naul, Brian
McFee, Caio Oliveira, Camilo Lamus, Carol Willing, Cass, CeShine Lee, Charles Truong, Chyi-Kwei Yau, CJ Carey,
codevig, Colin Ni, Dan Shiebler, Daniel, Daniel Hnyk, David Ellis, David Nicholson, David Staub, David Thaler,
David Warshaw, Davide Lasagna, Deborah, deﬁnitelyuncertain, Didi Bar-Zev, djipey, dsquareindia, edwinENSAE,
Elias Kuthe, Elvis DOHMATOB, Ethan White, Fabian Pedregosa, Fabio Ticconi, ﬁsache, Florian Wilhelm, Francis,
Francis O’Donovan, Gael Varoquaux, Ganiev Ibraim, ghg, Gilles Louppe, Giorgio Patrini, Giovanni Cherubin, Gio-
vanni Lanzani, Glenn Qian, Gordon Mohr, govin-vatsan, Graham Clenaghan, Greg Reda, Greg Stupp, Guillaume
Lemaitre, Gustav Mörtberg, halwai, Harizo Rajaona, Harry Mavroforakis, hashcode55, hdmetor, Henry Lin, Hob-
son Lane, Hugo Bowne-Anderson, Igor Andriushchenko, Imaculate, Inki Hwang, Isaac Sijaranamual, Ishank Gulati,
Issam Laradji, Iver Jordal, jackmartin, Jacob Schreiber, Jake Vanderplas, James Fiedler, James Routley, Jan Zikes,
1.7. Release history
31
scikit-learn user guide, Release 0.18.2
Janna Brettingen, jarfa, Jason Laska, jblackburne, jeff levesque, Jeffrey Blackburne, Jeffrey04, Jeremy Hintz, jere-
mynixon, Jeroen, Jessica Yung, Jill-Jênn Vie, Jimmy Jia, Jiyuan Qian, Joel Nothman, johannah, John, John Boersma,
John Kirkham, John Moeller, jonathan.striebel, joncrall, Jordi, Joseph Munoz, Joshua Cook, JPFrancoia, jrﬁedler,
JulianKahnert, juliathebrave, kaichogami, KamalakerDadi, Kenneth Lyons, Kevin Wang, kingjr, kjell, Konstantin
Podshumok, Kornel Kielczewski, Krishna Kalyan, krishnakalyan3, Kvle Putnam, Kyle Jackson, Lars Buitinck, ldavid,
LeiG, LeightonZhang, Leland McInnes, Liang-Chi Hsieh, Lilian Besson, lizsz, Loic Esteve, Louis Tiao, Léonie Borne,
Mads Jensen, Maniteja Nandana, Manoj Kumar, Manvendra Singh, Marco, Mario Krell, Mark Bao, Mark Szepieniec,
Martin Madsen, MartinBpr, MaryanMorel, Massil, Matheus, Mathieu Blondel, Mathieu Dubois, Matteo, Matthias Ek-
man, Max Moroz, Michael Scherer, michiaki ariga, Mikhail Korobov, Moussa Taiﬁ, mrandrewandrade, Mridul Seth,
nadya-p, Naoya Kanai, Nate George, Nelle Varoquaux, Nelson Liu, Nick James, NickleDave, Nico, Nicolas Goix,
Nikolay Mayorov, ningchi, nlathia, okbalefthanded, Okhlopkov, Olivier Grisel, Panos Louridas, Paul Strickland, Per-
rine Letellier, pestrickland, Peter Fischer, Pieter, Ping-Yao, Chang, practicalswift, Preston Parry, Qimu Zheng, Rachit
Kansal, Raghav RV, Ralf Gommers, Ramana.S, Rammig, Randy Olson, Rob Alexander, Robert Lutz, Robin Schucker,
Rohan Jain, Ruifeng Zheng, Ryan Yu, Rémy Léone, saihttam, Saiwing Yeung, Sam Shleifer, Samuel St-Jean, Sar-
taj Singh, Sasank Chilamkurthy, saurabh.bansod, Scott Andrews, Scott Lowe, seales, Sebastian Raschka, Sebastian
Saeger, Sebastián Vanrell, Sergei Lebedev, shagun Sodhani, shanmuga cv, Shashank Shekhar, shawpan, shengxid-
uan, Shota, shuckle16, Skipper Seabold, sklearn-ci, SmedbergM, srvanrell, Sébastien Lerique, Taranjeet, themrmax,
Thierry, Thierry Guillemot, Thomas, Thomas Hallock, Thomas Moreau, Tim Head, tKammy, toastedcornﬂakes, Tom,
TomDLT, Toshihiro Kamishima, tracer0tong, Trent Hauck, trevorstephens, Tue Vo, Varun, Varun Jewalikar, Viach-
eslav, Vighnesh Birodkar, Vikram, Villu Ruusmann, Vinayak Mehta, walter, waterponey, Wenhua Yang, Wenjian
Huang, Will Welch, wyseguy7, xyguo, yanlend, Yaroslav Halchenko, yelite, Yen, YenChenLin, Yichuan Liu, Yoav
Ram, Yoshiki, Zheng RuiFeng, zivori, Óscar Nájera
1.7.4 Version 0.17.1
February 18, 2016
Changelog
Bug ﬁxes
• Upgrade vendored joblib to version 0.9.4 that ﬁxes an important bug in joblib.Parallel that can silently
yield to wrong results when working on datasets larger than 1MB: https://github.com/joblib/joblib/blob/0.9.4/
CHANGES.rst
• Fixed reading of Bunch pickles generated with scikit-learn version <= 0.16. This can affect users who have
already downloaded a dataset with scikit-learn 0.16 and are loading it with scikit-learn 0.17. See #6196 for how
this affected datasets.fetch_20newsgroups. By Loic Esteve.
• Fixed a bug that prevented using ROC AUC score to perform grid search on several CPU / cores on large arrays.
See #6147 By Olivier Grisel.
• Fixed
a
bug
that
prevented
to
properly
set
the
presort
parameter
in
ensemble.GradientBoostingRegressor. See #5857 By Andrew McCulloh.
• Fixed a joblib error when evaluating the perplexity of a decomposition.LatentDirichletAllocation
model. See #6258 By Chyi-Kwei Yau.
1.7.5 Version 0.17
November 5, 2015
32
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
Changelog
New features
• All the Scaler classes but preprocessing.RobustScaler can be ﬁtted online by calling partial_ﬁt. By
Giorgio Patrini.
• The new class ensemble.VotingClassifier implements a “majority rule” / “soft voting” ensemble
classiﬁer to combine estimators for classiﬁcation. By Sebastian Raschka.
• The
new
class
preprocessing.RobustScaler
provides
an
alternative
to
preprocessing.StandardScaler for feature-wise centering and range normalization that is ro-
bust to outliers. By Thomas Unterthiner.
• The
new
class
preprocessing.MaxAbsScaler
provides
an
alternative
to
preprocessing.MinMaxScaler for feature-wise range normalization when the data is already
centered or sparse. By Thomas Unterthiner.
• The new class preprocessing.FunctionTransformer turns a Python function into a Pipeline-
compatible transformer object. By Joe Jevnik.
• The new classes cross_validation.LabelKFold and cross_validation.LabelShuffleSplit
generate
train-test
folds,
respectively
similar
to
cross_validation.KFold
and
cross_validation.ShuffleSplit, except that the folds are conditioned on a label array.
By
Brian McFee, Jean Kossaiﬁand Gilles Louppe.
• decomposition.LatentDirichletAllocation implements the Latent Dirichlet Allocation topic
model with online variational inference. By Chyi-Kwei Yau, with code based on an implementation by Matt
Hoffman. (#3659)
• The new solver sag implements a Stochastic Average Gradient descent and is available in both
linear_model.LogisticRegression and linear_model.Ridge. This solver is very efﬁcient for
large datasets. By Danny Sullivan and Tom Dupre la Tour. (#4738)
• The new solver cd implements a Coordinate Descent in decomposition.NMF. Previous solver based on
Projected Gradient is still available setting new parameter solver to pg, but is deprecated and will be removed
in 0.19, along with decomposition.ProjectedGradientNMF and parameters sparseness, eta,
beta and nls_max_iter. New parameters alpha and l1_ratio control L1 and L2 regularization, and
shuffle adds a shufﬂing step in the cd solver. By Tom Dupre la Tour and Mathieu Blondel.
Enhancements
• manifold.TSNE now supports approximate optimization via the Barnes-Hut method, leading to much faster
ﬁtting. By Christopher Erick Moody. (#4025)
• cluster.mean_shift_.MeanShift
now
supports
parallel
execution,
as
implemented
in
the
mean_shift function. By Martino Sorbaro.
• naive_bayes.GaussianNB now supports ﬁtting with sample_weight. By Jan Hendrik Metzen.
• dummy.DummyClassifier now supports a prior ﬁtting strategy. By Arnaud Joly.
• Added a fit_predict method for mixture.GMM and subclasses. By Cory Lorenz.
• Added the metrics.label_ranking_loss metric. By Arnaud Joly.
• Added the metrics.cohen_kappa_score metric.
• Added a warm_start constructor parameter to the bagging ensemble models to increase the size of the en-
semble. By Tim Head.
1.7. Release history
33
scikit-learn user guide, Release 0.18.2
• Added option to use multi-output regression metrics without averaging. By Konstantin Shmelkov and Michael
Eickenberg.
• Added stratify option to cross_validation.train_test_split for stratiﬁed splitting.
By
Miroslav Batchkarov.
• The
tree.export_graphviz
function
now
supports
aesthetic
improvements
for
tree.DecisionTreeClassifier
and
tree.DecisionTreeRegressor,
including
options
for coloring nodes by their majority class or impurity, showing variable names, and using node proportions
instead of raw sample counts. By Trevor Stephens.
• Improved speed of newton-cg solver in linear_model.LogisticRegression, by avoiding loss com-
putation. By Mathieu Blondel and Tom Dupre la Tour.
• The class_weight="auto" heuristic in classiﬁers supporting class_weight was deprecated and re-
placed by the class_weight="balanced" option, which has a simpler formula and interpretation. By
Hanna Wallach and Andreas Müller.
• Add
class_weight
parameter
to
automatically
weight
samples
by
class
frequency
for
linear_model.PassiveAgressiveClassifier. By Trevor Stephens.
• Added backlinks from the API reference pages to the user guide. By Andreas Müller.
• The labels parameter to sklearn.metrics.f1_score, sklearn.metrics.fbeta_score,
sklearn.metrics.recall_score and sklearn.metrics.precision_score has been ex-
tended. It is now possible to ignore one or more labels, such as where a multiclass problem has a majority
class to ignore. By Joel Nothman.
• Add sample_weight support to linear_model.RidgeClassifier. By Trevor Stephens.
• Provide an option for sparse output from sklearn.metrics.pairwise.cosine_similarity. By
Jaidev Deshpande.
• Add minmax_scale to provide a function interface for MinMaxScaler. By Thomas Unterthiner.
• dump_svmlight_file now handles multi-label datasets. By Chih-Wei Chang.
• RCV1 dataset loader (sklearn.datasets.fetch_rcv1). By Tom Dupre la Tour.
• The “Wisconsin Breast Cancer” classical two-class classiﬁcation dataset is now included in scikit-learn, avail-
able with sklearn.dataset.load_breast_cancer.
• Upgraded to joblib 0.9.3 to beneﬁt from the new automatic batching of short tasks. This makes it possible for
scikit-learn to beneﬁt from parallelism when many very short tasks are executed in parallel, for instance by the
grid_search.GridSearchCV meta-estimator with n_jobs > 1 used with a large grid of parameters
on a small dataset. By Vlad Niculae, Olivier Grisel and Loic Esteve.
• For more details about changes in joblib 0.9.3 see the release notes: https://github.com/joblib/joblib/blob/master/
CHANGES.rst#release-093
• Improved speed (3 times per iteration) of decomposition.DictLearning with coordinate descent
method from linear_model.Lasso. By Arthur Mensch.
• Parallel processing (threaded) for queries of nearest neighbors (using the ball-tree) by Nikolay Mayorov.
• Allow datasets.make_multilabel_classification to output a sparse y. By Kashif Rasul.
• cluster.DBSCAN now accepts a sparse matrix of precomputed distances, allowing memory-efﬁcient distance
precomputation. By Joel Nothman.
• tree.DecisionTreeClassifier now exposes an apply method for retrieving the leaf indices samples
are predicted as. By Daniel Galvez and Gilles Louppe.
34
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Speed up decision tree regressors, random forest regressors, extra trees regressors and gradient boosting estima-
tors by computing a proxy of the impurity improvement during the tree growth. The proxy quantity is such that
the split that maximizes this value also maximizes the impurity improvement. By Arnaud Joly, Jacob Schreiber
and Gilles Louppe.
• Speed up tree based methods by reducing the number of computations needed when computing the impurity
measure taking into account linear relationship of the computed statistics. The effect is particularly visible with
extra trees and on datasets with categorical or sparse features. By Arnaud Joly.
• ensemble.GradientBoostingRegressor
and ensemble.GradientBoostingClassifier
now expose an apply method for retrieving the leaf indices each sample ends up in under each try. By Ja-
cob Schreiber.
• Add sample_weight support to linear_model.LinearRegression. By Sonny Hu. (##4881)
• Add n_iter_without_progress to manifold.TSNE to control the stopping criterion. By Santi Vil-
lalba. (#5186)
• Added optional parameter random_state in linear_model.Ridge , to set the seed of the pseudo random
generator used in sag solver. By Tom Dupre la Tour.
• Added optional parameter warm_start in linear_model.LogisticRegression. If set to True, the
solvers lbfgs, newton-cg and sag will be initialized with the coefﬁcients computed in the previous ﬁt. By
Tom Dupre la Tour.
• Added
sample_weight
support
to
linear_model.LogisticRegression
for
the
lbfgs,
newton-cg, and sag solvers. By Valentin Stolbunov. Support added to the liblinear solver. By Manoj
Kumar.
• Added
optional
parameter
presort
to
ensemble.GradientBoostingRegressor
and
ensemble.GradientBoostingClassifier,
keeping default behavior the same.
This allows
gradient boosters to turn off presorting when building deep trees or using sparse data. By Jacob Schreiber.
• Altered metrics.roc_curve to drop unnecessary thresholds by default. By Graham Clenaghan.
• Added feature_selection.SelectFromModel meta-transformer which can be used along with es-
timators that have coef_ or feature_importances_ attribute to select important features of the input data. By
Maheshakya Wijewardena, Joel Nothman and Manoj Kumar.
• Added metrics.pairwise.laplacian_kernel. By Clyde Fare.
• covariance.GraphLasso allows separate control of the convergence criterion for the Elastic-Net subprob-
lem via the enet_tol parameter.
• Improved verbosity in decomposition.DictionaryLearning.
• ensemble.RandomForestClassifier and ensemble.RandomForestRegressor no longer ex-
plicitly store the samples used in bagging, resulting in a much reduced memory footprint for storing random
forest models.
• Added positive option to linear_model.Lars and linear_model.lars_path to force coefﬁ-
cients to be positive. (#5131)
• Added the X_norm_squared parameter to metrics.pairwise.euclidean_distances to provide
precomputed squared norms for X.
• Added the fit_predict method to pipeline.Pipeline.
• Added the preprocessing.min_max_scale function.
1.7. Release history
35
scikit-learn user guide, Release 0.18.2
Bug ﬁxes
• Fixed non-determinism in dummy.DummyClassifier with sparse multi-label output. By Andreas Müller.
• Fixed the output shape of linear_model.RANSACRegressor to (n_samples,). By Andreas Müller.
• Fixed bug in decomposition.DictLearning when n_jobs < 0. By Andreas Müller.
• Fixed bug where grid_search.RandomizedSearchCV could consume a lot of memory for large discrete
grids. By Joel Nothman.
• Fixed bug in linear_model.LogisticRegressionCV where penalty was ignored in the ﬁnal ﬁt. By
Manoj Kumar.
• Fixed bug in ensemble.forest.ForestClassifier while computing oob_score and X is a
sparse.csc_matrix. By Ankur Ankan.
• All regressors now consistently handle and warn when given y that is of shape (n_samples,1). By Andreas
Müller and Henry Lin. (#5431)
• Fix in cluster.KMeans cluster reassignment for sparse input by Lars Buitinck.
• Fixed a bug in lda.LDA that could cause asymmetric covariance matrices when using shrinkage. By Martin
Billinger.
• Fixed cross_validation.cross_val_predict for estimators with sparse predictions. By Buddha
Prakash.
• Fixed the predict_proba method of linear_model.LogisticRegression to use soft-max instead
of one-vs-rest normalization. By Manoj Kumar. (#5182)
• Fixed
the
partial_fit
method
of
linear_model.SGDClassifier
when
called
with
average=True. By Andrew Lamb. (#5282)
• Dataset fetchers use different ﬁlenames under Python 2 and Python 3 to avoid pickling compatibility issues. By
Olivier Grisel. (#5355)
• Fixed a bug in naive_bayes.GaussianNB which caused classiﬁcation results to depend on scale. By Jake
Vanderplas.
• Fixed temporarily linear_model.Ridge, which was incorrect when ﬁtting the intercept in the case of
sparse data. The ﬁx automatically changes the solver to ‘sag’ in this case. #5360 by Tom Dupre la Tour.
• Fixed a performance bug in decomposition.RandomizedPCA on data with a large number of features
and fewer samples. (#4478) By Andreas Müller, Loic Esteve and Giorgio Patrini.
• Fixed bug in cross_decomposition.PLS that yielded unstable and platform dependent output, and failed
on ﬁt_transform. By Arthur Mensch.
• Fixes to the Bunch class used to store datasets.
• Fixed ensemble.plot_partial_dependence ignoring the percentiles parameter.
• Providing a set as vocabulary in CountVectorizer no longer leads to inconsistent results when pickling.
• Fixed
the
conditions
on
when
a
precomputed
Gram
matrix
needs
to
be
recomputed
in
linear_model.LinearRegression,
linear_model.OrthogonalMatchingPursuit,
linear_model.Lasso and linear_model.ElasticNet.
• Fixed
inconsistent
memory
layout
in
the
coordinate
descent
solver
that
affected
linear_model.DictionaryLearning and covariance.GraphLasso.
(#5337) By Olivier
Grisel.
• manifold.LocallyLinearEmbedding no longer ignores the reg parameter.
36
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Nearest Neighbor estimators with custom distance metrics can now be pickled. (#4362)
• Fixed a bug in pipeline.FeatureUnion where transformer_weights were not properly handled
when performing grid-searches.
• Fixed a bug in linear_model.LogisticRegression and linear_model.LogisticRegressionCV
when using class_weight='balanced'```or ``class_weight='auto'. By Tom Dupre la Tour.
• Fixed bug #5495 when doing OVR(SVC(decision_function_shape=”ovr”)). Fixed by Elvis Dohmatob.
API changes summary
• Attribute data_min, data_max and data_range in preprocessing.MinMaxScaler are deprecated and
won’t be available from 0.19. Instead, the class now exposes data_min_, data_max_ and data_range_. By
Giorgio Patrini.
• All Scaler classes now have an scale_ attribute, the feature-wise rescaling applied by their transform methods.
The old attribute std_ in preprocessing.StandardScaler is deprecated and superseded by scale_; it
won’t be available in 0.19. By Giorgio Patrini.
• svm.SVC` and svm.NuSVC now have an decision_function_shape parameter to make their decision
function of shape (n_samples,n_classes) by setting decision_function_shape='ovr'. This
will be the default behavior starting in 0.19. By Andreas Müller.
• Passing 1D data arrays as input to estimators is now deprecated as it caused confusion in how the array ele-
ments should be interpreted as features or as samples. All data arrays are now expected to be explicitly shaped
(n_samples,n_features). By Vighnesh Birodkar.
• lda.LDA and qda.QDA have been moved to discriminant_analysis.LinearDiscriminantAnalysis
and discriminant_analysis.QuadraticDiscriminantAnalysis.
• The
store_covariance
and
tol
parameters
have
been
moved
from
the
ﬁt
method
to
the
constructor
in
discriminant_analysis.LinearDiscriminantAnalysis
and
the
store_covariances and tol parameters have been moved from the ﬁt method to the constructor
in discriminant_analysis.QuadraticDiscriminantAnalysis.
• Models inheriting from _LearntSelectorMixin will no longer support the transform methods. (i.e, Ran-
domForests, GradientBoosting, LogisticRegression, DecisionTrees, SVMs and SGD related models). Wrap
these models around the metatransfomer feature_selection.SelectFromModel to remove features
(according to coefs_ or feature_importances_) which are below a certain threshold value instead.
• cluster.KMeans re-runs cluster-assignments in case of non-convergence, to ensure consistency of
predict(X) and labels_. By Vighnesh Birodkar.
• Classiﬁer and Regressor models are now tagged as such using the _estimator_type attribute.
• Cross-validation iterators always provide indices into training and test set, not boolean masks.
• The decision_function on all regressors was deprecated and will be removed in 0.19. Use predict
instead.
• datasets.load_lfw_pairs
is
deprecated
and
will
be
removed
in
0.19.
Use
datasets.fetch_lfw_pairs instead.
• The deprecated hmm module was removed.
• The deprecated Bootstrap cross-validation iterator was removed.
• The
deprecated
Ward
and
WardAgglomerative
classes
have
been
removed.
Use
clustering.AgglomerativeClustering instead.
• cross_validation.check_cv is now a public function.
1.7. Release history
37
scikit-learn user guide, Release 0.18.2
• The property residues_ of linear_model.LinearRegression is deprecated and will be removed in
0.19.
• The deprecated n_jobs parameter of linear_model.LinearRegression has been moved to the con-
structor.
• Removed deprecated class_weight parameter from linear_model.SGDClassifier‘s fit method.
Use the construction parameter instead.
• The deprecated support for the sequence of sequences (or list of lists) multilabel format was removed. To convert
to and from the supported binary indicator matrix format, use MultiLabelBinarizer.
• The behavior of calling the inverse_transform method of Pipeline.pipeline will change in 0.19.
It will no longer reshape one-dimensional input to two-dimensional input.
• The
deprecated
attributes
indicator_matrix_,
multilabel_
and
classes_
of
preprocessing.LabelBinarizer were removed.
• Using gamma=0 in svm.SVC and svm.SVR to automatically set the gamma to 1.
/ n_features is
deprecated and will be removed in 0.19. Use gamma="auto" instead.
Code Contributors
Aaron Schumacher, Adithya Ganesh, akitty, Alexandre Gramfort, Alexey Grigorev, Ali Baharev, Allen Riddell, Ando
Saabas, Andreas Mueller, Andrew Lamb, Anish Shah, Ankur Ankan, Anthony Erlinger, Ari Rouvinen, Arnaud Joly,
Arnaud Rachez, Arthur Mensch, banilo, Barmaley.exe, benjaminirving, Boyuan Deng, Brett Naul, Brian McFee,
Buddha Prakash, Chi Zhang, Chih-Wei Chang, Christof Angermueller, Christoph Gohlke, Christophe Bourguignat,
Christopher Erick Moody, Chyi-Kwei Yau, Cindy Sridharan, CJ Carey, Clyde-fare, Cory Lorenz, Dan Blanchard,
Daniel Galvez, Daniel Kronovet, Danny Sullivan, Data1010, David, David D Lowe, David Dotson, djipey, Dmitry
Spikhalskiy, Donne Martin, Dougal J. Sutherland, Dougal Sutherland, edson duarte, Eduardo Caro, Eric Larson, Eric
Martin, Erich Schubert, Fernando Carrillo, Frank C. Eckert, Frank Zalkow, Gael Varoquaux, Ganiev Ibraim, Gilles
Louppe, Giorgio Patrini, giorgiop, Graham Clenaghan, Gryllos Prokopis, gwulfs, Henry Lin, Hsuan-Tien Lin, Im-
manuel Bayer, Ishank Gulati, Jack Martin, Jacob Schreiber, Jaidev Deshpande, Jake Vanderplas, Jan Hendrik Metzen,
Jean Kossaiﬁ, Jeffrey04, Jeremy, jfraj, Jiali Mei, Joe Jevnik, Joel Nothman, John Kirkham, John Wittenauer, Joseph,
Joshua Loyal, Jungkook Park, KamalakerDadi, Kashif Rasul, Keith Goodman, Kian Ho, Konstantin Shmelkov, Kyler
Brown, Lars Buitinck, Lilian Besson, Loic Esteve, Louis Tiao, maheshakya, Maheshakya Wijewardena, Manoj Ku-
mar, MarkTab marktab.net, Martin Ku, Martin Spacek, MartinBpr, martinosorb, MaryanMorel, Masafumi Oyamada,
Mathieu Blondel, Matt Krump, Matti Lyra, Maxim Kolganov, mbillinger, mhg, Michael Heilman, Michael Patterson,
Miroslav Batchkarov, Nelle Varoquaux, Nicolas, Nikolay Mayorov, Olivier Grisel, Omer Katz, Óscar Nájera, Pauli
Virtanen, Peter Fischer, Peter Prettenhofer, Phil Roth, pianomania, Preston Parry, Raghav RV, Rob Zinkov, Robert
Layton, Rohan Ramanath, Saket Choudhary, Sam Zhang, santi, saurabh.bansod, scls19fr, Sebastian Raschka, Sebas-
tian Saeger, Shivan Sornarajah, SimonPL, sinhrks, Skipper Seabold, Sonny Hu, sseg, Stephen Hoover, Steven De
Gryze, Steven Seguin, Theodore Vasiloudis, Thomas Unterthiner, Tiago Freitas Pereira, Tian Wang, Tim Head, Timo-
thy Hopper, tokoroten, Tom Dupré la Tour, Trevor Stephens, Valentin Stolbunov, Vighnesh Birodkar, Vinayak Mehta,
Vincent, Vincent Michel, vstolbunov, wangz10, Wei Xue, Yucheng Low, Yury Zhauniarovich, Zac Stewart, zhai_pro,
Zichen Wang
1.7.6 Version 0.16.1
April 14, 2015
38
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
Changelog
Bug ﬁxes
• Allow input data larger than block_size in covariance.LedoitWolf by Andreas Müller.
• Fix
a
bug
in
isotonic.IsotonicRegression
deduplication
that
caused
unstable
result
in
calibration.CalibratedClassifierCV by Jan Hendrik Metzen.
• Fix sorting of labels in func:preprocessing.label_binarize by Michael Heilman.
• Fix
several
stability
and
convergence
issues
in
cross_decomposition.CCA
and
cross_decomposition.PLSCanonical by Andreas Müller
• Fix a bug in cluster.KMeans when precompute_distances=False on fortran-ordered data.
• Fix a speed regression in ensemble.RandomForestClassifier‘s predict and predict_proba
by Andreas Müller.
• Fix a regression where utils.shuffle converted lists and dataframes to arrays, by Olivier Grisel
1.7.7 Version 0.16
March 26, 2015
Highlights
• Speed improvements (notably in cluster.DBSCAN), reduced memory requirements, bug-ﬁxes and better
default settings.
• Multinomial Logistic regression and a path algorithm in linear_model.LogisticRegressionCV.
• Out-of core learning of PCA via decomposition.IncrementalPCA.
• Probability callibration of classiﬁers using calibration.CalibratedClassifierCV.
• cluster.Birch clustering method for large-scale datasets.
• Scalable
approximate
nearest
neighbors
search
with
Locality-sensitive
hashing
forests
in
neighbors.LSHForest.
• Improved error messages and better validation when using malformed input data.
• More robust integration with pandas dataframes.
Changelog
New features
• The new neighbors.LSHForest implements locality-sensitive hashing for approximate nearest neighbors
search. By Maheshakya Wijewardena.
• Added svm.LinearSVR. This class uses the liblinear implementation of Support Vector Regression which is
much faster for large sample sizes than svm.SVR with linear kernel. By Fabian Pedregosa and Qiang Luo.
• Incremental ﬁt for GaussianNB.
• Added sample_weight support to dummy.DummyClassifier and dummy.DummyRegressor. By
Arnaud Joly.
1.7. Release history
39
scikit-learn user guide, Release 0.18.2
• Added the metrics.label_ranking_average_precision_score metrics. By Arnaud Joly.
• Add the metrics.coverage_error metrics. By Arnaud Joly.
• Added linear_model.LogisticRegressionCV. By Manoj Kumar, Fabian Pedregosa, Gael Varoquaux
and Alexandre Gramfort.
• Added warm_start constructor parameter to make it possible for any trained forest model to grow additional
trees incrementally. By Laurent Direr.
• Added
sample_weight
support
to
ensemble.GradientBoostingClassifier
and
ensemble.GradientBoostingRegressor. By Peter Prettenhofer.
• Added decomposition.IncrementalPCA, an implementation of the PCA algorithm that supports out-
of-core learning with a partial_fit method. By Kyle Kastner.
• Averaged SGD for SGDClassifier and SGDRegressor By Danny Sullivan.
• Added cross_val_predict function which computes cross-validated estimates. By Luis Pedro Coelho
• Added linear_model.TheilSenRegressor, a robust generalized-median-based estimator. By Florian
Wilhelm.
• Added metrics.median_absolute_error, a robust metric. By Gael Varoquaux and Florian Wilhelm.
• Add cluster.Birch, an online clustering algorithm. By Manoj Kumar, Alexandre Gramfort and Joel Noth-
man.
• Added shrinkage support to discriminant_analysis.LinearDiscriminantAnalysis using two
new solvers. By Clemens Brunner and Martin Billinger.
• Added kernel_ridge.KernelRidge, an implementation of kernelized ridge regression. By Mathieu
Blondel and Jan Hendrik Metzen.
• All solvers in linear_model.Ridge now support sample_weight. By Mathieu Blondel.
• Added cross_validation.PredefinedSplit cross-validation for ﬁxed user-provided cross-validation
folds. By Thomas Unterthiner.
• Added calibration.CalibratedClassifierCV, an approach for calibrating the predicted probabili-
ties of a classiﬁer. By Alexandre Gramfort, Jan Hendrik Metzen, Mathieu Blondel and Balazs Kegl.
Enhancements
• Add option return_distance in hierarchical.ward_tree to return distances between nodes for
both structured and unstructured versions of the algorithm. By Matteo Visconti di Oleggio Castello. The same
option was added in hierarchical.linkage_tree. By Manoj Kumar
• Add support for sample weights in scorer objects. Metrics with sample weight support will automatically beneﬁt
from it. By Noel Dawe and Vlad Niculae.
• Added newton-cg and lbfgs solver support in linear_model.LogisticRegression. By Manoj Ku-
mar.
• Add
selection="random"
parameter
to
implement
stochastic
coordinate
descent
for
linear_model.Lasso, linear_model.ElasticNet and related. By Manoj Kumar.
• Add
sample_weight
parameter
to
metrics.jaccard_similarity_score
and
metrics.log_loss. By Jatin Shah.
• Support
sparse
multilabel
indicator
representation
in
preprocessing.LabelBinarizer
and
multiclass.OneVsRestClassifier (by Hamzeh Alsalhi with thanks to Rohit Sivaprasad), as
well as evaluation metrics (by Joel Nothman).
40
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Add sample_weight parameter to metrics.jaccard_similarity_score. By Jatin Shah.
• Add support for multiclass in metrics.hinge_loss. Added labels=None as optional parameter. By Saurabh
Jha.
• Add sample_weight parameter to metrics.hinge_loss. By Saurabh Jha.
• Add multi_class="multinomial" option in linear_model.LogisticRegression to imple-
ment a Logistic Regression solver that minimizes the cross-entropy or multinomial loss instead of the default
One-vs-Rest setting. Supports lbfgs and newton-cg solvers. By Lars Buitinck and Manoj Kumar. Solver option
newton-cg by Simon Wu.
• DictVectorizer can now perform fit_transform on an iterable in a single pass, when giving the option
sort=False. By Dan Blanchard.
• GridSearchCV and RandomizedSearchCV can now be conﬁgured to work with estimators that may fail
and raise errors on individual folds. This option is controlled by the error_score parameter. This does not affect
errors raised on re-ﬁt. By Michal Romaniuk.
• Add digits parameter to metrics.classiﬁcation_report to allow report to show different precision of ﬂoating
point numbers. By Ian Gilmore.
• Add a quantile prediction strategy to the dummy.DummyRegressor. By Aaron Staple.
• Add handle_unknown option to preprocessing.OneHotEncoder to handle unknown categorical fea-
tures more gracefully during transform. By Manoj Kumar.
• Added support for sparse input data to decision trees and their ensembles. By Fares Hedyati and Arnaud Joly.
• Optimized cluster.AffinityPropagation by reducing the number of memory allocations of large
temporary data-structures. By Antony Lee.
• Parellization of the computation of feature importances in random forest. By Olivier Grisel and Arnaud Joly.
• Add n_iter_ attribute to estimators that accept a max_iter attribute in their constructor. By Manoj Kumar.
• Added decision function for multiclass.OneVsOneClassifier By Raghav RV and Kyle Beauchamp.
• neighbors.kneighbors_graph and radius_neighbors_graph support non-Euclidean metrics.
By Manoj Kumar
• Parameter connectivity in cluster.AgglomerativeClustering and family now accept callables
that return a connectivity matrix. By Manoj Kumar.
• Sparse support for paired_distances. By Joel Nothman.
• cluster.DBSCAN now supports sparse input and sample weights and has been optimized: the inner loop has
been rewritten in Cython and radius neighbors queries are now computed in batch. By Joel Nothman and Lars
Buitinck.
• Add
class_weight
parameter
to
automatically
weight
samples
by
class
frequency
for
ensemble.RandomForestClassifier,
tree.DecisionTreeClassifier,
ensemble.ExtraTreesClassifier and tree.ExtraTreeClassifier. By Trevor Stephens.
• grid_search.RandomizedSearchCV now does sampling without replacement if all parameters are
given as lists. By Andreas Müller.
• Parallelized calculation of pairwise_distances is now supported for scipy metrics and custom callables.
By Joel Nothman.
• Allow the ﬁtting and scoring of all clustering algorithms in pipeline.Pipeline. By Andreas Müller.
• More robust seeding and improved error messages in cluster.MeanShift by Andreas Müller.
1.7. Release history
41
scikit-learn user guide, Release 0.18.2
• Make the stopping criterion for mixture.GMM, mixture.DPGMM and mixture.VBGMM less dependent
on the number of samples by thresholding the average log-likelihood change instead of its sum over all samples.
By Hervé Bredin.
• The outcome of manifold.spectral_embedding was made deterministic by ﬂipping the sign of eigen-
vectors. By Hasil Sharma.
• Signiﬁcant performance and memory usage improvements in preprocessing.PolynomialFeatures.
By Eric Martin.
• Numerical
stability
improvements
for
preprocessing.StandardScaler
and
preprocessing.scale. By Nicolas Goix
• svm.SVC ﬁtted on sparse input now implements decision_function. By Rob Zinkov and Andreas
Müller.
• cross_validation.train_test_split now preserves the input type, instead of converting to numpy
arrays.
Documentation improvements
• Added example of using FeatureUnion for heterogeneous input. By Matt Terry
• Documentation on scorers was improved, to highlight the handling of loss functions. By Matt Pico.
• A discrepancy between liblinear output and scikit-learn’s wrappers is now noted. By Manoj Kumar.
• Improved documentation generation: examples referring to a class or function are now shown in a gallery on
the class/function’s API reference page. By Joel Nothman.
• More explicit documentation of sample generators and of data transformation. By Joel Nothman.
• sklearn.neighbors.BallTree and sklearn.neighbors.KDTree used to point to empty pages
stating that they are aliases of BinaryTree. This has been ﬁxed to show the correct class docs. By Manoj Kumar.
• Added silhouette plots for analysis of KMeans clustering using metrics.silhouette_samples and
metrics.silhouette_score. See Selecting the number of clusters with silhouette analysis on KMeans
clustering
Bug ﬁxes
• Metaestimators
now
support
ducktyping
for
the
presence
of
decision_function,
predict_proba and other methods.
This ﬁxes behavior of grid_search.GridSearchCV,
grid_search.RandomizedSearchCV,
pipeline.Pipeline,
feature_selection.RFE,
feature_selection.RFECV when nested. By Joel Nothman
• The scoring attribute of grid-search and cross-validation methods is no longer ignored when a
grid_search.GridSearchCV is given as a base estimator or the base estimator doesn’t have predict.
• The function hierarchical.ward_tree now returns the children in the same order for both the structured
and unstructured versions. By Matteo Visconti di Oleggio Castello.
• feature_selection.RFECV now correctly handles cases when step is not equal to 1. By Nikolay
Mayorov
• The decomposition.PCA now undoes whitening in its inverse_transform. Also, its components_
now always have unit length. By Michael Eickenberg.
• Fix incomplete download of the dataset when datasets.download_20newsgroups is called. By Manoj
Kumar.
42
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Various ﬁxes to the Gaussian processes subpackage by Vincent Dubourg and Jan Hendrik Metzen.
• Calling partial_fit with class_weight=='auto' throws an appropriate error message and suggests
a work around. By Danny Sullivan.
• RBFSampler with gamma=g formerly approximated rbf_kernel with gamma=g/2.; the deﬁnition of
gamma is now consistent, which may substantially change your results if you use a ﬁxed value. (If you cross-
validated over gamma, it probably doesn’t matter too much.) By Dougal Sutherland.
• Pipeline object delegate the classes_ attribute to the underlying estimator. It allows, for instance, to make
bagging of a pipeline object. By Arnaud Joly
• neighbors.NearestCentroid now uses the median as the centroid when metric is set to manhattan.
It was using the mean before. By Manoj Kumar
• Fix
numerical
stability
issues
in
linear_model.SGDClassifier
and
linear_model.SGDRegressor by clipping large gradients and ensuring that weight decay rescal-
ing is always positive (for large l2 regularization and large learning rate values). By Olivier Grisel
• When compute_full_tree is set to “auto”, the full tree is built when n_clusters is high and is early stopped when
n_clusters is low, while the behavior should be vice-versa in cluster.AgglomerativeClustering (and
friends). This has been ﬁxed By Manoj Kumar
• Fix lazy centering of data in linear_model.enet_path and linear_model.lasso_path. It was
centered around one. It has been changed to be centered around the origin. By Manoj Kumar
• Fix handling of precomputed afﬁnity matrices in cluster.AgglomerativeClustering when using
connectivity constraints. By Cathy Deng
• Correct partial_fit handling of class_prior for sklearn.naive_bayes.MultinomialNB and
sklearn.naive_bayes.BernoulliNB. By Trevor Stephens.
• Fixed a crash in metrics.precision_recall_fscore_support when using unsorted labels in the
multi-label setting. By Andreas Müller.
• Avoid
skipping
the
ﬁrst
nearest
neighbor
in
the
methods
radius_neighbors,
kneighbors,
kneighbors_graph
and
radius_neighbors_graph
in
sklearn.neighbors.NearestNeighbors and family, when the query data is not the same as ﬁt
data. By Manoj Kumar.
• Fix log-density calculation in the mixture.GMM with tied covariance. By Will Dawson
• Fixed a scaling error in feature_selection.SelectFdr where a factor n_features was missing. By
Andrew Tulloch
• Fix zero division in neighbors.KNeighborsRegressor and related classes when using distance weight-
ing and having identical data points. By Garret-R.
• Fixed round off errors with non positive-deﬁnite covariance matrices in GMM. By Alexis Mignon.
• Fixed a error in the computation of conditional probabilities in naive_bayes.BernoulliNB. By Hanna
Wallach.
• Make the method radius_neighbors of neighbors.NearestNeighbors return the samples lying
on the boundary for algorithm='brute'. By Yan Yi.
• Flip
sign
of
dual_coef_
of
svm.SVC
to
make
it
consistent
with
the
documentation
and
decision_function. By Artem Sobolev.
• Fixed handling of ties in isotonic.IsotonicRegression. We now use the weighted average of targets
(secondary method). By Andreas Müller and Michael Bommarito.
1.7. Release history
43
scikit-learn user guide, Release 0.18.2
API changes summary
• GridSearchCV and cross_val_score and other meta-estimators don’t convert pandas DataFrames into
arrays any more, allowing DataFrame speciﬁc operations in custom estimators.
• multiclass.fit_ovr,
multiclass.predict_ovr,
predict_proba_ovr,
multiclass.fit_ovo,
multiclass.predict_ovo,
multiclass.fit_ecoc
and
multiclass.predict_ecoc are deprecated. Use the underlying estimators instead.
• Nearest neighbors estimators used to take arbitrary keyword arguments and pass these to their distance metric.
This will no longer be supported in scikit-learn 0.18; use the metric_params argument instead.
• n_jobs parameter of the ﬁt method shifted to the constructor of the LinearRegression class.
• The predict_proba method of multiclass.OneVsRestClassifier now returns two probabilities
per sample in the multiclass case; this is consistent with other estimators and with the method’s documenta-
tion, but previous versions accidentally returned only the positive probability. Fixed by Will Lamond and Lars
Buitinck.
• Change default value of precompute in ElasticNet and Lasso to False. Setting precompute to “auto” was
found to be slower when n_samples > n_features since the computation of the Gram matrix is computationally
expensive and outweighs the beneﬁt of ﬁtting the Gram for just one alpha. precompute="auto" is now
deprecated and will be removed in 0.18 By Manoj Kumar.
• Expose positive option in linear_model.enet_path and linear_model.enet_path which
constrains coefﬁcients to be positive. By Manoj Kumar.
• Users
should
now
supply
an
explicit
average
parameter
to
sklearn.metrics.f1_score,
sklearn.metrics.fbeta_score,
sklearn.metrics.recall_score
and
sklearn.metrics.precision_score when performing multiclass or multilabel (i.e.
not binary)
classiﬁcation. By Joel Nothman.
• scoring parameter for cross validation now accepts ‘f1_micro’, ‘f1_macro’ or ‘f1_weighted’. ‘f1’ is now for
binary classiﬁcation only. Similar changes apply to ‘precision’ and ‘recall’. By Joel Nothman.
• The fit_intercept, normalize and return_models parameters in linear_model.enet_path
and linear_model.lasso_path have been removed. They were deprecated since 0.14
• From
now
onwards,
all
estimators
will
uniformly
raise
NotFittedError
(utils.validation.NotFittedError), when any of the predict like methods are called be-
fore the model is ﬁt. By Raghav RV.
• Input data validation was refactored for more consistent input validation. The check_arrays function was
replaced by check_array and check_X_y. By Andreas Müller.
• Allow X=None in the methods radius_neighbors,
kneighbors,
kneighbors_graph and
radius_neighbors_graph in sklearn.neighbors.NearestNeighbors and family.
If set to
None, then for every sample this avoids setting the sample itself as the ﬁrst nearest neighbor. By Manoj Kumar.
• Add
parameter
include_self
in
neighbors.kneighbors_graph
and
neighbors.radius_neighbors_graph which has to be explicitly set by the user.
If set to True,
then the sample itself is considered as the ﬁrst nearest neighbor.
• thresh parameter is deprecated in favor of new tol parameter in GMM, DPGMM and VBGMM. See Enhancements
section for details. By Hervé Bredin.
• Estimators will treat input with dtype object as numeric when possible. By Andreas Müller
• Estimators now raise ValueError consistently when ﬁtted on empty data (less than 1 sample or less than 1 feature
for 2D input). By Olivier Grisel.
44
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• The
shuffle
option
of
linear_model.SGDClassifier,
linear_model.SGDRegressor,
linear_model.Perceptron,
linear_model.PassiveAgressiveClassifier
and
linear_model.PassiveAgressiveRegressor now defaults to True.
• cluster.DBSCAN now uses a deterministic initialization. The random_state parameter is deprecated. By
Erich Schubert.
Code Contributors
A. Flaxman, Aaron Schumacher, Aaron Staple, abhishek thakur, Akshay, akshayah3, Aldrian Obaja, Alexander
Fabisch, Alexandre Gramfort, Alexis Mignon, Anders Aagaard, Andreas Mueller, Andreas van Cranenburgh, An-
drew Tulloch, Andrew Walker, Antony Lee, Arnaud Joly, banilo, Barmaley.exe, Ben Davies, Benedikt Koehler, bhsu,
Boris Feld, Borja Ayerdi, Boyuan Deng, Brent Pedersen, Brian Wignall, Brooke Osborn, Calvin Giles, Cathy Deng,
Celeo, cgohlke, chebee7i, Christian Stade-Schuldt, Christof Angermueller, Chyi-Kwei Yau, CJ Carey, Clemens Brun-
ner, Daiki Aminaka, Dan Blanchard, danfrankj, Danny Sullivan, David Fletcher, Dmitrijs Milajevs, Dougal J. Suther-
land, Erich Schubert, Fabian Pedregosa, Florian Wilhelm, ﬂoydsoft, Félix-Antoine Fortin, Gael Varoquaux, Garrett-R,
Gilles Louppe, gpassino, gwulfs, Hampus Bengtsson, Hamzeh Alsalhi, Hanna Wallach, Harry Mavroforakis, Hasil
Sharma, Helder, Herve Bredin, Hsiang-Fu Yu, Hugues SALAMIN, Ian Gilmore, Ilambharathi Kanniah, Imran Haque,
isms, Jake VanderPlas, Jan Dlabal, Jan Hendrik Metzen, Jatin Shah, Javier López Peña, jdcaballero, Jean Kossaiﬁ, Jeff
Hammerbacher, Joel Nothman, Jonathan Helmus, Joseph, Kaicheng Zhang, Kevin Markham, Kyle Beauchamp, Kyle
Kastner, Lagacherie Matthieu, Lars Buitinck, Laurent Direr, leepei, Loic Esteve, Luis Pedro Coelho, Lukas Michel-
bacher, maheshakya, Manoj Kumar, Manuel, Mario Michael Krell, Martin, Martin Billinger, Martin Ku, Mateusz
Susik, Mathieu Blondel, Matt Pico, Matt Terry, Matteo Visconti dOC, Matti Lyra, Max Linke, Mehdi Cherti, Michael
Bommarito, Michael Eickenberg, Michal Romaniuk, MLG, mr.Shu, Nelle Varoquaux, Nicola Montecchio, Nicolas,
Nikolay Mayorov, Noel Dawe, Okal Billy, Olivier Grisel, Óscar Nájera, Paolo Puggioni, Peter Prettenhofer, Pratap
Vardhan, pvnguyen, queqichao, Rafael Carrascosa, Raghav R V, Rahiel Kasim, Randall Mason, Rob Zinkov, Robert
Bradshaw, Saket Choudhary, Sam Nicholls, Samuel Charron, Saurabh Jha, sethdandridge, sinhrks, snuderl, Stefan
Otte, Stefan van der Walt, Steve Tjoa, swu, Sylvain Zimmer, tejesh95, terrycojones, Thomas Delteil, Thomas Un-
terthiner, Tomas Kazmar, trevorstephens, tttthomasssss, Tzu-Ming Kuo, ugurcaliskan, ugurthemaster, Vinayak Mehta,
Vincent Dubourg, Vjacheslav Murashkin, Vlad Niculae, wadawson, Wei Xue, Will Lamond, Wu Jiang, x0l, Xinfan
Meng, Yan Yi, Yu-Chin
1.7.8 Version 0.15.2
September 4, 2014
Bug ﬁxes
• Fixed handling of the p parameter of the Minkowski distance that was previously ignored in nearest neighbors
models. By Nikolay Mayorov.
• Fixed duplicated alphas in linear_model.LassoLars with early stopping on 32 bit Python. By Olivier
Grisel and Fabian Pedregosa.
• Fixed the build under Windows when scikit-learn is built with MSVC while NumPy is built with MinGW. By
Olivier Grisel and Federico Vaggi.
• Fixed an array index overﬂow bug in the coordinate descent solver. By Gael Varoquaux.
• Better handling of numpy 1.9 deprecation warnings. By Gael Varoquaux.
• Removed unnecessary data copy in cluster.KMeans. By Gael Varoquaux.
• Explicitly close open ﬁles to avoid ResourceWarnings under Python 3. By Calvin Giles.
1.7. Release history
45
scikit-learn user guide, Release 0.18.2
• The transform of discriminant_analysis.LinearDiscriminantAnalysis now projects the
input on the most discriminant directions. By Martin Billinger.
• Fixed potential overﬂow in _tree.safe_realloc by Lars Buitinck.
• Performance optimization in isotonic.IsotonicRegression. By Robert Bradshaw.
• nose is non-longer a runtime dependency to import sklearn, only for running the tests. By Joel Nothman.
• Many documentation and website ﬁxes by Joel Nothman, Lars Buitinck Matt Pico, and others.
1.7.9 Version 0.15.1
August 1, 2014
Bug ﬁxes
• Made
cross_validation.cross_val_score
use
cross_validation.KFold
instead
of
cross_validation.StratifiedKFold on multi-output classiﬁcation problems. By Nikolay Mayorov.
• Support unseen labels preprocessing.LabelBinarizer to restore the default behavior of 0.14.1 for
backward compatibility. By Hamzeh Alsalhi.
• Fixed the cluster.KMeans stopping criterion that prevented early convergence detection. By Edward Raff
and Gael Varoquaux.
• Fixed the behavior of multiclass.OneVsOneClassifier. in case of ties at the per-class vote level by
computing the correct per-class sum of prediction scores. By Andreas Müller.
• Made cross_validation.cross_val_score and grid_search.GridSearchCV accept Python
lists as input data. This is especially useful for cross-validation and model selection of text processing pipelines.
By Andreas Müller.
• Fixed data input checks of most estimators to accept input data that implements the NumPy __array__
protocol. This is the case for for pandas.Series and pandas.DataFrame in recent versions of pandas.
By Gael Varoquaux.
• Fixed a regression for linear_model.SGDClassifier with class_weight="auto" on data with
non-contiguous labels. By Olivier Grisel.
1.7.10 Version 0.15
July 15, 2014
Highlights
• Many speed and memory improvements all across the code
• Huge speed and memory improvements to random forests (and extra trees) that also beneﬁt better from parallel
computing.
• Incremental ﬁt to BernoulliRBM
• Added cluster.AgglomerativeClustering for hierarchical agglomerative clustering with average
linkage, complete linkage and ward strategies.
• Added linear_model.RANSACRegressor for robust regression models.
46
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Added dimensionality reduction with manifold.TSNE which can be used to visualize high-dimensional data.
Changelog
New features
• Added ensemble.BaggingClassifier and ensemble.BaggingRegressor meta-estimators for
ensembling any kind of base estimator. See the Bagging section of the user guide for details and examples.
By Gilles Louppe.
• New unsupervised feature selection algorithm feature_selection.VarianceThreshold, by Lars
Buitinck.
• Added linear_model.RANSACRegressor meta-estimator for the robust ﬁtting of regression models. By
Johannes Schönberger.
• Added cluster.AgglomerativeClustering for hierarchical agglomerative clustering with average
linkage, complete linkage and ward strategies, by Nelle Varoquaux and Gael Varoquaux.
• Shorthand constructors pipeline.make_pipeline and pipeline.make_union were added by Lars
Buitinck.
• Shufﬂe option for cross_validation.StratifiedKFold. By Jeffrey Blackburne.
• Incremental learning (partial_fit) for Gaussian Naive Bayes by Imran Haque.
• Added partial_fit to BernoulliRBM By Danny Sullivan.
• Added learning_curve utility to chart performance with respect to training size. See Plotting Learning
Curves. By Alexander Fabisch.
• Add positive option in LassoCV and ElasticNetCV. By Brian Wignall and Alexandre Gramfort.
• Added linear_model.MultiTaskElasticNetCV and linear_model.MultiTaskLassoCV. By
Manoj Kumar.
• Added manifold.TSNE. By Alexander Fabisch.
Enhancements
• Add
sparse
input
support
to
ensemble.AdaBoostClassifier
and
ensemble.AdaBoostRegressor meta-estimators. By Hamzeh Alsalhi.
• Memory improvements of decision trees, by Arnaud Joly.
• Decision trees can now be built in best-ﬁrst manner by using max_leaf_nodes as the stopping criteria.
Refactored the tree code to use either a stack or a priority queue for tree building. By Peter Prettenhofer and
Gilles Louppe.
• Decision trees can now be ﬁtted on fortran- and c-style arrays, and non-continuous arrays without the need to
make a copy. If the input array has a different dtype than np.float32, a fortran- style copy will be made
since fortran-style memory layout has speed advantages. By Peter Prettenhofer and Gilles Louppe.
• Speed improvement of regression trees by optimizing the the computation of the mean square error criterion.
This lead to speed improvement of the tree, forest and gradient boosting tree modules. By Arnaud Joly
• The img_to_graph and grid_tograph functions in sklearn.feature_extraction.image now
return np.ndarray instead of np.matrix when return_as=np.ndarray. See the Notes section for
more information on compatibility.
1.7. Release history
47
scikit-learn user guide, Release 0.18.2
• Changed the internal storage of decision trees to use a struct array. This ﬁxed some small bugs, while improving
code and providing a small speed gain. By Joel Nothman.
• Reduce memory usage and overhead when ﬁtting and predicting with forests of randomized trees in parallel
with n_jobs != 1 by leveraging new threading backend of joblib 0.8 and releasing the GIL in the tree ﬁtting
Cython code. By Olivier Grisel and Gilles Louppe.
• Speed improvement of the sklearn.ensemble.gradient_boosting module. By Gilles Louppe and
Peter Prettenhofer.
• Various enhancements to the sklearn.ensemble.gradient_boosting module: a warm_start ar-
gument to ﬁt additional trees, a max_leaf_nodes argument to ﬁt GBM style trees, a monitor ﬁt argument
to inspect the estimator during training, and refactoring of the verbose code. By Peter Prettenhofer.
• Faster sklearn.ensemble.ExtraTrees by caching feature values. By Arnaud Joly.
• Faster depth-based tree building algorithm such as decision tree, random forest, extra trees or gradient tree
boosting (with depth based growing strategy) by avoiding trying to split on found constant features in the sample
subset. By Arnaud Joly.
• Add min_weight_fraction_leaf pre-pruning parameter to tree-based methods: the minimum weighted
fraction of the input samples required to be at a leaf node. By Noel Dawe.
• Added metrics.pairwise_distances_argmin_min, by Philippe Gervais.
• Added predict method to cluster.AffinityPropagation and cluster.MeanShift, by Mathieu
Blondel.
• Vector and matrix multiplications have been optimised throughout the library by Denis Engemann, and Alexan-
dre Gramfort. In particular, they should take less memory with older NumPy versions (prior to 1.7.2).
• Precision-recall and ROC examples now use train_test_split, and have more explanation of why these metrics
are useful. By Kyle Kastner
• The training algorithm for decomposition.NMF is faster for sparse matrices and has much lower memory
complexity, meaning it will scale up gracefully to large datasets. By Lars Buitinck.
• Added svd_method option with default value to “randomized” to decomposition.FactorAnalysis to
save memory and signiﬁcantly speedup computation by Denis Engemann, and Alexandre Gramfort.
• Changed cross_validation.StratifiedKFold to try and preserve as much of the original ordering of
samples as possible so as not to hide overﬁtting on datasets with a non-negligible level of samples dependency.
By Daniel Nouri and Olivier Grisel.
• Add multi-output support to gaussian_process.GaussianProcess by John Novak.
• Support for precomputed distance matrices in nearest neighbor estimators by Robert Layton and Joel Nothman.
• Norm computations optimized for NumPy 1.6 and later versions by Lars Buitinck. In particular, the k-means
algorithm no longer needs a temporary data structure the size of its input.
• dummy.DummyClassifier can now be used to predict a constant output value. By Manoj Kumar.
• dummy.DummyRegressor has now a strategy parameter which allows to predict the mean, the median of the
training set or a constant output value. By Maheshakya Wijewardena.
• Multi-label
classiﬁcation
output
in
multilabel
indicator
format
is
now
supported
by
metrics.roc_auc_score and metrics.average_precision_score by Arnaud Joly.
• Signiﬁcant
performance
improvements
(more
than
100x
speedup
for
large
problems)
in
isotonic.IsotonicRegression by Andrew Tulloch.
• Speed and memory usage improvements to the SGD algorithm for linear models: it now uses threads, not
separate processes, when n_jobs>1. By Lars Buitinck.
48
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Grid search and cross validation allow NaNs in the input arrays so that preprocessors such as
preprocessing.Imputer can be trained within the cross validation loop, avoiding potentially skewed
results.
• Ridge regression can now deal with sample weights in feature space (only sample space until then). By Michael
Eickenberg. Both solutions are provided by the Cholesky solver.
• Several
classiﬁcation
and
regression
metrics
now
support
weighted
samples
with
the
new
sample_weight
argument:
metrics.accuracy_score,
metrics.zero_one_loss,
metrics.precision_score,
metrics.average_precision_score,
metrics.f1_score,
metrics.fbeta_score,
metrics.recall_score,
metrics.roc_auc_score,
metrics.explained_variance_score,
metrics.mean_squared_error,
metrics.mean_absolute_error, metrics.r2_score. By Noel Dawe.
• Speed up of the sample generator datasets.make_multilabel_classification. By Joel Nothman.
Documentation improvements
• The Working With Text Data tutorial has now been worked in to the main documentation’s tutorial section.
Includes exercises and skeletons for tutorial presentation. Original tutorial created by several authors including
Olivier Grisel, Lars Buitinck and many others. Tutorial integration into the scikit-learn documentation by Jaques
Grobler
• Added Computational Performance documentation. Discussion and examples of prediction latency / throughput
and different factors that have inﬂuence over speed. Additional tips for building faster models and choosing a
relevant compromise between speed and predictive power. By Eustache Diemert.
Bug ﬁxes
• Fixed bug in decomposition.MiniBatchDictionaryLearning : partial_fit was not working
properly.
• Fixed bug in linear_model.stochastic_gradient : l1_ratio was used as (1.0 -l1_ratio)
.
• Fixed bug in multiclass.OneVsOneClassifier with string labels
• Fixed a bug in LassoCV and ElasticNetCV: they would not pre-compute the Gram matrix with
precompute=True or precompute="auto" and n_samples > n_features. By Manoj Kumar.
• Fixed incorrect estimation of the degrees of freedom in feature_selection.f_regression when vari-
ates are not centered. By Virgile Fritsch.
• Fixed a race condition in parallel processing with pre_dispatch != "all" (for instance,
in
cross_val_score). By Olivier Grisel.
• Raise error in cluster.FeatureAgglomeration and cluster.WardAgglomeration when no
samples are given, rather than returning meaningless clustering.
• Fixed bug in gradient_boosting.GradientBoostingRegressor with loss='huber': gamma
might have not been initialized.
• Fixed feature importances as computed with a forest of randomized trees when ﬁt with sample_weight !=
None and/or with bootstrap=True. By Gilles Louppe.
1.7. Release history
49
scikit-learn user guide, Release 0.18.2
API changes summary
• sklearn.hmm is deprecated. Its removal is planned for the 0.17 release.
• Use of covariance.EllipticEnvelop has now been removed after deprecation.
Please use
covariance.EllipticEnvelope instead.
• cluster.Ward is deprecated. Use cluster.AgglomerativeClustering instead.
• cluster.WardClustering is deprecated. Use
• cluster.AgglomerativeClustering instead.
• cross_validation.Bootstrap
is
deprecated.
cross_validation.KFold
or
cross_validation.ShuffleSplit are recommended instead.
• Direct support for the sequence of sequences (or list of lists) multilabel format is deprecated. To convert to and
from the supported binary indicator matrix format, use MultiLabelBinarizer. By Joel Nothman.
• Add score method to PCA following the model of probabilistic PCA and deprecate ProbabilisticPCA
model whose score implementation is not correct. The computation now also exploits the matrix inversion
lemma for faster computation. By Alexandre Gramfort.
• The score method of FactorAnalysis now returns the average log-likelihood of the samples.
Use
score_samples to get log-likelihood of each sample. By Alexandre Gramfort.
• Generating boolean masks (the setting indices=False) from cross-validation generators is deprecated. Sup-
port for masks will be removed in 0.17. The generators have produced arrays of indices by default since 0.10.
By Joel Nothman.
• 1-d arrays containing strings with dtype=object (as used in Pandas) are now considered valid classiﬁcation
targets. This ﬁxes a regression from version 0.13 in some classiﬁers. By Joel Nothman.
• Fix wrong explained_variance_ratio_ attribute in RandomizedPCA. By Alexandre Gramfort.
• Fit alphas for each l1_ratio instead of mean_l1_ratio in linear_model.ElasticNetCV
and linear_model.LassoCV. This changes the shape of alphas_ from (n_alphas,)
to
(n_l1_ratio,n_alphas) if the l1_ratio provided is a 1-D array like object of length greater than
one. By Manoj Kumar.
• Fix linear_model.ElasticNetCV and linear_model.LassoCV when ﬁtting intercept and input
data is sparse. The automatic grid of alphas was not computed correctly and the scaling with normalize was
wrong. By Manoj Kumar.
• Fix wrong maximal number of features drawn (max_features) at each split for decision trees, random forests
and gradient tree boosting. Previously, the count for the number of drawn features started only after one non
constant features in the split. This bug ﬁx will affect computational and generalization performance of those
algorithms in the presence of constant features. To get back previous generalization performance, you should
modify the value of max_features. By Arnaud Joly.
• Fix
wrong
maximal
number
of
features
drawn
(max_features)
at
each
split
for
ensemble.ExtraTreesClassifier
and
ensemble.ExtraTreesRegressor.
Previously,
only non constant features in the split was counted as drawn. Now constant features are counted as drawn.
Furthermore at least one feature must be non constant in order to make a valid split. This bug ﬁx will affect
computational and generalization performance of extra trees in the presence of constant features. To get back
previous generalization performance, you should modify the value of max_features. By Arnaud Joly.
• Fix utils.compute_class_weight when class_weight=="auto". Previously it was broken for
input of non-integer dtype and the weighted array that was returned was wrong. By Manoj Kumar.
• Fix cross_validation.Bootstrap to return ValueError when n_train + n_test > n. By
Ronald Phlypo.
50
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
People
List of contributors for release 0.15 by number of commits.
• 312 Olivier Grisel
• 275 Lars Buitinck
• 221 Gael Varoquaux
• 148 Arnaud Joly
• 134 Johannes Schönberger
• 119 Gilles Louppe
• 113 Joel Nothman
• 111 Alexandre Gramfort
• 95 Jaques Grobler
• 89 Denis Engemann
• 83 Peter Prettenhofer
• 83 Alexander Fabisch
• 62 Mathieu Blondel
• 60 Eustache Diemert
• 60 Nelle Varoquaux
• 49 Michael Bommarito
• 45 Manoj-Kumar-S
• 28 Kyle Kastner
• 26 Andreas Mueller
• 22 Noel Dawe
• 21 Maheshakya Wijewardena
• 21 Brooke Osborn
• 21 Hamzeh Alsalhi
• 21 Jake VanderPlas
• 21 Philippe Gervais
• 19 Bala Subrahmanyam Varanasi
• 12 Ronald Phlypo
• 10 Mikhail Korobov
• 8 Thomas Unterthiner
• 8 Jeffrey Blackburne
• 8 eltermann
• 8 bwignall
• 7 Ankit Agrawal
• 7 CJ Carey
1.7. Release history
51
scikit-learn user guide, Release 0.18.2
• 6 Daniel Nouri
• 6 Chen Liu
• 6 Michael Eickenberg
• 6 ugurthemaster
• 5 Aaron Schumacher
• 5 Baptiste Lagarde
• 5 Rajat Khanduja
• 5 Robert McGibbon
• 5 Sergio Pascual
• 4 Alexis Metaireau
• 4 Ignacio Rossi
• 4 Virgile Fritsch
• 4 Sebastian Säger
• 4 Ilambharathi Kanniah
• 4 sdenton4
• 4 Robert Layton
• 4 Alyssa
• 4 Amos Waterland
• 3 Andrew Tulloch
• 3 murad
• 3 Steven Maude
• 3 Karol Pysniak
• 3 Jacques Kvam
• 3 cgohlke
• 3 cjlin
• 3 Michael Becker
• 3 hamzeh
• 3 Eric Jacobsen
• 3 john collins
• 3 kaushik94
• 3 Erwin Marsi
• 2 csytracy
• 2 LK
• 2 Vlad Niculae
• 2 Laurent Direr
• 2 Erik Shilts
52
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• 2 Raul Garreta
• 2 Yoshiki Vázquez Baeza
• 2 Yung Siang Liau
• 2 abhishek thakur
• 2 James Yu
• 2 Rohit Sivaprasad
• 2 Roland Szabo
• 2 amormachine
• 2 Alexis Mignon
• 2 Oscar Carlsson
• 2 Nantas Nardelli
• 2 jess010
• 2 kowalski87
• 2 Andrew Clegg
• 2 Federico Vaggi
• 2 Simon Frid
• 2 Félix-Antoine Fortin
• 1 Ralf Gommers
• 1 t-aft
• 1 Ronan Amicel
• 1 Rupesh Kumar Srivastava
• 1 Ryan Wang
• 1 Samuel Charron
• 1 Samuel St-Jean
• 1 Fabian Pedregosa
• 1 Skipper Seabold
• 1 Stefan Walk
• 1 Stefan van der Walt
• 1 Stephan Hoyer
• 1 Allen Riddell
• 1 Valentin Haenel
• 1 Vijay Ramesh
• 1 Will Myers
• 1 Yaroslav Halchenko
• 1 Yoni Ben-Meshulam
• 1 Yury V. Zaytsev
1.7. Release history
53
scikit-learn user guide, Release 0.18.2
• 1 adrinjalali
• 1 ai8rahim
• 1 alemagnani
• 1 alex
• 1 benjamin wilson
• 1 chalmerlowe
• 1 dzikie dro˙zd˙ze
• 1 jamestwebber
• 1 matrixorz
• 1 popo
• 1 samuela
• 1 François Boulogne
• 1 Alexander Measure
• 1 Ethan White
• 1 Guilherme Trein
• 1 Hendrik Heuer
• 1 IvicaJovic
• 1 Jan Hendrik Metzen
• 1 Jean Michel Rouly
• 1 Eduardo Ariño de la Rubia
• 1 Jelle Zijlstra
• 1 Eddy L O Jansson
• 1 Denis
• 1 John
• 1 John Schmidt
• 1 Jorge Cañardo Alastuey
• 1 Joseph Perla
• 1 Joshua Vredevoogd
• 1 José Ricardo
• 1 Julien Miotte
• 1 Kemal Eren
• 1 Kenta Sato
• 1 David Cournapeau
• 1 Kyle Kelley
• 1 Daniele Medri
• 1 Laurent Luce
54
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• 1 Laurent Pierron
• 1 Luis Pedro Coelho
• 1 DanielWeitzenfeld
• 1 Craig Thompson
• 1 Chyi-Kwei Yau
• 1 Matthew Brett
• 1 Matthias Feurer
• 1 Max Linke
• 1 Chris Filo Gorgolewski
• 1 Charles Earl
• 1 Michael Hanke
• 1 Michele Orrù
• 1 Bryan Lunt
• 1 Brian Kearns
• 1 Paul Butler
• 1 Paweł Mandera
• 1 Peter
• 1 Andrew Ash
• 1 Pietro Zambelli
• 1 staubda
1.7.11 Version 0.14
August 7, 2013
Changelog
• Missing
values
with
sparse
and
dense
matrices
can
be
imputed
with
the
transformer
preprocessing.Imputer by Nicolas Trésegnie.
• The core implementation of decisions trees has been rewritten from scratch, allowing for faster tree induction
and lower memory consumption in all tree-based estimators. By Gilles Louppe.
• Added ensemble.AdaBoostClassifier and ensemble.AdaBoostRegressor, by Noel Dawe and
Gilles Louppe. See the AdaBoost section of the user guide for details and examples.
• Added grid_search.RandomizedSearchCV and grid_search.ParameterSampler for ran-
domized hyperparameter optimization. By Andreas Müller.
• Added
biclustering
algorithms
(sklearn.cluster.bicluster.SpectralCoclustering
and
sklearn.cluster.bicluster.SpectralBiclustering),
data
generation
methods
(sklearn.datasets.make_biclusters
and
sklearn.datasets.make_checkerboard),
and scoring metrics (sklearn.metrics.consensus_score). By Kemal Eren.
• Added Restricted Boltzmann Machines (neural_network.BernoulliRBM). By Yann Dauphin.
1.7. Release history
55
scikit-learn user guide, Release 0.18.2
• Python 3 support by Justin Vincent, Lars Buitinck, Subhodeep Moitra and Olivier Grisel. All tests now pass
under Python 3.3.
• Ability to pass one penalty (alpha value) per target in linear_model.Ridge, by @eickenberg and Mathieu
Blondel.
• Fixed sklearn.linear_model.stochastic_gradient.py L2 regularization issue (minor practical
signiﬁcance). By Norbert Crombach and Mathieu Blondel .
• Added an interactive version of Andreas Müller‘s Machine Learning Cheat Sheet (for scikit-learn) to the docu-
mentation. See Choosing the right estimator. By Jaques Grobler.
• grid_search.GridSearchCV and cross_validation.cross_val_score now support the use
of advanced scoring function such as area under the ROC curve and f-beta scores. See The scoring parameter:
deﬁning model evaluation rules for details. By Andreas Müller and Lars Buitinck. Passing a function from
sklearn.metrics as score_func is deprecated.
• Multi-label
classiﬁcation
output
is
now
supported
by
metrics.accuracy_score,
metrics.zero_one_loss,
metrics.f1_score,
metrics.fbeta_score,
metrics.classification_report,
metrics.precision_score
and
metrics.recall_score by Arnaud Joly.
• Two new metrics metrics.hamming_loss and metrics.jaccard_similarity_score are added
with multi-label support by Arnaud Joly.
• Speed and memory usage improvements in feature_extraction.text.CountVectorizer and
feature_extraction.text.TfidfVectorizer, by Jochen Wersdörfer and Roman Sinayev.
• The
min_df
parameter
in
feature_extraction.text.CountVectorizer
and
feature_extraction.text.TfidfVectorizer, which used to be 2, has been reset to 1 to
avoid unpleasant surprises (empty vocabularies) for novice users who try it out on tiny document collections. A
value of at least 2 is still recommended for practical use.
• svm.LinearSVC, linear_model.SGDClassifier and linear_model.SGDRegressor now
have a sparsify method that converts their coef_ into a sparse matrix, meaning stored models trained
using these estimators can be made much more compact.
• linear_model.SGDClassifier now produces multiclass probability estimates when trained under log
loss or modiﬁed Huber loss.
• Hyperlinks to documentation in example code on the website by Martin Luessi.
• Fixed bug in preprocessing.MinMaxScaler causing incorrect scaling of the features for non-default
feature_range settings. By Andreas Müller.
• max_features in tree.DecisionTreeClassifier, tree.DecisionTreeRegressor and all
derived ensemble estimators now supports percentage values. By Gilles Louppe.
• Performance improvements in isotonic.IsotonicRegression by Nelle Varoquaux.
• metrics.accuracy_score has an option normalize to return the fraction or the number of correctly clas-
siﬁed sample by Arnaud Joly.
• Added metrics.log_loss that computes log loss, aka cross-entropy loss. By Jochen Wersdörfer and Lars
Buitinck.
• A bug that caused ensemble.AdaBoostClassifier‘s to output incorrect probabilities has been ﬁxed.
• Feature selectors now share a mixin providing consistent transform, inverse_transform and
get_support methods. By Joel Nothman.
• A ﬁtted grid_search.GridSearchCV or grid_search.RandomizedSearchCV can now generally
be pickled. By Joel Nothman.
56
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Refactored
and
vectorized
implementation
of
metrics.roc_curve
and
metrics.precision_recall_curve. By Joel Nothman.
• The new estimator sklearn.decomposition.TruncatedSVD performs dimensionality reduction using
SVD on sparse matrices, and can be used for latent semantic analysis (LSA). By Lars Buitinck.
• Added self-contained example of out-of-core learning on text data Out-of-core classiﬁcation of text documents.
By Eustache Diemert.
• The default number of components for sklearn.decomposition.RandomizedPCA is now correctly
documented to be n_features. This was the default behavior, so programs using it will continue to work as
they did.
• sklearn.cluster.KMeans now ﬁts several orders of magnitude faster on sparse data (the speedup depends
on the sparsity). By Lars Buitinck.
• Reduce memory footprint of FastICA by Denis Engemann and Alexandre Gramfort.
• Verbose output in sklearn.ensemble.gradient_boosting now uses a column format and prints
progress in decreasing frequency. It also shows the remaining time. By Peter Prettenhofer.
• sklearn.ensemble.gradient_boosting provides out-of-bag improvement oob_improvement_
rather than the OOB score for model selection. An example that shows how to use OOB estimates to select the
number of trees was added. By Peter Prettenhofer.
• Most metrics now support string labels for multiclass classiﬁcation by Arnaud Joly and Lars Buitinck.
• New OrthogonalMatchingPursuitCV class by Alexandre Gramfort and Vlad Niculae.
• Fixed a bug in sklearn.covariance.GraphLassoCV: the ‘alphas’ parameter now works as expected
when given a list of values. By Philippe Gervais.
• Fixed an important bug in sklearn.covariance.GraphLassoCV that prevented all folds provided by
a CV object to be used (only the ﬁrst 3 were used). When providing a CV object, execution time may thus
increase signiﬁcantly compared to the previous version (bug results are correct now). By Philippe Gervais.
• cross_validation.cross_val_score and the grid_search module is now tested with multi-
output data by Arnaud Joly.
• datasets.make_multilabel_classification can now return the output in label indicator multil-
abel format by Arnaud Joly.
• K-nearest neighbors, neighbors.KNeighborsRegressor and neighbors.RadiusNeighborsRegressor,
and
radius
neighbors,
neighbors.RadiusNeighborsRegressor
and
neighbors.RadiusNeighborsClassifier support multioutput data by Arnaud Joly.
• Random state in LibSVM-based estimators (svm.SVC, NuSVC, OneClassSVM, svm.SVR, svm.NuSVR)
can now be controlled. This is useful to ensure consistency in the probability estimates for the classiﬁers trained
with probability=True. By Vlad Niculae.
• Out-of-core learning support for discrete naive Bayes classiﬁers sklearn.naive_bayes.MultinomialNB
and sklearn.naive_bayes.BernoulliNB by adding the partial_fit method by Olivier Grisel.
• New website design and navigation by Gilles Louppe, Nelle Varoquaux, Vincent Michel and Andreas Müller.
• Improved documentation on multi-class, multi-label and multi-output classiﬁcation by Yannick Schwartz and
Arnaud Joly.
• Better input and error handling in the metrics module by Arnaud Joly and Joel Nothman.
• Speed optimization of the hmm module by Mikhail Korobov
• Signiﬁcant speed improvements for sklearn.cluster.DBSCAN by cleverless
1.7. Release history
57
scikit-learn user guide, Release 0.18.2
API changes summary
• The auc_score was renamed roc_auc_score.
• Testing scikit-learn with sklearn.test() is deprecated. Use nosetests sklearn from the command
line.
• Feature importances in tree.DecisionTreeClassifier, tree.DecisionTreeRegressor and all
derived ensemble estimators are now computed on the ﬂy when accessing the feature_importances_
attribute. Setting compute_importances=True is no longer required. By Gilles Louppe.
• linear_model.lasso_path and linear_model.enet_path can return its results in the same format
as that of linear_model.lars_path. This is done by setting the return_models parameter to False.
By Jaques Grobler and Alexandre Gramfort
• grid_search.IterGrid was renamed to grid_search.ParameterGrid.
• Fixed bug in KFold causing imperfect class balance in some cases. By Alexandre Gramfort and Tadej Janež.
• sklearn.neighbors.BallTree has been refactored, and a sklearn.neighbors.KDTree has been
added which shares the same interface. The Ball Tree now works with a wide variety of distance metrics.
Both classes have many new methods, including single-tree and dual-tree queries, breadth-ﬁrst and depth-ﬁrst
searching, and more advanced queries such as kernel density estimation and 2-point correlation functions. By
Jake Vanderplas
• Support for scipy.spatial.cKDTree within neighbors queries has been removed, and the functionality replaced
with the new KDTree class.
• sklearn.neighbors.KernelDensity has been added, which performs efﬁcient kernel density estima-
tion with a variety of kernels.
• sklearn.decomposition.KernelPCA now always returns output with n_components components,
unless the new parameter remove_zero_eig is set to True. This new behavior is consistent with the way
kernel PCA was always documented; previously, the removal of components with zero eigenvalues was tacitly
performed on all data.
• gcv_mode="auto"
no
longer
tries
to
perform
SVD
on
a
densiﬁed
sparse
matrix
in
sklearn.linear_model.RidgeCV.
• Sparse matrix support in sklearn.decomposition.RandomizedPCA is now deprecated in favor of the
new TruncatedSVD.
• cross_validation.KFold and cross_validation.StratifiedKFold now enforce n_folds >=
2 otherwise a ValueError is raised. By Olivier Grisel.
• datasets.load_files‘s charset and charset_errors parameters were renamed encoding and
decode_errors.
• Attribute
oob_score_
in
sklearn.ensemble.GradientBoostingRegressor
and
sklearn.ensemble.GradientBoostingClassifier is deprecated and has been replaced by
oob_improvement_ .
• Attributes in OrthogonalMatchingPursuit have been deprecated (copy_X, Gram, ...) and precompute_gram
renamed precompute for consistency. See #2224.
• sklearn.preprocessing.StandardScaler now converts integer input to ﬂoat, and raises a warning.
Previously it rounded for dense integer input.
• sklearn.multiclass.OneVsRestClassifier now has a decision_function method.
This
will return the distance of each sample from the decision boundary for each class, as long as the underlying
estimators implement the decision_function method. By Kyle Kastner.
• Better input validation, warning on unexpected shapes for y.
58
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
People
List of contributors for release 0.14 by number of commits.
• 277 Gilles Louppe
• 245 Lars Buitinck
• 187 Andreas Mueller
• 124 Arnaud Joly
• 112 Jaques Grobler
• 109 Gael Varoquaux
• 107 Olivier Grisel
• 102 Noel Dawe
• 99 Kemal Eren
• 79 Joel Nothman
• 75 Jake VanderPlas
• 73 Nelle Varoquaux
• 71 Vlad Niculae
• 65 Peter Prettenhofer
• 64 Alexandre Gramfort
• 54 Mathieu Blondel
• 38 Nicolas Trésegnie
• 35 eustache
• 27 Denis Engemann
• 25 Yann N. Dauphin
• 19 Justin Vincent
• 17 Robert Layton
• 15 Doug Coleman
• 14 Michael Eickenberg
• 13 Robert Marchman
• 11 Fabian Pedregosa
• 11 Philippe Gervais
• 10 Jim Holmström
• 10 Tadej Janež
• 10 syhw
• 9 Mikhail Korobov
• 9 Steven De Gryze
• 8 sergeyf
• 7 Ben Root
1.7. Release history
59
scikit-learn user guide, Release 0.18.2
• 7 Hrishikesh Huilgolkar
• 6 Kyle Kastner
• 6 Martin Luessi
• 6 Rob Speer
• 5 Federico Vaggi
• 5 Raul Garreta
• 5 Rob Zinkov
• 4 Ken Geis
• 3 A. Flaxman
• 3 Denton Cockburn
• 3 Dougal Sutherland
• 3 Ian Ozsvald
• 3 Johannes Schönberger
• 3 Robert McGibbon
• 3 Roman Sinayev
• 3 Szabo Roland
• 2 Diego Molla
• 2 Imran Haque
• 2 Jochen Wersdörfer
• 2 Sergey Karayev
• 2 Yannick Schwartz
• 2 jamestwebber
• 1 Abhijeet Kolhe
• 1 Alexander Fabisch
• 1 Bastiaan van den Berg
• 1 Benjamin Peterson
• 1 Daniel Velkov
• 1 Fazlul Shahriar
• 1 Felix Brockherde
• 1 Félix-Antoine Fortin
• 1 Harikrishnan S
• 1 Jack Hale
• 1 JakeMick
• 1 James McDermott
• 1 John Benediktsson
• 1 John Zwinck
60
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• 1 Joshua Vredevoogd
• 1 Justin Pati
• 1 Kevin Hughes
• 1 Kyle Kelley
• 1 Matthias Ekman
• 1 Miroslav Shubernetskiy
• 1 Naoki Orii
• 1 Norbert Crombach
• 1 Rafael Cunha de Almeida
• 1 Rolando Espinoza La fuente
• 1 Seamus Abshere
• 1 Sergey Feldman
• 1 Sergio Medina
• 1 Stefano Lattarini
• 1 Steve Koch
• 1 Sturla Molden
• 1 Thomas Jarosch
• 1 Yaroslav Halchenko
1.7.12 Version 0.13.1
February 23, 2013
The 0.13.1 release only ﬁxes some bugs and does not add any new functionality.
Changelog
• Fixed a testing error caused by the function cross_validation.train_test_split being interpreted
as a test by Yaroslav Halchenko.
• Fixed a bug in the reassignment of small clusters in the cluster.MiniBatchKMeans by Gael Varoquaux.
• Fixed default value of gamma in decomposition.KernelPCA by Lars Buitinck.
• Updated joblib to 0.7.0d by Gael Varoquaux.
• Fixed scaling of the deviance in ensemble.GradientBoostingClassifier by Peter Prettenhofer.
• Better tie-breaking in multiclass.OneVsOneClassifier by Andreas Müller.
• Other small improvements to tests and documentation.
1.7. Release history
61
scikit-learn user guide, Release 0.18.2
People
List of contributors for release 0.13.1 by number of commits.
• 16 Lars Buitinck
• 12 Andreas Müller
• 8 Gael Varoquaux
• 5 Robert Marchman
• 3 Peter Prettenhofer
• 2 Hrishikesh Huilgolkar
• 1 Bastiaan van den Berg
• 1 Diego Molla
• 1 Gilles Louppe
• 1 Mathieu Blondel
• 1 Nelle Varoquaux
• 1 Rafael Cunha de Almeida
• 1 Rolando Espinoza La fuente
• 1 Vlad Niculae
• 1 Yaroslav Halchenko
1.7.13 Version 0.13
January 21, 2013
New Estimator Classes
• dummy.DummyClassifier and dummy.DummyRegressor, two data-independent predictors by Mathieu
Blondel. Useful to sanity-check your estimators. See Dummy estimators in the user guide. Multioutput support
added by Arnaud Joly.
• decomposition.FactorAnalysis, a transformer implementing the classical factor analysis, by Chris-
tian Osendorfer and Alexandre Gramfort. See Factor Analysis in the user guide.
• feature_extraction.FeatureHasher,
a
transformer
implementing
the
“hashing
trick”
for
fast,
low-memory
feature
extraction
from
string
ﬁelds
by
Lars
Buitinck
and
feature_extraction.text.HashingVectorizer for text documents by Olivier Grisel See
Feature hashing and Vectorizing a large text corpus with the hashing trick for the documentation and sample
usage.
• pipeline.FeatureUnion, a transformer that concatenates results of several other transformers by Andreas
Müller. See FeatureUnion: composite feature spaces in the user guide.
• random_projection.GaussianRandomProjection, random_projection.SparseRandomProjection
and the function random_projection.johnson_lindenstrauss_min_dim. The ﬁrst two are trans-
formers implementing Gaussian and sparse random projection matrix by Olivier Grisel and Arnaud Joly. See
Random Projection in the user guide.
62
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• kernel_approximation.Nystroem, a transformer for approximating arbitrary kernels by Andreas
Müller. See Nystroem Method for Kernel Approximation in the user guide.
• preprocessing.OneHotEncoder, a transformer that computes binary encodings of categorical features
by Andreas Müller. See Encoding categorical features in the user guide.
• linear_model.PassiveAggressiveClassifier and linear_model.PassiveAggressiveRegressor,
predictors implementing an efﬁcient stochastic optimization for linear models by Rob Zinkov and Mathieu
Blondel. See Passive Aggressive Algorithms in the user guide.
• ensemble.RandomTreesEmbedding, a transformer for creating high-dimensional sparse representations
using ensembles of totally random trees by Andreas Müller. See Totally Random Trees Embedding in the user
guide.
• manifold.SpectralEmbedding and function manifold.spectral_embedding, implementing
the “laplacian eigenmaps” transformation for non-linear dimensionality reduction by Wei Li. See Spectral
Embedding in the user guide.
• isotonic.IsotonicRegression by Fabian Pedregosa, Alexandre Gramfort and Nelle Varoquaux,
Changelog
• metrics.zero_one_loss (formerly metrics.zero_one) now has option for normalized output that
reports the fraction of misclassiﬁcations, rather than the raw number of misclassiﬁcations. By Kyle Beauchamp.
• tree.DecisionTreeClassifier and all derived ensemble models now support sample weighting, by
Noel Dawe and Gilles Louppe.
• Speedup improvement when using bootstrap samples in forests of randomized trees, by Peter Prettenhofer and
Gilles Louppe.
• Partial dependence plots for Gradient Tree Boosting in ensemble.partial_dependence.partial_dependence
by Peter Prettenhofer. See Partial Dependence Plots for an example.
• The table of contents on the website has now been made expandable by Jaques Grobler.
• feature_selection.SelectPercentile now breaks ties deterministically instead of returning all
equally ranked features.
• feature_selection.SelectKBest and feature_selection.SelectPercentile are more
numerically stable since they use scores, rather than p-values, to rank results. This means that they might
sometimes select different features than they did previously.
• Ridge regression and ridge classiﬁcation ﬁtting with sparse_cg solver no longer has quadratic memory com-
plexity, by Lars Buitinck and Fabian Pedregosa.
• Ridge regression and ridge classiﬁcation now support a new fast solver called lsqr, by Mathieu Blondel.
• Speed up of metrics.precision_recall_curve by Conrad Lee.
• Added support for reading/writing svmlight ﬁles with pairwise preference attribute (qid in svmlight ﬁle format)
in datasets.dump_svmlight_file and datasets.load_svmlight_file by Fabian Pedregosa.
• Faster and more robust metrics.confusion_matrix and Clustering performance evaluation by Wei Li.
• cross_validation.cross_val_score now works with precomputed kernels and afﬁnity matrices, by
Andreas Müller.
• LARS algorithm made more numerically stable with heuristics to drop regressors too correlated as well as to
stop the path when numerical noise becomes predominant, by Gael Varoquaux.
• Faster implementation of metrics.precision_recall_curve by Conrad Lee.
1.7. Release history
63
scikit-learn user guide, Release 0.18.2
• New kernel metrics.chi2_kernel by Andreas Müller, often used in computer vision applications.
• Fix of longstanding bug in naive_bayes.BernoulliNB ﬁxed by Shaun Jackman.
• Implemented predict_proba in multiclass.OneVsRestClassifier, by Andrew Winterman.
• Improve consistency in gradient boosting: estimators ensemble.GradientBoostingRegressor and
ensemble.GradientBoostingClassifier use the estimator tree.DecisionTreeRegressor
instead of the tree._tree.Tree data structure by Arnaud Joly.
• Fixed a ﬂoating point exception in the decision trees module, by Seberg.
• Fix metrics.roc_curve fails when y_true has only one class by Wei Li.
• Add the metrics.mean_absolute_error function which computes the mean absolute error.
The
metrics.mean_squared_error, metrics.mean_absolute_error and metrics.r2_score
metrics support multioutput by Arnaud Joly.
• Fixed class_weight support in svm.LinearSVC and linear_model.LogisticRegression by
Andreas Müller. The meaning of class_weight was reversed as erroneously higher weight meant less
positives of a given class in earlier releases.
• Improve narrative documentation and consistency in sklearn.metrics for regression and classiﬁcation
metrics by Arnaud Joly.
• Fixed a bug in sklearn.svm.SVC when using csr-matrices with unsorted indices by Xinfan Meng and An-
dreas Müller.
• MiniBatchKMeans: Add random reassignment of cluster centers with little observations attached to them,
by Gael Varoquaux.
API changes summary
• Renamed
all
occurrences
of
n_atoms
to
n_components
for
con-
sistency.
This
applies
to
decomposition.DictionaryLearning,
decomposition.MiniBatchDictionaryLearning,
decomposition.dict_learning,
decomposition.dict_learning_online.
• Renamed
all
occurrences
of
max_iters
to
max_iter
for
consis-
tency.
This
applies
to
semi_supervised.LabelPropagation
and
semi_supervised.label_propagation.LabelSpreading.
• Renamed
all
occurrences
of
learn_rate
to
learning_rate
for
consistency
in
ensemble.BaseGradientBoosting and ensemble.GradientBoostingRegressor.
• The module sklearn.linear_model.sparse is gone. Sparse matrix support was already integrated into
the “regular” linear models.
• sklearn.metrics.mean_square_error, which incorrectly returned the accumulated error, was re-
moved. Use mean_squared_error instead.
• Passing class_weight parameters to fit methods is no longer supported. Pass them to estimator construc-
tors instead.
• GMMs no longer have decode and rvs methods. Use the score, predict or sample methods instead.
• The solver ﬁt option in Ridge regression and classiﬁcation is now deprecated and will be removed in v0.14.
Use the constructor option instead.
• feature_extraction.text.DictVectorizer now returns sparse matrices in the CSR format, in-
stead of COO.
64
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Renamed k in cross_validation.KFold
and cross_validation.StratifiedKFold
to
n_folds, renamed n_bootstraps to n_iter in cross_validation.Bootstrap.
• Renamed
all
occurrences
of
n_iterations
to
n_iter
for
consistency.
This
applies
to
cross_validation.ShuffleSplit,
cross_validation.StratifiedShuffleSplit,
utils.randomized_range_finder and utils.randomized_svd.
• Replaced
rho
in
linear_model.ElasticNet
and
linear_model.SGDClassifier
by
l1_ratio.
The rho parameter had different meanings; l1_ratio was introduced to avoid confu-
sion.
It has the same meaning as previously rho in linear_model.ElasticNet and (1-rho) in
linear_model.SGDClassifier.
• linear_model.LassoLars and linear_model.Lars now store a list of paths in the case of multiple
targets, rather than an array of paths.
• The attribute gmm of hmm.GMMHMM was renamed to gmm_ to adhere more strictly with the API.
• cluster.spectral_embedding was moved to manifold.spectral_embedding.
• Renamed eig_tol in manifold.spectral_embedding, cluster.SpectralClustering to
eigen_tol, renamed mode to eigen_solver.
• Renamed mode in manifold.spectral_embedding and cluster.SpectralClustering to
eigen_solver.
• classes_ and n_classes_ attributes of tree.DecisionTreeClassifier and all derived ensemble
models are now ﬂat in case of single output problems and nested in case of multi-output problems.
• The estimators_ attribute of ensemble.gradient_boosting.GradientBoostingRegressor
and
ensemble.gradient_boosting.GradientBoostingClassifier
is
now
an
array
of
:class:’tree.DecisionTreeRegressor’.
• Renamed chunk_size to batch_size in decomposition.MiniBatchDictionaryLearning
and decomposition.MiniBatchSparsePCA for consistency.
• svm.SVC and svm.NuSVC now provide a classes_ attribute and support arbitrary dtypes for labels y.
Also, the dtype returned by predict now reﬂects the dtype of y during fit (used to be np.float).
• Changed
default
test_size
in
cross_validation.train_test_split
to
None,
added
pos-
sibility to infer test_size from train_size in cross_validation.ShuffleSplit and
cross_validation.StratifiedShuffleSplit.
• Renamed
function
sklearn.metrics.zero_one
to
sklearn.metrics.zero_one_loss.
Be aware that the default behavior in sklearn.metrics.zero_one_loss is different from
sklearn.metrics.zero_one: normalize=False is changed to normalize=True.
• Renamed function metrics.zero_one_score to metrics.accuracy_score.
• datasets.make_circles now has the same number of inner and outer points.
• In the Naive Bayes classiﬁers, the class_prior parameter was moved from fit to __init__.
People
List of contributors for release 0.13 by number of commits.
• 364 Andreas Müller
• 143 Arnaud Joly
• 137 Peter Prettenhofer
• 131 Gael Varoquaux
1.7. Release history
65
scikit-learn user guide, Release 0.18.2
• 117 Mathieu Blondel
• 108 Lars Buitinck
• 106 Wei Li
• 101 Olivier Grisel
• 65 Vlad Niculae
• 54 Gilles Louppe
• 40 Jaques Grobler
• 38 Alexandre Gramfort
• 30 Rob Zinkov
• 19 Aymeric Masurelle
• 18 Andrew Winterman
• 17 Fabian Pedregosa
• 17 Nelle Varoquaux
• 16 Christian Osendorfer
• 14 Daniel Nouri
• 13 Virgile Fritsch
• 13 syhw
• 12 Satrajit Ghosh
• 10 Corey Lynch
• 10 Kyle Beauchamp
• 9 Brian Cheung
• 9 Immanuel Bayer
• 9 mr.Shu
• 8 Conrad Lee
• 8 James Bergstra
• 7 Tadej Janež
• 6 Brian Cajes
• 6 Jake Vanderplas
• 6 Michael
• 6 Noel Dawe
• 6 Tiago Nunes
• 6 cow
• 5 Anze
• 5 Shiqiao Du
• 4 Christian Jauvin
• 4 Jacques Kvam
66
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• 4 Richard T. Guy
• 4 Robert Layton
• 3 Alexandre Abraham
• 3 Doug Coleman
• 3 Scott Dickerson
• 2 ApproximateIdentity
• 2 John Benediktsson
• 2 Mark Veronda
• 2 Matti Lyra
• 2 Mikhail Korobov
• 2 Xinfan Meng
• 1 Alejandro Weinstein
• 1 Alexandre Passos
• 1 Christoph Deil
• 1 Eugene Nizhibitsky
• 1 Kenneth C. Arnold
• 1 Luis Pedro Coelho
• 1 Miroslav Batchkarov
• 1 Pavel
• 1 Sebastian Berg
• 1 Shaun Jackman
• 1 Subhodeep Moitra
• 1 bob
• 1 dengemann
• 1 emanuele
• 1 x006
1.7.14 Version 0.12.1
October 8, 2012
The 0.12.1 release is a bug-ﬁx release with no additional features, but is instead a set of bug ﬁxes
Changelog
• Improved numerical stability in spectral embedding by Gael Varoquaux
• Doctest under windows 64bit by Gael Varoquaux
• Documentation ﬁxes for elastic net by Andreas Müller and Alexandre Gramfort
• Proper behavior with fortran-ordered NumPy arrays by Gael Varoquaux
1.7. Release history
67
scikit-learn user guide, Release 0.18.2
• Make GridSearchCV work with non-CSR sparse matrix by Lars Buitinck
• Fix parallel computing in MDS by Gael Varoquaux
• Fix Unicode support in count vectorizer by Andreas Müller
• Fix MinCovDet breaking with X.shape = (3, 1) by Virgile Fritsch
• Fix clone of SGD objects by Peter Prettenhofer
• Stabilize GMM by Virgile Fritsch
People
• 14 Peter Prettenhofer
• 12 Gael Varoquaux
• 10 Andreas Müller
• 5 Lars Buitinck
• 3 Virgile Fritsch
• 1 Alexandre Gramfort
• 1 Gilles Louppe
• 1 Mathieu Blondel
1.7.15 Version 0.12
September 4, 2012
Changelog
• Various speed improvements of the decision trees module, by Gilles Louppe.
• ensemble.GradientBoostingRegressor
and ensemble.GradientBoostingClassifier
now support feature subsampling via the max_features argument, by Peter Prettenhofer.
• Added Huber and Quantile loss functions to ensemble.GradientBoostingRegressor, by Peter Pret-
tenhofer.
• Decision trees and forests of randomized trees now support multi-output classiﬁcation and regression problems,
by Gilles Louppe.
• Added preprocessing.LabelEncoder, a simple utility class to normalize labels or transform non-
numerical labels, by Mathieu Blondel.
• Added the epsilon-insensitive loss and the ability to make probabilistic predictions with the modiﬁed huber loss
in Stochastic Gradient Descent, by Mathieu Blondel.
• Added Multi-dimensional Scaling (MDS), by Nelle Varoquaux.
• SVMlight ﬁle format loader now detects compressed (gzip/bzip2) ﬁles and decompresses them on the ﬂy, by
Lars Buitinck.
• SVMlight ﬁle format serializer now preserves double precision ﬂoating point values, by Olivier Grisel.
• A common testing framework for all estimators was added, by Andreas Müller.
• Understandable error messages for estimators that do not accept sparse input by Gael Varoquaux
68
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Speedups in hierarchical clustering by Gael Varoquaux. In particular building the tree now supports early
stopping. This is useful when the number of clusters is not small compared to the number of samples.
• Add MultiTaskLasso and MultiTaskElasticNet for joint feature selection, by Alexandre Gramfort.
• Added metrics.auc_score and metrics.average_precision_score convenience functions by
Andreas Müller.
• Improved sparse matrix support in the Feature selection module by Andreas Müller.
• New word boundaries-aware character n-gram analyzer for the Text feature extraction module by @kernc.
• Fixed bug in spectral clustering that led to single point clusters by Andreas Müller.
• In feature_extraction.text.CountVectorizer, added an option to ignore infrequent words,
min_df by Andreas Müller.
• Add support for multiple targets in some linear models (ElasticNet, Lasso and OrthogonalMatchingPursuit) by
Vlad Niculae and Alexandre Gramfort.
• Fixes in decomposition.ProbabilisticPCA score function by Wei Li.
• Fixed feature importance computation in Gradient Tree Boosting.
API changes summary
• The old scikits.learn package has disappeared; all code should import from sklearn instead, which
was introduced in 0.9.
• In metrics.roc_curve, the thresholds array is now returned with it’s order reversed, in order to keep
it consistent with the order of the returned fpr and tpr.
• In hmm objects, like hmm.GaussianHMM, hmm.MultinomialHMM, etc., all parameters must be passed to
the object when initialising it and not through fit. Now fit will only accept the data as an input parameter.
• For all SVM classes, a faulty behavior of gamma was ﬁxed. Previously, the default gamma value was only
computed the ﬁrst time fit was called and then stored. It is now recalculated on every call to fit.
• All Base classes are now abstract meta classes so that they can not be instantiated.
• cluster.ward_tree now also returns the parent array. This is necessary for early-stopping in which case
the tree is not completely built.
• In feature_extraction.text.CountVectorizer the parameters min_n and max_n were joined to
the parameter n_gram_range to enable grid-searching both at once.
• In feature_extraction.text.CountVectorizer, words that appear only in one document are now
ignored by default. To reproduce the previous behavior, set min_df=1.
• Fixed API inconsistency: linear_model.SGDClassifier.predict_proba now returns 2d array
when ﬁt on two classes.
• Fixed API inconsistency: discriminant_analysis.QuadraticDiscriminantAnalysis.decision_function
and
discriminant_analysis.LinearDiscriminantAnalysis.decision_function
now
return 1d arrays when ﬁt on two classes.
• Grid of alphas used for ﬁtting linear_model.LassoCV and linear_model.ElasticNetCV is now
stored in the attribute alphas_ rather than overriding the init parameter alphas.
• Linear models when alpha is estimated by cross-validation store the estimated value in the alpha_ attribute
rather than just alpha or best_alpha.
• ensemble.GradientBoostingClassifier now supports ensemble.GradientBoostingClassifier.staged
and ensemble.GradientBoostingClassifier.staged_predict.
1.7. Release history
69
scikit-learn user guide, Release 0.18.2
• svm.sparse.SVC and other sparse SVM classes are now deprecated. The all classes in the Support Vector
Machines module now automatically select the sparse or dense representation base on the input.
• All clustering algorithms now interpret the array X given to fit as input data,
in particular
cluster.SpectralClustering and cluster.AffinityPropagation which previously expected
afﬁnity matrices.
• For clustering algorithms that take the desired number of clusters as a parameter, this parameter is now called
n_clusters.
People
• 267 Andreas Müller
• 94 Gilles Louppe
• 89 Gael Varoquaux
• 79 Peter Prettenhofer
• 60 Mathieu Blondel
• 57 Alexandre Gramfort
• 52 Vlad Niculae
• 45 Lars Buitinck
• 44 Nelle Varoquaux
• 37 Jaques Grobler
• 30 Alexis Mignon
• 30 Immanuel Bayer
• 27 Olivier Grisel
• 16 Subhodeep Moitra
• 13 Yannick Schwartz
• 12 @kernc
• 11 Virgile Fritsch
• 9 Daniel Duckworth
• 9 Fabian Pedregosa
• 9 Robert Layton
• 8 John Benediktsson
• 7 Marko Burjek
• 5 Nicolas Pinto
• 4 Alexandre Abraham
• 4 Jake Vanderplas
• 3 Brian Holt
• 3 Edouard Duchesnay
• 3 Florian Hoenig
70
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• 3 ﬂyingimmidev
• 2 Francois Savard
• 2 Hannes Schulz
• 2 Peter Welinder
• 2 Yaroslav Halchenko
• 2 Wei Li
• 1 Alex Companioni
• 1 Brandyn A. White
• 1 Bussonnier Matthias
• 1 Charles-Pierre Astolﬁ
• 1 Dan O’Huiginn
• 1 David Cournapeau
• 1 Keith Goodman
• 1 Ludwig Schwardt
• 1 Olivier Hervieu
• 1 Sergio Medina
• 1 Shiqiao Du
• 1 Tim Sheerman-Chase
• 1 buguen
1.7.16 Version 0.11
May 7, 2012
Changelog
Highlights
• Gradient boosted regression trees (Gradient Tree Boosting) for classiﬁcation and regression by Peter Pretten-
hofer and Scott White .
• Simple
dict-based
feature
loader
with
support
for
categorical
variables
(feature_extraction.DictVectorizer) by Lars Buitinck.
• Added Matthews correlation coefﬁcient (metrics.matthews_corrcoef) and added macro and micro av-
erage options to metrics.precision_score, metrics.recall_score and metrics.f1_score
by Satrajit Ghosh.
• Out of Bag Estimates of generalization error for Ensemble methods by Andreas Müller.
• Randomized sparse models: Randomized sparse linear models for feature selection, by Alexandre Gramfort and
Gael Varoquaux
• Label Propagation for semi-supervised learning, by Clay Woolam. Note the semi-supervised API is still work
in progress, and may change.
1.7. Release history
71
scikit-learn user guide, Release 0.18.2
• Added BIC/AIC model selection to classical Gaussian mixture models and uniﬁed the API with the remainder
of scikit-learn, by Bertrand Thirion
• Added
sklearn.cross_validation.StratifiedShuffleSplit,
which
is
a
sklearn.cross_validation.ShuffleSplit with balanced splits, by Yannick Schwartz.
• sklearn.neighbors.NearestCentroid classiﬁer added, along with a shrink_threshold param-
eter, which implements shrunken centroid classiﬁcation, by Robert Layton.
Other changes
• Merged dense and sparse implementations of Stochastic Gradient Descent module and exposed utility extension
types for sequential datasets seq_dataset and weight vectors weight_vector by Peter Prettenhofer.
• Added partial_fit (support for online/minibatch learning) and warm_start to the Stochastic Gradient De-
scent module by Mathieu Blondel.
• Dense
and
sparse
implementations
of
Support
Vector
Machines
classes
and
linear_model.LogisticRegression merged by Lars Buitinck.
• Regressors can now be used as base estimator in the Multiclass and multilabel algorithms module by Mathieu
Blondel.
• Added
n_jobs
option
to
metrics.pairwise.pairwise_distances
and
metrics.pairwise.pairwise_kernels for parallel computation, by Mathieu Blondel.
• K-means can now be run in parallel, using the n_jobs argument to either K-means or KMeans, by Robert
Layton.
• Improved Cross-validation: evaluating estimator performance and Tuning the hyper-parameters of an estima-
tor documentation and introduced the new cross_validation.train_test_split helper function by
Olivier Grisel
• svm.SVC members coef_ and intercept_ changed sign for consistency with decision_function;
for kernel==linear, coef_ was ﬁxed in the one-vs-one case, by Andreas Müller.
• Performance improvements to efﬁcient leave-one-out cross-validated Ridge regression, esp.
for the
n_samples > n_features case, in linear_model.RidgeCV, by Reuben Fletcher-Costin.
• Refactoring and simpliﬁcation of the Text feature extraction API and ﬁxed a bug that caused possible negative
IDF, by Olivier Grisel.
• Beam pruning option in _BaseHMM module has been removed since it is difﬁcult to Cythonize. If you are
interested in contributing a Cython version, you can use the python version in the git history as a reference.
• Classes in Nearest Neighbors now support arbitrary Minkowski metric for nearest neighbors searches. The
metric can be speciﬁed by argument p.
API changes summary
• covariance.EllipticEnvelop is now deprecated - Please use covariance.EllipticEnvelope
instead.
• NeighborsClassifier and NeighborsRegressor are gone in the module Nearest Neighbors. Use
the classes KNeighborsClassifier, RadiusNeighborsClassifier, KNeighborsRegressor
and/or RadiusNeighborsRegressor instead.
• Sparse classes in the Stochastic Gradient Descent module are now deprecated.
72
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• In mixture.GMM, mixture.DPGMM and mixture.VBGMM, parameters must be passed to an object when
initialising it and not through fit. Now fit will only accept the data as an input parameter.
• methods rvs and decode in GMM module are now deprecated. sample and score or predict should be
used instead.
• attribute _scores and _pvalues in univariate feature selection objects are now deprecated. scores_ or
pvalues_ should be used instead.
• In LogisticRegression, LinearSVC, SVC and NuSVC, the class_weight parameter is now an ini-
tialization parameter, not a parameter to ﬁt. This makes grid searches over this parameter possible.
• LFW data is now always shape (n_samples,n_features) to be consistent with the Olivetti faces
dataset. Use images and pairs attribute to access the natural images shapes instead.
• In svm.LinearSVC, the meaning of the multi_class parameter changed. Options now are 'ovr' and
'crammer_singer', with 'ovr' being the default. This does not change the default behavior but hopefully
is less confusing.
• Class
feature_selection.text.Vectorizer
is
deprecated
and
replaced
by
feature_selection.text.TfidfVectorizer.
• The preprocessor / analyzer nested structure for text feature extraction has been removed. All those features are
now directly passed as ﬂat constructor arguments to feature_selection.text.TfidfVectorizer
and feature_selection.text.CountVectorizer, in particular the following parameters are now
used:
– analyzer can be 'word' or 'char' to switch the default analysis scheme, or use a speciﬁc python
callable (as previously).
– tokenizer and preprocessor have been introduced to make it still possible to customize those steps
with the new API.
– input explicitly control how to interpret the sequence passed to fit and predict: ﬁlenames, ﬁle
objects or direct (byte or Unicode) strings.
– charset decoding is explicit and strict by default.
– the vocabulary, ﬁtted or not is now stored in the vocabulary_ attribute to be consistent with the
project conventions.
• Class
feature_selection.text.TfidfVectorizer
now
derives
directly
from
feature_selection.text.CountVectorizer to make grid search trivial.
• methods rvs in _BaseHMM module are now deprecated. sample should be used instead.
• Beam pruning option in _BaseHMM module is removed since it is difﬁcult to be Cythonized. If you are inter-
ested, you can look in the history codes by git.
• The SVMlight format loader now supports ﬁles with both zero-based and one-based column indices, since both
occur “in the wild”.
• Arguments in class ShuffleSplit are now consistent with StratifiedShuffleSplit. Arguments
test_fraction and train_fraction are deprecated and renamed to test_size and train_size
and can accept both float and int.
• Arguments in class Bootstrap are now consistent with StratifiedShuffleSplit.
Arguments
n_test and n_train are deprecated and renamed to test_size and train_size and can accept both
float and int.
• Argument p added to classes in Nearest Neighbors to specify an arbitrary Minkowski metric for nearest neigh-
bors searches.
1.7. Release history
73
scikit-learn user guide, Release 0.18.2
People
• 282 Andreas Müller
• 239 Peter Prettenhofer
• 198 Gael Varoquaux
• 129 Olivier Grisel
• 114 Mathieu Blondel
• 103 Clay Woolam
• 96 Lars Buitinck
• 88 Jaques Grobler
• 82 Alexandre Gramfort
• 50 Bertrand Thirion
• 42 Robert Layton
• 28 ﬂyingimmidev
• 26 Jake Vanderplas
• 26 Shiqiao Du
• 21 Satrajit Ghosh
• 17 David Marek
• 17 Gilles Louppe
• 14 Vlad Niculae
• 11 Yannick Schwartz
• 10 Fabian Pedregosa
• 9 fcostin
• 7 Nick Wilson
• 5 Adrien Gaidon
• 5 Nicolas Pinto
• 4 David Warde-Farley
• 5 Nelle Varoquaux
• 5 Emmanuelle Gouillart
• 3 Joonas Sillanpää
• 3 Paolo Losi
• 2 Charles McCarthy
• 2 Roy Hyunjin Han
• 2 Scott White
• 2 ibayer
• 1 Brandyn White
• 1 Carlos Scheidegger
74
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• 1 Claire Revillet
• 1 Conrad Lee
• 1 Edouard Duchesnay
• 1 Jan Hendrik Metzen
• 1 Meng Xinfan
• 1 Rob Zinkov
• 1 Shiqiao
• 1 Udi Weinsberg
• 1 Virgile Fritsch
• 1 Xinfan Meng
• 1 Yaroslav Halchenko
• 1 jansoe
• 1 Leon Palafox
1.7.17 Version 0.10
January 11, 2012
Changelog
• Python 2.5 compatibility was dropped; the minimum Python version needed to use scikit-learn is now 2.6.
• Sparse inverse covariance estimation using the graph Lasso, with associated cross-validated estimator, by Gael
Varoquaux
• New Tree module by Brian Holt, Peter Prettenhofer, Satrajit Ghosh and Gilles Louppe. The module comes with
complete documentation and examples.
• Fixed a bug in the RFE module by Gilles Louppe (issue #378).
• Fixed a memory leak in Support Vector Machines module by Brian Holt (issue #367).
• Faster tests by Fabian Pedregosa and others.
• Silhouette Coefﬁcient cluster analysis evaluation metric added as sklearn.metrics.silhouette_score
by Robert Layton.
• Fixed a bug in K-means in the handling of the n_init parameter: the clustering algorithm used to be run
n_init times but the last solution was retained instead of the best solution by Olivier Grisel.
• Minor refactoring in Stochastic Gradient Descent module; consolidated dense and sparse predict methods; En-
hanced test time performance by converting model parameters to fortran-style arrays after ﬁtting (only multi-
class).
• Adjusted Mutual Information metric added as sklearn.metrics.adjusted_mutual_info_score
by Robert Layton.
• Models like SVC/SVR/LinearSVC/LogisticRegression from libsvm/liblinear now support scaling of C regular-
ization parameter by the number of samples by Alexandre Gramfort.
• New Ensemble Methods module by Gilles Louppe and Brian Holt. The module comes with the random forest
algorithm and the extra-trees method, along with documentation and examples.
1.7. Release history
75
scikit-learn user guide, Release 0.18.2
• Novelty and Outlier Detection: outlier and novelty detection, by Virgile Fritsch.
• Kernel Approximation: a transform implementing kernel approximation for fast SGD on non-linear kernels by
Andreas Müller.
• Fixed a bug due to atom swapping in Orthogonal Matching Pursuit (OMP) by Vlad Niculae.
• Sparse coding with a precomputed dictionary by Vlad Niculae.
• Mini Batch K-Means performance improvements by Olivier Grisel.
• K-means support for sparse matrices by Mathieu Blondel.
• Improved documentation for developers and for the sklearn.utils module, by Jake Vanderplas.
• Vectorized 20newsgroups dataset loader (sklearn.datasets.fetch_20newsgroups_vectorized)
by Mathieu Blondel.
• Multiclass and multilabel algorithms by Lars Buitinck.
• Utilities for fast computation of mean and variance for sparse matrices by Mathieu Blondel.
• Make sklearn.preprocessing.scale and sklearn.preprocessing.Scaler work on sparse
matrices by Olivier Grisel
• Feature importances using decision trees and/or forest of trees, by Gilles Louppe.
• Parallel implementation of forests of randomized trees by Gilles Louppe.
• sklearn.cross_validation.ShuffleSplit can subsample the train sets as well as the test sets by
Olivier Grisel.
• Errors in the build of the documentation ﬁxed by Andreas Müller.
API changes summary
Here are the code migration instructions when upgrading from scikit-learn version 0.9:
• Some estimators that may overwrite their inputs to save memory previously had overwrite_ parameters;
these have been replaced with copy_ parameters with exactly the opposite meaning.
This particularly affects some of the estimators in linear_model. The default behavior is still to copy
everything passed in.
• The SVMlight dataset loader sklearn.datasets.load_svmlight_file no longer supports loading
two ﬁles at once; use load_svmlight_files instead. Also, the (unused) buffer_mb parameter is gone.
• Sparse estimators in the Stochastic Gradient Descent module use dense parameter vector coef_ instead of
sparse_coef_. This signiﬁcantly improves test time performance.
• The Covariance estimation module now has a robust estimator of covariance, the Minimum Covariance Deter-
minant estimator.
• Cluster evaluation metrics in metrics.cluster have been refactored but the changes are back-
wards compatible.
They have been moved to the metrics.cluster.supervised, along with
metrics.cluster.unsupervised which contains the Silhouette Coefﬁcient.
• The permutation_test_score function now behaves the same way as cross_val_score (i.e. uses
the mean score across the folds.)
• Cross Validation generators now use integer indices (indices=True) by default instead of boolean masks.
This make it more intuitive to use with sparse matrix data.
76
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• The functions used for sparse coding, sparse_encode and sparse_encode_parallel have been com-
bined into sklearn.decomposition.sparse_encode, and the shapes of the arrays have been trans-
posed for consistency with the matrix factorization setting, as opposed to the regression setting.
• Fixed an off-by-one error in the SVMlight/LibSVM ﬁle format handling;
ﬁles generated using
sklearn.datasets.dump_svmlight_file should be re-generated. (They should continue to work,
but accidentally had one extra column of zeros prepended.)
• BaseDictionaryLearning class replaced by SparseCodingMixin.
• sklearn.utils.extmath.fast_svd has been renamed sklearn.utils.extmath.randomized_svd
and the default oversampling is now ﬁxed to 10 additional random vectors instead of doubling the number of
components to extract. The new behavior follows the reference paper.
People
The following people contributed to scikit-learn since last release:
• 246 Andreas Müller
• 242 Olivier Grisel
• 220 Gilles Louppe
• 183 Brian Holt
• 166 Gael Varoquaux
• 144 Lars Buitinck
• 73 Vlad Niculae
• 65 Peter Prettenhofer
• 64 Fabian Pedregosa
• 60 Robert Layton
• 55 Mathieu Blondel
• 52 Jake Vanderplas
• 44 Noel Dawe
• 38 Alexandre Gramfort
• 24 Virgile Fritsch
• 23 Satrajit Ghosh
• 3 Jan Hendrik Metzen
• 3 Kenneth C. Arnold
• 3 Shiqiao Du
• 3 Tim Sheerman-Chase
• 3 Yaroslav Halchenko
• 2 Bala Subrahmanyam Varanasi
• 2 DraXus
• 2 Michael Eickenberg
• 1 Bogdan Trach
1.7. Release history
77
scikit-learn user guide, Release 0.18.2
• 1 Félix-Antoine Fortin
• 1 Juan Manuel Caicedo Carvajal
• 1 Nelle Varoquaux
• 1 Nicolas Pinto
• 1 Tiziano Zito
• 1 Xinfan Meng
1.7.18 Version 0.9
September 21, 2011
scikit-learn 0.9 was released on September 2011, three months after the 0.8 release and includes the new modules
Manifold learning, The Dirichlet Process as well as several new algorithms and documentation improvements.
This release also includes the dictionary-learning work developed by Vlad Niculae as part of the Google Summer of
Code program.
78
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
Changelog
• New Manifold learning module by Jake Vanderplas and Fabian Pedregosa.
• New Dirichlet Process Gaussian Mixture Model by Alexandre Passos
1.7. Release history
79
scikit-learn user guide, Release 0.18.2
• Nearest Neighbors module refactoring by Jake Vanderplas : general refactoring, support for sparse matrices in
input, speed and documentation improvements. See the next section for a full list of API changes.
• Improvements on the Feature selection module by Gilles Louppe : refactoring of the RFE classes, documenta-
tion rewrite, increased efﬁciency and minor API changes.
• Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA) by Vlad Niculae, Gael Varo-
quaux and Alexandre Gramfort
• Printing an estimator now behaves independently of architectures and Python version thanks to Jean Kossaiﬁ.
• Loader for libsvm/svmlight format by Mathieu Blondel and Lars Buitinck
• Documentation improvements: thumbnails in example gallery by Fabian Pedregosa.
• Important bugﬁxes in Support Vector Machines module (segfaults, bad performance) by Fabian Pedregosa.
• Added Multinomial Naive Bayes and Bernoulli Naive Bayes by Lars Buitinck
• Text feature extraction optimizations by Lars Buitinck
• Chi-Square feature selection (feature_selection.univariate_selection.chi2) by Lars Buit-
inck.
• Sample generators module refactoring by Gilles Louppe
• Multiclass and multilabel algorithms by Mathieu Blondel
• Ball tree rewrite by Jake Vanderplas
• Implementation of DBSCAN algorithm by Robert Layton
• Kmeans predict and transform by Robert Layton
• Preprocessing module refactoring by Olivier Grisel
• Faster mean shift by Conrad Lee
• New Bootstrap, Random permutations cross-validation a.k.a. Shufﬂe & Split and various other improve-
ments in cross validation schemes by Olivier Grisel and Gael Varoquaux
• Adjusted Rand index and V-Measure clustering evaluation metrics by Olivier Grisel
• Added Orthogonal Matching Pursuit by Vlad Niculae
• Added 2D-patch extractor utilities in the Feature extraction module by Vlad Niculae
• Implementation of linear_model.LassoLarsCV (cross-validated Lasso solver using the Lars algorithm)
and linear_model.LassoLarsIC (BIC/AIC model selection in Lars) by Gael Varoquaux and Alexandre
Gramfort
• Scalability improvements to metrics.roc_curve by Olivier Hervieu
• Distance
helper
functions
metrics.pairwise.pairwise_distances
and
metrics.pairwise.pairwise_kernels by Robert Layton
• Mini-Batch K-Means by Nelle Varoquaux and Peter Prettenhofer.
• Downloading datasets from the mldata.org repository utilities by Pietro Berkes.
• The Olivetti faces dataset by David Warde-Farley.
API changes summary
Here are the code migration instructions when upgrading from scikit-learn version 0.8:
80
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• The scikits.learn package was renamed sklearn. There is still a scikits.learn package alias for
backward compatibility.
Third-party projects with a dependency on scikit-learn 0.9+ should upgrade their codebase. For instance, under
Linux / MacOSX just run (make a backup ﬁrst!):
find -name "*.py" | xargs sed -i 's/\bscikits.learn\b/sklearn/g'
• Estimators no longer accept model parameters as fit arguments:
instead all parameters must be only
be passed as constructor arguments or using the now public set_params method inherited from
base.BaseEstimator.
Some estimators can still accept keyword arguments on the fit but this is restricted to data-dependent values
(e.g. a Gram matrix or an afﬁnity matrix that are precomputed from the X data matrix.
• The cross_val package has been renamed to cross_validation although there is also a cross_val
package alias in place for backward compatibility.
Third-party projects with a dependency on scikit-learn 0.9+ should upgrade their codebase. For instance, under
Linux / MacOSX just run (make a backup ﬁrst!):
find -name "*.py" | xargs sed -i 's/\bcross_val\b/cross_validation/g'
• The score_func argument of the sklearn.cross_validation.cross_val_score function is
now expected to accept y_test and y_predicted as only arguments for classiﬁcation and regression tasks
or X_test for unsupervised estimators.
• gamma parameter for support vector machine algorithms is set to 1 / n_features by default, instead of 1
/ n_samples.
• The sklearn.hmm has been marked as orphaned: it will be removed from scikit-learn in version 0.11 unless
someone steps up to contribute documentation, examples and ﬁx lurking numerical stability issues.
• sklearn.neighbors has been made into a submodule.
The two previously available estimators,
NeighborsClassifier and NeighborsRegressor have been marked as deprecated. Their function-
ality has been divided among ﬁve new classes: NearestNeighbors for unsupervised neighbors searches,
KNeighborsClassifier & RadiusNeighborsClassifier for supervised classiﬁcation problems,
and KNeighborsRegressor & RadiusNeighborsRegressor for supervised regression problems.
• sklearn.ball_tree.BallTree has been moved to sklearn.neighbors.BallTree. Using the
former will generate a warning.
• sklearn.linear_model.LARS() and related classes (LassoLARS, LassoLARSCV, etc.) have been re-
named to sklearn.linear_model.Lars().
• All distance metrics and kernels in sklearn.metrics.pairwise now have a Y parameter, which by
default is None. If not given, the result is the distance (or kernel similarity) between each sample in Y. If given,
the result is the pairwise distance (or kernel similarity) between samples in X to Y.
• sklearn.metrics.pairwise.l1_distance is now called manhattan_distance, and by default
returns the pairwise distance. For the component wise distance, set the parameter sum_over_features to
False.
Backward compatibility package aliases and other deprecated classes and functions will be removed in version 0.11.
People
38 people contributed to this release.
• 387 Vlad Niculae
1.7. Release history
81
scikit-learn user guide, Release 0.18.2
• 320 Olivier Grisel
• 192 Lars Buitinck
• 179 Gael Varoquaux
• 168 Fabian Pedregosa (INRIA, Parietal Team)
• 127 Jake Vanderplas
• 120 Mathieu Blondel
• 85 Alexandre Passos
• 67 Alexandre Gramfort
• 57 Peter Prettenhofer
• 56 Gilles Louppe
• 42 Robert Layton
• 38 Nelle Varoquaux
• 32 Jean Kossaiﬁ
• 30 Conrad Lee
• 22 Pietro Berkes
• 18 andy
• 17 David Warde-Farley
• 12 Brian Holt
• 11 Robert
• 8 Amit Aides
• 8 Virgile Fritsch
• 7 Yaroslav Halchenko
• 6 Salvatore Masecchia
• 5 Paolo Losi
• 4 Vincent Schut
• 3 Alexis Metaireau
• 3 Bryan Silverthorn
• 3 Andreas Müller
• 2 Minwoo Jake Lee
• 1 Emmanuelle Gouillart
• 1 Keith Goodman
• 1 Lucas Wiman
• 1 Nicolas Pinto
• 1 Thouis (Ray) Jones
• 1 Tim Sheerman-Chase
82
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
1.7.19 Version 0.8
May 11, 2011
scikit-learn 0.8 was released on May 2011, one month after the ﬁrst “international” scikit-learn coding sprint and is
marked by the inclusion of important modules: Hierarchical clustering, Cross decomposition, Non-negative matrix
factorization (NMF or NNMF), initial support for Python 3 and by important enhancements and bug ﬁxes.
Changelog
Several new modules where introduced during this release:
• New Hierarchical clustering module by Vincent Michel, Bertrand Thirion, Alexandre Gramfort and Gael Varo-
quaux.
• Kernel PCA implementation by Mathieu Blondel
• The Labeled Faces in the Wild face recognition dataset by Olivier Grisel.
• New Cross decomposition module by Edouard Duchesnay.
• Non-negative matrix factorization (NMF or NNMF) module Vlad Niculae
• Implementation of the Oracle Approximating Shrinkage algorithm by Virgile Fritsch in the Covariance estima-
tion module.
Some other modules beneﬁted from signiﬁcant improvements or cleanups.
• Initial support for Python 3: builds and imports cleanly, some modules are usable while others have failing tests
by Fabian Pedregosa.
• decomposition.PCA is now usable from the Pipeline object by Olivier Grisel.
• Guide How to optimize for speed by Olivier Grisel.
• Fixes for memory leaks in libsvm bindings, 64-bit safer BallTree by Lars Buitinck.
• bug and style ﬁxing in K-means algorithm by Jan Schlüter.
• Add attribute converged to Gaussian Mixture Models by Vincent Schut.
• Implemented transform, predict_log_proba in discriminant_analysis.LinearDiscriminantAnalysis
By Mathieu Blondel.
• Refactoring in the Support Vector Machines module and bug ﬁxes by Fabian Pedregosa, Gael Varoquaux and
Amit Aides.
• Refactored SGD module (removed code duplication, better variable naming), added interface for sample weight
by Peter Prettenhofer.
• Wrapped BallTree with Cython by Thouis (Ray) Jones.
• Added function svm.l1_min_c by Paolo Losi.
• Typos, doc style, etc. by Yaroslav Halchenko, Gael Varoquaux, Olivier Grisel, Yann Malet, Nicolas Pinto, Lars
Buitinck and Fabian Pedregosa.
People
People that made this release possible preceded by number of commits:
• 159 Olivier Grisel
1.7. Release history
83
scikit-learn user guide, Release 0.18.2
• 96 Gael Varoquaux
• 96 Vlad Niculae
• 94 Fabian Pedregosa
• 36 Alexandre Gramfort
• 32 Paolo Losi
• 31 Edouard Duchesnay
• 30 Mathieu Blondel
• 25 Peter Prettenhofer
• 22 Nicolas Pinto
• 11 Virgile Fritsch
• 7 Lars Buitinck
• 6 Vincent Michel
• 5 Bertrand Thirion
• 4 Thouis (Ray) Jones
• 4 Vincent Schut
• 3 Jan Schlüter
• 2 Julien Miotte
• 2 Matthieu Perrot
• 2 Yann Malet
• 2 Yaroslav Halchenko
• 1 Amit Aides
• 1 Andreas Müller
• 1 Feth Arezki
• 1 Meng Xinfan
1.7.20 Version 0.7
March 2, 2011
scikit-learn 0.7 was released in March 2011, roughly three months after the 0.6 release. This release is marked by the
speed improvements in existing algorithms like k-Nearest Neighbors and K-Means algorithm and by the inclusion of
an efﬁcient algorithm for computing the Ridge Generalized Cross Validation solution. Unlike the preceding release,
no new modules where added to this release.
Changelog
• Performance improvements for Gaussian Mixture Model sampling [Jan Schlüter].
• Implementation of efﬁcient leave-one-out cross-validated Ridge in linear_model.RidgeCV [Mathieu
Blondel]
84
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• Better handling of collinearity and early stopping in linear_model.lars_path [Alexandre Gramfort and
Fabian Pedregosa].
• Fixes for liblinear ordering of labels and sign of coefﬁcients [Dan Yamins, Paolo Losi, Mathieu Blondel and
Fabian Pedregosa].
• Performance improvements for Nearest Neighbors algorithm in high-dimensional spaces [Fabian Pedregosa].
• Performance improvements for cluster.KMeans [Gael Varoquaux and James Bergstra].
• Sanity checks for SVM-based classes [Mathieu Blondel].
• Refactoring of neighbors.NeighborsClassifier and neighbors.kneighbors_graph: added
different algorithms for the k-Nearest Neighbor Search and implemented a more stable algorithm for ﬁnding
barycenter weights. Also added some developer documentation for this module, see notes_neighbors for more
information [Fabian Pedregosa].
• Documentation improvements: Added pca.RandomizedPCA and linear_model.LogisticRegression
to the class reference. Also added references of matrices used for clustering and other ﬁxes [Gael Varoquaux,
Fabian Pedregosa, Mathieu Blondel, Olivier Grisel, Virgile Fritsch , Emmanuelle Gouillart]
• Binded decision_function in classes that make use of liblinear,
dense and sparse variants,
like
svm.LinearSVC or linear_model.LogisticRegression [Fabian Pedregosa].
• Performance
and
API
improvements
to
metrics.euclidean_distances
and
to
pca.RandomizedPCA [James Bergstra].
• Fix compilation issues under NetBSD [Kamel Ibn Hassen Derouiche]
• Allow input sequences of different lengths in hmm.GaussianHMM [Ron Weiss].
• Fix bug in afﬁnity propagation caused by incorrect indexing [Xinfan Meng]
People
People that made this release possible preceded by number of commits:
• 85 Fabian Pedregosa
• 67 Mathieu Blondel
• 20 Alexandre Gramfort
• 19 James Bergstra
• 14 Dan Yamins
• 13 Olivier Grisel
• 12 Gael Varoquaux
• 4 Edouard Duchesnay
• 4 Ron Weiss
• 2 Satrajit Ghosh
• 2 Vincent Dubourg
• 1 Emmanuelle Gouillart
• 1 Kamel Ibn Hassen Derouiche
• 1 Paolo Losi
• 1 VirgileFritsch
1.7. Release history
85
scikit-learn user guide, Release 0.18.2
• 1 Yaroslav Halchenko
• 1 Xinfan Meng
1.7.21 Version 0.6
December 21, 2010
scikit-learn 0.6 was released on December 2010. It is marked by the inclusion of several new modules and a general
renaming of old ones. It is also marked by the inclusion of new example, including applications to real-world datasets.
Changelog
• New stochastic gradient descent module by Peter Prettenhofer. The module comes with complete documentation
and examples.
• Improved svm module: memory consumption has been reduced by 50%, heuristic to automatically set class
weights, possibility to assign weights to samples (see SVM: Weighted samples for an example).
• New Gaussian Processes module by Vincent Dubourg.
This module also has great documenta-
tion and some very neat examples.
See example_gaussian_process_plot_gp_regression.py or exam-
ple_gaussian_process_plot_gp_probabilistic_classiﬁcation_after_regression.py for a taste of what can be done.
• It is now possible to use liblinear’s Multi-class SVC (option multi_class in svm.LinearSVC)
• New features and performance improvements of text feature extraction.
• Improved sparse matrix support, both in main classes (grid_search.GridSearchCV) as in modules
sklearn.svm.sparse and sklearn.linear_model.sparse.
• Lots of cool new examples and a new section that uses real-world datasets was created. These include: Faces
recognition example using eigenfaces and SVMs, Species distribution modeling, Libsvm GUI, Wikipedia princi-
pal eigenvector and others.
• Faster Least Angle Regression algorithm. It is now 2x faster than the R version on worst case and up to 10x
times faster on some cases.
• Faster
coordinate
descent
algorithm.
In
particular,
the
full
path
version
of
lasso
(linear_model.lasso_path) is more than 200x times faster than before.
• It is now possible to get probability estimates from a linear_model.LogisticRegression model.
• module renaming: the glm module has been renamed to linear_model, the gmm module has been included into
the more general mixture model and the sgd module has been included in linear_model.
• Lots of bug ﬁxes and documentation improvements.
People
People that made this release possible preceded by number of commits:
• 207 Olivier Grisel
• 167 Fabian Pedregosa
• 97 Peter Prettenhofer
• 68 Alexandre Gramfort
• 59 Mathieu Blondel
86
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
• 55 Gael Varoquaux
• 33 Vincent Dubourg
• 21 Ron Weiss
• 9 Bertrand Thirion
• 3 Alexandre Passos
• 3 Anne-Laure Fouque
• 2 Ronan Amicel
• 1 Christian Osendorfer
1.7.22 Version 0.5
October 11, 2010
Changelog
New classes
• Support for sparse matrices in some classiﬁers of modules svm and linear_model (see svm.sparse.SVC,
svm.sparse.SVR,
svm.sparse.LinearSVC,
linear_model.sparse.Lasso,
linear_model.sparse.ElasticNet)
• New pipeline.Pipeline object to compose different estimators.
• Recursive Feature Elimination routines in module Feature selection.
• Addition
of
various
classes
capable
of
cross
validation
in
the
linear_model
module
(linear_model.LassoCV, linear_model.ElasticNetCV, etc.).
• New, more efﬁcient LARS algorithm implementation. The Lasso variant of the algorithm is also implemented.
See linear_model.lars_path, linear_model.Lars and linear_model.LassoLars.
• New Hidden Markov Models module (see classes hmm.GaussianHMM, hmm.MultinomialHMM,
hmm.GMMHMM)
• New module feature_extraction (see class reference)
• New FastICA algorithm in module sklearn.fastica
Documentation
• Improved documentation for many modules, now separating narrative documentation from the class reference.
As an example, see documentation for the SVM module and the complete class reference.
Fixes
• API changes: adhere variable names to PEP-8, give more meaningful names.
• Fixes for svm module to run on a shared memory context (multiprocessing).
• It is again possible to generate latex (and thus PDF) from the sphinx docs.
1.7. Release history
87
scikit-learn user guide, Release 0.18.2
Examples
• new examples using some of the mlcomp datasets: sphx_glr_auto_examples_mlcomp_sparse_document_classif
(since removed) and Classiﬁcation of text documents using sparse features
• Many more examples. See here the full list of examples.
External dependencies
• Joblib is now a dependency of this package, although it is shipped with (sklearn.externals.joblib).
Removed modules
• Module ann (Artiﬁcial Neural Networks) has been removed from the distribution. Users wanting this sort of
algorithms should take a look into pybrain.
Misc
• New sphinx theme for the web page.
Authors
The following is a list of authors for this release, preceded by number of commits:
• 262 Fabian Pedregosa
• 240 Gael Varoquaux
• 149 Alexandre Gramfort
• 116 Olivier Grisel
• 40 Vincent Michel
• 38 Ron Weiss
• 23 Matthieu Perrot
• 10 Bertrand Thirion
• 7 Yaroslav Halchenko
• 9 VirgileFritsch
• 6 Edouard Duchesnay
• 4 Mathieu Blondel
• 1 Ariel Rokem
• 1 Matthieu Brucher
1.7.23 Version 0.4
August 26, 2010
88
Chapter 1. Welcome to scikit-learn
scikit-learn user guide, Release 0.18.2
Changelog
Major changes in this release include:
• Coordinate Descent algorithm (Lasso, ElasticNet) refactoring & speed improvements (roughly 100x times
faster).
• Coordinate Descent Refactoring (and bug ﬁxing) for consistency with R’s package GLMNET.
• New metrics module.
• New GMM module contributed by Ron Weiss.
• Implementation of the LARS algorithm (without Lasso variant for now).
• feature_selection module redesign.
• Migration to GIT as version control system.
• Removal of obsolete attrselect module.
• Rename of private compiled extensions (added underscore).
• Removal of legacy unmaintained code.
• Documentation improvements (both docstring and rst).
• Improvement of the build system to (optionally) link with MKL. Also, provide a lite BLAS implementation in
case no system-wide BLAS is found.
• Lots of new examples.
• Many, many bug ﬁxes ...
Authors
The committer list for this release is the following (preceded by number of commits):
• 143 Fabian Pedregosa
• 35 Alexandre Gramfort
• 34 Olivier Grisel
• 11 Gael Varoquaux
• 5 Yaroslav Halchenko
• 2 Vincent Michel
• 1 Chris Filo Gorgolewski
1.7.24 Earlier versions
Earlier versions included contributions by Fred Mailhot, David Cooke, David Huard, Dave Morrill, Ed Schoﬁeld,
Travis Oliphant, Pearu Peterson.
1.7. Release history
89
scikit-learn user guide, Release 0.18.2
90
Chapter 1. Welcome to scikit-learn
CHAPTER
TWO
SCIKIT-LEARN TUTORIALS
2.1 An introduction to machine learning with scikit-learn
Section contents
In this section, we introduce the machine learning vocabulary that we use throughout scikit-learn and give a simple
learning example.
2.1.1 Machine learning: the problem setting
In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data.
If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), it is
said to have several attributes or features.
We can separate learning problems in a few large categories:
• supervised learning, in which the data comes with additional attributes that we want to predict (Click here to go
to the scikit-learn supervised learning page).This problem can be either:
– classiﬁcation: samples belong to two or more classes and we want to learn from already labeled data how
to predict the class of unlabeled data. An example of classiﬁcation problem would be the handwritten digit
recognition example, in which the aim is to assign each input vector to one of a ﬁnite number of discrete
categories. Another way to think of classiﬁcation is as a discrete (as opposed to continuous) form of
supervised learning where one has a limited number of categories and for each of the n samples provided,
one is to try to label them with the correct category or class.
– regression: if the desired output consists of one or more continuous variables, then the task is called
regression. An example of a regression problem would be the prediction of the length of a salmon as a
function of its age and weight.
• unsupervised learning, in which the training data consists of a set of input vectors x without any corresponding
target values. The goal in such problems may be to discover groups of similar examples within the data, where
it is called clustering, or to determine the distribution of data within the input space, known as density estima-
tion, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of
visualization (Click here to go to the Scikit-Learn unsupervised learning page).
91
scikit-learn user guide, Release 0.18.2
Training set and testing set
Machine learning is about learning some properties of a data set and applying them to new data. This is why a
common practice in machine learning to evaluate an algorithm is to split the data at hand into two sets, one that we
call the training set on which we learn data properties and one that we call the testing set on which we test these
properties.
2.1.2 Loading an example dataset
scikit-learn comes with a few standard datasets, for instance the iris and digits datasets for classiﬁcation and the boston
house prices dataset for regression.
In the following, we start a Python interpreter from our shell and then load the iris and digits datasets. Our
notational convention is that $ denotes the shell prompt while >>> denotes the Python interpreter prompt:
$ python
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> digits = datasets.load_digits()
A dataset is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in
the .data member, which is a n_samples,n_features array. In the case of supervised problem, one or more
response variables are stored in the .target member. More details on the different datasets can be found in the
dedicated section.
For instance, in the case of the digits dataset, digits.data gives access to the features that can be used to classify
the digits samples:
>>> print(digits.data)
[[
0.
0.
5. ...,
0.
0.
0.]
[
0.
0.
0. ...,
10.
0.
0.]
[
0.
0.
0. ...,
16.
9.
0.]
...,
[
0.
0.
1. ...,
6.
0.
0.]
[
0.
0.
2. ...,
12.
0.
0.]
[
0.
0.
10. ...,
12.
1.
0.]]
and digits.target gives the ground truth for the digit dataset, that is the number corresponding to each digit
image that we are trying to learn:
>>> digits.target
array([0, 1, 2, ..., 8, 9, 8])
Shape of the data arrays
The data is always a 2D array, shape (n_samples,n_features), although the original data may have had a
different shape. In the case of the digits, each original sample is an image of shape (8,8) and can be accessed
using:
>>> digits.images[0]
array([[
0.,
0.,
5.,
13.,
9.,
1.,
0.,
0.],
[
0.,
0.,
13.,
15.,
10.,
15.,
5.,
0.],
[
0.,
3.,
15.,
2.,
0.,
11.,
8.,
0.],
[
0.,
4.,
12.,
0.,
0.,
8.,
8.,
0.],
[
0.,
5.,
8.,
0.,
0.,
9.,
8.,
0.],
[
0.,
4.,
11.,
0.,
1.,
12.,
7.,
0.],
[
0.,
2.,
14.,
5.,
10.,
12.,
0.,
0.],
[
0.,
0.,
6.,
13.,
10.,
0.,
0.,
0.]])
92
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
The simple example on this dataset illustrates how starting from the original problem one can shape the data for
consumption in scikit-learn.
Loading from external datasets
To load from an external dataset, please refer to loading external datasets.
2.1.3 Learning and predicting
In the case of the digits dataset, the task is to predict, given an image, which digit it represents. We are given samples
of each of the 10 possible classes (the digits zero through nine) on which we ﬁt an estimator to be able to predict the
classes to which unseen samples belong.
In scikit-learn, an estimator for classiﬁcation is a Python object that implements the methods fit(X,y) and
predict(T).
An example of an estimator is the class sklearn.svm.SVC that implements support vector classiﬁcation. The
constructor of an estimator takes as arguments the parameters of the model, but for the time being, we will consider
the estimator as a black box:
>>> from sklearn import svm
>>> clf = svm.SVC(gamma=0.001, C=100.)
Choosing the parameters of the model
In this example we set the value of gamma manually. It is possible to automatically ﬁnd good values for the
parameters by using tools such as grid search and cross validation.
We call our estimator instance clf, as it is a classiﬁer. It now must be ﬁtted to the model, that is, it must learn from
the model. This is done by passing our training set to the fit method. As a training set, let us use all the images of
our dataset apart from the last one. We select this training set with the [:-1] Python syntax, which produces a new
array that contains all but the last entry of digits.data:
>>> clf.fit(digits.data[:-1], digits.target[:-1])
SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
Now you can predict new values, in particular, we can ask to the classiﬁer what is the digit of our last image in the
digits dataset, which we have not used to train the classiﬁer:
>>> clf.predict(digits.data[-1:])
array([8])
2.1. An introduction to machine learning with scikit-learn
93
scikit-learn user guide, Release 0.18.2
The corresponding image is the following:
As you can see, it is a challenging task: the
images are of poor resolution. Do you agree with the classiﬁer?
A complete example of this classiﬁcation problem is available as an example that you can run and study: Recognizing
hand-written digits.
2.1.4 Model persistence
It is possible to save a model in the scikit by using Python’s built-in persistence model, namely pickle:
>>> from sklearn import svm
>>> from sklearn import datasets
>>> clf = svm.SVC()
>>> iris = datasets.load_iris()
>>> X, y = iris.data, iris.target
>>> clf.fit(X, y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
>>> import pickle
>>> s = pickle.dumps(clf)
>>> clf2 = pickle.loads(s)
>>> clf2.predict(X[0:1])
array([0])
>>> y[0]
0
In the speciﬁc case of the scikit, it may be more interesting to use joblib’s replacement of pickle (joblib.dump &
joblib.load), which is more efﬁcient on big data, but can only pickle to the disk and not to a string:
>>> from sklearn.externals import joblib
>>> joblib.dump(clf, 'filename.pkl')
Later you can load back the pickled model (possibly in another Python process) with:
>>> clf = joblib.load('filename.pkl')
Note: joblib.dump and joblib.load functions also accept ﬁle-like object instead of ﬁlenames. More infor-
mation on data persistence with Joblib is available here.
Note that pickle has some security and maintainability issues. Please refer to section Model persistence for more
detailed information about model persistence with scikit-learn.
94
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
2.1.5 Conventions
scikit-learn estimators follow certain rules to make their behavior more predictive.
Type casting
Unless otherwise speciﬁed, input will be cast to float64:
>>> import numpy as np
>>> from sklearn import random_projection
>>> rng = np.random.RandomState(0)
>>> X = rng.rand(10, 2000)
>>> X = np.array(X, dtype='float32')
>>> X.dtype
dtype('float32')
>>> transformer = random_projection.GaussianRandomProjection()
>>> X_new = transformer.fit_transform(X)
>>> X_new.dtype
dtype('float64')
In this example, X is float32, which is cast to float64 by fit_transform(X).
Regression targets are cast to float64, classiﬁcation targets are maintained:
>>> from sklearn import datasets
>>> from sklearn.svm import SVC
>>> iris = datasets.load_iris()
>>> clf = SVC()
>>> clf.fit(iris.data, iris.target)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
>>> list(clf.predict(iris.data[:3]))
[0, 0, 0]
>>> clf.fit(iris.data, iris.target_names[iris.target])
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
>>> list(clf.predict(iris.data[:3]))
['setosa', 'setosa', 'setosa']
Here, the ﬁrst predict() returns an integer array, since iris.target (an integer array) was used in fit. The
second predict() returns a string array, since iris.target_names was for ﬁtting.
Reﬁtting and updating parameters
Hyper-parameters
of
an
estimator
can
be
updated
after
it
has
been
constructed
via
the
sklearn.pipeline.Pipeline.set_params method.
Calling fit() more than once will overwrite
what was learned by any previous fit():
2.1. An introduction to machine learning with scikit-learn
95
scikit-learn user guide, Release 0.18.2
>>> import numpy as np
>>> from sklearn.svm import SVC
>>> rng = np.random.RandomState(0)
>>> X = rng.rand(100, 10)
>>> y = rng.binomial(1, 0.5, 100)
>>> X_test = rng.rand(5, 10)
>>> clf = SVC()
>>> clf.set_params(kernel='linear').fit(X, y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
>>> clf.predict(X_test)
array([1, 0, 1, 1, 0])
>>> clf.set_params(kernel='rbf').fit(X, y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
>>> clf.predict(X_test)
array([0, 0, 0, 1, 0])
Here, the default kernel rbf is ﬁrst changed to linear after the estimator has been constructed via SVC(), and
changed back to rbf to reﬁt the estimator and to make a second prediction.
Multiclass vs. multilabel ﬁtting
When using multiclass classifiers, the learning and prediction task that is performed is dependent on the
format of the target data ﬁt upon:
>>> from sklearn.svm import SVC
>>> from sklearn.multiclass import OneVsRestClassifier
>>> from sklearn.preprocessing import LabelBinarizer
>>> X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]
>>> y = [0, 0, 1, 1, 2]
>>> classif = OneVsRestClassifier(estimator=SVC(random_state=0))
>>> classif.fit(X, y).predict(X)
array([0, 0, 1, 1, 2])
In the above case, the classiﬁer is ﬁt on a 1d array of multiclass labels and the predict() method therefore provides
corresponding multiclass predictions. It is also possible to ﬁt upon a 2d array of binary label indicators:
>>> y = LabelBinarizer().fit_transform(y)
>>> classif.fit(X, y).predict(X)
array([[1, 0, 0],
[1, 0, 0],
[0, 1, 0],
[0, 0, 0],
[0, 0, 0]])
Here, the classiﬁer is fit() on a 2d binary label representation of y, using the LabelBinarizer. In this case
predict() returns a 2d array representing the corresponding multilabel predictions.
96
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
Note that the fourth and ﬁfth instances returned all zeroes, indicating that they matched none of the three labels fit
upon. With multilabel outputs, it is similarly possible for an instance to be assigned multiple labels:
>> from sklearn.preprocessing import MultiLabelBinarizer
>> y = [[0, 1], [0, 2], [1, 3], [0, 2, 3], [2, 4]]
>> y = preprocessing.MultiLabelBinarizer().fit_transform(y)
>> classif.fit(X, y).predict(X)
array([[1, 1, 0, 0, 0],
[1, 0, 1, 0, 0],
[0, 1, 0, 1, 0],
[1, 0, 1, 1, 0],
[0, 0, 1, 0, 1]])
In this case, the classiﬁer is ﬁt upon instances each assigned multiple labels. The MultiLabelBinarizer is
used to binarize the 2d array of multilabels to fit upon. As a result, predict() returns a 2d array with multiple
predicted labels for each instance.
2.2 A tutorial on statistical-learning for scientiﬁc data processing
Statistical learning
Machine learning is a technique with a growing importance, as the size of the datasets experimental sciences are fac-
ing is rapidly growing. Problems it tackles range from building a prediction function linking different observations,
to classifying observations, or learning the structure in an unlabeled dataset.
This tutorial will explore statistical learning, the use of machine learning techniques with the goal of statistical
inference: drawing conclusions on the data at hand.
Scikit-learn is a Python module integrating classic machine learning algorithms in the tightly-knit world of scientiﬁc
Python packages (NumPy, SciPy, matplotlib).
2.2.1 Statistical learning: the setting and the estimator object in scikit-learn
Datasets
Scikit-learn deals with learning information from one or more datasets that are represented as 2D arrays. They can be
understood as a list of multi-dimensional observations. We say that the ﬁrst axis of these arrays is the samples axis,
while the second is the features axis.
A simple example shipped with the scikit: iris dataset
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> data = iris.data
>>> data.shape
(150, 4)
It is made of 150 observations of irises, each described by 4 features: their sepal and petal length and width, as
detailed in iris.DESCR.
2.2. A tutorial on statistical-learning for scientiﬁc data processing
97
scikit-learn user guide, Release 0.18.2
When the data is not initially in the (n_samples,n_features) shape, it needs to be preprocessed in order to be
used by scikit-learn.
An example of reshaping data would be the digits dataset
The digits dataset is made of 1797 8x8 images of hand-written digits
>>> digits = datasets.load_digits()
>>> digits.images.shape
(1797, 8, 8)
>>> import matplotlib.pyplot as plt
>>> plt.imshow(digits.images[-1], cmap=plt.cm.gray_r)
<matplotlib.image.AxesImage object at ...>
To use this dataset with the scikit, we transform each 8x8 image into a feature vector of length 64
>>> data = digits.images.reshape((digits.images.shape[0], -1))
Estimators objects
Fitting data: the main API implemented by scikit-learn is that of the estimator. An estimator is any object that learns
from data; it may be a classiﬁcation, regression or clustering algorithm or a transformer that extracts/ﬁlters useful
features from raw data.
All estimator objects expose a fit method that takes a dataset (usually a 2-d array):
>>> estimator.fit(data)
Estimator parameters: All the parameters of an estimator can be set when it is instantiated or by modifying the
corresponding attribute:
>>> estimator = Estimator(param1=1, param2=2)
>>> estimator.param1
1
Estimated parameters: When data is ﬁtted with an estimator, parameters are estimated from the data at hand. All the
estimated parameters are attributes of the estimator object ending by an underscore:
>>> estimator.estimated_param_
98
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
2.2.2 Supervised learning: predicting an output variable from high-dimensional ob-
servations
The problem solved in supervised learning
Supervised learning consists in learning the link between two datasets: the observed data X and an external variable
y that we are trying to predict, usually called “target” or “labels”. Most often, y is a 1D array of length n_samples.
All supervised estimators in scikit-learn implement a fit(X,y) method to ﬁt the model and a predict(X)
method that, given unlabeled observations X, returns the predicted labels y.
Vocabulary: classiﬁcation and regression
If the prediction task is to classify the observations in a set of ﬁnite labels, in other words to “name” the objects
observed, the task is said to be a classiﬁcation task. On the other hand, if the goal is to predict a continuous target
variable, it is said to be a regression task.
When doing classiﬁcation in scikit-learn, y is a vector of integers or strings.
Note: See the Introduction to machine learning with scikit-learn Tutorial for a quick run-through on the basic
machine learning vocabulary used within scikit-learn.
Nearest neighbor and the curse of dimensionality
Classifying irises:
The iris dataset is a classiﬁcation task
consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal
length and width:
2.2. A tutorial on statistical-learning for scientiﬁc data processing
99
scikit-learn user guide, Release 0.18.2
>>> import numpy as np
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> iris_X = iris.data
>>> iris_y = iris.target
>>> np.unique(iris_y)
array([0, 1, 2])
k-Nearest neighbors classiﬁer
The simplest possible classiﬁer is the nearest neighbor: given a new observation X_test, ﬁnd in the training set (i.e.
the data used to train the estimator) the observation with the closest feature vector. (Please see the Nearest Neighbors
section of the online Scikit-learn documentation for more information about this type of classiﬁer.)
Training set and testing set
While experimenting with any learning algorithm, it is important not to test the prediction of an estimator on the
data used to ﬁt the estimator as this would not be evaluating the performance of the estimator on new data. This is
why datasets are often split into train and test data.
KNN (k nearest neighbors) classiﬁcation example:
>>> # Split iris data in train and test data
>>> # A random permutation, to split the data randomly
>>> np.random.seed(0)
>>> indices = np.random.permutation(len(iris_X))
>>> iris_X_train = iris_X[indices[:-10]]
>>> iris_y_train = iris_y[indices[:-10]]
>>> iris_X_test
= iris_X[indices[-10:]]
>>> iris_y_test
= iris_y[indices[-10:]]
>>> # Create and fit a nearest-neighbor classifier
>>> from sklearn.neighbors import KNeighborsClassifier
100
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
>>> knn = KNeighborsClassifier()
>>> knn.fit(iris_X_train, iris_y_train)
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
metric_params=None, n_jobs=1, n_neighbors=5, p=2,
weights='uniform')
>>> knn.predict(iris_X_test)
array([1, 2, 1, 0, 0, 0, 2, 1, 2, 0])
>>> iris_y_test
array([1, 1, 1, 0, 0, 0, 2, 1, 2, 0])
The curse of dimensionality
For an estimator to be effective, you need the distance between neighboring points to be less than some value 𝑑, which
depends on the problem. In one dimension, this requires on average 𝑛1/𝑑points. In the context of the above 𝑘-NN
example, if the data is described by just one feature with values ranging from 0 to 1 and with 𝑛training observations,
then new data will be no further away than 1/𝑛. Therefore, the nearest neighbor decision rule will be efﬁcient as soon
as 1/𝑛is small compared to the scale of between-class feature variations.
If the number of features is 𝑝, you now require 𝑛1/𝑑𝑝points. Let’s say that we require 10 points in one dimension:
now 10𝑝points are required in 𝑝dimensions to pave the [0, 1] space. As 𝑝becomes large, the number of training points
required for a good estimator grows exponentially.
For example, if each point is just a single number (8 bytes), then an effective 𝑘-NN estimator in a paltry 𝑝20 di-
mensions would require more training data than the current estimated size of the entire internet (±1000 Exabytes or
so).
This is called the curse of dimensionality and is a core problem that machine learning addresses.
Linear model: from regression to sparsity
Diabetes dataset
The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442
patients, and an indication of disease progression after one year:
>>> diabetes = datasets.load_diabetes()
>>> diabetes_X_train = diabetes.data[:-20]
>>> diabetes_X_test
= diabetes.data[-20:]
>>> diabetes_y_train = diabetes.target[:-20]
>>> diabetes_y_test
= diabetes.target[-20:]
The task at hand is to predict disease progression from physiological variables.
Linear regression
LinearRegression,
in its simplest form,
ﬁts a linear model to the data set by adjusting a set
of parameters in order to make the sum of the squared residuals of the model as small as possible.
2.2. A tutorial on statistical-learning for scientiﬁc data processing
101
scikit-learn user guide, Release 0.18.2
Linear models: 𝑦= 𝑋𝛽+ 𝜖
• 𝑋: data
• 𝑦: target variable
• 𝛽: Coefﬁcients
• 𝜖: Observation noise
>>> from sklearn import linear_model
>>> regr = linear_model.LinearRegression()
>>> regr.fit(diabetes_X_train, diabetes_y_train)
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> print(regr.coef_)
[
0.30349955 -237.63931533
510.53060544
327.73698041 -814.13170937
492.81458798
102.84845219
184.60648906
743.51961675
76.09517222]
>>> # The mean square error
>>> np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)
2004.56760268...
>>> # Explained variance score: 1 is perfect prediction
>>> # and 0 means that there is no linear relationship
>>> # between X and y.
>>> regr.score(diabetes_X_test, diabetes_y_test)
0.5850753022690...
Shrinkage
If
there
are
few
data
points
per
dimension,
noise
in
the
observations
induces
high
variance:
102
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
>>> X = np.c_[ .5, 1].T
>>> y = [.5, 1]
>>> test = np.c_[ 0, 2].T
>>> regr = linear_model.LinearRegression()
>>> import matplotlib.pyplot as plt
>>> plt.figure()
>>> np.random.seed(0)
>>> for _ in range(6):
...
this_X = .1*np.random.normal(size=(2, 1)) + X
...
regr.fit(this_X, y)
...
plt.plot(test, regr.predict(test))
...
plt.scatter(this_X, y, s=3)
A solution in high-dimensional statistical learning is to shrink the regression coefﬁcients to zero:
any
two randomly chosen set of observations are likely to be uncorrelated.
This is called Ridge regression:
>>> regr = linear_model.Ridge(alpha=.1)
>>> plt.figure()
>>> np.random.seed(0)
>>> for _ in range(6):
...
this_X = .1*np.random.normal(size=(2, 1)) + X
...
regr.fit(this_X, y)
...
plt.plot(test, regr.predict(test))
...
plt.scatter(this_X, y, s=3)
This is an example of bias/variance tradeoff: the larger the ridge alpha parameter, the higher the bias and the lower
the variance.
We can choose alpha to minimize left out error, this time using the diabetes dataset rather than our synthetic data:
>>> alphas = np.logspace(-4, -1, 6)
>>> from __future__ import print_function
>>> print([regr.set_params(alpha=alpha
...
).fit(diabetes_X_train, diabetes_y_train,
...
).score(diabetes_X_test, diabetes_y_test) for alpha in alphas])
[0.5851110683883..., 0.5852073015444..., 0.5854677540698..., 0.5855512036503..., 0.
˓→5830717085554..., 0.57058999437...]
2.2. A tutorial on statistical-learning for scientiﬁc data processing
103
scikit-learn user guide, Release 0.18.2
Note: Capturing in the ﬁtted parameters noise that prevents the model to generalize to new data is called overﬁtting.
The bias introduced by the ridge regression is called a regularization.
Sparsity
Fitting only features 1 and 2
Note: A representation of the full diabetes dataset would involve 11 dimensions (10 feature dimensions and one of
the target variable). It is hard to develop an intuition on such representation, but it may be useful to keep in mind that
it would be a fairly empty space.
We can see that, although feature 2 has a strong coefﬁcient on the full model, it conveys little information on y when
considered with feature 1.
To improve the conditioning of the problem (i.e. mitigating the The curse of dimensionality), it would be interesting
to select only the informative features and set non-informative ones, like feature 2 to 0. Ridge regression will decrease
their contribution, but not set them to zero. Another penalization approach, called Lasso (least absolute shrinkage and
selection operator), can set some coefﬁcients to zero. Such methods are called sparse method and sparsity can be
seen as an application of Occam’s razor: prefer simpler models.
>>> regr = linear_model.Lasso()
>>> scores = [regr.set_params(alpha=alpha
...
).fit(diabetes_X_train, diabetes_y_train
...
).score(diabetes_X_test, diabetes_y_test)
...
for alpha in alphas]
>>> best_alpha = alphas[scores.index(max(scores))]
>>> regr.alpha = best_alpha
104
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
>>> regr.fit(diabetes_X_train, diabetes_y_train)
Lasso(alpha=0.025118864315095794, copy_X=True, fit_intercept=True,
max_iter=1000, normalize=False, positive=False, precompute=False,
random_state=None, selection='cyclic', tol=0.0001, warm_start=False)
>>> print(regr.coef_)
[
0.
-212.43764548
517.19478111
313.77959962 -160.8303982
-0.
-187.19554705
69.38229038
508.66011217
71.84239008]
Different algorithms for the same problem
Different algorithms can be used to solve the same mathematical problem. For instance the Lasso object in
scikit-learn solves the lasso regression problem using a coordinate decent method, that is efﬁcient on large datasets.
However, scikit-learn also provides the LassoLars object using the LARS algorthm, which is very efﬁcient for
problems in which the weight vector estimated is very sparse (i.e. problems with very few observations).
Classiﬁcation
For classiﬁcation, as in the labeling iris task, linear regression is not
the right approach as it will give too much weight to data far from the decision frontier. A linear approach is to ﬁt a
sigmoid function or logistic function:
𝑦= sigmoid(𝑋𝛽−offset) + 𝜖=
1
1 + exp(−𝑋𝛽+ offset) + 𝜖
>>> logistic = linear_model.LogisticRegression(C=1e5)
>>> logistic.fit(iris_X_train, iris_y_train)
LogisticRegression(C=100000.0, class_weight=None, dual=False,
fit_intercept=True, intercept_scaling=1, max_iter=100,
multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
2.2. A tutorial on statistical-learning for scientiﬁc data processing
105
scikit-learn user guide, Release 0.18.2
This is known as LogisticRegression.
Multiclass classiﬁcation
If you have several classes to predict, an option often used is to ﬁt one-versus-all classiﬁers and then use a voting
heuristic for the ﬁnal decision.
Shrinkage and sparsity with logistic regression
The C parameter controls the amount of regularization in the LogisticRegression object: a large value
for C results in less regularization.
penalty="l2" gives Shrinkage (i.e.
non-sparse coefﬁcients), while
penalty="l1" gives Sparsity.
Exercise
Try classifying the digits dataset with nearest neighbors and a linear model. Leave out the last 10% and test
prediction performance on these observations.
from sklearn import datasets, neighbors, linear_model
digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target
Solution: ../../auto_examples/exercises/digits_classification_exercise.py
Support vector machines (SVMs)
Linear SVMs
Support Vector Machines belong to the discriminant model family: they try to ﬁnd a combination of samples to build
a plane maximizing the margin between the two classes. Regularization is set by the C parameter: a small value for C
means the margin is calculated using many or all of the observations around the separating line (more regularization);
a large value for C means the margin is calculated on observations close to the separating line (less regularization).
106
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
Unregularized SVM
Regularized SVM (default)
Example:
• Plot different SVM classiﬁers in the iris dataset
SVMs can be used in regression –SVR (Support Vector Regression)–, or in classiﬁcation –SVC (Support Vector Clas-
siﬁcation).
>>> from sklearn import svm
>>> svc = svm.SVC(kernel='linear')
>>> svc.fit(iris_X_train, iris_y_train)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
Warning: Normalizing data
For many estimators, including the SVMs, having datasets with unit standard deviation for each feature is important
to get good prediction.
Using kernels
Classes are not always linearly separable in feature space. The solution is to build a decision function that is not linear
but may be polynomial instead. This is done using the kernel trick that can be seen as creating a decision energy by
positioning kernels on observations:
Linear kernel
Polynomial kernel
>>> svc = svm.SVC(kernel='linear')
>>> svc = svm.SVC(kernel='poly',
...
degree=3)
>>> # degree: polynomial degree
2.2. A tutorial on statistical-learning for scientiﬁc data processing
107
scikit-learn user guide, Release 0.18.2
RBF kernel (Radial Basis Function)
>>> svc = svm.SVC(kernel='rbf')
>>> # gamma: inverse of size of
>>> # radial kernel
Interactive example
See the SVM GUI to download svm_gui.py; add data points of both classes with right and left button, ﬁt the
model and change parameters and data.
Exercise
Try classifying classes 1 and 2 from the iris dataset with SVMs, with the 2 ﬁrst features. Leave out 10% of each
class and test prediction performance on these observations.
Warning: the classes are ordered, do not leave out the last 10%, you would be testing on only one class.
Hint: You can use the decision_function method on a grid to get intuitions.
108
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
iris = datasets.load_iris()
X = iris.data
y = iris.target
X = X[y != 0, :2]
y = y[y != 0]
Solution: ../../auto_examples/exercises/plot_iris_exercise.py
2.2.3 Model selection: choosing estimators and their parameters
Score, and cross-validated scores
As we have seen, every estimator exposes a score method that can judge the quality of the ﬁt (or the prediction) on
new data. Bigger is better.
>>> from sklearn import datasets, svm
>>> digits = datasets.load_digits()
>>> X_digits = digits.data
>>> y_digits = digits.target
>>> svc = svm.SVC(C=1, kernel='linear')
>>> svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])
0.97999999999999998
To get a better measure of prediction accuracy (which we can use as a proxy for goodness of ﬁt of the model), we can
successively split the data in folds that we use for training and testing:
>>> import numpy as np
>>> X_folds = np.array_split(X_digits, 3)
>>> y_folds = np.array_split(y_digits, 3)
>>> scores = list()
>>> for k in range(3):
...
# We use 'list' to copy, in order to 'pop' later on
...
X_train = list(X_folds)
...
X_test
= X_train.pop(k)
...
X_train = np.concatenate(X_train)
...
y_train = list(y_folds)
...
y_test
= y_train.pop(k)
...
y_train = np.concatenate(y_train)
...
scores.append(svc.fit(X_train, y_train).score(X_test, y_test))
>>> print(scores)
[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]
This is called a KFold cross-validation.
Cross-validation generators
Scikit-learn has a collection of classes which can be used to generate lists of train/test indices for popular cross-
validation strategies.
They expose a split method which accepts the input dataset to be split and yields the train/test set indices for each
iteration of the chosen cross-validation strategy.
This example shows an example usage of the split method.
2.2. A tutorial on statistical-learning for scientiﬁc data processing
109
scikit-learn user guide, Release 0.18.2
>>> from sklearn.model_selection import KFold, cross_val_score
>>> X = ["a", "a", "b", "c", "c", "c"]
>>> k_fold = KFold(n_splits=3)
>>> for train_indices, test_indices in k_fold.split(X):
...
print('Train: %s | test: %s' % (train_indices, test_indices))
Train: [2 3 4 5] | test: [0 1]
Train: [0 1 4 5] | test: [2 3]
Train: [0 1 2 3] | test: [4 5]
The cross-validation can then be performed easily:
>>> kfold = KFold(n_splits=3)
>>> [svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])
...
for train, test in k_fold.split(X_digits)]
[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]
The cross-validation score can be directly calculated using the cross_val_score helper. Given an estimator, the
cross-validation object and the input dataset, the cross_val_score splits the data repeatedly into a training and a
testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration
of cross-validation.
By default the estimator’s score method is used to compute the individual scores.
Refer the metrics module to learn more on the available scoring methods.
>>> cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)
array([ 0.93489149,
0.95659432,
0.93989983])
n_jobs=-1 means that the computation will be dispatched on all the CPUs of the computer.
Alternatively, the scoring argument can be provided to specify an alternative scoring method.
>>> cross_val_score(svc, X_digits, y_digits, cv=k_fold,
...
scoring='precision_macro')
array([ 0.93969761,
0.95911415,
0.94041254])
Cross-validation generators
KFold (n_splits, shufﬂe,
random_state)
StratifiedKFold (n_iter,
test_size, train_size,
random_state)
GroupKFold (n_splits, shufﬂe,
random_state)
Splits it into K folds, trains on K-1
and then tests on the left-out.
Same as K-Fold but preserves the
class distribution within each fold.
Ensures that the same group is not
in both testing and training sets.
ShuffleSplit (n_iter,
test_size, train_size,
random_state)
StratifiedShuffleSplit
GroupShuffleSplit
Generates train/test indices
based on random permutation.
Same as shufﬂe split but preserves the
class distribution within each iteration.
Ensures that the same group is not
in both testing and training sets.
LeaveOneGroupOut ()
LeavePGroupsOut (p)
LeaveOneOut ()
Takes a group array to group observations.
Leave P groups out.
Leave one observation out.
LeavePOut (p)
PredefinedSplit
Leave P observations out.
Generates train/test indices based on predeﬁned splits.
110
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
Exercise
On the digits dataset, plot the cross-validation
score of a SVC estimator with an linear kernel as a function of parameter C (use a logarithmic grid of points,
from 1 to 10).
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn import datasets, svm
digits = datasets.load_digits()
X = digits.data
y = digits.target
svc = svm.SVC(kernel='linear')
C_s = np.logspace(-10, 0, 10)
Solution: Cross-validation on Digits Dataset Exercise
Grid-search and cross-validated estimators
Grid-search
scikit-learn provides an object that, given data, computes the score during the ﬁt of an estimator on a parameter grid and
chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction
and exposes an estimator API:
>>> from sklearn.model_selection import GridSearchCV, cross_val_score
>>> Cs = np.logspace(-6, -1, 10)
>>> clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),
...
n_jobs=-1)
>>> clf.fit(X_digits[:1000], y_digits[:1000])
GridSearchCV(cv=None,...
>>> clf.best_score_
0.925...
>>> clf.best_estimator_.C
0.0077...
2.2. A tutorial on statistical-learning for scientiﬁc data processing
111
scikit-learn user guide, Release 0.18.2
>>> # Prediction performance on test set is not as good as on train set
>>> clf.score(X_digits[1000:], y_digits[1000:])
0.943...
By default, the GridSearchCV uses a 3-fold cross-validation. However, if it detects that a classiﬁer is passed, rather
than a regressor, it uses a stratiﬁed 3-fold.
Nested cross-validation
>>> cross_val_score(clf, X_digits, y_digits)
...
array([ 0.938...,
0.963...,
0.944...])
Two cross-validation loops are performed in parallel: one by the GridSearchCV estimator to set gamma and the
other one by cross_val_score to measure the prediction performance of the estimator. The resulting scores
are unbiased estimates of the prediction score on new data.
Warning: You cannot nest objects with parallel computing (n_jobs different than 1).
Cross-validated estimators
Cross-validation to set a parameter can be done more efﬁciently on an algorithm-by-algorithm basis. This is why, for
certain estimators, scikit-learn exposes Cross-validation: evaluating estimator performance estimators that set their
parameter automatically by cross-validation:
>>> from sklearn import linear_model, datasets
>>> lasso = linear_model.LassoCV()
>>> diabetes = datasets.load_diabetes()
>>> X_diabetes = diabetes.data
>>> y_diabetes = diabetes.target
>>> lasso.fit(X_diabetes, y_diabetes)
LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,
max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,
precompute='auto', random_state=None, selection='cyclic', tol=0.0001,
verbose=False)
>>> # The estimator chose automatically its lambda:
>>> lasso.alpha_
0.01229...
These estimators are called similarly to their counterparts, with ‘CV’ appended to their name.
Exercise
On the diabetes dataset, ﬁnd the optimal regularization parameter alpha.
Bonus: How much can you trust the selection of alpha?
from sklearn import datasets
from sklearn.linear_model import LassoCV
from sklearn.linear_model import Lasso
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
diabetes = datasets.load_diabetes()
112
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
Solution: Cross-validation on diabetes Dataset Exercise
2.2.4 Unsupervised learning: seeking representations of the data
Clustering: grouping observations together
The problem solved in clustering
Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label
them: we could try a clustering task: split the observations into well-separated group called clusters.
K-means clustering
Note that there exist a lot of different clustering criteria and associated algorithms. The simplest clustering algorithm
is K-means.
>>> from sklearn import cluster, datasets
>>> iris = datasets.load_iris()
>>> X_iris = iris.data
>>> y_iris = iris.target
>>> k_means = cluster.KMeans(n_clusters=3)
>>> k_means.fit(X_iris)
KMeans(algorithm='auto', copy_x=True, init='k-means++', ...
>>> print(k_means.labels_[::10])
[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]
>>> print(y_iris[::10])
[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]
Warning:
There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of
clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-
learn employs several tricks to mitigate this issue.
Bad initialization
8 clusters
Ground truth
2.2. A tutorial on statistical-learning for scientiﬁc data processing
113
scikit-learn user guide, Release 0.18.2
Don’t over-interpret clustering results
Application example: vector quantization
Clustering in general and KMeans, in particular, can be seen as a way of choosing a small number of exemplars to
compress the information. The problem is sometimes known as vector quantization. For instance, this can be used
to posterize an image:
>>> import scipy as sp
>>> try:
...
face = sp.face(gray=True)
... except AttributeError:
...
from scipy import misc
...
face = misc.face(gray=True)
>>> X = face.reshape((-1, 1)) # We need an (n_sample, n_feature) array
>>> k_means = cluster.KMeans(n_clusters=5, n_init=1)
>>> k_means.fit(X)
KMeans(algorithm='auto', copy_x=True, init='k-means++', ...
>>> values = k_means.cluster_centers_.squeeze()
>>> labels = k_means.labels_
>>> face_compressed = np.choose(labels, values)
>>> face_compressed.shape = face.shape
Raw image
K-means quantization
Equal bins
Image histogram
Hierarchical agglomerative clustering: Ward
A Hierarchical clustering method is a type of cluster analysis that aims to build a hierarchy of clusters. In general, the
various approaches of this technique are either:
• Agglomerative - bottom-up approaches: each observation starts in its own cluster, and clusters are iterativelly
merged in such a way to minimize a linkage criterion. This approach is particularly interesting when the clus-
ters of interest are made of only a few observations. When the number of clusters is large, it is much more
computationally efﬁcient than k-means.
• Divisive - top-down approaches: all observations start in one cluster, which is iteratively split as one moves
down the hierarchy. For estimating large numbers of clusters, this approach is both slow (due to all observations
starting as one cluster, which it splits recursively) and statistically ill-posed.
Connectivity-constrained clustering
With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connec-
tivity graph. Graphs in the scikit are represented by their adjacency matrix. Often, a sparse matrix is used. This
can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when
114
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
clustering an image:
import matplotlib.pyplot as plt
from sklearn.feature_extraction.image import grid_to_graph
from sklearn.cluster import AgglomerativeClustering
from sklearn.utils.testing import SkipTest
from sklearn.utils.fixes import sp_version
if sp_version < (0, 12):
raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "
"thus does not include the scipy.misc.face() image.")
###############################################################################
# Generate data
try:
face = sp.face(gray=True)
except AttributeError:
# Newer versions of scipy have face in misc
from scipy import misc
face = misc.face(gray=True)
# Resize it to 10% of the original size to speed up the processing
face = sp.misc.imresize(face, 0.10) / 255.
Feature agglomeration
We have seen that sparsity could be used to mitigate the curse of dimensionality, i.e an insufﬁcient amount of ob-
servations compared to the number of features. Another approach is to merge together similar features: feature
agglomeration. This approach can be implemented by clustering in the feature direction, in other words clustering
2.2. A tutorial on statistical-learning for scientiﬁc data processing
115
scikit-learn user guide, Release 0.18.2
the transposed data.
>>> digits = datasets.load_digits()
>>> images = digits.images
>>> X = np.reshape(images, (len(images), -1))
>>> connectivity = grid_to_graph(*images[0].shape)
>>> agglo = cluster.FeatureAgglomeration(connectivity=connectivity,
...
n_clusters=32)
>>> agglo.fit(X)
FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',...
>>> X_reduced = agglo.transform(X)
>>> X_approx = agglo.inverse_transform(X_reduced)
>>> images_approx = np.reshape(X_approx, images.shape)
transform and inverse_transform methods
Some estimators expose a transform method, for instance to reduce the dimensionality of the dataset.
Decompositions: from a signal to components and loadings
Components and loadings
If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational
basis: we want to learn loadings L and a set of components C such that X = L C. Different criteria exist to choose
the components
Principal component analysis: PCA
Principal component analysis (PCA) selects the successive components that explain the maximum variance in the
signal.
116
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
The point cloud spanned by the observations above is very ﬂat in one direction: one of the three univariate features
can almost be exactly computed using the other two. PCA ﬁnds the directions in which the data is not ﬂat
When used to transform data, PCA can reduce the dimensionality of the data by projecting on a principal subspace.
>>> # Create a signal with only 2 useful dimensions
>>> x1 = np.random.normal(size=100)
>>> x2 = np.random.normal(size=100)
>>> x3 = x1 + x2
>>> X = np.c_[x1, x2, x3]
>>> from sklearn import decomposition
>>> pca = decomposition.PCA()
>>> pca.fit(X)
PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
svd_solver='auto', tol=0.0, whiten=False)
>>> print(pca.explained_variance_)
[
2.18565811e+00
1.19346747e+00
8.43026679e-32]
>>> # As we can see, only the 2 first components are useful
>>> pca.n_components = 2
>>> X_reduced = pca.fit_transform(X)
>>> X_reduced.shape
(100, 2)
Independent Component Analysis: ICA
Independent component analysis (ICA) selects components so that the distribution of their loadings carries
a maximum amount of independent information.
It is able to recover non-Gaussian independent signals:
2.2. A tutorial on statistical-learning for scientiﬁc data processing
117
scikit-learn user guide, Release 0.18.2
>>> # Generate sample data
>>> time = np.linspace(0, 10, 2000)
>>> s1 = np.sin(2 * time)
# Signal 1 : sinusoidal signal
>>> s2 = np.sign(np.sin(3 * time))
# Signal 2 : square signal
>>> S = np.c_[s1, s2]
>>> S += 0.2 * np.random.normal(size=S.shape)
# Add noise
>>> S /= S.std(axis=0)
# Standardize data
>>> # Mix data
>>> A = np.array([[1, 1], [0.5, 2]])
# Mixing matrix
>>> X = np.dot(S, A.T)
# Generate observations
>>> # Compute ICA
>>> ica = decomposition.FastICA()
>>> S_ = ica.fit_transform(X)
# Get the estimated sources
>>> A_ = ica.mixing_.T
>>> np.allclose(X,
np.dot(S_, A_) + ica.mean_)
True
118
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
2.2.5 Putting it all together
Pipelining
We have seen that some estimators can transform data and that some estimators can predict variables. We can also
create combined estimators:
from sklearn import linear_model, decomposition, datasets
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
logistic = linear_model.LogisticRegression()
pca = decomposition.PCA()
pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])
digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target
###############################################################################
# Plot the PCA spectrum
pca.fit(X_digits)
plt.figure(1, figsize=(4, 3))
plt.clf()
plt.axes([.2, .2, .7, .7])
plt.plot(pca.explained_variance_, linewidth=2)
plt.axis('tight')
plt.xlabel('n_components')
plt.ylabel('explained_variance_')
###############################################################################
# Prediction
n_components = [20, 40, 64]
Cs = np.logspace(-4, 4, 3)
#Parameters of pipelines can be set using ‘__’ separated parameter names:
estimator = GridSearchCV(pipe,
dict(pca__n_components=n_components,
logistic__C=Cs))
estimator.fit(X_digits, y_digits)
plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,
2.2. A tutorial on statistical-learning for scientiﬁc data processing
119
scikit-learn user guide, Release 0.18.2
linestyle=':', label='n_components chosen')
plt.legend(prop=dict(size=12))
Face recognition with eigenfaces
The dataset used in this example is a preprocessed excerpt of the “Labeled Faces in the Wild”, also known as LFW:
http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)
"""
===================================================
Faces recognition example using eigenfaces and SVMs
===================================================
The dataset used in this example is a preprocessed excerpt of the
"Labeled Faces in the Wild", aka LFW_:
http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)
.. _LFW: http://vis-www.cs.umass.edu/lfw/
Expected results for the top 5 most represented people in the dataset:
================== ============ ======= ========== =======
precision
recall
f1-score
support
================== ============ ======= ========== =======
Ariel Sharon
0.67
0.92
0.77
13
Colin Powell
0.75
0.78
0.76
60
Donald Rumsfeld
0.78
0.67
0.72
27
George W Bush
0.86
0.86
0.86
146
Gerhard Schroeder
0.76
0.76
0.76
25
Hugo Chavez
0.67
0.67
0.67
15
Tony Blair
0.81
0.69
0.75
36
avg / total
0.80
0.80
0.80
322
================== ============ ======= ========== =======
"""
from __future__ import print_function
from time import time
import logging
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA
from sklearn.svm import SVC
print(__doc__)
# Display progress logs on stdout
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')
120
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
###############################################################################
# Download the data, if not already on disk and load it as numpy arrays
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
# introspect the images arrays to find the shapes (for plotting)
n_samples, h, w = lfw_people.images.shape
# for machine learning we use the 2 data directly (as relative pixel
# positions info is ignored by this model)
X = lfw_people.data
n_features = X.shape[1]
# the label to predict is the id of the person
y = lfw_people.target
target_names = lfw_people.target_names
n_classes = target_names.shape[0]
print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)
print("n_classes: %d" % n_classes)
###############################################################################
# Split into a training set and a test set using a stratified k fold
# split into a training and testing set
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.25, random_state=42)
###############################################################################
# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled
# dataset): unsupervised feature extraction / dimensionality reduction
n_components = 150
print("Extracting the top %d eigenfaces from %d faces"
% (n_components, X_train.shape[0]))
t0 = time()
pca = PCA(n_components=n_components, svd_solver='randomized',
whiten=True).fit(X_train)
print("done in %0.3fs" % (time() - t0))
eigenfaces = pca.components_.reshape((n_components, h, w))
print("Projecting the input data on the eigenfaces orthonormal basis")
t0 = time()
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print("done in %0.3fs" % (time() - t0))
###############################################################################
# Train a SVM classification model
2.2. A tutorial on statistical-learning for scientiﬁc data processing
121
scikit-learn user guide, Release 0.18.2
print("Fitting the classifier to the training set")
t0 = time()
param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }
clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)
clf = clf.fit(X_train_pca, y_train)
print("done in %0.3fs" % (time() - t0))
print("Best estimator found by grid search:")
print(clf.best_estimator_)
###############################################################################
# Quantitative evaluation of the model quality on the test set
print("Predicting people's names on the test set")
t0 = time()
y_pred = clf.predict(X_test_pca)
print("done in %0.3fs" % (time() - t0))
print(classification_report(y_test, y_pred, target_names=target_names))
print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))
###############################################################################
# Qualitative evaluation of the predictions using matplotlib
def plot_gallery(images, titles, h, w, n_row=3, n_col=4):
"""Helper function to plot a gallery of portraits"""
plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))
plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)
for i in range(n_row * n_col):
plt.subplot(n_row, n_col, i + 1)
plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)
plt.title(titles[i], size=12)
plt.xticks(())
plt.yticks(())
# plot the result of the prediction on a portion of the test set
def title(y_pred, y_test, target_names, i):
pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]
true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]
return 'predicted: %s\ntrue:
%s' % (pred_name, true_name)
prediction_titles = [title(y_pred, y_test, target_names, i)
for i in range(y_pred.shape[0])]
plot_gallery(X_test, prediction_titles, h, w)
# plot the gallery of the most significative eigenfaces
eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
plot_gallery(eigenfaces, eigenface_titles, h, w)
plt.show()
122
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
Prediction
Eigenfaces
Expected results for the top 5 most represented people in the dataset:
precision
recall
f1-score
support
Gerhard_Schroeder
0.91
0.75
0.82
28
Donald_Rumsfeld
0.84
0.82
0.83
33
Tony_Blair
0.65
0.82
0.73
34
Colin_Powell
0.78
0.88
0.83
58
George_W_Bush
0.93
0.86
0.90
129
avg / total
0.86
0.84
0.85
282
Open problem: Stock Market Structure
Can we predict the variation in stock prices for Google over a given time frame?
Learning a graph structure
2.2.6 Finding help
The project mailing list
If you encounter a bug with scikit-learn or something that needs clariﬁcation in the docstring or the online
documentation, please feel free to ask on the Mailing List
Q&A communities with Machine Learning practitioners
Quora.com Quora has a topic for Machine Learning related questions that also features some
interesting discussions: https://www.quora.com/topic/Machine-Learning
Stack Exchange The Stack Exchange family of sites hosts multiple subdomains for Machine
Learning questions.
– _’An excellent free online course for Machine Learning taught by Professor Andrew Ng of Stanford’: https://www.
coursera.org/learn/machine-learning
– _’Another excellent free online course that takes a more general approach to Artiﬁcial Intelligence’: https://www.
udacity.com/course/intro-to-artiﬁcial-intelligence--cs271
2.2. A tutorial on statistical-learning for scientiﬁc data processing
123
scikit-learn user guide, Release 0.18.2
2.3 Working With Text Data
The goal of this guide is to explore some of the main scikit-learn tools on a single practical task: analysing a
collection of text documents (newsgroups posts) on twenty different topics.
In this section we will see how to:
• load the ﬁle contents and the categories
• extract feature vectors suitable for machine learning
• train a linear model to perform categorization
• use a grid search strategy to ﬁnd a good conﬁguration of both the feature extraction components and the classiﬁer
2.3.1 Tutorial setup
To get started with this tutorial, you ﬁrstly must have the scikit-learn and all of its required dependencies installed.
Please refer to the installation instructions page for more information and for per-system instructions.
The source of this tutorial can be found within your scikit-learn folder:
scikit-learn/doc/tutorial/text_analytics/
The tutorial folder, should contain the following folders:
• *.rst files - the source of the tutorial document written with sphinx
• data - folder to put the datasets used during the tutorial
• skeletons - sample incomplete scripts for the exercises
• solutions - solutions of the exercises
You
can
already
copy
the
skeletons
into
a
new
folder
somewhere
on
your
hard-drive
named
sklearn_tut_workspace where you will edit your own ﬁles for the exercises while keeping the original
skeletons intact:
% cp -r skeletons work_directory/sklearn_tut_workspace
Machine Learning algorithms need data.
Go to each $TUTORIAL_HOME/data sub-folder and run the
fetch_data.py script from there (after having read them ﬁrst).
For instance:
% cd $TUTORIAL_HOME/data/languages
% less fetch_data.py
% python fetch_data.py
2.3.2 Loading the 20 newsgroups dataset
The dataset is called “Twenty Newsgroups”. Here is the ofﬁcial description, quoted from the website:
The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned
(nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected
by Ken Lang, probably for his paper “Newsweeder: Learning to ﬁlter netnews,” though he does not explic-
itly mention this collection. The 20 newsgroups collection has become a popular data set for experiments
in text applications of machine learning techniques, such as text classiﬁcation and text clustering.
124
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
In the following we will use the built-in dataset loader for 20 newsgroups from scikit-learn. Alternatively, it is possible
to download the dataset manually from the web-site and use the sklearn.datasets.load_files function by
pointing it to the 20news-bydate-train subfolder of the uncompressed archive folder.
In order to get faster execution times for this ﬁrst example we will work on a partial dataset with only 4 categories out
of the 20 available in the dataset:
>>> categories = ['alt.atheism', 'soc.religion.christian',
...
'comp.graphics', 'sci.med']
We can now load the list of ﬁles matching those categories as follows:
>>> from sklearn.datasets import fetch_20newsgroups
>>> twenty_train = fetch_20newsgroups(subset='train',
...
categories=categories, shuffle=True, random_state=42)
The returned dataset is a scikit-learn “bunch”: a simple holder object with ﬁelds that can be both accessed
as python dict keys or object attributes for convenience, for instance the target_names holds the list of the
requested category names:
>>> twenty_train.target_names
['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']
The ﬁles themselves are loaded in memory in the data attribute. For reference the ﬁlenames are also available:
>>> len(twenty_train.data)
2257
>>> len(twenty_train.filenames)
2257
Let’s print the ﬁrst lines of the ﬁrst loaded ﬁle:
>>> print("\n".join(twenty_train.data[0].split("\n")[:3]))
From: sd345@city.ac.uk (Michael Collier)
Subject: Converting images to HP LaserJet III?
Nntp-Posting-Host: hampton
>>> print(twenty_train.target_names[twenty_train.target[0]])
comp.graphics
Supervised learning algorithms will require a category label for each document in the training set. In this case the cat-
egory is the name of the newsgroup which also happens to be the name of the folder holding the individual documents.
For speed and space efﬁciency reasons scikit-learn loads the target attribute as an array of integers that corre-
sponds to the index of the category name in the target_names list. The category integer id of each sample is stored
in the target attribute:
>>> twenty_train.target[:10]
array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])
It is possible to get back the category names as follows:
>>> for t in twenty_train.target[:10]:
...
print(twenty_train.target_names[t])
...
comp.graphics
comp.graphics
soc.religion.christian
2.3. Working With Text Data
125
scikit-learn user guide, Release 0.18.2
soc.religion.christian
soc.religion.christian
soc.religion.christian
soc.religion.christian
sci.med
sci.med
sci.med
You can notice that the samples have been shufﬂed randomly (with a ﬁxed RNG seed): this is useful if you select only
the ﬁrst samples to quickly train a model and get a ﬁrst idea of the results before re-training on the complete dataset
later.
2.3.3 Extracting features from text ﬁles
In order to perform machine learning on text documents, we ﬁrst need to turn the text content into numerical feature
vectors.
Bags of words
The most intuitive way to do so is the bags of words representation:
1. assign a ﬁxed integer id to each word occurring in any document of the training set (for instance by building a
dictionary from words to integer indices).
2. for each document #i, count the number of occurrences of each word w and store it in X[i,j] as the value of
feature #j where j is the index of word w in the dictionary
The bags of words representation implies that n_features is the number of distinct words in the corpus: this
number is typically larger than 100,000.
If n_samples == 10000, storing X as a numpy array of type ﬂoat32 would require 10000 x 100000 x 4 bytes =
4GB in RAM which is barely manageable on today’s computers.
Fortunately, most values in X will be zeros since for a given document less than a couple thousands of distinct words
will be used. For this reason we say that bags of words are typically high-dimensional sparse datasets. We can save
a lot of memory by only storing the non-zero parts of the feature vectors in memory.
scipy.sparse matrices are data structures that do exactly this, and scikit-learn has built-in support for these
structures.
Tokenizing text with scikit-learn
Text preprocessing, tokenizing and ﬁltering of stopwords are included in a high level component that is able to build a
dictionary of features and transform documents to feature vectors:
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> count_vect = CountVectorizer()
>>> X_train_counts = count_vect.fit_transform(twenty_train.data)
>>> X_train_counts.shape
(2257, 35788)
CountVectorizer supports counts of N-grams of words or consecutive characters. Once ﬁtted, the vectorizer has
built a dictionary of feature indices:
126
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
>>> count_vect.vocabulary_.get(u'algorithm')
4690
The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.
From occurrences to frequencies
Occurrence count is a good start but there is an issue: longer documents will have higher average count values than
shorter documents, even though they might talk about the same topics.
To avoid these potential discrepancies it sufﬁces to divide the number of occurrences of each word in a document by
the total number of words in the document: these new features are called tf for Term Frequencies.
Another reﬁnement on top of tf is to downscale weights for words that occur in many documents in the corpus and are
therefore less informative than those that occur only in a smaller portion of the corpus.
This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.
Both tf and tf–idf can be computed as follows:
>>> from sklearn.feature_extraction.text import TfidfTransformer
>>> tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)
>>> X_train_tf = tf_transformer.transform(X_train_counts)
>>> X_train_tf.shape
(2257, 35788)
In the above example-code, we ﬁrstly use the fit(..)
method to ﬁt our estimator to the data and secondly
the transform(..) method to transform our count-matrix to a tf-idf representation. These two steps can be
combined to achieve the same end result faster by skipping redundant processing. This is done through using the
fit_transform(..) method as shown below, and as mentioned in the note in the previous section:
>>> tfidf_transformer = TfidfTransformer()
>>> X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
>>> X_train_tfidf.shape
(2257, 35788)
2.3.4 Training a classiﬁer
Now that we have our features, we can train a classiﬁer to try to predict the category of a post. Let’s start with a
naïve Bayes classiﬁer, which provides a nice baseline for this task. scikit-learn includes several variants of this
classiﬁer; the one most suitable for word counts is the multinomial variant:
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)
To try to predict the outcome on a new document we need to extract the features using almost the same feature extract-
ing chain as before. The difference is that we call transform instead of fit_transform on the transformers,
since they have already been ﬁt to the training set:
>>> docs_new = ['God is love', 'OpenGL on the GPU is fast']
>>> X_new_counts = count_vect.transform(docs_new)
>>> X_new_tfidf = tfidf_transformer.transform(X_new_counts)
>>> predicted = clf.predict(X_new_tfidf)
>>> for doc, category in zip(docs_new, predicted):
2.3. Working With Text Data
127
scikit-learn user guide, Release 0.18.2
...
print('%r => %s' % (doc, twenty_train.target_names[category]))
...
'God is love' => soc.religion.christian
'OpenGL on the GPU is fast' => comp.graphics
2.3.5 Building a pipeline
In order to make the vectorizer => transformer => classiﬁer easier to work with, scikit-learn provides a
Pipeline class that behaves like a compound classiﬁer:
>>> from sklearn.pipeline import Pipeline
>>> text_clf = Pipeline([('vect', CountVectorizer()),
...
('tfidf', TfidfTransformer()),
...
('clf', MultinomialNB()),
... ])
The names vect, tfidf and clf (classiﬁer) are arbitrary. We shall see their use in the section on grid search, below.
We can now train the model with a single command:
>>> text_clf = text_clf.fit(twenty_train.data, twenty_train.target)
2.3.6 Evaluation of the performance on the test set
Evaluating the predictive accuracy of the model is equally easy:
>>> import numpy as np
>>> twenty_test = fetch_20newsgroups(subset='test',
...
categories=categories, shuffle=True, random_state=42)
>>> docs_test = twenty_test.data
>>> predicted = text_clf.predict(docs_test)
>>> np.mean(predicted == twenty_test.target)
0.834...
I.e., we achieved 83.4% accuracy. Let’s see if we can do better with a linear support vector machine (SVM), which is
widely regarded as one of the best text classiﬁcation algorithms (although it’s also a bit slower than naïve Bayes). We
can change the learner by just plugging a different classiﬁer object into our pipeline:
>>> from sklearn.linear_model import SGDClassifier
>>> text_clf = Pipeline([('vect', CountVectorizer()),
...
('tfidf', TfidfTransformer()),
...
('clf', SGDClassifier(loss='hinge', penalty='l2',
...
alpha=1e-3, n_iter=5, random_
˓→state=42)),
... ])
>>> _ = text_clf.fit(twenty_train.data, twenty_train.target)
>>> predicted = text_clf.predict(docs_test)
>>> np.mean(predicted == twenty_test.target)
0.912...
scikit-learn further provides utilities for more detailed performance analysis of the results:
>>> from sklearn import metrics
>>> print(metrics.classification_report(twenty_test.target, predicted,
...
target_names=twenty_test.target_names))
128
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
...
precision
recall
f1-score
support
alt.atheism
0.95
0.81
0.87
319
comp.graphics
0.88
0.97
0.92
389
sci.med
0.94
0.90
0.92
396
soc.religion.christian
0.90
0.95
0.93
398
avg / total
0.92
0.91
0.91
1502
>>> metrics.confusion_matrix(twenty_test.target, predicted)
array([[258,
11,
15,
35],
[
4, 379,
3,
3],
[
5,
33, 355,
3],
[
5,
10,
4, 379]])
As expected the confusion matrix shows that posts from the newsgroups on atheism and christian are more often
confused for one another than with computer graphics.
2.3.7 Parameter tuning using grid search
We’ve already encountered some parameters such as use_idf in the TfidfTransformer. Classiﬁers tend to have
many parameters as well; e.g., MultinomialNB includes a smoothing parameter alpha and SGDClassifier
has a penalty parameter alpha and conﬁgurable loss and penalty terms in the objective function (see the module
documentation, or use the Python help function, to get a description of these).
Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of
the best parameters on a grid of possible values. We try out all classiﬁers on either words or bigrams, with or without
idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:
>>> from sklearn.model_selection import GridSearchCV
>>> parameters = {'vect__ngram_range': [(1, 1), (1, 2)],
...
'tfidf__use_idf': (True, False),
...
'clf__alpha': (1e-2, 1e-3),
... }
Obviously, such an exhaustive search can be expensive. If we have multiple CPU cores at our disposal, we can tell
the grid searcher to try these eight parameter combinations in parallel with the n_jobs parameter. If we give this
parameter a value of -1, grid search will detect how many cores are installed and uses them all:
>>> gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
The grid search instance behaves like a normal scikit-learn model. Let’s perform the search on a smaller subset
of the training data to speed up the computation:
>>> gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])
The result of calling fit on a GridSearchCV object is a classiﬁer that we can use to predict:
>>> twenty_train.target_names[gs_clf.predict(['God is love'])[0]]
'soc.religion.christian'
The object’s best_score_ and best_params_ attributes store the best mean score and the parameters setting
corresponding to that score:
2.3. Working With Text Data
129
scikit-learn user guide, Release 0.18.2
>>> gs_clf.best_score_
0.900...
>>> for param_name in sorted(parameters.keys()):
...
print("%s: %r" % (param_name, gs_clf.best_params_[param_name]))
...
clf__alpha: 0.001
tfidf__use_idf: True
vect__ngram_range: (1, 1)
A more detailed summary of the search is available at gs_clf.cv_results_.
The cv_results_ parameter can be easily imported into pandas as a DataFrame for further inspection.
Exercises
To do the exercises, copy the content of the ‘skeletons’ folder as a new folder named ‘workspace’:
% cp -r skeletons workspace
You can then edit the content of the workspace without fear of loosing the original exercise instructions.
Then ﬁre an ipython shell and run the work-in-progress script with:
[1] %run workspace/exercise_XX_script.py arg1 arg2 arg3
If an exception is triggered, use %debug to ﬁre-up a post mortem ipdb session.
Reﬁne the implementation and iterate until the exercise is solved.
For each exercise, the skeleton ﬁle provides all the necessary import statements, boilerplate code to load the
data and sample code to evaluate the predictive accurracy of the model.
2.3.8 Exercise 1: Language identiﬁcation
• Write a text classiﬁcation pipeline using a custom preprocessor and CharNGramAnalyzer using data from
Wikipedia articles as training set.
• Evaluate the performance on some held out test set.
ipython command line:
%run workspace/exercise_01_language_train_model.py data/languages/paragraphs/
2.3.9 Exercise 2: Sentiment Analysis on movie reviews
• Write a text classiﬁcation pipeline to classify movie reviews as either positive or negative.
• Find a good set of parameters using grid search.
• Evaluate the performance on a held out test set.
ipython command line:
%run workspace/exercise_02_sentiment.py data/movie_reviews/txt_sentoken/
130
Chapter 2. scikit-learn Tutorials
scikit-learn user guide, Release 0.18.2
2.3.10 Exercise 3: CLI text classiﬁcation utility
Using the results of the previous exercises and the cPickle module of the standard library, write a command line
utility that detects the language of some text provided on stdin and estimate the polarity (positive or negative) if the
text is written in English.
Bonus point if the utility is able to give a conﬁdence level for its predictions.
2.3.11 Where to from here
Here are a few suggestions to help further your scikit-learn intuition upon the completion of this tutorial:
• Try playing around with the analyzer and token normalisation under CountVectorizer
• If you don’t have labels, try using Clustering on your problem.
• If you have multiple labels per document, e.g categories, have a look at the Multiclass and multilabel section
• Try using Truncated SVD for latent semantic analysis.
• Have a look at using Out-of-core Classiﬁcation to learn from data that would not ﬁt into the computer main
memory.
• Have a look at the Hashing Vectorizer as a memory efﬁcient alternative to CountVectorizer.
2.4 Choosing the right estimator
Often the hardest part of solving a machine learning problem can be ﬁnding the right estimator for the job.
Different estimators are better suited for different types of data and different problems.
The ﬂowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which
estimators to try on your data.
Click on any estimator in the chart below to see its documentation.
2.5 External Resources, Videos and Talks
For written tutorials, see the Tutorial section of the documentation.
2.5.1 New to Scientiﬁc Python?
For those that are still new to the scientiﬁc Python ecosystem, we highly recommend the Python Scientiﬁc Lecture
Notes. This will help you ﬁnd your footing a bit and will deﬁnitely improve your scikit-learn experience. A basic
understanding of NumPy arrays is recommended to make the most of scikit-learn.
2.5.2 External Tutorials
There are several online tutorials available which are geared toward speciﬁc subject areas:
• Machine Learning for NeuroImaging in Python
• Machine Learning for Astronomical Data Analysis
2.4. Choosing the right estimator
131
scikit-learn user guide, Release 0.18.2
2.5.3 Videos
• An introduction to scikit-learn Part I and Part II at Scipy 2013 by Gael Varoquaux, Jake Vanderplas and Olivier
Grisel. Notebooks on github.
• Introduction to scikit-learn by Gael Varoquaux at ICML 2010
A three minute video from a very early stage of the scikit, explaining the basic idea and approach we
are following.
• Introduction to statistical learning with scikit-learn by Gael Varoquaux at SciPy 2011
An extensive tutorial, consisting of four sessions of one hour. The tutorial covers the basics of ma-
chine learning, many algorithms and how to apply them using scikit-learn. The material correspond-
ing is now in the scikit-learn documentation section A tutorial on statistical-learning for scientiﬁc
data processing.
• Statistical Learning for Text Classiﬁcation with scikit-learn and NLTK (and slides) by Olivier Grisel at PyCon
2011
Thirty minute introduction to text classiﬁcation. Explains how to use NLTK and scikit-learn to solve
real-world text classiﬁcation tasks and compares against cloud-based solutions.
• Introduction to Interactive Predictive Analytics in Python with scikit-learn by Olivier Grisel at PyCon 2012
3-hours long introduction to prediction tasks using scikit-learn.
• scikit-learn - Machine Learning in Python by Jake Vanderplas at the 2012 PyData workshop at Google
Interactive demonstration of some scikit-learn features. 75 minutes.
• scikit-learn tutorial by Jake Vanderplas at PyData NYC 2012
Presentation using the online tutorial, 45 minutes.
Note: Doctest Mode
The code-examples in the above tutorials are written in a python-console format. If you wish to easily execute these
examples in IPython, use:
%doctest_mode
in the IPython-console. You can then simply copy and paste the examples directly into IPython without having to
worry about removing the >>> manually.
132
Chapter 2. scikit-learn Tutorials
CHAPTER
THREE
USER GUIDE
3.1 Supervised learning
3.1.1 Generalized Linear Models
The following are a set of methods intended for regression in which the target value is expected to be a linear combi-
nation of the input variables. In mathematical notion, if ˆ𝑦is the predicted value.
ˆ𝑦(𝑤, 𝑥) = 𝑤0 + 𝑤1𝑥1 + ... + 𝑤𝑝𝑥𝑝
Across the module, we designate the vector 𝑤= (𝑤1, ..., 𝑤𝑝) as coef_ and 𝑤0 as intercept_.
To perform classiﬁcation with generalized linear models, see Logistic regression.
Ordinary Least Squares
LinearRegression ﬁts a linear model with coefﬁcients 𝑤= (𝑤1, ..., 𝑤𝑝) to minimize the residual sum of squares
between the observed responses in the dataset, and the responses predicted by the linear approximation. Mathemati-
cally it solves a problem of the form:
𝑚𝑖𝑛
𝑤||𝑋𝑤−𝑦||2
2
LinearRegression will take in its fit method arrays X, y and will store the coefﬁcients 𝑤of the linear model
in its coef_ member:
133
scikit-learn user guide, Release 0.18.2
>>> from sklearn import linear_model
>>> reg = linear_model.LinearRegression()
>>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> reg.coef_
array([ 0.5,
0.5])
However, coefﬁcient estimates for Ordinary Least Squares rely on the independence of the model terms. When terms
are correlated and the columns of the design matrix 𝑋have an approximate linear dependence, the design matrix
becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the
observed response, producing a large variance. This situation of multicollinearity can arise, for example, when data
are collected without an experimental design.
Examples:
• Linear Regression Example
Ordinary Least Squares Complexity
This method computes the least squares solution using a singular value decomposition of X. If X is a matrix of size (n,
p) this method has a cost of 𝑂(𝑛𝑝2), assuming that 𝑛≥𝑝.
Ridge Regression
Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of
coefﬁcients. The ridge coefﬁcients minimize a penalized residual sum of squares,
𝑚𝑖𝑛
𝑤||𝑋𝑤−𝑦||2
2 + 𝛼||𝑤||2
2
Here, 𝛼≥0 is a complexity parameter that controls the amount of shrinkage: the larger the value of 𝛼, the greater the
amount of shrinkage and thus the coefﬁcients become more robust to collinearity.
As with other linear models, Ridge will take in its fit method arrays X, y and will store the coefﬁcients 𝑤of the
linear model in its coef_ member:
134
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> from sklearn import linear_model
>>> reg = linear_model.Ridge (alpha = .5)
>>> reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
normalize=False, random_state=None, solver='auto', tol=0.001)
>>> reg.coef_
array([ 0.34545455,
0.34545455])
>>> reg.intercept_
0.13636...
Examples:
• Plot Ridge coefﬁcients as a function of the regularization
• Classiﬁcation of text documents using sparse features
Ridge Complexity
This method has the same order of complexity than an Ordinary Least Squares.
Setting the regularization parameter: generalized Cross-Validation
RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. The object works in
the same way as GridSearchCV except that it defaults to Generalized Cross-Validation (GCV), an efﬁcient form of
leave-one-out cross-validation:
>>> from sklearn import linear_model
>>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
normalize=False)
>>> reg.alpha_
0.1
References
• “Notes on Regularized Least Squares”, Rifkin & Lippert (technical report, course slides).
Lasso
The Lasso is a linear model that estimates sparse coefﬁcients. It is useful in some contexts due to its tendency
to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given
solution is dependent. For this reason, the Lasso and its variants are fundamental to the ﬁeld of compressed sensing.
Under certain conditions, it can recover the exact set of non-zero weights (see Compressive sensing: tomography
reconstruction with L1 prior (Lasso)).
Mathematically, it consists of a linear model trained with ℓ1 prior as regularizer. The objective function to minimize
3.1. Supervised learning
135
scikit-learn user guide, Release 0.18.2
is:
𝑚𝑖𝑛
𝑤
1
2𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠
||𝑋𝑤−𝑦||2
2 + 𝛼||𝑤||1
The lasso estimate thus solves the minimization of the least-squares penalty with 𝛼||𝑤||1 added, where 𝛼is a constant
and ||𝑤||1 is the ℓ1-norm of the parameter vector.
The implementation in the class Lasso uses coordinate descent as the algorithm to ﬁt the coefﬁcients. See Least
Angle Regression for another implementation:
>>> from sklearn import linear_model
>>> reg = linear_model.Lasso(alpha = 0.1)
>>> reg.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, positive=False, precompute=False, random_state=None,
selection='cyclic', tol=0.0001, warm_start=False)
>>> reg.predict([[1, 1]])
array([ 0.8])
Also useful for lower-level tasks is the function lasso_path that computes the coefﬁcients along the full path of
possible values.
Examples:
• Lasso and Elastic Net for Sparse Signals
• Compressive sensing: tomography reconstruction with L1 prior (Lasso)
Note: Feature selection with Lasso
As the Lasso regression yields sparse models, it can thus be used to perform feature selection, as detailed in L1-based
feature selection.
Note: Randomized sparsity
For feature selection or sparse recovery, it may be interesting to use Randomized sparse models.
Setting regularization parameter
The alpha parameter controls the degree of sparsity of the coefﬁcients estimated.
Using cross-validation
scikit-learn exposes objects that set the Lasso alpha parameter by cross-validation: LassoCV and LassoLarsCV.
LassoLarsCV is based on the Least Angle Regression algorithm explained below.
For high-dimensional datasets with many collinear regressors, LassoCV is most often preferable.
However,
LassoLarsCV has the advantage of exploring more relevant values of alpha parameter, and if the number of samples
is very small compared to the number of observations, it is often faster than LassoCV.
136
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Information-criteria based model selection
Alternatively, the estimator LassoLarsIC proposes to use the Akaike information criterion (AIC) and the Bayes
Information criterion (BIC). It is a computationally cheaper alternative to ﬁnd the optimal value of alpha as the regu-
larization path is computed only once instead of k+1 times when using k-fold cross-validation. However, such criteria
needs a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results)
and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when
the problem is badly conditioned (more features than samples).
Examples:
• Lasso model selection: Cross-Validation / AIC / BIC
Multi-task Lasso
The MultiTaskLasso is a linear model that estimates sparse coefﬁcients for multiple regression problems jointly:
y is a 2D array, of shape (n_samples,n_tasks). The constraint is that the selected features are the same for all
the regression problems, also called tasks.
3.1. Supervised learning
137
scikit-learn user guide, Release 0.18.2
The following ﬁgure compares the location of the non-zeros in W obtained with a simple Lasso or a MultiTaskLasso.
The Lasso estimates yields scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.
Fitting a time-series model, imposing that any active feature be active at all times.
Examples:
• Joint feature selection with multi-task Lasso
Mathematically, it consists of a linear model trained with a mixed ℓ1 ℓ2 prior as regularizer. The objective function to
minimize is:
𝑚𝑖𝑛
𝑤
1
2𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠
||𝑋𝑊−𝑌||2
𝐹𝑟𝑜+ 𝛼||𝑊||21
where 𝐹𝑟𝑜indicates the Frobenius norm:
||𝐴||𝐹𝑟𝑜=
√︃∑︁
𝑖𝑗
𝑎2
𝑖𝑗
and ℓ1 ℓ2 reads:
||𝐴||21 =
∑︁
𝑖
√︃∑︁
𝑗
𝑎2
𝑖𝑗
The implementation in the class MultiTaskLasso uses coordinate descent as the algorithm to ﬁt the coefﬁcients.
Elastic Net
ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for
learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization
properties of Ridge. We control the convex combination of L1 and L2 using the l1_ratio parameter.
Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one
of these at random, while elastic-net is likely to pick both.
A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge’s
stability under rotation.
138
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The objective function to minimize is in this case
𝑚𝑖𝑛
𝑤
1
2𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠
||𝑋𝑤−𝑦||2
2 + 𝛼𝜌||𝑤||1 + 𝛼(1 −𝜌)
2
||𝑤||2
2
The class ElasticNetCV can be used to set the parameters alpha (𝛼) and l1_ratio (𝜌) by cross-validation.
Examples:
• Lasso and Elastic Net for Sparse Signals
• Lasso and Elastic Net
Multi-task Elastic Net
The MultiTaskElasticNet is an elastic-net model that estimates sparse coefﬁcients for multiple regression prob-
lems jointly: Y is a 2D array, of shape (n_samples,n_tasks). The constraint is that the selected features are the
same for all the regression problems, also called tasks.
Mathematically, it consists of a linear model trained with a mixed ℓ1 ℓ2 prior and ℓ2 prior as regularizer. The objective
function to minimize is:
𝑚𝑖𝑛
𝑊
1
2𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠
||𝑋𝑊−𝑌||2
𝐹𝑟𝑜+ 𝛼𝜌||𝑊||21 + 𝛼(1 −𝜌)
2
||𝑊||2
𝐹𝑟𝑜
The implementation in the class MultiTaskElasticNet uses coordinate descent as the algorithm to ﬁt the coef-
ﬁcients.
The class MultiTaskElasticNetCV can be used to set the parameters alpha (𝛼) and l1_ratio (𝜌) by cross-
validation.
Least Angle Regression
Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron,
Trevor Hastie, Iain Johnstone and Robert Tibshirani.
The advantages of LARS are:
3.1. Supervised learning
139
scikit-learn user guide, Release 0.18.2
• It is numerically efﬁcient in contexts where p >> n (i.e., when the number of dimensions is signiﬁcantly greater
than the number of points)
• It is computationally just as fast as forward selection and has the same order of complexity as an ordinary least
squares.
• It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune
the model.
• If two variables are almost equally correlated with the response, then their coefﬁcients should increase at ap-
proximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.
• It is easily modiﬁed to produce solutions for other estimators, like the Lasso.
The disadvantages of the LARS method include:
• Because LARS is based upon an iterative reﬁtting of the residuals, it would appear to be especially sensitive to
the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al.
(2004) Annals of Statistics article.
The LARS model can be used using estimator Lars, or its low-level implementation lars_path.
LARS Lasso
LassoLars is a lasso model implemented using the LARS algorithm, and unlike the implementation based on
coordinate_descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefﬁcients.
>>> from sklearn import linear_model
>>> reg = linear_model.LassoLars(alpha=.1)
>>> reg.fit([[0, 0], [1, 1]], [0, 1])
LassoLars(alpha=0.1, copy_X=True, eps=..., fit_intercept=True,
fit_path=True, max_iter=500, normalize=True, positive=False,
precompute='auto', verbose=False)
>>> reg.coef_
array([ 0.717157...,
0.
])
Examples:
140
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• Lasso path using LARS
The Lars algorithm provides the full path of the coefﬁcients along the regularization parameter almost for free, thus a
common operation consist of retrieving the path with function lars_path
Mathematical formulation
The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated
parameters are increased in a direction equiangular to each one’s correlations with the residual.
Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the
L1 norm of the parameter vector. The full coefﬁcients path is stored in the array coef_path_, which has size
(n_features, max_features+1). The ﬁrst column is always zero.
References:
• Original Algorithm is detailed in the paper Least Angle Regression by Hastie et al.
Orthogonal Matching Pursuit (OMP)
OrthogonalMatchingPursuit and orthogonal_mp implements the OMP algorithm for approximating the
ﬁt of a linear model with constraints imposed on the number of non-zero coefﬁcients (ie. the L 0 pseudo-norm).
Being a forward feature selection method like Least Angle Regression, orthogonal matching pursuit can approximate
the optimum solution vector with a ﬁxed number of non-zero elements:
arg min ||𝑦−𝑋𝛾||2
2 subject to ||𝛾||0 ≤𝑛𝑛𝑜𝑛𝑧𝑒𝑟𝑜_𝑐𝑜𝑒𝑓𝑠
Alternatively, orthogonal matching pursuit can target a speciﬁc error instead of a speciﬁc number of non-zero coefﬁ-
cients. This can be expressed as:
arg min ||𝛾||0 subject to ||𝑦−𝑋𝛾||2
2 ≤tol
OMP is based on a greedy algorithm that includes at each step the atom most highly correlated with the current
residual. It is similar to the simpler matching pursuit (MP) method, but better in that at each iteration, the residual is
recomputed using an orthogonal projection on the space of the previously chosen dictionary elements.
Examples:
• Orthogonal Matching Pursuit
References:
• http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf
• Matching pursuits with time-frequency dictionaries, S. G. Mallat, Z. Zhang,
3.1. Supervised learning
141
scikit-learn user guide, Release 0.18.2
Bayesian Regression
Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the
regularization parameter is not set in a hard sense but tuned to the data at hand.
This can be done by introducing uninformative priors over the hyper parameters of the model. The ℓ2 regularization
used in Ridge Regression is equivalent to ﬁnding a maximum a-postiori solution under a Gaussian prior over the
parameters 𝑤with precision 𝜆−1. Instead of setting lambda manually, it is possible to treat it as a random variable to
be estimated from the data.
To obtain a fully probabilistic model, the output 𝑦is assumed to be Gaussian distributed around 𝑋𝑤:
𝑝(𝑦|𝑋, 𝑤, 𝛼) = 𝒩(𝑦|𝑋𝑤, 𝛼)
Alpha is again treated as a random variable that is to be estimated from the data.
The advantages of Bayesian Regression are:
• It adapts to the data at hand.
• It can be used to include regularization parameters in the estimation procedure.
The disadvantages of Bayesian regression include:
• Inference of the model can be time consuming.
References
• A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning
• Original Algorithm is detailed in the book Bayesian learning for neural networks by Radford M. Neal
Bayesian Ridge Regression
BayesianRidge estimates a probabilistic model of the regression problem as described above. The prior for the
parameter 𝑤is given by a spherical Gaussian:
𝑝(𝑤|𝜆) = 𝒩(𝑤|0, 𝜆−1Ip)
The priors over 𝛼and 𝜆are chosen to be gamma distributions, the conjugate prior for the precision of the Gaussian.
The resulting model is called Bayesian Ridge Regression, and is similar to the classical Ridge. The parameters
𝑤, 𝛼and 𝜆are estimated jointly during the ﬁt of the model. The remaining hyperparameters are the parameters of
the gamma priors over 𝛼and 𝜆. These are usually chosen to be non-informative. The parameters are estimated by
maximizing the marginal log likelihood.
By default 𝛼1 = 𝛼2 = 𝜆1 = 𝜆2 = 1.𝑒−6.
Bayesian Ridge Regression is used for regression:
>>> from sklearn import linear_model
>>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]
>>> Y = [0., 1., 2., 3.]
>>> reg = linear_model.BayesianRidge()
>>> reg.fit(X, Y)
BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,
fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,
normalize=False, tol=0.001, verbose=False)
142
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
After being ﬁtted, the model can then be used to predict new values:
>>> reg.predict ([[1, 0.]])
array([ 0.50000013])
The weights 𝑤of the model can be access:
>>> reg.coef_
array([ 0.49999993,
0.49999993])
Due to the Bayesian framework, the weights found are slightly different to the ones found by Ordinary Least Squares.
However, Bayesian Ridge Regression is more robust to ill-posed problem.
Examples:
• Bayesian Ridge Regression
References
• More details can be found in the article Bayesian Interpolation by MacKay, David J. C.
Automatic Relevance Determination - ARD
ARDRegression is very similar to Bayesian Ridge Regression, but can lead to sparser weights 𝑤
1
2.
ARDRegression poses a different prior over 𝑤, by dropping the assumption of the Gaussian being spherical.
Instead, the distribution over 𝑤is assumed to be an axis-parallel, elliptical Gaussian distribution.
This means each weight 𝑤𝑖is drawn from a Gaussian distribution, centered on zero and with a precision 𝜆𝑖:
𝑝(𝑤|𝜆) = 𝒩(𝑤|0, 𝐴−1)
with 𝑑𝑖𝑎𝑔(𝐴) = 𝜆= {𝜆1, ..., 𝜆𝑝}.
1 Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1
2 David Wipf and Srikantan Nagarajan: A new view of automatic relevance determination
3.1. Supervised learning
143
scikit-learn user guide, Release 0.18.2
In contrast to Bayesian Ridge Regression, each coordinate of 𝑤𝑖has its own standard deviation 𝜆𝑖. The prior over all
𝜆𝑖is chosen to be the same gamma distribution given by hyperparameters 𝜆1 and 𝜆2.
ARD is also known in the literature as Sparse Bayesian Learning and Relevance Vector Machine 3 4.
Examples:
• Automatic Relevance Determination Regression (ARD)
References:
Logistic regression
Logistic regression, despite its name, is a linear model for classiﬁcation rather than regression. Logistic regression is
also known in the literature as logit regression, maximum-entropy classiﬁcation (MaxEnt) or the log-linear classiﬁer.
In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.
The implementation of logistic regression in scikit-learn can be accessed from class LogisticRegression. This
implementation can ﬁt binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.
As an optimization problem, binary class L2 penalized logistic regression minimizes the following cost function:
𝑚𝑖𝑛
𝑤,𝑐
1
2𝑤𝑇𝑤+ 𝐶
𝑛
∑︁
𝑖=1
log(exp(−𝑦𝑖(𝑋𝑇
𝑖𝑤+ 𝑐)) + 1).
Similarly, L1 regularized logistic regression solves the following optimization problem
𝑚𝑖𝑛
𝑤,𝑐‖𝑤‖1 + 𝐶
𝑛
∑︁
𝑖=1
log(exp(−𝑦𝑖(𝑋𝑇
𝑖𝑤+ 𝑐)) + 1).
The solvers implemented in the class LogisticRegression are “liblinear”, “newton-cg”, “lbfgs” and “sag”:
3 Michael E. Tipping: Sparse Bayesian Learning and the Relevance Vector Machine
4 Tristan Fletcher: Relevance Vector Machines explained
144
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The solver “liblinear” uses a coordinate descent (CD) algorithm, and relies on the excellent C++ LIBLINEAR library,
which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multino-
mial (multiclass) model; instead, the optimization problem is decomposed in a “one-vs-rest” fashion so separate binary
classiﬁers are trained for all classes. This happens under the hood, so LogisticRegression instances using this
solver behave as multiclass classiﬁers. For L1 penalization sklearn.svm.l1_min_c allows to calculate the lower
bound for C in order to get a non “null” (all feature weights to zero) model.
The “lbfgs”, “sag” and “newton-cg” solvers only support L2 penalization and are found to converge faster for some
high dimensional data. Setting multi_class to “multinomial” with these solvers learns a true multinomial logistic
regression model 5, which means that its probability estimates should be better calibrated than the default “one-
vs-rest” setting. The “lbfgs”, “sag” and “newton-cg”” solvers cannot optimize L1-penalized models, therefore the
“multinomial” setting does not learn sparse models.
The solver “sag” uses a Stochastic Average Gradient descent 6. It is faster than other solvers for large datasets, when
both the number of samples and the number of features are large.
In a nutshell, one may choose the solver with the following rules:
Case
Solver
Small dataset or L1 penalty
“liblinear”
Multinomial loss or large dataset
“lbfgs”, “sag” or “newton-cg”
Very Large dataset
“sag”
For large dataset, you may also consider using SGDClassifier with ‘log’ loss.
Examples:
• L1 Penalty and Sparsity in Logistic Regression
• Path with L1- Logistic Regression
• Plot multinomial and One-vs-Rest Logistic Regression
Differences from liblinear:
There might be a difference in the scores obtained between LogisticRegression with solver=liblinear
or LinearSVC and the external liblinear library directly, when fit_intercept=False and the ﬁt coef_
(or) the data to be predicted are zeroes. This is because for the sample(s) with decision_function zero,
LogisticRegression and LinearSVC predict the negative class, while liblinear predicts the positive class.
Note that a model with fit_intercept=False and having many samples with decision_function zero,
is likely to be a underﬁt, bad model and you are advised to set fit_intercept=True and increase the inter-
cept_scaling.
Note: Feature selection with sparse logistic regression
A logistic regression with L1 penalty yields sparse models, and can thus be used to perform feature selection, as
detailed in L1-based feature selection.
LogisticRegressionCV implements Logistic Regression with builtin cross-validation to ﬁnd out the optimal
C parameter. “newton-cg”, “sag” and “lbfgs” solvers are found to be faster for high-dimensional dense data, due to
warm-starting. For the multiclass case, if multi_class option is set to “ovr”, an optimal C is obtained for each class and
if the multi_class option is set to “multinomial”, an optimal C is obtained by minimizing the cross- entropy loss.
5 Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4
6 Mark Schmidt, Nicolas Le Roux, and Francis Bach: Minimizing Finite Sums with the Stochastic Average Gradient.
3.1. Supervised learning
145
scikit-learn user guide, Release 0.18.2
References:
Stochastic Gradient Descent - SGD
Stochastic gradient descent is a simple yet very efﬁcient approach to ﬁt linear models. It is particularly useful when the
number of samples (and the number of features) is very large. The partial_fit method allows only/out-of-core
learning.
The classes SGDClassifier and SGDRegressor provide functionality to ﬁt linear models for classiﬁca-
tion and regression using different (convex) loss functions and different penalties.
E.g., with loss="log",
SGDClassifier ﬁts a logistic regression model, while with loss="hinge" it ﬁts a linear support vector ma-
chine (SVM).
References
• Stochastic Gradient Descent
Perceptron
The Perceptron is another simple algorithm suitable for large scale learning. By default:
• It does not require a learning rate.
• It is not regularized (penalized).
• It updates its model only on mistakes.
The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the
resulting models are sparser.
Passive Aggressive Algorithms
The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Per-
ceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization
parameter C.
For
classiﬁcation,
PassiveAggressiveClassifier
can
be
used
with
loss='hinge'
(PA-I)
or
loss='squared_hinge' (PA-II). For regression, PassiveAggressiveRegressor can be used with
loss='epsilon_insensitive' (PA-I) or loss='squared_epsilon_insensitive' (PA-II).
References:
• “Online Passive-Aggressive Algorithms” K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer -
JMLR 7 (2006)
Robustness regression: outliers and modeling errors
Robust regression is interested in ﬁtting a regression model in the presence of corrupt data: either outliers, or error in
the model.
146
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Different scenario and useful concepts
There are different things to keep in mind when dealing with data corrupted by outliers:
• Outliers in X or in y?
Outliers in the y direction
Outliers in the X direction
• Fraction of outliers versus amplitude of error
The number of outlying points matters, but also how much they are outliers.
Small outliers
Large outliers
An important notion of robust ﬁtting is that of breakdown point: the fraction of data that can be outlying for the ﬁt to
start missing the inlying data.
Note that in general, robust ﬁtting in high-dimensional setting (large n_features) is very hard. The robust models here
will probably not work in these settings.
Trade-offs: which estimator?
3.1. Supervised learning
147
scikit-learn user guide, Release 0.18.2
Scikit-learn provides 3 robust regression estimators: RANSAC, Theil Sen and HuberRegressor
• HuberRegressor should be faster than RANSAC and Theil Sen unless the number of samples are
very large, i.e n_samples >> n_features. This is because RANSAC and Theil Sen ﬁt on
smaller subsets of the data. However, both Theil Sen and RANSAC are unlikely to be as robust as
HuberRegressor for the default parameters.
• RANSAC is faster than Theil Sen and scales much better with the number of samples
• RANSAC will deal better with large outliers in the y direction (most common situation)
• Theil Sen will cope better with medium-size outliers in the X direction, but this property will
disappear in large dimensional settings.
When in doubt, use RANSAC
RANSAC: RANdom SAmple Consensus
RANSAC (RANdom SAmple Consensus) ﬁts a model from random subsets of inliers from the complete data set.
RANSAC is a non-deterministic algorithm producing only a reasonable result with a certain probability, which is de-
pendent on the number of iterations (see max_trials parameter). It is typically used for linear and non-linear regression
problems and is especially popular in the ﬁelds of photogrammetric computer vision.
The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers,
which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then
estimated only from the determined inliers.
Details of the algorithm
Each iteration performs the following steps:
1. Select min_samples random samples from the original data and check whether the set of data is valid (see
is_data_valid).
2. Fit a model to the random subset (base_estimator.fit) and check whether the estimated model is valid
(see is_model_valid).
148
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3. Classify
all
data
as
inliers
or
outliers
by
calculating
the
residuals
to
the
estimated
model
(base_estimator.predict(X) -y) - all data samples with absolute residuals smaller than the
residual_threshold are considered as inliers.
4. Save ﬁtted model as best model if number of inlier samples is maximal. In case the current estimated model has
the same number of inliers, it is only considered as the best model if it has better score.
These steps are performed either a maximum number of times (max_trials) or until one of the special stop criteria
are met (see stop_n_inliers and stop_score). The ﬁnal model is estimated using all inlier samples (consensus
set) of the previously determined best model.
The is_data_valid and is_model_valid functions allow to identify and reject degenerate combinations of
random sub-samples. If the estimated model is not needed for identifying degenerate cases, is_data_valid should
be used as it is called prior to ﬁtting the model and thus leading to better computational performance.
Examples:
• Robust linear model estimation using RANSAC
• Robust linear estimator ﬁtting
References:
• https://en.wikipedia.org/wiki/RANSAC
• “Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Auto-
mated Cartography” Martin A. Fischler and Robert C. Bolles - SRI International (1981)
• “Performance Evaluation of RANSAC Family” Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)
Theil-Sen estimator: generalized-median-based estimator
The TheilSenRegressor estimator uses a generalization of the median in multiple dimensions. It is thus robust
to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of
the problem. It looses its robustness properties and becomes no better than an ordinary least squares in high dimension.
Examples:
• Theil-Sen Regression
• Robust linear estimator ﬁtting
References:
• https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator
Theoretical considerations
TheilSenRegressor is comparable to the Ordinary Least Squares (OLS) in terms of asymptotic efﬁciency and as
an unbiased estimator. In contrast to OLS, Theil-Sen is a non-parametric method which means it makes no assumption
3.1. Supervised learning
149
scikit-learn user guide, Release 0.18.2
about the underlying distribution of the data. Since Theil-Sen is a median-based estimator, it is more robust against
corrupted data aka outliers. In univariate setting, Theil-Sen has a breakdown point of about 29.3% in case of a simple
linear regression which means that it can tolerate arbitrary corrupted data of up to 29.3%.
The implementation of TheilSenRegressor in scikit-learn follows a generalization to a multivariate linear re-
gression model [#f1]_ using the spatial median which is a generalization of the median to multiple dimensions 8.
In terms of time and space complexity, Theil-Sen scales according to
(︂𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠
𝑛𝑠𝑢𝑏𝑠𝑎𝑚𝑝𝑙𝑒𝑠
)︂
which makes it infeasible to be applied exhaustively to problems with a large number of samples and features. There-
fore, the magnitude of a subpopulation can be chosen to limit the time and space complexity by considering only a
random subset of all possible combinations.
Examples:
• Theil-Sen Regression
References:
Huber Regression
The HuberRegressor is different to Ridge because it applies a linear loss to samples that are classiﬁed as outliers.
A sample is classiﬁed as an inlier if the absolute error of that sample is lesser than a certain threshold. It differs from
TheilSenRegressor and RANSACRegressor because it does not ignore the effect of the outliers but gives a
lesser weight to them.
The loss function that HuberRegressor minimizes is given by
𝑚𝑖𝑛
𝑤,𝜎
𝑛
∑︁
𝑖=1
(︂
𝜎+ 𝐻𝑚
(︂𝑋𝑖𝑤−𝑦𝑖
𝜎
)︂
𝜎
)︂
+ 𝛼||𝑤||2
2
8
20. Kärkkäinen and S. Äyrämö: On Computation of Spatial Median for Robust Data Mining.
150
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
where
𝐻𝑚(𝑧) =
{︃
𝑧2,
if |𝑧| < 𝜖,
2𝜖|𝑧| −𝜖2,
otherwise
It is advised to set the parameter epsilon to 1.35 to achieve 95% statistical efﬁciency.
Notes
The HuberRegressor differs from using SGDRegressor with loss set to huber in the following ways.
• HuberRegressor is scaling invariant. Once epsilon is set, scaling X and y down or up by different values
would produce the same robustness to outliers as before. as compared to SGDRegressor where epsilon
has to be set again when X and y are scaled.
• HuberRegressor should be more efﬁcient to use on data with small number of samples while
SGDRegressor needs a number of passes on the training data to produce the same robustness.
Examples:
• HuberRegressor vs Ridge on dataset with strong outliers
References:
Also, this estimator is different from the R implementation of Robust Regression (http://www.ats.ucla.edu/stat/r/dae/
rreg.htm) because the R implementation does a weighted least squares implementation with weights given to each
sample on the basis of how much the residual is greater than a certain threshold.
Polynomial regression: extending linear models with basis functions
One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This
approach maintains the generally fast performance of linear methods, while allowing them to ﬁt a much wider range
of data.
3.1. Supervised learning
151
scikit-learn user guide, Release 0.18.2
For example, a simple linear regression can be extended by constructing polynomial features from the coefﬁcients.
In the standard linear regression case, you might have a model that looks like this for two-dimensional data:
ˆ𝑦(𝑤, 𝑥) = 𝑤0 + 𝑤1𝑥1 + 𝑤2𝑥2
If we want to ﬁt a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials,
so that the model looks like this:
ˆ𝑦(𝑤, 𝑥) = 𝑤0 + 𝑤1𝑥1 + 𝑤2𝑥2 + 𝑤3𝑥1𝑥2 + 𝑤4𝑥2
1 + 𝑤5𝑥2
2
The (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new variable
𝑧= [𝑥1, 𝑥2, 𝑥1𝑥2, 𝑥2
1, 𝑥2
2]
With this re-labeling of the data, our problem can be written
ˆ𝑦(𝑤, 𝑥) = 𝑤0 + 𝑤1𝑧1 + 𝑤2𝑧2 + 𝑤3𝑧3 + 𝑤4𝑧4 + 𝑤5𝑧5
We see that the resulting polynomial regression is in the same class of linear models we’d considered above (i.e. the
model is linear in 𝑤) and can be solved by the same techniques. By considering linear ﬁts within a higher-dimensional
space built with these basis functions, the model has the ﬂexibility to ﬁt a much broader range of data.
Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:
This ﬁgure is created using the PolynomialFeatures preprocessor. This preprocessor transforms an input data
matrix into a new data matrix of a given degree. It can be used as follows:
>>> from sklearn.preprocessing import PolynomialFeatures
>>> import numpy as np
>>> X = np.arange(6).reshape(3, 2)
>>> X
array([[0, 1],
[2, 3],
[4, 5]])
>>> poly = PolynomialFeatures(degree=2)
>>> poly.fit_transform(X)
array([[
1.,
0.,
1.,
0.,
0.,
1.],
[
1.,
2.,
3.,
4.,
6.,
9.],
[
1.,
4.,
5.,
16.,
20.,
25.]])
152
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The features of X have been transformed from [𝑥1, 𝑥2] to [1, 𝑥1, 𝑥2, 𝑥2
1, 𝑥1𝑥2, 𝑥2
2], and can now be used within any
linear model.
This sort of preprocessing can be streamlined with the Pipeline tools. A single object representing a simple polynomial
regression can be created and used as follows:
>>> from sklearn.preprocessing import PolynomialFeatures
>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.pipeline import Pipeline
>>> import numpy as np
>>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),
...
('linear', LinearRegression(fit_intercept=False))])
>>> # fit to an order-3 polynomial data
>>> x = np.arange(5)
>>> y = 3 - 2 * x + x ** 2 - x ** 3
>>> model = model.fit(x[:, np.newaxis], y)
>>> model.named_steps['linear'].coef_
array([ 3., -2.,
1., -1.])
The linear model trained on polynomial features is able to exactly recover the input polynomial coefﬁcients.
In some cases it’s not necessary to include higher powers of any single feature, but only the so-called interaction
features that multiply together at most 𝑑distinct features. These can be gotten from PolynomialFeatures with
the setting interaction_only=True.
For example, when dealing with boolean features, 𝑥𝑛
𝑖= 𝑥𝑖for all 𝑛and is therefore useless; but 𝑥𝑖𝑥𝑗represents the
conjunction of two booleans. This way, we can solve the XOR problem with a linear classiﬁer:
>>> from sklearn.linear_model import Perceptron
>>> from sklearn.preprocessing import PolynomialFeatures
>>> import numpy as np
>>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
>>> y = X[:, 0] ^ X[:, 1]
>>> y
array([0, 1, 1, 0])
>>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)
>>> X
array([[1, 0, 0, 0],
[1, 0, 1, 0],
[1, 1, 0, 0],
[1, 1, 1, 1]])
>>> clf = Perceptron(fit_intercept=False, n_iter=10, shuffle=False).fit(X, y)
And the classiﬁer “predictions” are perfect:
>>> clf.predict(X)
array([0, 1, 1, 0])
>>> clf.score(X, y)
1.0
3.1.2 Linear and Quadratic Discriminant Analysis
Linear
Discriminant
Analysis
(discriminant_analysis.LinearDiscriminantAnalysis)
and
Quadratic
Discriminant
Analysis
(discriminant_analysis.QuadraticDiscriminantAnalysis)
are two classic classiﬁers, with, as their names suggest, a linear and a quadratic decision surface, respectively.
These classiﬁers are attractive because they have closed-form solutions that can be easily computed, are inherently
multiclass, have proven to work well in practice and have no hyperparameters to tune.
3.1. Supervised learning
153
scikit-learn user guide, Release 0.18.2
The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The
bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Dis-
criminant Analysis can learn quadratic boundaries and is therefore more ﬂexible.
Examples:
Linear and Quadratic Discriminant Analysis with conﬁdence ellipsoid: Comparison of LDA and QDA on synthetic
data.
Dimensionality reduction using Linear Discriminant Analysis
discriminant_analysis.LinearDiscriminantAnalysis can be used to perform supervised dimen-
sionality reduction, by projecting the input data to a linear subspace consisting of the directions which maximize
the separation between classes (in a precise sense discussed in the mathematics section below). The dimension of the
output is necessarily less than the number of classes, so this is a in general a rather strong dimensionality reduction,
and only makes senses in a multiclass setting.
This
is
implemented
in
discriminant_analysis.LinearDiscriminantAnalysis.transform.
The
desired
dimensionality
can
be
set
using
the
n_components
constructor
parameter.
This
pa-
rameter
has
no
inﬂuence
on
discriminant_analysis.LinearDiscriminantAnalysis.fit
or
discriminant_analysis.LinearDiscriminantAnalysis.predict.
Examples:
Comparison of LDA and PCA 2D projection of Iris dataset: Comparison of LDA and PCA for dimensionality
reduction of the Iris dataset
154
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Mathematical formulation of the LDA and QDA classiﬁers
Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution
of the data 𝑃(𝑋|𝑦= 𝑘) for each class 𝑘. Predictions can then be obtained by using Bayes’ rule:
𝑃(𝑦= 𝑘|𝑋) = 𝑃(𝑋|𝑦= 𝑘)𝑃(𝑦= 𝑘)
𝑃(𝑋)
=
𝑃(𝑋|𝑦= 𝑘)𝑃(𝑦= 𝑘)
∑︀
𝑙𝑃(𝑋|𝑦= 𝑙) · 𝑃(𝑦= 𝑙)
and we select the class 𝑘which maximizes this conditional probability.
More speciﬁcally, for linear and quadratic discriminant analysis, 𝑃(𝑋|𝑦) is modelled as a multivariate Gaussian
distribution with density:
𝑝(𝑋|𝑦= 𝑘) =
1
(2𝜋)𝑛|Σ𝑘|1/2 exp
(︂
−1
2(𝑋−𝜇𝑘)𝑡Σ−1
𝑘(𝑋−𝜇𝑘)
)︂
To use this model as a classiﬁer, we just need to estimate from the training data the class priors 𝑃(𝑦= 𝑘) (by the
proportion of instances of class 𝑘), the class means 𝜇𝑘(by the empirical sample class means) and the covariance
matrices (either by the empirical sample class covariance matrices, or by a regularized estimator: see the section on
shrinkage below).
In the case of LDA, the Gaussians for each class are assumed to share the same covariance matrix: Σ𝑘= Σ for all
𝑘. This leads to linear decision surfaces between, as can be seen by comparing the log-probability ratios log[𝑃(𝑦=
𝑘|𝑋)/𝑃(𝑦= 𝑙|𝑋)]:
log
(︂𝑃(𝑦= 𝑘|𝑋)
𝑃(𝑦= 𝑙|𝑋)
)︂
= 0 ⇔(𝜇𝑘−𝜇𝑙)Σ−1𝑋= 1
2(𝜇𝑡
𝑘Σ−1𝜇𝑘−𝜇𝑡
𝑙Σ−1𝜇𝑙)
In the case of QDA, there are no assumptions on the covariance matrices Σ𝑘of the Gaussians, leading to quadratic
decision surfaces. See 3 for more details.
Note: Relation with Gaussian Naive Bayes
If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be con-
ditionally independent in each class, and the resulting classiﬁer is equivalent to the Gaussian Naive Bayes classiﬁer
naive_bayes.GaussianNB.
Mathematical formulation of LDA dimensionality reduction
To understand the use of LDA in dimensionality reduction, it is useful to start with a geometric reformulation of the
LDA classiﬁcation rule explained above. We write 𝐾for the total number of target classes. Since in LDA we assume
that all classes have the same estimated covariance Σ, we can rescale the data so that this covariance is the identity:
𝑋* = 𝐷−1/2𝑈𝑡𝑋with Σ = 𝑈𝐷𝑈𝑡
Then one can show that to classify a data point after scaling is equivalent to ﬁnding the estimated class mean 𝜇*
𝑘which
is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the 𝐾−1
afﬁne subspace 𝐻𝐾generated by all the 𝜇*
𝑘for all classes. This shows that, implicit in the LDA classiﬁer, there is a
dimensionality reduction by linear projection onto a 𝐾−1 dimensional space.
We can reduce the dimension even more, to a chosen 𝐿, by projecting onto the linear subspace 𝐻𝐿
which maximize the variance of the 𝜇*
𝑘after projection (in effect, we are doing a form of PCA for
the transformed class means 𝜇*
𝑘).
This 𝐿corresponds to the n_components parameter used in the
discriminant_analysis.LinearDiscriminantAnalysis.transform method. See 3 for more de-
tails.
3 “The Elements of Statistical Learning”, Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008.
3.1. Supervised learning
155
scikit-learn user guide, Release 0.18.2
Shrinkage
Shrinkage is a tool to improve estimation of covariance matrices in situations where the number of train-
ing samples is small compared to the number of features.
In this scenario, the empirical sample co-
variance is a poor estimator.
Shrinkage LDA can be used by setting the shrinkage parameter of the
discriminant_analysis.LinearDiscriminantAnalysis class to ‘auto’.
This automatically deter-
mines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf 4.
Note that currently shrinkage only works when setting the solver parameter to ‘lsqr’ or ‘eigen’.
The shrinkage parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to
no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete
shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix).
Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.
Estimation algorithms
The default solver is ‘svd’. It can perform both classiﬁcation and transform, and it does not rely on the calculation
of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the
‘svd’ solver cannot be used with shrinkage.
The ‘lsqr’ solver is an efﬁcient algorithm that only works for classiﬁcation. It supports shrinkage.
The ‘eigen’ solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used
for both classiﬁcation and transform, and it supports shrinkage. However, the ‘eigen’ solver needs to compute the
covariance matrix, so it might not be suitable for situations with a high number of features.
Examples:
4 Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.
156
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Normal and Shrinkage Linear Discriminant Analysis for classiﬁcation: Comparison of LDA classiﬁers with and
without shrinkage.
References:
3.1.3 Kernel ridge regression
Kernel ridge regression (KRR) [M2012] combines Ridge Regression (linear least squares with l2-norm regularization)
with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For
non-linear kernels, this corresponds to a non-linear function in the original space.
The form of the model learned by KernelRidge is identical to support vector regression (SVR). However, different
loss functions are used: KRR uses squared error loss while support vector regression uses 𝜖-insensitive loss, both
combined with l2 regularization. In contrast to SVR, ﬁtting KernelRidge can be done in closed-form and is typically
faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which
learns a sparse model for 𝜖> 0, at prediction-time.
The following ﬁgure compares KernelRidge and SVR on an artiﬁcial dataset, which consists of a sinusoidal target
function and strong noise added to every ﬁfth datapoint. The learned model of KernelRidge and SVR is plotted,
where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The
learned functions are very similar; however, ﬁtting KernelRidge is approx. seven times faster than ﬁtting SVR
(both with grid-search). However, prediction of 100000 target values is more than three times faster with SVR since it
has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.
The next ﬁgure compares the time for ﬁtting and prediction of KernelRidge and SVR for different sizes of the
training set. Fitting KernelRidge is faster than SVR for medium-sized training sets (less than 1000 samples);
however, for larger training sets SVR scales better. With regard to prediction time, SVR is faster than KernelRidge
for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the
prediction time depends on the parameters 𝜖and 𝐶of the SVR; 𝜖= 0 would correspond to a dense model.
References:
3.1.4 Support Vector Machines
Support vector machines (SVMs) are a set of supervised learning methods used for classiﬁcation, regression and
outliers detection.
The advantages of support vector machines are:
• Effective in high dimensional spaces.
• Still effective in cases where number of dimensions is greater than the number of samples.
• Uses a subset of training points in the decision function (called support vectors), so it is also memory efﬁcient.
• Versatile: different Kernel functions can be speciﬁed for the decision function. Common kernels are provided,
but it is also possible to specify custom kernels.
The disadvantages of support vector machines include:
• If the number of features is much greater than the number of samples, the method is likely to give poor perfor-
mances.
3.1. Supervised learning
157
scikit-learn user guide, Release 0.18.2
158
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
159
scikit-learn user guide, Release 0.18.2
• SVMs do not directly provide probability estimates, these are calculated using an expensive ﬁve-fold cross-
validation (see Scores and probabilities, below).
The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by
numpy.asarray) and sparse (any scipy.sparse) sample vectors as input. However, to use an SVM to make pre-
dictions for sparse data, it must have been ﬁt on such data. For optimal performance, use C-ordered numpy.ndarray
(dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.
Classiﬁcation
SVC, NuSVC and LinearSVC are classes capable of performing multi-class classiﬁcation on a dataset.
SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical
formulations (see section Mathematical formulation). On the other hand, LinearSVC is another implementation
of Support Vector Classiﬁcation for the case of a linear kernel. Note that LinearSVC does not accept keyword
kernel, as this is assumed to be linear. It also lacks some of the members of SVC and NuSVC, like support_.
As other classiﬁers,
SVC, NuSVC
and LinearSVC
take as input two arrays:
an array X of size
[n_samples,n_features] holding the training samples, and an array y of class labels (strings or integers),
size [n_samples]:
>>> from sklearn import svm
>>> X = [[0, 0], [1, 1]]
>>> y = [0, 1]
160
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> clf = svm.SVC()
>>> clf.fit(X, y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
After being ﬁtted, the model can then be used to predict new values:
>>> clf.predict([[2., 2.]])
array([1])
SVMs decision function depends on some subset of the training data, called the support vectors. Some properties of
these support vectors can be found in members support_vectors_, support_ and n_support:
>>> # get support vectors
>>> clf.support_vectors_
array([[ 0.,
0.],
[ 1.,
1.]])
>>> # get indices of support vectors
>>> clf.support_
array([0, 1]...)
>>> # get number of support vectors for each class
>>> clf.n_support_
array([1, 1]...)
Multi-class classiﬁcation
SVC and NuSVC implement the “one-against-one” approach (Knerr et al., 1990) for multi- class classiﬁca-
tion.
If n_class is the number of classes, then n_class * (n_class -1) / 2 classiﬁers are con-
structed and each one trains data from two classes.
To provide a consistent interface with other classiﬁers, the
decision_function_shape option allows to aggregate the results of the “one-against-one” classiﬁers to a deci-
sion function of shape (n_samples,n_classes):
>>> X = [[0], [1], [2], [3]]
>>> Y = [0, 1, 2, 3]
>>> clf = svm.SVC(decision_function_shape='ovo')
>>> clf.fit(X, Y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
>>> dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes: 4*3/2 = 6
6
>>> clf.decision_function_shape = "ovr"
>>> dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes
4
On the other hand, LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training n_class models. If
there are only two classes, only one model is trained:
>>> lin_clf = svm.LinearSVC()
>>> lin_clf.fit(X, Y)
3.1. Supervised learning
161
scikit-learn user guide, Release 0.18.2
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
intercept_scaling=1, loss='squared_hinge', max_iter=1000,
multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
verbose=0)
>>> dec = lin_clf.decision_function([[1]])
>>> dec.shape[1]
4
See Mathematical formulation for a complete description of the decision function.
Note that the LinearSVC also implements an alternative multi-class strategy, the so-called multi-class SVM formu-
lated by Crammer and Singer, by using the option multi_class='crammer_singer'. This method is consis-
tent, which is not true for one-vs-rest classiﬁcation. In practice, one-vs-rest classiﬁcation is usually preferred, since
the results are mostly similar, but the runtime is signiﬁcantly less.
For “one-vs-rest” LinearSVC the attributes coef_ and intercept_ have the shape [n_class,n_features]
and [n_class] respectively. Each row of the coefﬁcients corresponds to one of the n_class many “one-vs-rest”
classiﬁers and similar for the intercepts, in the order of the “one” class.
In the case of “one-vs-one” SVC, the layout of the attributes is a little more involved. In the case of having a linear
kernel, The layout of coef_ and intercept_ is similar to the one described for LinearSVC described above,
except that the shape of coef_ is [n_class * (n_class -1) / 2,n_features], corresponding to as
many binary classiﬁers. The order for classes 0 to n is “0 vs 1”, “0 vs 2” , ... “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . . .
“n-1 vs n”.
The shape of dual_coef_ is [n_class-1,n_SV] with a somewhat hard to grasp layout. The columns corre-
spond to the support vectors involved in any of the n_class * (n_class -1) / 2 “one-vs-one” classiﬁers.
Each of the support vectors is used in n_class -1 classiﬁers. The n_class -1 entries in each row correspond
to the dual coefﬁcients for these classiﬁers.
This might be made more clear by an example:
Consider a three class problem with class 0 having three support vectors 𝑣0
0, 𝑣1
0, 𝑣2
0 and class 1 and 2 having two
support vectors 𝑣0
1, 𝑣1
1 and 𝑣0
2, 𝑣1
2 respectively. For each support vector 𝑣𝑗
𝑖, there are two dual coefﬁcients. Let’s call
the coefﬁcient of support vector 𝑣𝑗
𝑖in the classiﬁer between classes 𝑖and 𝑘𝛼𝑗
𝑖,𝑘. Then dual_coef_ looks like this:
𝛼0
0,1
𝛼0
0,2
Coefﬁcients for SVs of class 0
𝛼1
0,1
𝛼1
0,2
𝛼2
0,1
𝛼2
0,2
𝛼0
1,0
𝛼0
1,2
Coefﬁcients for SVs of class 1
𝛼1
1,0
𝛼1
1,2
𝛼0
2,0
𝛼0
2,1
Coefﬁcients for SVs of class 2
𝛼1
2,0
𝛼1
2,1
Scores and probabilities
The SVC method decision_function gives per-class scores for each sample (or a single score per sample in the
binary case). When the constructor option probability is set to True, class membership probability estimates
(from the methods predict_proba and predict_log_proba) are enabled. In the binary case, the probabilities
are calibrated using Platt scaling: logistic regression on the SVM’s scores, ﬁt by an additional cross-validation on the
training data. In the multiclass case, this is extended as per Wu et al. (2004).
Needless to say, the cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition,
the probability estimates may be inconsistent with the scores, in the sense that the “argmax” of the scores may not be
the argmax of the probabilities. (E.g., in binary classiﬁcation, a sample may be labeled by predict as belonging
162
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
to a class that has probability <½ according to predict_proba.) Platt’s method is also known to have theoret-
ical issues. If conﬁdence scores are required, but these do not have to be probabilities, then it is advisable to set
probability=False and use decision_function instead of predict_proba.
References:
• Wu, Lin and Weng, “Probability estimates for multi-class classiﬁcation by pairwise coupling”, JMLR 5:975-
1005, 2004.
Unbalanced problems
In problems where it is desired to give more importance to certain classes or certain individual samples keywords
class_weight and sample_weight can be used.
SVC (but not NuSVC) implement a keyword class_weight in the fit method. It’s a dictionary of the form
{class_label :
value}, where value is a ﬂoating point number > 0 that sets the parameter C of class
class_label to C * value.
SVC, NuSVC, SVR, NuSVR and OneClassSVM implement also weights for individual samples in method fit
through keyword sample_weight. Similar to class_weight, these set the parameter C for the i-th example to
C * sample_weight[i].
Examples:
• Plot different SVM classiﬁers in the iris dataset,
• SVM: Maximum margin separating hyperplane,
3.1. Supervised learning
163
scikit-learn user guide, Release 0.18.2
• SVM: Separating hyperplane for unbalanced classes
• SVM-Anova: SVM with univariate feature selection,
• Non-linear SVM
• SVM: Weighted samples,
Regression
The method of Support Vector Classiﬁcation can be extended to solve regression problems. This method is called
Support Vector Regression.
The model produced by support vector classiﬁcation (as described above) depends only on a subset of the training
data, because the cost function for building the model does not care about training points that lie beyond the margin.
Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because
the cost function for building the model ignores any training data close to the model prediction.
There are three different implementations of Support Vector Regression:
SVR, NuSVR and LinearSVR.
LinearSVR provides a faster implementation than SVR but only considers linear kernels, while NuSVR implements
a slightly different formulation than SVR and LinearSVR. See Implementation details for further details.
As with classiﬁcation classes, the ﬁt method will take as argument vectors X, y, only that in this case y is expected to
have ﬂoating point values instead of integer values:
>>> from sklearn import svm
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = svm.SVR()
>>> clf.fit(X, y)
SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
>>> clf.predict([[1, 1]])
array([ 1.5])
Examples:
• Support Vector Regression (SVR) using linear and non-linear kernels
164
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Density estimation, novelty detection
One-class SVM is used for novelty detection, that is, given a set of samples, it will detect the soft boundary of that set
so as to classify new points as belonging to that set or not. The class that implements this is called OneClassSVM.
In this case, as it is a type of unsupervised learning, the ﬁt method will only take as input an array X, as there are no
class labels.
See, section Novelty and Outlier Detection for more details on this usage.
Examples:
• One-class SVM with non-linear kernel (RBF)
• Species distribution modeling
Complexity
Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the
number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support
vectors from the rest of the training data. The QP solver used by this libsvm-based implementation scales between
𝑂(𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠× 𝑛2
𝑠𝑎𝑚𝑝𝑙𝑒𝑠) and 𝑂(𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠× 𝑛3
𝑠𝑎𝑚𝑝𝑙𝑒𝑠) depending on how efﬁciently the libsvm cache is used in
practice (dataset dependent). If the data is very sparse 𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠should be replaced by the average number of non-
zero features in a sample vector.
Also note that for the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more
efﬁcient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features.
3.1. Supervised learning
165
scikit-learn user guide, Release 0.18.2
Tips on Practical Use
• Avoiding data copy: For SVC, SVR, NuSVC and NuSVR, if the data passed to certain methods is not C-ordered
contiguous, and double precision, it will be copied before calling the underlying C implementation. You can
check whether a given numpy array is C-contiguous by inspecting its flags attribute.
For LinearSVC (and LogisticRegression) any input passed as a numpy array will be copied and con-
verted to the liblinear internal sparse data representation (double precision ﬂoats and int32 indices of non-zero
components). If you want to ﬁt a large-scale linear classiﬁer without copying a dense numpy C-contiguous
double precision array as input we suggest to use the SGDClassifier class instead. The objective function
can be conﬁgured to be almost the same as the LinearSVC model.
• Kernel cache size: For SVC, SVR, nuSVC and NuSVR, the size of the kernel cache has a strong impact on run
times for larger problems. If you have enough RAM available, it is recommended to set cache_size to a
higher value than the default of 200(MB), such as 500(MB) or 1000(MB).
• Setting C: C is 1 by default and it’s a reasonable default choice. If you have a lot of noisy observations you
should decrease it. It corresponds to regularize more the estimation.
• Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data.
For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0
and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. See
section Preprocessing data for more details on scaling and normalization.
• Parameter nu in NuSVC/OneClassSVM/NuSVR approximates the fraction of training errors and support vec-
tors.
• In SVC, if data for classiﬁcation are unbalanced (e.g.
many positive and few negative),
set
class_weight='balanced' and/or try different penalty parameters C.
• The underlying LinearSVC implementation uses a random number generator to select features when ﬁtting
the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens,
try with a smaller tol parameter.
• Using L1 penalization as provided by LinearSVC(loss='l2',penalty='l1',dual=False) yields
a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision
function. Increasing C yields a more complex model (more feature are selected). The C value that yields a
“null” model (all weights equal to zero) can be calculated using l1_min_c.
Kernel functions
The kernel function can be any of the following:
• linear: ⟨𝑥, 𝑥′⟩.
• polynomial: (𝛾⟨𝑥, 𝑥′⟩+ 𝑟)𝑑. 𝑑is speciﬁed by keyword degree, 𝑟by coef0.
• rbf: exp(−𝛾|𝑥−𝑥′|2). 𝛾is speciﬁed by keyword gamma, must be greater than 0.
• sigmoid (tanh(𝛾⟨𝑥, 𝑥′⟩+ 𝑟)), where 𝑟is speciﬁed by coef0.
Different kernels are speciﬁed by keyword kernel at initialization:
>>> linear_svc = svm.SVC(kernel='linear')
>>> linear_svc.kernel
'linear'
>>> rbf_svc = svm.SVC(kernel='rbf')
>>> rbf_svc.kernel
'rbf'
166
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Custom Kernels
You can deﬁne your own kernels by either giving the kernel as a python function or by precomputing the Gram matrix.
Classiﬁers with custom kernels behave the same way as any other classiﬁers, except that:
• Field support_vectors_ is now empty, only indices of support vectors are stored in support_
• A reference (and not a copy) of the ﬁrst argument in the fit() method is stored for future reference. If that
array changes between the use of fit() and predict() you will have unexpected results.
Using Python functions as kernels
You can also use your own deﬁned kernels by passing a function to the keyword kernel in the constructor.
Your
kernel
must
take
as
arguments
two
matrices
of
shape
(n_samples_1,n_features),
(n_samples_2,n_features) and return a kernel matrix of shape (n_samples_1,n_samples_2).
The following code deﬁnes a linear kernel and creates a classiﬁer instance that will use that kernel:
>>> import numpy as np
>>> from sklearn import svm
>>> def my_kernel(X, Y):
...
return np.dot(X, Y.T)
...
>>> clf = svm.SVC(kernel=my_kernel)
Examples:
• SVM with custom kernel.
Using the Gram matrix
Set kernel='precomputed' and pass the Gram matrix instead of X in the ﬁt method. At the moment, the kernel
values between all training vectors and the test vectors must be provided.
>>> import numpy as np
>>> from sklearn import svm
>>> X = np.array([[0, 0], [1, 1]])
>>> y = [0, 1]
>>> clf = svm.SVC(kernel='precomputed')
>>> # linear kernel computation
>>> gram = np.dot(X, X.T)
>>> clf.fit(gram, y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto',
kernel='precomputed', max_iter=-1, probability=False,
random_state=None, shrinking=True, tol=0.001, verbose=False)
>>> # predict on training examples
>>> clf.predict(gram)
array([0, 1])
3.1. Supervised learning
167
scikit-learn user guide, Release 0.18.2
Parameters of the RBF Kernel
When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and
gamma. The parameter C, common to all SVM kernels, trades off misclassiﬁcation of training examples against
simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all
training examples correctly. gamma deﬁnes how much inﬂuence a single training example has. The larger gamma is,
the closer other examples must be to be affected.
Proper
choice
of
C
and
gamma
is
critical
to
the
SVM’s
performance.
One
is
advised
to
use
sklearn.model_selection.GridSearchCV with C and gamma spaced exponentially far apart to choose
good values.
Examples:
• RBF SVM parameters
Mathematical formulation
A support vector machine constructs a hyper-plane or set of hyper-planes in a high or inﬁnite dimensional space, which
can be used for classiﬁcation, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane
that has the largest distance to the nearest training data points of any class (so-called functional margin), since in
general the larger the margin the lower the generalization error of the classiﬁer.
168
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
SVC
Given training vectors 𝑥𝑖∈R𝑝, i=1,..., n, in two classes, and a vector 𝑦∈{1, −1}𝑛, SVC solves the following primal
problem:
min
𝑤,𝑏,𝜁
1
2𝑤𝑇𝑤+ 𝐶
𝑛
∑︁
𝑖=1
𝜁𝑖
subject to 𝑦𝑖(𝑤𝑇𝜑(𝑥𝑖) + 𝑏) ≥1 −𝜁𝑖,
𝜁𝑖≥0, 𝑖= 1, ..., 𝑛
Its dual is
min
𝛼
1
2𝛼𝑇𝑄𝛼−𝑒𝑇𝛼
subject to 𝑦𝑇𝛼= 0
0 ≤𝛼𝑖≤𝐶, 𝑖= 1, ..., 𝑛
where 𝑒is the vector of all ones, 𝐶> 0 is the upper bound, 𝑄is an 𝑛by 𝑛positive semideﬁnite matrix, 𝑄𝑖𝑗≡
𝑦𝑖𝑦𝑗𝐾(𝑥𝑖, 𝑥𝑗), where 𝐾(𝑥𝑖, 𝑥𝑗) = 𝜑(𝑥𝑖)𝑇𝜑(𝑥𝑗) is the kernel. Here training vectors are implicitly mapped into a
higher (maybe inﬁnite) dimensional space by the function 𝜑.
The decision function is:
sgn(
𝑛
∑︁
𝑖=1
𝑦𝑖𝛼𝑖𝐾(𝑥𝑖, 𝑥) + 𝜌)
Note: While SVM models derived from libsvm and liblinear use C as regularization parameter, most other estimators
use alpha. The relation between both is 𝐶= 𝑛_𝑠𝑎𝑚𝑝𝑙𝑒𝑠
𝑎𝑙𝑝ℎ𝑎
.
This parameters can be accessed through the members dual_coef_ which holds the product 𝑦𝑖𝛼𝑖,
support_vectors_ which holds the support vectors, and intercept_ which holds the independent term 𝜌
:
References:
• “Automatic Capacity Tuning of Very Large VC-dimension Classiﬁers”, I. Guyon, B. Boser, V. Vapnik -
Advances in neural information processing 1993.
• “Support-vector networks”, C. Cortes, V. Vapnik - Machine Learning, 20, 273-297 (1995).
NuSVC
We introduce a new parameter 𝜈which controls the number of support vectors and training errors. The parameter
𝜈∈(0, 1] is an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors.
It can be shown that the 𝜈-SVC formulation is a reparametrization of the 𝐶-SVC and therefore mathematically equiv-
alent.
3.1. Supervised learning
169
scikit-learn user guide, Release 0.18.2
SVR
Given training vectors 𝑥𝑖∈R𝑝, i=1,..., n, and a vector 𝑦∈R𝑛𝜀-SVR solves the following primal problem:
min
𝑤,𝑏,𝜁,𝜁*
1
2𝑤𝑇𝑤+ 𝐶
𝑛
∑︁
𝑖=1
(𝜁𝑖+ 𝜁*
𝑖)
subject to 𝑦𝑖−𝑤𝑇𝜑(𝑥𝑖) −𝑏≤𝜀+ 𝜁𝑖,
𝑤𝑇𝜑(𝑥𝑖) + 𝑏−𝑦𝑖≤𝜀+ 𝜁*
𝑖,
𝜁𝑖, 𝜁*
𝑖≥0, 𝑖= 1, ..., 𝑛
Its dual is
min
𝛼,𝛼*
1
2(𝛼−𝛼*)𝑇𝑄(𝛼−𝛼*) + 𝜀𝑒𝑇(𝛼+ 𝛼*) −𝑦𝑇(𝛼−𝛼*)
subject to 𝑒𝑇(𝛼−𝛼*) = 0
0 ≤𝛼𝑖, 𝛼*
𝑖≤𝐶, 𝑖= 1, ..., 𝑛
where 𝑒is the vector of all ones, 𝐶> 0 is the upper bound, 𝑄is an 𝑛by 𝑛positive semideﬁnite matrix, 𝑄𝑖𝑗≡
𝐾(𝑥𝑖, 𝑥𝑗) = 𝜑(𝑥𝑖)𝑇𝜑(𝑥𝑗) is the kernel. Here training vectors are implicitly mapped into a higher (maybe inﬁnite)
dimensional space by the function 𝜑.
The decision function is:
𝑛
∑︁
𝑖=1
(𝛼𝑖−𝛼*
𝑖)𝐾(𝑥𝑖, 𝑥) + 𝜌
These parameters can be accessed through the members dual_coef_ which holds the difference 𝛼𝑖−𝛼*
𝑖,
support_vectors_ which holds the support vectors, and intercept_ which holds the independent term 𝜌
References:
• “A Tutorial on Support Vector Regression”, Alex J. Smola, Bernhard Schölkopf - Statistics and Computing
archive Volume 14 Issue 3, August 2004, p. 199-222.
Implementation details
Internally, we use libsvm and liblinear to handle all computations. These libraries are wrapped using C and Cython.
References:
For a description of the implementation and details of the algorithms used, please refer to
• LIBSVM: A Library for Support Vector Machines.
• LIBLINEAR – A Library for Large Linear Classiﬁcation.
3.1.5 Stochastic Gradient Descent
Stochastic Gradient Descent (SGD) is a simple yet very efﬁcient approach to discriminative learning of linear clas-
siﬁers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though
170
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
SGD has been around in the machine learning community for a long time, it has received a considerable amount of
attention just recently in the context of large-scale learning.
SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text
classiﬁcation and natural language processing. Given that the data is sparse, the classiﬁers in this module easily scale
to problems with more than 10^5 training examples and more than 10^5 features.
The advantages of Stochastic Gradient Descent are:
• Efﬁciency.
• Ease of implementation (lots of opportunities for code tuning).
The disadvantages of Stochastic Gradient Descent include:
• SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.
• SGD is sensitive to feature scaling.
Classiﬁcation
Warning: Make sure you permute (shufﬂe) your training data before ﬁtting the model or use shuffle=True
to shufﬂe after each iteration.
The class SGDClassifier implements a plain stochastic gradient descent learning routine which supports different
loss functions and penalties for classiﬁcation.
As other classiﬁers, SGD has to be ﬁtted with two arrays: an array X of size [n_samples, n_features] holding the
training samples, and an array Y of size [n_samples] holding the target values (class labels) for the training samples:
3.1. Supervised learning
171
scikit-learn user guide, Release 0.18.2
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0., 0.], [1., 1.]]
>>> y = [0, 1]
>>> clf = SGDClassifier(loss="hinge", penalty="l2")
>>> clf.fit(X, y)
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
eta0=0.0, fit_intercept=True, l1_ratio=0.15,
learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
penalty='l2', power_t=0.5, random_state=None, shuffle=True,
verbose=0, warm_start=False)
After being ﬁtted, the model can then be used to predict new values:
>>> clf.predict([[2., 2.]])
array([1])
SGD ﬁts a linear model to the training data. The member coef_ holds the model parameters:
>>> clf.coef_
array([[ 9.9...,
9.9...]])
Member intercept_ holds the intercept (aka offset or bias):
>>> clf.intercept_
array([-9.9...])
Whether or not the model should use an intercept, i.e.
a biased hyperplane, is controlled by the parameter
fit_intercept.
To get the signed distance to the hyperplane use SGDClassifier.decision_function:
>>> clf.decision_function([[2., 2.]])
array([ 29.6...])
The concrete loss function can be set via the loss parameter. SGDClassifier supports the following loss func-
tions:
• loss="hinge": (soft-margin) linear Support Vector Machine,
• loss="modified_huber": smoothed hinge loss,
• loss="log": logistic regression,
• and all regression losses below.
The ﬁrst two loss functions are lazy, they only update the model parameters if an example violates the margin con-
straint, which makes training very efﬁcient and may result in sparser models, even when L2 penalty is used.
Using loss="log" or loss="modified_huber" enables the predict_proba method, which gives a vector
of probability estimates 𝑃(𝑦|𝑥) per sample 𝑥:
>>> clf = SGDClassifier(loss="log").fit(X, y)
>>> clf.predict_proba([[1., 1.]])
array([[ 0.00...,
0.99...]])
The concrete penalty can be set via the penalty parameter. SGD supports the following penalties:
• penalty="l2": L2 norm penalty on coef_.
• penalty="l1": L1 norm penalty on coef_.
172
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• penalty="elasticnet": Convex combination of L2 and L1; (1 -l1_ratio) * L2 + l1_ratio
* L1.
The default setting is penalty="l2". The L1 penalty leads to sparse solutions, driving most coefﬁcients to zero.
The Elastic Net solves some deﬁciencies of the L1 penalty in the presence of highly correlated attributes. The param-
eter l1_ratio controls the convex combination of L1 and L2 penalty.
SGDClassifier supports multi-class classiﬁcation by combining multiple binary classiﬁers in a “one versus all”
(OVA) scheme. For each of the 𝐾classes, a binary classiﬁer is learned that discriminates between that and all other
𝐾−1 classes. At testing time, we compute the conﬁdence score (i.e. the signed distances to the hyperplane) for each
classiﬁer and choose the class with the highest conﬁdence. The Figure below illustrates the OVA approach on the iris
dataset. The dashed lines represent the three OVA classiﬁers; the background colors show the decision surface induced
by the three classiﬁers.
In
the
case
of
multi-class
classiﬁcation
coef_
is
a
two-dimensionally
array
of
shape=[n_classes,n_features] and intercept_ is a one dimensional array of shape=[n_classes].
The i-th row of coef_ holds the weight vector of the OVA classiﬁer for the i-th class; classes are indexed in ascending
order (see attribute classes_). Note that, in principle, since they allow to create a probability model, loss="log"
and loss="modified_huber" are more suitable for one-vs-all classiﬁcation.
SGDClassifier supports both weighted classes and weighted instances via the ﬁt parameters class_weight
and sample_weight. See the examples below and the doc string of SGDClassifier.fit for further informa-
tion.
Examples:
• SGD: Maximum margin separating hyperplane,
• Plot multi-class SGD on the iris dataset
• SGD: Weighted samples
3.1. Supervised learning
173
scikit-learn user guide, Release 0.18.2
• Comparing various online solvers
• SVM: Separating hyperplane for unbalanced classes (See the Note)
SGDClassifier supports averaged SGD (ASGD). Averaging can be enabled by setting `average=True`.
ASGD works by averaging the coefﬁcients of the plain SGD over each iteration over a sample. When using ASGD
the learning rate can be larger and even constant leading on some datasets to a speed up in training time.
For classiﬁcation with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic
Average Gradient (SAG) algorithm, available as a solver in LogisticRegression.
Regression
The class SGDRegressor implements a plain stochastic gradient descent learning routine which supports different
loss functions and penalties to ﬁt linear regression models. SGDRegressor is well suited for regression prob-
lems with a large number of training samples (> 10.000), for other problems we recommend Ridge, Lasso, or
ElasticNet.
The concrete loss function can be set via the loss parameter. SGDRegressor supports the following loss functions:
• loss="squared_loss": Ordinary least squares,
• loss="huber": Huber loss for robust regression,
• loss="epsilon_insensitive": linear Support Vector Regression.
The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region
has to be speciﬁed via the parameter epsilon. This parameter depends on the scale of the target variables.
SGDRegressor supports averaged SGD as SGDClassifier.
Averaging can be enabled by setting
`average=True`.
For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with
Stochastic Average Gradient (SAG) algorithm, available as a solver in Ridge.
Stochastic Gradient Descent for sparse data
Note: The sparse implementation produces slightly different results than the dense implementation due to a shrunk
learning rate for the intercept.
There is built-in support for sparse data given in any matrix in a format supported by scipy.sparse. For maximum
efﬁciency, however, use the CSR matrix format as deﬁned in scipy.sparse.csr_matrix.
Examples:
• Classiﬁcation of text documents using sparse features
Complexity
The major advantage of SGD is its efﬁciency, which is basically linear in the number of training examples. If X is a
matrix of size (n, p) training has a cost of 𝑂(𝑘𝑛¯𝑝), where k is the number of iterations (epochs) and ¯𝑝is the average
number of non-zero attributes per sample.
174
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Recent theoretical results, however, show that the runtime to get some desired optimization accuracy does not increase
as the training set size increases.
Tips on Practical Use
• Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For
example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and
variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can
be easily done using StandardScaler:
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
# Don't cheat - fit only on training data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
# apply same transformation to test data
If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed.
• Finding a reasonable regularization term 𝛼is best done using GridSearchCV, usually in the range
10.0**-np.arange(1,7).
• Empirically, we found that SGD converges after observing approx. 10^6 training samples. Thus, a reasonable
ﬁrst guess for the number of iterations is n_iter = np.ceil(10**6 / n), where n is the size of the
training set.
• If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by
some constant c such that the average L2 norm of the training data equals one.
• We found that Averaged SGD works best with a larger number of features and a higher eta0
References:
• “Efﬁcient BackProp” Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998.
Mathematical formulation
Given a set of training examples (𝑥1, 𝑦1), . . . , (𝑥𝑛, 𝑦𝑛) where 𝑥𝑖∈R𝑛and 𝑦𝑖∈{−1, 1}, our goal is to learn a linear
scoring function 𝑓(𝑥) = 𝑤𝑇𝑥+ 𝑏with model parameters 𝑤∈R𝑚and intercept 𝑏∈R. In order to make predictions,
we simply look at the sign of 𝑓(𝑥). A common choice to ﬁnd the model parameters is by minimizing the regularized
training error given by
𝐸(𝑤, 𝑏) = 1
𝑛
𝑛
∑︁
𝑖=1
𝐿(𝑦𝑖, 𝑓(𝑥𝑖)) + 𝛼𝑅(𝑤)
where 𝐿is a loss function that measures model (mis)ﬁt and 𝑅is a regularization term (aka penalty) that penalizes
model complexity; 𝛼> 0 is a non-negative hyperparameter.
Different choices for 𝐿entail different classiﬁers such as
• Hinge: (soft-margin) Support Vector Machines.
• Log: Logistic Regression.
• Least-Squares: Ridge Regression.
• Epsilon-Insensitive: (soft-margin) Support Vector Regression.
3.1. Supervised learning
175
scikit-learn user guide, Release 0.18.2
All of the above loss functions can be regarded as an upper bound on the misclassiﬁcation error (Zero-one loss) as
shown in the Figure below.
Popular choices for the regularization term 𝑅include:
• L2 norm: 𝑅(𝑤) := 1
2
∑︀𝑛
𝑖=1 𝑤2
𝑖,
• L1 norm: 𝑅(𝑤) := ∑︀𝑛
𝑖=1 |𝑤𝑖|, which leads to sparse solutions.
• Elastic Net: 𝑅(𝑤) := 𝜌
2
∑︀𝑛
𝑖=1 𝑤2
𝑖+ (1 −𝜌) ∑︀𝑛
𝑖=1 |𝑤𝑖|, a convex combination of L2 and L1, where 𝜌is given
by 1 -l1_ratio.
The Figure below shows the contours of the different regularization terms in the parameter space when 𝑅(𝑤) = 1.
SGD
Stochastic gradient descent is an optimization method for unconstrained optimization problems. In contrast to (batch)
gradient descent, SGD approximates the true gradient of 𝐸(𝑤, 𝑏) by considering a single training example at a time.
The class SGDClassifier implements a ﬁrst-order SGD learning routine. The algorithm iterates over the training
examples and for each example updates the model parameters according to the update rule given by
𝑤←𝑤−𝜂(𝛼𝜕𝑅(𝑤)
𝜕𝑤
+ 𝜕𝐿(𝑤𝑇𝑥𝑖+ 𝑏, 𝑦𝑖)
𝜕𝑤
)
where 𝜂is the learning rate which controls the step-size in the parameter space. The intercept 𝑏is updated similarly
but without regularization.
The learning rate 𝜂can be either constant or gradually decaying. For classiﬁcation, the default learning rate schedule
(learning_rate='optimal') is given by
𝜂(𝑡) =
1
𝛼(𝑡0 + 𝑡)
176
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
where 𝑡is the time step (there are a total of n_samples * n_iter time steps), 𝑡0 is determined based on a heuristic
proposed by Léon Bottou such that the expected initial updates are comparable with the expected size of the weights
(this assuming that the norm of the training samples is approx. 1). The exact deﬁnition can be found in _init_t in
BaseSGD.
For regression the default learning rate schedule is inverse scaling (learning_rate='invscaling'), given by
𝜂(𝑡) =
𝑒𝑡𝑎0
𝑡𝑝𝑜𝑤𝑒𝑟_𝑡
where 𝑒𝑡𝑎0 and 𝑝𝑜𝑤𝑒𝑟_𝑡are hyperparameters chosen by the user via eta0 and power_t, resp.
For a constant learning rate use learning_rate='constant' and use eta0 to specify the learning rate.
The model parameters can be accessed through the members coef_ and intercept_:
• Member coef_ holds the weights 𝑤
• Member intercept_ holds 𝑏
References:
• “Solving large scale linear prediction problems using stochastic gradient descent algorithms” T. Zhang - In
Proceedings of ICML ‘04.
• “Regularization and variable selection via the elastic net” H. Zou, T. Hastie - Journal of the Royal Statistical
Society Series B, 67 (2), 301-320.
• “Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent” Xu, Wei
3.1. Supervised learning
177
scikit-learn user guide, Release 0.18.2
Implementation details
The implementation of SGD is inﬂuenced by the Stochastic Gradient SVM of Léon Bottou. Similar to SvmSGD,
the weight vector is represented as the product of a scalar and a vector which allows an efﬁcient weight update in
the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning
rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up
sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from
Shalev-Shwartz et al. 2007. For multi-class classiﬁcation, a “one versus all” approach is used. We use the truncated
gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written
in Cython.
References:
• “Stochastic Gradient Descent” L. Bottou - Website, 2010.
• “The Tradeoffs of Large Scale Machine Learning” L. Bottou - Website, 2011.
• “Pegasos: Primal estimated sub-gradient solver for svm” S. Shalev-Shwartz, Y. Singer, N. Srebro - In Pro-
ceedings of ICML ‘07.
• “Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty” Y. Tsu-
ruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL ‘09.
3.1.6 Nearest Neighbors
sklearn.neighbors provides functionality for unsupervised and supervised neighbors-based learning methods.
Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and
spectral clustering. Supervised neighbors-based learning comes in two ﬂavors: classiﬁcation for data with discrete
labels, and regression for data with continuous labels.
The principle behind nearest neighbor methods is to ﬁnd a predeﬁned number of training samples closest in distance
to the new point, and predict the label from these. The number of samples can be a user-deﬁned constant (k-nearest
neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can,
in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based meth-
ods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data
(possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree.).
Despite its simplicity, nearest neighbors has been successful in a large number of classiﬁcation and regression prob-
lems, including handwritten digits or satellite image scenes. Being a non-parametric method, it is often successful in
classiﬁcation situations where the decision boundary is very irregular.
The classes in sklearn.neighbors can handle either Numpy arrays or scipy.sparse matrices as input. For dense
matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics
are supported for searches.
There are many learning routines which rely on nearest neighbors at their core. One example is kernel density estima-
tion, discussed in the density estimation section.
Unsupervised Nearest Neighbors
NearestNeighbors implements unsupervised nearest neighbors learning. It acts as a uniform interface to three
different nearest neighbors algorithms: BallTree, KDTree, and a brute-force algorithm based on routines in
sklearn.metrics.pairwise. The choice of neighbors search algorithm is controlled through the keyword
'algorithm', which must be one of ['auto','ball_tree','kd_tree','brute']. When the default
178
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
value 'auto' is passed, the algorithm attempts to determine the best approach from the training data. For a discussion
of the strengths and weaknesses of each option, see Nearest Neighbor Algorithms.
Warning: Regarding the Nearest Neighbors algorithms, if two neighbors, neighbor 𝑘+1 and 𝑘, have
identical distances but different labels, the results will depend on the ordering of the training data.
Finding the Nearest Neighbors
For the simple task of ﬁnding the nearest neighbors between two sets of data, the unsupervised algorithms within
sklearn.neighbors can be used:
>>> from sklearn.neighbors import NearestNeighbors
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
>>> distances, indices = nbrs.kneighbors(X)
>>> indices
array([[0, 1],
[1, 0],
[2, 1],
[3, 4],
[4, 3],
[5, 4]]...)
>>> distances
array([[ 0.
,
1.
],
[ 0.
,
1.
],
[ 0.
,
1.41421356],
[ 0.
,
1.
],
[ 0.
,
1.
],
[ 0.
,
1.41421356]])
Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of
zero.
It is also possible to efﬁciently produce a sparse graph showing the connections between neighboring points:
>>> nbrs.kneighbors_graph(X).toarray()
array([[ 1.,
1.,
0.,
0.,
0.,
0.],
[ 1.,
1.,
0.,
0.,
0.,
0.],
[ 0.,
1.,
1.,
0.,
0.,
0.],
[ 0.,
0.,
0.,
1.,
1.,
0.],
[ 0.,
0.,
0.,
1.,
1.,
0.],
[ 0.,
0.,
0.,
0.,
1.,
1.]])
Our dataset is structured such that points nearby in index order are nearby in parameter space, leading to
an approximately block-diagonal matrix of K-nearest neighbors.
Such a sparse graph is useful in a vari-
ety of circumstances which make use of spatial relationships between points for unsupervised learning:
in
particular, see sklearn.manifold.Isomap, sklearn.manifold.LocallyLinearEmbedding, and
sklearn.cluster.SpectralClustering.
KDTree and BallTree Classes
Alternatively, one can use the KDTree or BallTree classes directly to ﬁnd nearest neighbors. This is the function-
ality wrapped by the NearestNeighbors class used above. The Ball Tree and KD Tree have the same interface;
3.1. Supervised learning
179
scikit-learn user guide, Release 0.18.2
we’ll show an example of using the KD Tree here:
>>> from sklearn.neighbors import KDTree
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> kdt = KDTree(X, leaf_size=30, metric='euclidean')
>>> kdt.query(X, k=2, return_distance=False)
array([[0, 1],
[1, 0],
[2, 1],
[3, 4],
[4, 3],
[5, 4]]...)
Refer to the KDTree and BallTree class documentation for more information on the options available for neighbors
searches, including speciﬁcation of query strategies, of various distance metrics, etc. For a list of available metrics,
see the documentation of the DistanceMetric class.
Nearest Neighbors Classiﬁcation
Neighbors-based classiﬁcation is a type of instance-based learning or non-generalizing learning: it does not attempt
to construct a general internal model, but simply stores instances of the training data. Classiﬁcation is computed from
a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the
most representatives within the nearest neighbors of the point.
scikit-learn implements two different nearest neighbors classiﬁers: KNeighborsClassifier implements learn-
ing based on the 𝑘nearest neighbors of each query point, where 𝑘is an integer value speciﬁed by the user.
RadiusNeighborsClassifier implements learning based on the number of neighbors within a ﬁxed radius
𝑟of each training point, where 𝑟is a ﬂoating-point value speciﬁed by the user.
The 𝑘-neighbors classiﬁcation in KNeighborsClassifier is the more commonly used of the two techniques.
The optimal choice of the value 𝑘is highly data-dependent: in general a larger 𝑘suppresses the effects of noise, but
makes the classiﬁcation boundaries less distinct.
In
cases
where
the
data
is
not
uniformly
sampled,
radius-based
neighbors
classiﬁcation
in
RadiusNeighborsClassifier can be a better choice.
The user speciﬁes a ﬁxed radius 𝑟, such that
points in sparser neighborhoods use fewer nearest neighbors for the classiﬁcation. For high-dimensional parameter
spaces, this method becomes less effective due to the so-called “curse of dimensionality”.
The basic nearest neighbors classiﬁcation uses uniform weights: that is, the value assigned to a query point is computed
from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors
such that nearer neighbors contribute more to the ﬁt. This can be accomplished through the weights keyword. The
default value, weights = 'uniform', assigns uniform weights to each neighbor. weights = 'distance'
assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-deﬁned function
of the distance can be supplied which is used to compute the weights.
180
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples:
• Nearest Neighbors Classiﬁcation: an example of classiﬁcation using nearest neighbors.
Nearest Neighbors Regression
Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables.
The label assigned to a query point is computed based the mean of the labels of its nearest neighbors.
scikit-learn implements two different neighbors regressors:
KNeighborsRegressor implements learning
based on the 𝑘nearest neighbors of each query point, where 𝑘is an integer value speciﬁed by the user.
RadiusNeighborsRegressor implements learning based on the neighbors within a ﬁxed radius 𝑟of the query
point, where 𝑟is a ﬂoating-point value speciﬁed by the user.
The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes
uniformly to the classiﬁcation of a query point. Under some circumstances, it can be advantageous to weight points
such that nearby points contribute more to the regression than faraway points. This can be accomplished through the
weights keyword. The default value, weights = 'uniform', assigns equal weights to all points. weights
= 'distance' assigns weights proportional to the inverse of the distance from the query point. Alternatively, a
user-deﬁned function of the distance can be supplied, which will be used to compute the weights.
The use of multi-output nearest neighbors for regression is demonstrated in Face completion with a multi-output
estimators. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of
3.1. Supervised learning
181
scikit-learn user guide, Release 0.18.2
the lower half of those faces.
Examples:
• Nearest Neighbors regression: an example of regression using nearest neighbors.
• Face completion with a multi-output estimators: an example of multi-output regression using nearest neigh-
bors.
Nearest Neighbor Algorithms
Brute Force
Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor
search implementation involves the brute-force computation of distances between all pairs of points in the dataset:
for 𝑁samples in 𝐷dimensions, this approach scales as 𝑂[𝐷𝑁2]. Efﬁcient brute-force neighbors searches can
be very competitive for small data samples.
However, as the number of samples 𝑁grows, the brute-force ap-
proach quickly becomes infeasible. In the classes within sklearn.neighbors, brute-force neighbors searches
are speciﬁed using the keyword algorithm = 'brute', and are computed using the routines available in
sklearn.metrics.pairwise.
K-D Tree
To address the computational inefﬁciencies of the brute-force approach, a variety of tree-based data structures have
been invented. In general, these structures attempt to reduce the required number of distance calculations by efﬁciently
encoding aggregate distance information for the sample. The basic idea is that if point 𝐴is very distant from point
𝐵, and point 𝐵is very close to point 𝐶, then we know that points 𝐴and 𝐶are very distant, without having to
182
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
183
scikit-learn user guide, Release 0.18.2
explicitly calculate their distance. In this way, the computational cost of a nearest neighbors search can be reduced to
𝑂[𝐷𝑁log(𝑁)] or better. This is a signiﬁcant improvement over brute-force for large 𝑁.
An early approach to taking advantage of this aggregate information was the KD tree data structure (short for K-
dimensional tree), which generalizes two-dimensional Quad-trees and 3-dimensional Oct-trees to an arbitrary number
of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data
axes, dividing it into nested orthotopic regions into which data points are ﬁled. The construction of a KD tree is very
fast: because partitioning is performed only along the data axes, no 𝐷-dimensional distances need to be computed.
Once constructed, the nearest neighbor of a query point can be determined with only 𝑂[log(𝑁)] distance computations.
Though the KD tree approach is very fast for low-dimensional (𝐷< 20) neighbors searches, it becomes inefﬁcient
as 𝐷grows very large: this is one manifestation of the so-called “curse of dimensionality”. In scikit-learn, KD tree
neighbors searches are speciﬁed using the keyword algorithm = 'kd_tree', and are computed using the class
KDTree.
References:
• “Multidimensional binary search trees used for associative searching”, Bentley, J.L., Communications of the
ACM (1975)
Ball Tree
To address the inefﬁciencies of KD Trees in higher dimensions, the ball tree data structure was developed. Where
KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes
tree construction more costly than that of the KD tree, but results in a data structure which can be very efﬁcient on
highly-structured data, even in very high dimensions.
A ball tree recursively divides the data into nodes deﬁned by a centroid 𝐶and radius 𝑟, such that each point in the
node lies within the hyper-sphere deﬁned by 𝑟and 𝐶. The number of candidate points for a neighbor search is reduced
through use of the triangle inequality:
|𝑥+ 𝑦| ≤|𝑥| + |𝑦|
With this setup, a single distance calculation between a test point and the centroid is sufﬁcient to determine a lower
and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes,
it can out-perform a KD-tree in high dimensions, though the actual performance is highly dependent on the structure
of the training data. In scikit-learn, ball-tree-based neighbors searches are speciﬁed using the keyword algorithm
= 'ball_tree', and are computed using the class sklearn.neighbors.BallTree. Alternatively, the user
can work with the BallTree class directly.
References:
• “Five balltree construction algorithms”, Omohundro, S.M., International Computer Science Institute Techni-
cal Report (1989)
Choice of Nearest Neighbors Algorithm
The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:
• number of samples 𝑁(i.e. n_samples) and dimensionality 𝐷(i.e. n_features).
– Brute force query time grows as 𝑂[𝐷𝑁]
184
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
– Ball tree query time grows as approximately 𝑂[𝐷log(𝑁)]
– KD tree query time changes with 𝐷in a way that is difﬁcult to precisely characterise. For small 𝐷(less
than 20 or so) the cost is approximately 𝑂[𝐷log(𝑁)], and the KD tree query can be very efﬁcient. For
larger 𝐷, the cost increases to nearly 𝑂[𝐷𝑁], and the overhead due to the tree structure can lead to queries
which are slower than brute force.
For small data sets (𝑁less than 30 or so), log(𝑁) is comparable to 𝑁, and brute force algorithms can be more
efﬁcient than a tree-based approach. Both KDTree and BallTree address this through providing a leaf size
parameter: this controls the number of samples at which a query switches to brute-force. This allows both
algorithms to approach the efﬁciency of a brute-force computation for small 𝑁.
• data structure: intrinsic dimensionality of the data and/or sparsity of the data. Intrinsic dimensionality refers
to the dimension 𝑑≤𝐷of a manifold on which the data lies, which can be linearly or non-linearly embedded
in the parameter space. Sparsity refers to the degree to which the data ﬁlls the parameter space (this is to be
distinguished from the concept as used in “sparse” matrices. The data matrix may have no zero entries, but the
structure can still be “sparse” in this sense).
– Brute force query time is unchanged by data structure.
– Ball tree and KD tree query times can be greatly inﬂuenced by data structure. In general, sparser data with a
smaller intrinsic dimensionality leads to faster query times. Because the KD tree internal representation is
aligned with the parameter axes, it will not generally show as much improvement as ball tree for arbitrarily
structured data.
Datasets used in machine learning tend to be very structured, and are very well-suited for tree-based queries.
• number of neighbors 𝑘requested for a query point.
– Brute force query time is largely unaffected by the value of 𝑘
– Ball tree and KD tree query time will become slower as 𝑘increases. This is due to two effects: ﬁrst, a
larger 𝑘leads to the necessity to search a larger portion of the parameter space. Second, using 𝑘> 1
requires internal queueing of results as the tree is traversed.
As 𝑘becomes large compared to 𝑁, the ability to prune branches in a tree-based query is reduced. In this
situation, Brute force queries can be more efﬁcient.
• number of query points. Both the ball tree and the KD Tree require a construction phase. The cost of this
construction becomes negligible when amortized over many queries. If only a small number of queries will
be performed, however, the construction can make up a signiﬁcant fraction of the total cost. If very few query
points will be required, brute force is better than a tree-based method.
Currently, algorithm = 'auto' selects 'kd_tree' if 𝑘
<
𝑁/2 and the 'effective_metric_'
is in the 'VALID_METRICS' list of 'kd_tree'.
It selects 'ball_tree' if 𝑘
<
𝑁/2 and the
'effective_metric_' is not in the 'VALID_METRICS' list of 'kd_tree'. It selects 'brute' if 𝑘>=
𝑁/2. This choice is based on the assumption that the number of query points is at least the same order as the number
of training points, and that leaf_size is close to its default value of 30.
Effect of leaf_size
As noted above, for small sample sizes a brute force search can be more efﬁcient than a tree-based query. This fact is
accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level
of this switch can be speciﬁed with the parameter leaf_size. This parameter choice has many effects:
construction time A larger leaf_size leads to a faster tree construction time, because fewer nodes need to be
created
3.1. Supervised learning
185
scikit-learn user guide, Release 0.18.2
query time Both a large or small leaf_size can lead to suboptimal query cost. For leaf_size approaching
1, the overhead involved in traversing nodes can signiﬁcantly slow query times. For leaf_size approach-
ing the size of the training set, queries become essentially brute force. A good compromise between these is
leaf_size = 30, the default value of the parameter.
memory As leaf_size increases, the memory required to store a tree structure decreases. This is especially
important in the case of ball tree, which stores a 𝐷-dimensional centroid for each node. The required storage
space for BallTree is approximately 1 / leaf_size times the size of the training set.
leaf_size is not referenced for brute force queries.
Nearest Centroid Classiﬁer
The NearestCentroid classiﬁer is a simple algorithm that represents each class by the centroid of its members.
In effect, this makes it similar to the label updating phase of the sklearn.KMeans algorithm. It also has no param-
eters to choose, making it a good baseline classiﬁer. It does, however, suffer on non-convex classes, as well as when
classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discrim-
inant Analysis (sklearn.discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic
Discriminant Analysis (sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis) for
more complex methods that do not make this assumption. Usage of the default NearestCentroid is simple:
>>> from sklearn.neighbors.nearest_centroid import NearestCentroid
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> clf = NearestCentroid()
>>> clf.fit(X, y)
NearestCentroid(metric='euclidean', shrink_threshold=None)
>>> print(clf.predict([[-0.8, -1]]))
[1]
Nearest Shrunken Centroid
The NearestCentroid classiﬁer has a shrink_threshold parameter, which implements the nearest shrunken
centroid classiﬁer. In effect, the value of each feature for each centroid is divided by the within-class variance of that
feature. The feature values are then reduced by shrink_threshold. Most notably, if a particular feature value
crosses zero, it is set to zero. In effect, this removes the feature from affecting the classiﬁcation. This is useful, for
example, for removing noisy features.
In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82.
186
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples:
• Nearest Centroid Classiﬁcation: an example of classiﬁcation using nearest centroid with different shrink
thresholds.
Approximate Nearest Neighbors
There are many efﬁcient exact nearest neighbor search algorithms for low dimensions 𝑑(approximately 50). However
these algorithms perform poorly with respect to space and query time when 𝑑increases. These algorithms are not any
better than comparing query point to each point from the database in a high dimension (see Brute Force). This is a
well-known consequence of the phenomenon called “The Curse of Dimensionality”.
There are certain applications where we do not need the exact nearest neighbors but having a “good guess” would
sufﬁce. When answers do not have to be exact, the LSHForest class implements an approximate nearest neigh-
bor search. Approximate nearest neighbor search methods have been designed to try to speedup query time with
high dimensional data. These techniques are useful when the aim is to characterize the neighborhood rather than
identifying the exact neighbors themselves (eg: k-nearest neighbors classiﬁcation and regression). Some of the most
popular approximate nearest neighbor search techniques are locality sensitive hashing, best bin ﬁt and balanced box-
decomposition tree based search.
3.1. Supervised learning
187
scikit-learn user guide, Release 0.18.2
Locality Sensitive Hashing Forest
The vanilla implementation of locality sensitive hashing has a hyper-parameter that is hard to tune in practice, therefore
scikit-learn implements a variant called LSHForest that has more reasonable hyperparameters. Both methods use
internally random hyperplanes to index the samples into buckets and actual cosine similarities are only computed
for samples that collide with the query hence achieving sublinear scaling. (see Mathematical description of Locality
Sensitive Hashing).
LSHForest has two main hyper-parameters: n_estimators and n_candidates. The accuracy of queries can
be controlled using these parameters as demonstrated in the following plots:
As a rule of thumb, a user can set n_estimators to a large enough value (e.g. between 10 and 50) and then adjust
n_candidates to trade off accuracy for query time.
For small data sets, the brute force method for exact nearest neighbor search can be faster than LSH Forest. However
LSH Forest has a sub-linear query time scalability with the index size. The exact break even point where LSH
Forest queries become faster than brute force depends on the dimensionality, structure of the dataset, required level
of precision, characteristics of the runtime environment such as availability of BLAS optimizations, number of CPU
cores and size of the CPU caches. Following graphs depict scalability of LSHForest queries with index size.
For ﬁxed LSHForest parameters, the accuracy of queries tends to slowly decrease with larger datasets. The error
bars on the previous plots represent standard deviation across different queries.
188
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
189
scikit-learn user guide, Release 0.18.2
Examples:
• Hyper-parameters of Approximate Nearest Neighbors: an example of the behavior of hyperparameters of
approximate nearest neighbor search using LSH Forest.
• Scalability of Approximate Nearest Neighbors: an example of scalability of approximate nearest neighbor
search using LSH Forest.
Mathematical description of Locality Sensitive Hashing
Locality sensitive hashing (LSH) techniques have been used in many areas where nearest neighbor search is performed
in high dimensions. The main concept behind LSH is to hash each data point in the database using multiple (often
simple) hash functions to form a digest (also called a hash). At this point the probability of collision - where two
objects have similar digests - is much higher for the points which are close to each other than that of the distant points.
We describe the requirements for a hash function family to be locality sensitive as follows.
A family 𝐻of functions from a domain 𝑆to a range 𝑈is called (𝑟, 𝑒, 𝑝1, 𝑝2)-sensitive, with 𝑟, 𝑒> 0, 𝑝1 > 𝑝2 > 0, if
for any 𝑝, 𝑞∈𝑆, the following conditions hold (𝐷is the distance function):
• If 𝐷(𝑝, 𝑞) <= 𝑟then 𝑃𝐻[ℎ(𝑝) = ℎ(𝑞)] >= 𝑝1,
• If 𝐷(𝑝, 𝑞) > 𝑟(1 + 𝑒) then 𝑃𝐻[ℎ(𝑝) = ℎ(𝑞)] <= 𝑝2.
As deﬁned, nearby points within a distance of 𝑟to each other are likely to collide with probability 𝑝1. In contrast,
distant points which are located with the distance more than 𝑟(1 + 𝑒) have a small probability of 𝑝2 of collision.
Suppose there is a family of LSH function 𝐻. An LSH index is built as follows:
1. Choose 𝑘functions ℎ1, ℎ2, ...ℎ𝑘uniformly at random (with replacement) from 𝐻. For any 𝑝∈𝑆, place 𝑝in
the bucket with label 𝑔(𝑝) = (ℎ1(𝑝), ℎ2(𝑝), ...ℎ𝑘(𝑝)). Observe that if each ℎ𝑖outputs one “digit”, each bucket
has a k-digit label.
2. Independently perform step 1 𝑙times to construct 𝑙separate estimators, with hash functions 𝑔1, 𝑔2, ...𝑔𝑙.
The reason to concatenate hash functions in the step 1 is to decrease the probability of the collision of distant points
as much as possible. The probability drops from 𝑝2 to 𝑝𝑘
2 which is negligibly small for large 𝑘. The choice of 𝑘is
strongly dependent on the data set size and structure and is therefore hard to tune in practice. There is a side effect of
having a large 𝑘; it has the potential of decreasing the chance of nearby points getting collided. To address this issue,
multiple estimators are constructed in step 2.
The requirement to tune 𝑘for a given dataset makes classical LSH cumbersome to use in practice. The LSH Forest
variant has benn designed to alleviate this requirement by automatically adjusting the number of digits used to hash
the samples.
LSH Forest is formulated with preﬁx trees with each leaf of a tree corresponding to an actual data point in the database.
There are 𝑙such trees which compose the forest and they are constructed using independently drawn random sequence
of hash functions from 𝐻. In this implementation, “Random Projections” is being used as the LSH technique which is
an approximation for the cosine distance. The length of the sequence of hash functions is kept ﬁxed at 32. Moreover,
a preﬁx tree is implemented using sorted arrays and binary search.
There are two phases of tree traversals used in order to answer a query to ﬁnd the 𝑚nearest neighbors of a point
𝑞. First, a top-down traversal is performed using a binary search to identify the leaf having the longest preﬁx match
(maximum depth) with 𝑞‘s label after subjecting 𝑞to the same hash functions. 𝑀>> 𝑚points (total candidates)
are extracted from the forest, moving up from the previously found maximum depth towards the root synchronously
across all trees in the bottom-up traversal. M is set to 𝑐𝑙where 𝑐, the number of candidates extracted from each tree,
is a constant. Finally, the similarity of each of these 𝑀points against point 𝑞is calculated and the top 𝑚points are
returned as the nearest neighbors of 𝑞. Since most of the time in these queries is spent calculating the distances to
190
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
candidates, the speedup compared to brute force search is approximately 𝑁/𝑀, where 𝑁is the number of points in
database.
References:
• “Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions”, Alexandr, A.,
Indyk, P., Foundations of Computer Science, 2006. FOCS ‘06. 47th Annual IEEE Symposium
• “LSH Forest: Self-Tuning Indexes for Similarity Search”, Bawa, M., Condie, T., Ganesan, P., WWW ‘05
Proceedings of the 14th international conference on World Wide Web Pages 651-660
3.1.7 Gaussian Processes
Gaussian Processes (GP) are a generic supervised learning method designed to solve regression and probabilistic
classiﬁcation problems.
The advantages of Gaussian processes are:
• The prediction interpolates the observations (at least for regular kernels).
• The prediction is probabilistic (Gaussian) so that one can compute empirical conﬁdence intervals and decide
based on those if one should reﬁt (online ﬁtting, adaptive ﬁtting) the prediction in some region of interest.
• Versatile: different kernels can be speciﬁed. Common kernels are provided, but it is also possible to specify
custom kernels.
The disadvantages of Gaussian processes include:
• They are not sparse, i.e., they use the whole samples/features information to perform the prediction.
• They lose efﬁciency in high dimensional spaces – namely when the number of features exceeds a few dozens.
Gaussian Process Regression (GPR)
The GaussianProcessRegressor implements Gaussian processes (GP) for regression purposes. For this, the
prior of the GP needs to be speciﬁed. The prior mean is assumed to be constant and zero (for normalize_y=False)
or the training data’s mean (for normalize_y=True). The prior’s covariance is speciﬁed by a passing a kernel
object. The hyperparameters of the kernel are optimized during ﬁtting of GaussianProcessRegressor by maximizing
the log-marginal-likelihood (LML) based on the passed optimizer. As the LML may have multiple local optima,
the optimizer can be started repeatedly by specifying n_restarts_optimizer. The ﬁrst run is always conducted
starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter
values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept
ﬁxed, None can be passed as optimizer.
The noise level in the targets can be speciﬁed by passing it via the parameter alpha, either globally as a scalar or
per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during ﬁtting as
it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An
alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can
estimate the global noise level from the data (see example below).
The implementation is based on Algorithm 2.1 of [RW2006]. In addition to the API of standard scikit-learn estimators,
GaussianProcessRegressor:
• allows prediction without prior ﬁtting (based on the GP prior)
• provides an additional method sample_y(X), which evaluates samples drawn from the GPR (prior or poste-
rior) at given inputs
3.1. Supervised learning
191
scikit-learn user guide, Release 0.18.2
• exposes a method log_marginal_likelihood(theta), which can be used externally for other ways of
selecting hyperparameters, e.g., via Markov chain Monte Carlo.
GPR examples
GPR with noise-level estimation
This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An
illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML.
The ﬁrst corresponds to a model with a high noise level and a large length scale, which explains all variations in the
data by noise.
The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-
free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the
hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important
to repeat the optimization several times for different initializations.
Comparison of GPR and Kernel Ridge Regression
Both kernel ridge regression (KRR) and GPR learn a target function by employing internally the “kernel trick”. KRR
learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in
192
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
193
scikit-learn user guide, Release 0.18.2
194
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge
regularization. GPR uses the kernel to deﬁne the covariance of a prior distribution over the target functions and uses
the observed training data to deﬁne a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution
over target functions is deﬁned, whose mean is used for prediction.
A major difference is that GPR can choose the kernel’s hyperparameters based on gradient-ascent on the marginal
likelihood function while KRR needs to perform a grid search on a cross-validated loss function (mean-squared error
loss). A further difference is that GPR learns a generative, probabilistic model of the target function and can thus
provide meaningful conﬁdence intervals and posterior samples along with the predictions while KRR only provides
predictions.
The following ﬁgure illustrates both methods on an artiﬁcial dataset, which consists of a sinusoidal target function
and strong noise. The ﬁgure compares the learned model of KRR and GPR based on a ExpSineSquared kernel,
which is suited for learning periodic functions. The kernel’s hyperparameters control the smoothness (length_scale)
and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an
additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.
The ﬁgure shows that both methods learn reasonable models of the target function. GPR correctly identiﬁes the peri-
odicity of the function to be roughly 2*𝜋(6.28), while KRR chooses the doubled periodicity 4*𝜋. Besides that, GPR
provides reasonable conﬁdence bounds on the prediction which are not available for KRR. A major difference between
the two methods is the time required for ﬁtting and predicting: while ﬁtting KRR is fast in principle, the grid-search
for hyperparameter optimization scales exponentially with the number of hyperparameters (“curse of dimensional-
ity”). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is
thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar;
however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting
the mean.
GPR on Mauna Loa CO2 data
This example is based on Section 5.4.3 of [RW2006]. It illustrates an example of complex kernel engineering and
hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly
average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Obser-
vatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time
3.1. Supervised learning
195
scikit-learn user guide, Release 0.18.2
t.
The kernel is composed of several terms that are responsible for explaining different properties of the signal:
• a long term, smooth rising trend is to be explained by an RBF kernel. The RBF kernel with a large length-scale
enforces this component to be smooth; it is not enforced that the trend is rising which leaves this choice to the
GP. The speciﬁc length-scale and the amplitude are free hyperparameters.
• a seasonal component, which is to be explained by the periodic ExpSineSquared kernel with a ﬁxed periodicity
of 1 year. The length-scale of this periodic component, controlling its smoothness, is a free parameter. In order
to allow decaying away from exact periodicity, the product with an RBF kernel is taken. The length-scale of this
RBF component controls the decay time and is a further free parameter.
• smaller, medium term irregularities are to be explained by a RationalQuadratic kernel component, whose length-
scale and alpha parameter, which determines the diffuseness of the length-scales, are to be determined. Ac-
cording to [RW2006], these irregularities can better be explained by a RationalQuadratic than an RBF kernel
component, probably because it can accommodate several length-scales.
• a “noise” term, consisting of an RBF kernel contribution, which shall explain the correlated noise components
such as local weather phenomena, and a WhiteKernel contribution for the white noise. The relative amplitudes
and the RBF’s length scale are further free parameters.
Maximizing the log-marginal-likelihood after subtracting the target’s mean yields the following kernel with an LML
of -83.214:
34.4**2 * RBF(length_scale=41.8)
+ 3.27**2 * RBF(length_scale=180) * ExpSineSquared(length_scale=1.44,
periodicity=1)
+ 0.446**2 * RationalQuadratic(alpha=17.7, length_scale=0.957)
+ 0.197**2 * RBF(length_scale=0.138) + WhiteKernel(noise_level=0.0336)
Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The
periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long
decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an
amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the
overall noise level is very small, indicating that the data can be very well explained by the model. The ﬁgure shows
also that the model makes very conﬁdent predictions until around 2015
Gaussian Process Classiﬁcation (GPC)
The GaussianProcessClassifier implements Gaussian processes (GP) for classiﬁcation purposes, more
speciﬁcally for probabilistic classiﬁcation, where test predictions take the form of class probabilities. GaussianPro-
cessClassiﬁer places a GP prior on a latent function 𝑓, which is then squashed through a link function to obtain the
probabilistic classiﬁcation. The latent function 𝑓is a so-called nuisance function, whose values are not observed and
are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and 𝑓is removed (inte-
grated out) during prediction. GaussianProcessClassiﬁer implements the logistic link function, for which the integral
cannot be computed analytically but is easily approximated in the binary case.
In contrast to the regression setting, the posterior of the latent function 𝑓is not Gaussian even for a GP prior since
a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to
the logistic link function (logit) is used. GaussianProcessClassiﬁer approximates the non-Gaussian posterior with a
Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of [RW2006].
The GP prior mean is assumed to be zero. The prior’s covariance is speciﬁed by a passing a kernel object. The hyper-
parameters of the kernel are optimized during ﬁtting of GaussianProcessRegressor by maximizing the log-marginal-
likelihood (LML) based on the passed optimizer. As the LML may have multiple local optima, the optimizer can
be started repeatedly by specifying n_restarts_optimizer. The ﬁrst run is always conducted starting from the
initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been
196
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
197
scikit-learn user guide, Release 0.18.2
chosen randomly from the range of allowed values. If the initial hyperparameters should be kept ﬁxed, None can be
passed as optimizer.
GaussianProcessClassifier supports multi-class classiﬁcation by performing either one-versus-rest or one-
versus-one based training and prediction. In one-versus-rest, one binary Gaussian process classiﬁer is ﬁtted for each
class, which is trained to separate this class from the rest. In “one_vs_one”, one binary Gaussian process classiﬁer is
ﬁtted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors
are combined into multi-class predictions. See the section on multi-class classiﬁcation for more details.
In the case of Gaussian process classiﬁcation, “one_vs_one” might be computationally cheaper since it has to solve
many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since
Gaussian process classiﬁcation scales cubically with the size of the dataset, this might be considerably faster. How-
ever, note that “one_vs_one” does not support predicting probability estimates but only plain predictions. Moreover,
note that GaussianProcessClassifier does not (yet) implement a true multi-class Laplace approximation in-
ternally, but as discussed aboved is based on solving several binary classiﬁcation tasks internally, which are combined
using one-versus-rest or one-versus-one.
GPC examples
Probabilistic predictions with GPC
This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparam-
eters. The ﬁrst ﬁgure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the
hyperparameters corresponding to the maximum log-marginal-likelihood (LML).
While the hyperparameters chosen by optimizing LML have a considerable larger LML, they perform slightly worse
according to the log-loss on test data. The ﬁgure shows that this is because they exhibit a steep change of the class
probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the
class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by
GPC.
The second ﬁgure shows the log-marginal-likelihood for different choices of the kernel’s hyperparameters, highlighting
the two choices of the hyperparameters used in the ﬁrst ﬁgure by black dots.
Illustration of GPC on the XOR dataset
This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary
kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because
the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as
RBF often obtain better results.
Gaussian process classiﬁcation (GPC) on iris dataset
This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-
dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classiﬁcation. The
anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two
feature dimensions.
Kernels for Gaussian Processes
Kernels (also called “covariance functions” in the context of GPs) are a crucial ingredient of GPs which determine
the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by deﬁning the
198
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
199
scikit-learn user guide, Release 0.18.2
200
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
“similarity” of two datapoints combined with the assumption that similar datapoints should have similar target values.
Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints
and not on their absolute values 𝑘(𝑥𝑖, 𝑥𝑗) = 𝑘(𝑑(𝑥𝑖, 𝑥𝑗)) and are thus invariant to translations in the input space,
while non-stationary kernels depend also on the speciﬁc values of the datapoints. Stationary kernels can further be
subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input
space. For more details, we refer to Chapter 4 of [RW2006].
Gaussian Process Kernel API
The main usage of a Kernel is to compute the GP’s covariance between datapoints. For this, the method __call__
of the kernel can be called. This method can either be used to compute the “auto-covariance” of all pairs of datapoints
in a 2d array X, or the “cross-covariance” of all combinations of datapoints of a 2d array X with datapoints in a 2d
array Y. The following identity holds true for all kernels k (except for the WhiteKernel): k(X) == K(X,Y=X)
If only the diagonal of the auto-covariance is being used, the method diag() of a kernel can be called, which is more
computationally efﬁcient than the equivalent call to __call__: np.diag(k(X,X)) == k.diag(X)
Kernels are parameterized by a vector 𝜃of hyperparameters. These hyperparameters can for instance control length-
scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of of the kernel’s
auto-covariance with respect to 𝜃via setting eval_gradient=True in the __call__ method. This gradient is
used by the Gaussian process (both regressor and classiﬁer) in computing the gradient of the log-marginal-likelihood,
which in turn is used to determine the value of 𝜃, which maximizes the log-marginal-likelihood, via gradient ascent.
For each hyperparameter, the initial value and the bounds need to be speciﬁed when creating an instance of the kernel.
The current value of 𝜃can be get and set via the property theta of the kernel object. Moreover, the bounds of the
hyperparameters can be accessed by the property bounds of the kernel. Note that both properties (theta and bounds)
return log-transformed values of the internally used values since those are typically more amenable to gradient-based
optimization. The speciﬁcation of each hyperparameter is stored in the form of an instance of Hyperparameter
in the respective kernel. Note that a kernel using a hyperparameter with name “x” must have the attributes self.x and
self.x_bounds.
The abstract base class for all kernels is Kernel. Kernel implements a similar interface as Estimator, providing
the methods get_params(), set_params(), and clone(). This allows setting kernel values also via meta-
estimators such as Pipeline or GridSearch. Note that due to the nested structure of kernels (by applying kernel
3.1. Supervised learning
201
scikit-learn user guide, Release 0.18.2
operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary
kernel operator, parameters of the left operand are preﬁxed with k1__ and parameters of the right operand with k2__.
An additional convenience method is clone_with_theta(theta), which returns a cloned version of the kernel
but with the hyperparameters set to theta. An illustrative example:
>>> from sklearn.gaussian_process.kernels import ConstantKernel, RBF
>>> kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) *
˓→RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0,
˓→length_scale_bounds=(0.0, 10.0))
>>> for hyperparameter in kernel.hyperparameters: print(hyperparameter)
Hyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[
˓→0.,
10.]]), n_elements=1, fixed=False)
Hyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[
0.,
˓→
10.]]), n_elements=1, fixed=False)
Hyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[
0.,
˓→10.]]), n_elements=1, fixed=False)
>>> params = kernel.get_params()
>>> for key in sorted(params): print("%s : %s" % (key, params[key]))
k1 : 1**2 * RBF(length_scale=0.5)
k1__k1 : 1**2
k1__k1__constant_value : 1.0
k1__k1__constant_value_bounds : (0.0, 10.0)
k1__k2 : RBF(length_scale=0.5)
k1__k2__length_scale : 0.5
k1__k2__length_scale_bounds : (0.0, 10.0)
k2 : RBF(length_scale=2)
k2__length_scale : 2.0
k2__length_scale_bounds : (0.0, 10.0)
>>> print(kernel.theta)
# Note: log-transformed
[ 0.
-0.69314718
0.69314718]
>>> print(kernel.bounds)
# Note: log-transformed
[[
-inf
2.30258509]
[
-inf
2.30258509]
[
-inf
2.30258509]]
All Gaussian process kernels are interoperable with sklearn.metrics.pairwise and vice versa: instances
of subclasses of Kernel can be passed as metric to pairwise_kernels‘‘ from sklearn.metrics.pairwise.
Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class PairwiseKernel.
The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support
only isotropic distances. The parameter gamma is considered to be a hyperparameter and may be optimized. The other
kernel parameters are set directly at initialization and are kept ﬁxed.
Basic kernels
The ConstantKernel kernel can be used as part of a Product kernel where it scales the magnitude of the other
factor (kernel) or as part of a Sum kernel, where it modiﬁes the mean of the Gaussian process. It depends on a
parameter 𝑐𝑜𝑛𝑠𝑡𝑎𝑛𝑡_𝑣𝑎𝑙𝑢𝑒. It is deﬁned as:
𝑘(𝑥𝑖, 𝑥𝑗) = 𝑐𝑜𝑛𝑠𝑡𝑎𝑛𝑡_𝑣𝑎𝑙𝑢𝑒∀𝑥1, 𝑥2
The main use-case of the WhiteKernel kernel is as part of a sum-kernel where it explains the noise-component of
the signal. Tuning its parameter 𝑛𝑜𝑖𝑠𝑒_𝑙𝑒𝑣𝑒𝑙corresponds to estimating the noise-level. It is deﬁned as:e
𝑘(𝑥𝑖, 𝑥𝑗) = 𝑛𝑜𝑖𝑠𝑒_𝑙𝑒𝑣𝑒𝑙if 𝑥𝑖== 𝑥𝑗else 0
202
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Kernel operators
Kernel operators take one or two base kernels and combine them into a new kernel. The Sum kernel takes two kernels
𝑘1 and 𝑘2 and combines them via 𝑘𝑠𝑢𝑚(𝑋, 𝑌) = 𝑘1(𝑋, 𝑌) + 𝑘2(𝑋, 𝑌). The Product kernel takes two kernels 𝑘1
and 𝑘2 and combines them via 𝑘𝑝𝑟𝑜𝑑𝑢𝑐𝑡(𝑋, 𝑌) = 𝑘1(𝑋, 𝑌) * 𝑘2(𝑋, 𝑌). The Exponentiation kernel takes one
base kernel and a scalar parameter 𝑒𝑥𝑝𝑜𝑛𝑒𝑛𝑡and combines them via 𝑘𝑒𝑥𝑝(𝑋, 𝑌) = 𝑘(𝑋, 𝑌)exponent.
Radial-basis function (RBF) kernel
The RBF kernel is a stationary kernel. It is also known as the “squared exponential” kernel. It is parameterized by a
length-scale parameter 𝑙> 0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same
number of dimensions as the inputs 𝑥(anisotropic variant of the kernel). The kernel is given by:
𝑘(𝑥𝑖, 𝑥𝑗) = exp
(︂
−1
2𝑑(𝑥𝑖/𝑙, 𝑥𝑗/𝑙)2
)︂
This kernel is inﬁnitely differentiable, which implies that GPs with this kernel as covariance function have mean square
derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are
shown in the following ﬁgure:
Matérn kernel
The Matern kernel is a stationary kernel and a generalization of the RBF kernel. It has an additional parameter 𝜈
which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter 𝑙> 0, which
can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs 𝑥
(anisotropic variant of the kernel). The kernel is given by:
𝑘(𝑥𝑖, 𝑥𝑗) = 𝜎2
1
Γ(𝜈)2𝜈−1
(︃
𝛾
√
2𝜈𝑑(𝑥𝑖/𝑙, 𝑥𝑗/𝑙)
)︃𝜈
𝐾𝜈
(︃
𝛾
√
2𝜈𝑑(𝑥𝑖/𝑙, 𝑥𝑗/𝑙)
)︃
,
As 𝜈→∞, the Matérn kernel converges to the RBF kernel. When 𝜈= 1/2, the Matérn kernel becomes identical to
the absolute exponential kernel, i.e.,
𝑘(𝑥𝑖, 𝑥𝑗) = 𝜎2 exp
(︃
−𝛾𝑑(𝑥𝑖/𝑙, 𝑥𝑗/𝑙)
)︃
𝜈= 1
2
In particular, 𝜈= 3/2:
𝑘(𝑥𝑖, 𝑥𝑗) = 𝜎2
(︃
1 + 𝛾
√
3𝑑(𝑥𝑖/𝑙, 𝑥𝑗/𝑙)
)︃
exp
(︃
−𝛾
√
3𝑑(𝑥𝑖/𝑙, 𝑥𝑗/𝑙)
)︃
𝜈= 3
2
and 𝜈= 5/2:
𝑘(𝑥𝑖, 𝑥𝑗) = 𝜎2
(︃
1 + 𝛾
√
5𝑑(𝑥𝑖/𝑙, 𝑥𝑗/𝑙) + 5
3𝛾2𝑑(𝑥𝑖/𝑙, 𝑥𝑗/𝑙)2
)︃
exp
(︃
−𝛾
√
5𝑑(𝑥𝑖/𝑙, 𝑥𝑗/𝑙)
)︃
𝜈= 5
2
are popular choices for learning functions that are not inﬁnitely differentiable (as assumed by the RBF kernel) but at
least once (𝜈= 3/2) or twice differentiable (𝜈= 5/2).
The ﬂexibility of controlling the smoothness of the learned function via 𝜈allows adapting to the properties of the
true underlying functional relation. The prior and posterior of a GP resulting from a Matérn kernel are shown in the
following ﬁgure:
See [RW2006], pp84 for further details regarding the different variants of the Matérn kernel.
3.1. Supervised learning
203
scikit-learn user guide, Release 0.18.2
204
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
205
scikit-learn user guide, Release 0.18.2
Rational quadratic kernel
The RationalQuadratic kernel can be seen as a scale mixture (an inﬁnite sum) of RBF kernels with different
characteristic length-scales. It is parameterized by a length-scale parameter 𝑙> 0 and a scale mixture parameter 𝛼> 0
Only the isotropic variant where 𝑙is a scalar is supported at the moment. The kernel is given by:
𝑘(𝑥𝑖, 𝑥𝑗) =
(︂
1 + 𝑑(𝑥𝑖, 𝑥𝑗)2
2𝛼𝑙2
)︂𝛼
The prior and posterior of a GP resulting from an RBF kernel are shown in the following ﬁgure:
206
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Exp-Sine-Squared kernel
The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter
𝑙> 0 and a periodicity parameter 𝑝> 0. Only the isotropic variant where 𝑙is a scalar is supported at the moment.
The kernel is given by:
𝑘(𝑥𝑖, 𝑥𝑗) = exp (−2sin(𝜋/𝑝* 𝑑(𝑥𝑖, 𝑥𝑗))/𝑙)2
The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following ﬁgure:
3.1. Supervised learning
207
scikit-learn user guide, Release 0.18.2
Dot-Product kernel
The DotProduct kernel is non-stationary and can be obtained from linear regression by putting 𝑁(0, 1) priors on
the coefﬁcients of 𝑥𝑑(𝑑= 1, ..., 𝐷) and a prior of 𝑁(0, 𝜎2
0) on the bias. The DotProduct kernel is invariant to a
rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter 𝜎2
0. For 𝜎2
0 = 0,
the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by
𝑘(𝑥𝑖, 𝑥𝑗) = 𝜎2
0 + 𝑥𝑖· 𝑥𝑗
The DotProduct kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the
following ﬁgure:
208
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
References
• [RW2006] Gaussian Processes for Machine Learning, Carl Eduard Rasmussen and Christopher K.I. Williams,
MIT Press 2006. Link to an ofﬁcial complete PDF version of the book here .
Legacy Gaussian Processes
In this section, the implementation of Gaussian processes used in scikit-learn until release 0.16.1 is described. Note
that this implementation is deprecated and will be removed in version 0.18.
An introductory regression example
Say we want to surrogate the function 𝑔(𝑥) = 𝑥sin(𝑥). To do so, the function is evaluated onto a design of experi-
ments. Then, we deﬁne a GaussianProcess model whose regression and correlation models might be speciﬁed using
additional kwargs, and ask for the model to be ﬁtted to the data. Depending on the number of parameters provided at
instantiation, the ﬁtting procedure may recourse to maximum likelihood estimation for the parameters or alternatively
it uses the given parameters.
>>> import numpy as np
>>> from sklearn import gaussian_process
>>> def f(x):
...
return x * np.sin(x)
>>> X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T
>>> y = f(X).ravel()
>>> x = np.atleast_2d(np.linspace(0, 10, 1000)).T
>>> gp = gaussian_process.GaussianProcess(theta0=1e-2, thetaL=1e-4, thetaU=1e-1)
>>> gp.fit(X, y)
GaussianProcess(beta0=None, corr=<function squared_exponential at 0x...>,
normalize=True, nugget=array(2.22...-15),
optimizer='fmin_cobyla', random_start=1, random_state=...
regr=<function constant at 0x...>, storage_mode='full',
theta0=array([[ 0.01]]), thetaL=array([[ 0.0001]]),
thetaU=array([[ 0.1]]), verbose=False)
>>> y_pred, sigma2_pred = gp.predict(x, eval_MSE=True)
Fitting Noisy Data
When the data to be ﬁt includes noise, the Gaussian process model can be used by specifying the variance of the noise
for each point. GaussianProcess takes a parameter nugget which is added to the diagonal of the correlation
matrix between training points: in general this is a type of Tikhonov regularization. In the special case of a squared-
exponential correlation function, this normalization is equivalent to specifying a fractional variance in the input. That
is
nugget𝑖=
[︂𝜎𝑖
𝑦𝑖
]︂2
With nugget and corr properly set, Gaussian Processes can be used to robustly recover an underlying function
from noisy data.
3.1. Supervised learning
209
scikit-learn user guide, Release 0.18.2
Mathematical formulation
The initial assumption
Suppose one wants to model the output of a computer experiment, say a mathematical function:
𝑔:R𝑛features →R
𝑋↦→𝑦= 𝑔(𝑋)
GPML starts with the assumption that this function is a conditional sample path of a Gaussian process 𝐺which is
additionally assumed to read as follows:
𝐺(𝑋) = 𝑓(𝑋)𝑇𝛽+ 𝑍(𝑋)
where 𝑓(𝑋)𝑇𝛽is a linear regression model and 𝑍(𝑋) is a zero-mean Gaussian process with a fully stationary covari-
ance function:
𝐶(𝑋, 𝑋′) = 𝜎2𝑅(|𝑋−𝑋′|)
𝜎2 being its variance and 𝑅being the correlation function which solely depends on the absolute relative distance
between each sample, possibly featurewise (this is the stationarity assumption).
From this basic formulation, note that GPML is nothing but an extension of a basic least squares linear regression
problem:
𝑔(𝑋) ≈𝑓(𝑋)𝑇𝛽
Except we additionally assume some spatial coherence (correlation) between the samples dictated by the correlation
function. Indeed, ordinary least squares assumes the correlation model 𝑅(|𝑋−𝑋′|) is one when 𝑋= 𝑋′ and zero
otherwise : a dirac correlation model – sometimes referred to as a nugget correlation model in the kriging literature.
The best linear unbiased prediction (BLUP)
We now derive the best linear unbiased prediction of the sample path 𝑔conditioned on the observations:
ˆ𝐺(𝑋) = 𝐺(𝑋|𝑦1 = 𝑔(𝑋1), ..., 𝑦𝑛samples = 𝑔(𝑋𝑛samples))
It is derived from its given properties:
• It is linear (a linear combination of the observations)
ˆ𝐺(𝑋) ≡𝑎(𝑋)𝑇𝑦
• It is unbiased
E[𝐺(𝑋) −ˆ𝐺(𝑋)] = 0
• It is the best (in the Mean Squared Error sense)
ˆ𝐺(𝑋)* = arg min
^
𝐺(𝑋)
E[(𝐺(𝑋) −ˆ𝐺(𝑋))2]
So that the optimal weight vector 𝑎(𝑋) is solution of the following equality constrained optimization problem:
𝑎(𝑋)* = arg min
𝑎(𝑋) E[(𝐺(𝑋) −𝑎(𝑋)𝑇𝑦)2]
s.t. E[𝐺(𝑋) −𝑎(𝑋)𝑇𝑦] = 0
210
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Rewriting this constrained optimization problem in the form of a Lagrangian and looking further for the ﬁrst order
optimality conditions to be satisﬁed, one ends up with a closed form expression for the sought predictor – see references
for the complete proof.
In the end, the BLUP is shown to be a Gaussian random variate with mean:
𝜇^𝑌(𝑋) = 𝑓(𝑋)𝑇ˆ𝛽+ 𝑟(𝑋)𝑇𝛾
and variance:
𝜎2
^𝑌(𝑋) = 𝜎2
𝑌(1 −𝑟(𝑋)𝑇𝑅−1 𝑟(𝑋) + 𝑢(𝑋)𝑇(𝐹𝑇𝑅−1 𝐹)−1 𝑢(𝑋))
where we have introduced:
• the correlation matrix whose terms are deﬁned wrt the autocorrelation function and its built-in parameters 𝜃:
𝑅𝑖𝑗= 𝑅(|𝑋𝑖−𝑋𝑗|, 𝜃), 𝑖, 𝑗= 1, ..., 𝑚
• the vector of cross-correlations between the point where the prediction is made and the points in the DOE:
𝑟𝑖= 𝑅(|𝑋−𝑋𝑖|, 𝜃), 𝑖= 1, ..., 𝑚
• the regression matrix (eg the Vandermonde matrix if 𝑓is a polynomial basis):
𝐹𝑖𝑗= 𝑓𝑖(𝑋𝑗), 𝑖= 1, ..., 𝑝, 𝑗= 1, ..., 𝑚
• the generalized least square regression weights:
ˆ𝛽= (𝐹𝑇𝑅−1 𝐹)−1 𝐹𝑇𝑅−1 𝑌
• and the vectors:
𝛾= 𝑅−1(𝑌−𝐹ˆ𝛽)
𝑢(𝑋) = 𝐹𝑇𝑅−1 𝑟(𝑋) −𝑓(𝑋)
It is important to notice that the probabilistic response of a Gaussian Process predictor is fully analytic and mostly relies
on basic linear algebra operations. More precisely the mean prediction is the sum of two simple linear combinations
(dot products), and the variance requires two matrix inversions, but the correlation matrix can be decomposed only
once using a Cholesky decomposition algorithm.
The empirical best linear unbiased predictor (EBLUP)
Until now, both the autocorrelation and regression models were assumed given. In practice however they are never
known in advance so that one has to make (motivated) empirical choices for these models Correlation Models.
Provided these choices are made, one should estimate the remaining unknown parameters involved in the BLUP. To
do so, one uses the set of provided observations in conjunction with some inference technique. The present implemen-
tation, which is based on the DACE’s Matlab toolbox uses the maximum likelihood estimation technique – see DACE
manual in references for the complete equations. This maximum likelihood estimation problem is turned into a global
optimization problem onto the autocorrelation parameters. In the present implementation, this global optimization is
solved by means of the fmin_cobyla optimization function from scipy.optimize. In the case of anisotropy however, we
provide an implementation of Welch’s componentwise optimization algorithm – see references.
3.1. Supervised learning
211
scikit-learn user guide, Release 0.18.2
Correlation Models
Common correlation models matches some famous SVM’s kernels because they are mostly built on equivalent as-
sumptions. They must fulﬁll Mercer’s conditions and should additionally remain stationary. Note however, that the
choice of the correlation model should be made in agreement with the known properties of the original experiment
from which the observations come. For instance:
• If the original experiment is known to be inﬁnitely differentiable (smooth), then one should use the squared-
exponential correlation model.
• If it’s not, then one should rather use the exponential correlation model.
• Note also that there exists a correlation model that takes the degree of derivability as input: this is the Matern
correlation model, but it’s not implemented here (TODO).
For a more detailed discussion on the selection of appropriate correlation models, see the book by Rasmussen &
Williams in references.
Regression Models
Common linear regression models involve zero- (constant), ﬁrst- and second-order polynomials. But one may specify
its own in the form of a Python function that takes the features X as input and that returns a vector containing the
values of the functional set. The only constraint is that the number of functions must not exceed the number of
available observations so that the underlying regression problem is not underdetermined.
Implementation details
The implementation is based on a translation of the DACE Matlab toolbox.
References:
• DACE, A Matlab Kriging Toolbox S Lophaven, HB Nielsen, J Sondergaard 2002,
• W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell, and M.D. Morris (1992). Screening, predicting,
and computer experiments. Technometrics, 34(1) 15–25.
3.1.8 Cross decomposition
The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the
canonical correlation analysis (CCA).
These families of algorithms are useful to ﬁnd linear relations between two multivariate datasets: the X and Y argu-
ments of the fit method are 2D arrays.
Cross decomposition algorithms ﬁnd the fundamental relations between two matrices (X and Y). They are latent
variable approaches to modeling the covariance structures in these two spaces. They will try to ﬁnd the multidi-
mensional direction in the X space that explains the maximum multidimensional variance direction in the Y space.
PLS-regression is particularly suited when the matrix of predictors has more variables than observations, and when
there is multicollinearity among X values. By contrast, standard regression will fail in these cases.
Classes included in this module are PLSRegression PLSCanonical, CCA and PLSSVD
212
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Reference:
• JA Wegelin A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case
Examples:
• Compare cross decomposition methods
3.1.9 Naive Bayes
Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive”
assumption of independence between every pair of features. Given a class variable 𝑦and a dependent feature vector
𝑥1 through 𝑥𝑛, Bayes’ theorem states the following relationship:
𝑃(𝑦| 𝑥1, . . . , 𝑥𝑛) = 𝑃(𝑦)𝑃(𝑥1, . . . 𝑥𝑛| 𝑦)
𝑃(𝑥1, . . . , 𝑥𝑛)
Using the naive independence assumption that
𝑃(𝑥𝑖|𝑦, 𝑥1, . . . , 𝑥𝑖−1, 𝑥𝑖+1, . . . , 𝑥𝑛) = 𝑃(𝑥𝑖|𝑦),
for all 𝑖, this relationship is simpliﬁed to
𝑃(𝑦| 𝑥1, . . . , 𝑥𝑛) = 𝑃(𝑦) ∏︀𝑛
𝑖=1 𝑃(𝑥𝑖| 𝑦)
𝑃(𝑥1, . . . , 𝑥𝑛)
3.1. Supervised learning
213
scikit-learn user guide, Release 0.18.2
Since 𝑃(𝑥1, . . . , 𝑥𝑛) is constant given the input, we can use the following classiﬁcation rule:
𝑃(𝑦| 𝑥1, . . . , 𝑥𝑛) ∝𝑃(𝑦)
𝑛
∏︁
𝑖=1
𝑃(𝑥𝑖| 𝑦)
⇓
ˆ𝑦= arg max
𝑦
𝑃(𝑦)
𝑛
∏︁
𝑖=1
𝑃(𝑥𝑖| 𝑦),
and we can use Maximum A Posteriori (MAP) estimation to estimate 𝑃(𝑦) and 𝑃(𝑥𝑖| 𝑦); the former is then the
relative frequency of class 𝑦in the training set.
The different naive Bayes classiﬁers differ mainly by the assumptions they make regarding the distribution of 𝑃(𝑥𝑖|
𝑦).
In spite of their apparently over-simpliﬁed assumptions, naive Bayes classiﬁers have worked quite well in many real-
world situations, famously document classiﬁcation and spam ﬁltering. They require a small amount of training data to
estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data
it does, see the references below.)
Naive Bayes learners and classiﬁers can be extremely fast compared to more sophisticated methods. The decoupling
of the class conditional feature distributions means that each distribution can be independently estimated as a one
dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.
On the ﬂip side, although naive Bayes is known as a decent classiﬁer, it is known to be a bad estimator, so the
probability outputs from predict_proba are not to be taken too seriously.
References:
• H. Zhang (2004). The optimality of Naive Bayes. Proc. FLAIRS.
Gaussian Naive Bayes
GaussianNB implements the Gaussian Naive Bayes algorithm for classiﬁcation. The likelihood of the features is
assumed to be Gaussian:
𝑃(𝑥𝑖| 𝑦) =
1
√︁
2𝜋𝜎2𝑦
exp
(︂
−(𝑥𝑖−𝜇𝑦)2
2𝜎2𝑦
)︂
The parameters 𝜎𝑦and 𝜇𝑦are estimated using maximum likelihood.
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> from sklearn.naive_bayes import GaussianNB
>>> gnb = GaussianNB()
>>> y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)
>>> print("Number of mislabeled points out of a total %d points : %d"
...
% (iris.data.shape[0],(iris.target != y_pred).sum()))
Number of mislabeled points out of a total 150 points : 6
Multinomial Naive Bayes
MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two
classic naive Bayes variants used in text classiﬁcation (where the data are typically represented as word vector counts,
214
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors 𝜃𝑦=
(𝜃𝑦1, . . . , 𝜃𝑦𝑛) for each class 𝑦, where 𝑛is the number of features (in text classiﬁcation, the size of the vocabulary)
and 𝜃𝑦𝑖is the probability 𝑃(𝑥𝑖| 𝑦) of feature 𝑖appearing in a sample belonging to class 𝑦.
The parameters 𝜃𝑦is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:
ˆ𝜃𝑦𝑖= 𝑁𝑦𝑖+ 𝛼
𝑁𝑦+ 𝛼𝑛
where 𝑁𝑦𝑖= ∑︀
𝑥∈𝑇𝑥𝑖is the number of times feature 𝑖appears in a sample of class 𝑦in the training set 𝑇, and
𝑁𝑦= ∑︀|𝑇|
𝑖=1 𝑁𝑦𝑖is the total count of all features for class 𝑦.
The smoothing priors 𝛼≥0 accounts for features not present in the learning samples and prevents zero probabilities
in further computations. Setting 𝛼= 1 is called Laplace smoothing, while 𝛼< 1 is called Lidstone smoothing.
Bernoulli Naive Bayes
BernoulliNB implements the naive Bayes training and classiﬁcation algorithms for data that is distributed ac-
cording to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a
binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued
feature vectors; if handed any other kind of data, a BernoulliNB instance may binarize its input (depending on the
binarize parameter).
The decision rule for Bernoulli naive Bayes is based on
𝑃(𝑥𝑖| 𝑦) = 𝑃(𝑖| 𝑦)𝑥𝑖+ (1 −𝑃(𝑖| 𝑦))(1 −𝑥𝑖)
which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature 𝑖that is an
indicator for class 𝑦, where the multinomial variant would simply ignore a non-occurring feature.
In the case of text classiﬁcation, word occurrence vectors (rather than word count vectors) may be used to train and
use this classiﬁer. BernoulliNB might perform better on some datasets, especially those with shorter documents.
It is advisable to evaluate both models, if time permits.
References:
• C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge Uni-
versity Press, pp. 234-265.
• A. McCallum and K. Nigam (1998). A comparison of event models for Naive Bayes text classiﬁcation. Proc.
AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.
• V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam ﬁltering with Naive Bayes – Which Naive
Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).
Out-of-core naive Bayes model ﬁtting
Naive Bayes models can be used to tackle large scale classiﬁcation problems for which the full training set might not ﬁt
in memory. To handle this case, MultinomialNB, BernoulliNB, and GaussianNB expose a partial_fit
method that can be used incrementally as done with other classiﬁers as demonstrated in Out-of-core classiﬁcation of
text documents. All naive Bayes classiﬁers support sample weighting.
Contrary to the fit method, the ﬁrst call to partial_fit needs to be passed the list of all the expected class labels.
For an overview of available strategies in scikit-learn, see also the out-of-core learning documentation.
3.1. Supervised learning
215
scikit-learn user guide, Release 0.18.2
Note:
The partial_fit method call of naive Bayes models introduces some computational overhead. It is
recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows.
3.1.10 Decision Trees
Decision Trees (DTs) are a non-parametric supervised learning method used for classiﬁcation and regression. The
goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the
data features.
For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else
decision rules. The deeper the tree, the more complex the decision rules and the ﬁtter the model.
Some advantages of decision trees are:
• Simple to understand and to interpret. Trees can be visualised.
• Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be
created and blank values to be removed. Note however that this module does not support missing values.
• The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.
• Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing
datasets that have only one type of variable. See algorithms for more information.
• Able to handle multi-output problems.
• Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily
explained by boolean logic. By contrast, in a black box model (e.g., in an artiﬁcial neural network), results may
be more difﬁcult to interpret.
• Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the
model.
216
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• Performs well even if its assumptions are somewhat violated by the true model from which the data were
generated.
The disadvantages of decision trees include:
• Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overﬁt-
ting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required
at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.
• Decision trees can be unstable because small variations in the data might result in a completely different tree
being generated. This problem is mitigated by using decision trees within an ensemble.
• The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality
and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic
algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms
cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in
an ensemble learner, where the features and samples are randomly sampled with replacement.
• There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity
or multiplexer problems.
• Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the
dataset prior to ﬁtting with the decision tree.
Classiﬁcation
DecisionTreeClassifier is a class capable of performing multi-class classiﬁcation on a dataset.
As with other classiﬁers, DecisionTreeClassifier takes as input two arrays: an array X, sparse or dense,
of size [n_samples,n_features] holding the training samples, and an array Y of integer values, size
[n_samples], holding the class labels for the training samples:
>>> from sklearn import tree
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(X, Y)
After being ﬁtted, the model can then be used to predict the class of samples:
>>> clf.predict([[2., 2.]])
array([1])
Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class
in a leaf:
>>> clf.predict_proba([[2., 2.]])
array([[ 0.,
1.]])
DecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classiﬁcation and multiclass
(where the labels are [0, ..., K-1]) classiﬁcation.
Using the Iris dataset, we can construct a tree as follows:
>>> from sklearn.datasets import load_iris
>>> from sklearn import tree
>>> iris = load_iris()
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(iris.data, iris.target)
3.1. Supervised learning
217
scikit-learn user guide, Release 0.18.2
Once trained, we can export the tree in Graphviz format using the export_graphviz exporter. Below is an example
export of a tree trained on the entire iris dataset:
>>> with open("iris.dot", 'w') as f:
...
f = tree.export_graphviz(clf, out_file=f)
Then we can use Graphviz’s dot tool to create a PDF ﬁle (or any other supported ﬁle type): dot -Tpdf iris.dot
-o iris.pdf.
>>> import os
>>> os.unlink('iris.dot')
Alternatively, if we have Python module pydotplus installed, we can generate a PDF ﬁle (or any other supported
ﬁle type) directly in Python:
>>> import pydotplus
>>> dot_data = tree.export_graphviz(clf, out_file=None)
>>> graph = pydotplus.graph_from_dot_data(dot_data)
>>> graph.write_pdf("iris.pdf")
The export_graphviz exporter also supports a variety of aesthetic options, including coloring nodes by their
class (or value for regression) and using explicit variable and class names if desired. IPython notebooks can also
render these plots inline using the Image() function:
>>> from IPython.display import Image
>>> dot_data = tree.export_graphviz(clf, out_file=None,
feature_names=iris.feature_names,
class_names=iris.target_names,
filled=True, rounded=True,
special_characters=True)
>>> graph = pydotplus.graph_from_dot_data(dot_data)
>>> Image(graph.create_png())
After being ﬁtted, the model can then be used to predict the class of samples:
>>> clf.predict(iris.data[:1, :])
array([0])
Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class
in a leaf:
>>> clf.predict_proba(iris.data[:1, :])
array([[ 1.,
0.,
0.]])
Examples:
• Plot the decision surface of a decision tree on the iris dataset
Regression
Decision trees can also be applied to regression problems, using the DecisionTreeRegressor class.
As in the classiﬁcation setting, the ﬁt method will take as argument arrays X and y, only that in this case y is expected
to have ﬂoating point values instead of integer values:
218
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
petal length (cm) ≤ 2.45
gini = 0.6667
samples = 150
value = [50, 50, 50]
class = setosa
gini = 0.0
samples = 50
value = [50, 0, 0]
class = setosa
True
petal width (cm) ≤ 1.75
gini = 0.5
samples = 100
value = [0, 50, 50]
class = versicolor
False
petal length (cm) ≤ 4.95
gini = 0.168
samples = 54
value = [0, 49, 5]
class = versicolor
petal length (cm) ≤ 4.85
gini = 0.0425
samples = 46
value = [0, 1, 45]
class = virginica
petal width (cm) ≤ 1.65
gini = 0.0408
samples = 48
value = [0, 47, 1]
class = versicolor
petal width (cm) ≤ 1.55
gini = 0.4444
samples = 6
value = [0, 2, 4]
class = virginica
gini = 0.0
samples = 47
value = [0, 47, 0]
class = versicolor
gini = 0.0
samples = 1
value = [0, 0, 1]
class = virginica
gini = 0.0
samples = 3
value = [0, 0, 3]
class = virginica
sepal length (cm) ≤ 6.95
gini = 0.4444
samples = 3
value = [0, 2, 1]
class = versicolor
gini = 0.0
samples = 2
value = [0, 2, 0]
class = versicolor
gini = 0.0
samples = 1
value = [0, 0, 1]
class = virginica
sepal length (cm) ≤ 5.95
gini = 0.4444
samples = 3
value = [0, 1, 2]
class = virginica
gini = 0.0
samples = 43
value = [0, 0, 43]
class = virginica
gini = 0.0
samples = 1
value = [0, 1, 0]
class = versicolor
gini = 0.0
samples = 2
value = [0, 0, 2]
class = virginica
3.1. Supervised learning
219
scikit-learn user guide, Release 0.18.2
>>> from sklearn import tree
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = tree.DecisionTreeRegressor()
>>> clf = clf.fit(X, y)
>>> clf.predict([[1, 1]])
array([ 0.5])
Examples:
• Decision Tree Regression
Multi-output problems
A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array
of size [n_samples,n_outputs].
When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n
independent models, i.e. one for each output, and then to use those models to independently predict each one of the
n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an
often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower
training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may
often be increased.
With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the
following changes:
• Store n output values in leaves, instead of 1;
• Use splitting criteria that compute the average reduction across all n outputs.
220
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
This
module
offers
support
for
multi-output
problems
by
implementing
this
strategy
in
both
DecisionTreeClassifier and DecisionTreeRegressor.
If a decision tree is ﬁt on an output
array Y of size [n_samples,n_outputs] then the resulting estimator will:
• Output n_output values upon predict;
• Output a list of n_output arrays of class probabilities upon predict_proba.
The use of multi-output trees for regression is demonstrated in Multi-output Decision Tree Regression. In this example,
the input X is a single real value and the outputs Y are the sine and cosine of X.
The use of multi-output trees for classiﬁcation is demonstrated in Face completion with a multi-output estimators. In
this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of
those faces.
Examples:
• Multi-output Decision Tree Regression
• Face completion with a multi-output estimators
References:
• M. Dumont et al, Fast multi-class image annotation with random subwindows and multiple output randomized
trees, International Conference on Computer Vision Theory and Applications 2009
3.1. Supervised learning
221
scikit-learn user guide, Release 0.18.2
222
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Complexity
In general, the run time cost to construct a balanced binary tree is 𝑂(𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠log(𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠)) and query
time 𝑂(log(𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠)). Although the tree construction algorithm attempts to generate balanced trees, they will not
always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of
searching through 𝑂(𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠) to ﬁnd the feature that offers the largest reduction in entropy. This has a cost of
𝑂(𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠log(𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠)) at each node, leading to a total cost over the entire trees (by summing the cost at
each node) of 𝑂(𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠𝑛2
𝑠𝑎𝑚𝑝𝑙𝑒𝑠log(𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠)).
Scikit-learn offers a more efﬁcient implementation for the construction of decision trees. A naive implementation
(as above) would recompute the class label histograms (for classiﬁcation) or the means (for regression) at for each
new split point along a given feature. Presorting the feature over all relevant samples, and retaining a running la-
bel count, will reduce the complexity at each node to 𝑂(𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠log(𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠)), which results in a total cost of
𝑂(𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠log(𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠)). This is an option for all tree based algorithms. By default it is turned on for
gradient boosting, where in general it makes training faster, but turned off for all other algorithms as it tends to slow
down training when training deep trees.
Tips on practical use
• Decision trees tend to overﬁt on data with a large number of features. Getting the right ratio of samples to
number of features is important, since a tree with few samples in high dimensional space is very likely to
overﬁt.
• Consider performing dimensionality reduction (PCA, ICA, or Feature selection) beforehand to give your tree a
better chance of ﬁnding features that are discriminative.
• Visualise your tree as you are training by using the export function. Use max_depth=3 as an initial tree
depth to get a feel for how the tree is ﬁtting to your data, and then increase the depth.
• Remember that the number of samples required to populate the tree doubles for each additional level the tree
grows to. Use max_depth to control the size of the tree to prevent overﬁtting.
• Use min_samples_split or min_samples_leaf to control the number of samples at a leaf node. A
very small number will usually mean the tree will overﬁt, whereas a large number will prevent the tree from
learning the data. Try min_samples_leaf=5 as an initial value. If the sample size varies greatly, a ﬂoat
number can be used as percentage in these two parameters. The main difference between the two is that
min_samples_leaf guarantees a minimum number of samples in a leaf, while min_samples_split
can create arbitrary small leaves, though min_samples_split is more common in the literature.
• Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant.
Class balancing can be done by sampling an equal number of samples from each class, or preferably by nor-
malizing the sum of the sample weights (sample_weight) for each class to the same value. Also note that
weight-based pre-pruning criteria, such as min_weight_fraction_leaf, will then be less biased toward
dominant classes than criteria that are not aware of the sample weights, like min_samples_leaf.
• If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning
criterion such as min_weight_fraction_leaf, which ensure that leaf nodes contain at least a fraction of
the overall sum of the sample weights.
• All decision trees use np.float32 arrays internally. If training data is not in this format, a copy of the dataset
will be made.
• If the input matrix X is very sparse, it is recommended to convert to sparse csc_matrix before calling ﬁt and
sparse csr_matrix before calling predict. Training time can be orders of magnitude faster for a sparse matrix
input compared to a dense matrix when features have zero values in most of the samples.
3.1. Supervised learning
223
scikit-learn user guide, Release 0.18.2
Tree algorithms: ID3, C4.5, C5.0 and CART
What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented
in scikit-learn?
ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, ﬁnding
for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical
targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the
tree to generalise to unseen data.
C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically deﬁning
a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of
intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy
of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a
rule’s precondition if the accuracy of the rule improves without it.
C5.0 is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets
than C4.5 while being more accurate.
CART (Classiﬁcation and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target
variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold
that yield the largest information gain at each node.
scikit-learn uses an optimised version of the CART algorithm.
Mathematical formulation
Given training vectors 𝑥𝑖∈𝑅𝑛, i=1,..., l and a label vector 𝑦∈𝑅𝑙, a decision tree recursively partitions the space
such that the samples with the same labels are grouped together.
Let the data at node 𝑚be represented by 𝑄. For each candidate split 𝜃= (𝑗, 𝑡𝑚) consisting of a feature 𝑗and threshold
𝑡𝑚, partition the data into 𝑄𝑙𝑒𝑓𝑡(𝜃) and 𝑄𝑟𝑖𝑔ℎ𝑡(𝜃) subsets
𝑄𝑙𝑒𝑓𝑡(𝜃) = (𝑥, 𝑦)|𝑥𝑗<= 𝑡𝑚
𝑄𝑟𝑖𝑔ℎ𝑡(𝜃) = 𝑄∖𝑄𝑙𝑒𝑓𝑡(𝜃)
The impurity at 𝑚is computed using an impurity function 𝐻(), the choice of which depends on the task being solved
(classiﬁcation or regression)
𝐺(𝑄, 𝜃) = 𝑛𝑙𝑒𝑓𝑡
𝑁𝑚
𝐻(𝑄𝑙𝑒𝑓𝑡(𝜃)) + 𝑛𝑟𝑖𝑔ℎ𝑡
𝑁𝑚
𝐻(𝑄𝑟𝑖𝑔ℎ𝑡(𝜃))
Select the parameters that minimises the impurity
𝜃* = argmin𝜃𝐺(𝑄, 𝜃)
Recurse for subsets 𝑄𝑙𝑒𝑓𝑡(𝜃*) and 𝑄𝑟𝑖𝑔ℎ𝑡(𝜃*) until the maximum allowable depth is reached, 𝑁𝑚< min𝑠𝑎𝑚𝑝𝑙𝑒𝑠or
𝑁𝑚= 1.
Classiﬁcation criteria
If a target is a classiﬁcation outcome taking on values 0,1,...,K-1, for node 𝑚, representing a region 𝑅𝑚with 𝑁𝑚
observations, let
𝑝𝑚𝑘= 1/𝑁𝑚
∑︁
𝑥𝑖∈𝑅𝑚
𝐼(𝑦𝑖= 𝑘)
224
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
be the proportion of class k observations in node 𝑚
Common measures of impurity are Gini
𝐻(𝑋𝑚) =
∑︁
𝑘
𝑝𝑚𝑘(1 −𝑝𝑚𝑘)
Cross-Entropy
𝐻(𝑋𝑚) = −
∑︁
𝑘
𝑝𝑚𝑘log(𝑝𝑚𝑘)
and Misclassiﬁcation
𝐻(𝑋𝑚) = 1 −max(𝑝𝑚𝑘)
Regression criteria
If the target is a continuous value, then for node 𝑚, representing a region 𝑅𝑚with 𝑁𝑚observations, a common
criterion to minimise is the Mean Squared Error
𝑐𝑚=
1
𝑁𝑚
∑︁
𝑖∈𝑁𝑚
𝑦𝑖
𝐻(𝑋𝑚) =
1
𝑁𝑚
∑︁
𝑖∈𝑁𝑚
(𝑦𝑖−𝑐𝑚)2
References:
• https://en.wikipedia.org/wiki/Decision_tree_learning
• https://en.wikipedia.org/wiki/Predictive_analytics
• L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression Trees. Wadsworth, Belmont,
CA, 1984.
• J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.
• T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009.
3.1.11 Ensemble methods
The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning
algorithm in order to improve generalizability / robustness over a single estimator.
Two families of ensemble methods are usually distinguished:
• In averaging methods, the driving principle is to build several estimators independently and then to average
their predictions. On average, the combined estimator is usually better than any of the single base estimator
because its variance is reduced.
Examples: Bagging methods, Forests of randomized trees, ...
• By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the
combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.
Examples: AdaBoost, Gradient Tree Boosting, ...
3.1. Supervised learning
225
scikit-learn user guide, Release 0.18.2
Bagging meta-estimator
In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box
estimator on random subsets of the original training set and then aggregate their individual predictions to form a ﬁnal
prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by
introducing randomization into its construction procedure and then making an ensemble out of it. In many cases,
bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary
to adapt the underlying base algorithm. As they provide a way to reduce overﬁtting, bagging methods work best with
strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually
work best with weak models (e.g., shallow decision trees).
Bagging methods come in many ﬂavours but mostly differ from each other by the way they draw random subsets of
the training set:
• When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known
as Pasting [B1999].
• When samples are drawn with replacement, then the method is known as Bagging [B1996].
• When random subsets of the dataset are drawn as random subsets of the features, then the method is known as
Random Subspaces [H1998].
• Finally, when base estimators are built on subsets of both samples and features, then the method is known as
Random Patches [LG2012].
In scikit-learn,
bagging methods are offered as a uniﬁed BaggingClassifier meta-estimator (resp.
BaggingRegressor), taking as input a user-speciﬁed base estimator along with parameters specifying the strategy
to draw random subsets. In particular, max_samples and max_features control the size of the subsets (in terms
of samples and features), while bootstrap and bootstrap_features control whether samples and features
are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can
be estimated with the out-of-bag samples by setting oob_score=True. As an example, the snippet below illustrates
how to instantiate a bagging ensemble of KNeighborsClassifier base estimators, each built on random subsets
of 50% of the samples and 50% of the features.
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> bagging = BaggingClassifier(KNeighborsClassifier(),
...
max_samples=0.5, max_features=0.5)
Examples:
• Single estimator versus bagging: bias-variance decomposition
References
Forests of randomized trees
The sklearn.ensemble module includes two averaging algorithms based on randomized decision trees: the Ran-
domForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques [B1998]
speciﬁcally designed for trees. This means a diverse set of classiﬁers is created by introducing randomness in the
classiﬁer construction. The prediction of the ensemble is given as the averaged prediction of the individual classiﬁers.
226
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
As other classiﬁers, forest classiﬁers have to be ﬁtted with two arrays:
a sparse or dense array X of size
[n_samples,n_features] holding the training samples, and an array Y of size [n_samples] holding the
target values (class labels) for the training samples:
>>> from sklearn.ensemble import RandomForestClassifier
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = RandomForestClassifier(n_estimators=10)
>>> clf = clf.fit(X, Y)
Like decision trees,
forests of trees also extend to multi-output problems (if Y is an array of size
[n_samples,n_outputs]).
Random Forests
In random forests (see RandomForestClassifier and RandomForestRegressor classes), each tree in the
ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition,
when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all
features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this
randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but,
due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding
an overall better model.
In contrast to the original publication [B2001], the scikit-learn implementation combines classiﬁers by averaging their
probabilistic prediction, instead of letting each classiﬁer vote for a single class.
Extremely Randomized Trees
In extremely randomized trees (see ExtraTreesClassifier and ExtraTreesRegressor classes), random-
ness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features
is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candi-
date feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to
reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias:
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import make_blobs
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.ensemble import ExtraTreesClassifier
>>> from sklearn.tree import DecisionTreeClassifier
>>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,
...
random_state=0)
>>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
...
random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean()
0.97...
>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
...
min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean()
0.999...
3.1. Supervised learning
227
scikit-learn user guide, Release 0.18.2
>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
...
min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean() > 0.999
True
Parameters
The main parameters to adjust when using these methods is n_estimators and max_features. The former
is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition,
note that results will stop getting signiﬁcantly better beyond a critical number of trees. The latter is the size of
the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance,
but also the greater the increase in bias. Empirical good default values are max_features=n_features for
regression problems, and max_features=sqrt(n_features) for classiﬁcation tasks (where n_features is
the number of features in the data). Good results are often achieved when setting max_depth=None in combination
with min_samples_split=1 (i.e., when fully developing the trees). Bear in mind though that these values are
usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be
cross-validated. In addition, note that in random forests, bootstrap samples are used by default (bootstrap=True)
while the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using bootstrap
sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by
setting oob_score=True.
Parallelization
Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions
through the n_jobs parameter. If n_jobs=k then computations are partitioned into k jobs, and run on k cores of
the machine. If n_jobs=-1 then all cores available on the machine are used. Note that because of inter-process
communication overhead, the speedup might not be linear (i.e., using k jobs will unfortunately not be k times as fast).
228
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Signiﬁcant speedup can still be achieved though when building a large number of trees, or when building a single tree
requires a fair amount of time (e.g., on large datasets).
Examples:
• Plot the decision surfaces of ensembles of trees on the iris dataset
• Pixel importances with a parallel forest of trees
• Face completion with a multi-output estimators
References
Feature importance evaluation
The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance
of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute
to the ﬁnal prediction decision of a larger fraction of the input samples. The expected fraction of the samples they
contribute to can thus be used as an estimate of the relative importance of the features.
By averaging those expected activity rates over several randomized trees one can reduce the variance of such an
estimate and use it for feature selection.
The following example shows a color-coded representation of the relative importances of each individual pixel for a
face recognition task using a ExtraTreesClassifier model.
In practice those estimates are stored as an attribute named feature_importances_ on the ﬁtted model. This
is an array with shape (n_features,) whose values are positive and sum to 1.0. The higher the value, the more
important is the contribution of the matching feature to the prediction function.
Examples:
• Pixel importances with a parallel forest of trees
• Feature importances with forests of trees
Totally Random Trees Embedding
RandomTreesEmbedding implements an unsupervised transformation of the data. Using a forest of completely
random trees, RandomTreesEmbedding encodes the data by the indices of the leaves a data point ends up in. This
index is then encoded in a one-of-K manner, leading to a high dimensional, sparse binary coding. This coding can be
computed very efﬁciently and can then be used as a basis for other learning tasks. The size and sparsity of the code
can be inﬂuenced by choosing the number of trees and the maximum depth per tree. For each tree in the ensemble, the
coding contains one entry of one. The size of the coding is at most n_estimators * 2 ** max_depth, the
maximum number of leaves in the forest.
As neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit,
non-parametric density estimation.
3.1. Supervised learning
229
scikit-learn user guide, Release 0.18.2
Examples:
• Hashing feature transformation using Totally Random Trees
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap... compares non-linear dimen-
sionality reduction techniques on handwritten digits.
• Feature transformations with ensembles of trees compares supervised and unsupervised tree based feature
transformations.
See also:
Manifold learning techniques can also be useful to derive non-linear representations of feature space, also these ap-
proaches focus also on dimensionality reduction.
AdaBoost
The module sklearn.ensemble includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund
and Schapire [FS1995].
The core principle of AdaBoost is to ﬁt a sequence of weak learners (i.e., models that are only slightly better than
random guessing, such as small decision trees) on repeatedly modiﬁed versions of the data. The predictions from
all of them are then combined through a weighted majority vote (or sum) to produce the ﬁnal prediction. The data
modiﬁcations at each so-called boosting iteration consist of applying weights 𝑤1, 𝑤2, ..., 𝑤𝑁to each of the training
samples. Initially, those weights are all set to 𝑤𝑖= 1/𝑁, so that the ﬁrst step simply trains a weak learner on the
original data. For each successive iteration, the sample weights are individually modiﬁed and the learning algorithm is
230
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted
model induced at the previous step have their weights increased, whereas the weights are decreased for those that were
predicted correctly. As iterations proceed, examples that are difﬁcult to predict receive ever-increasing inﬂuence. Each
subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the
sequence [HTF].
AdaBoost can be used both for classiﬁcation and regression problems:
• For multi-class classiﬁcation, AdaBoostClassifier implements AdaBoost-SAMME and AdaBoost-
SAMME.R [ZZRH2009].
• For regression, AdaBoostRegressor implements AdaBoost.R2 [D1997].
Usage
The following example shows how to ﬁt an AdaBoost classiﬁer with 100 weak learners:
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import AdaBoostClassifier
>>> iris = load_iris()
>>> clf = AdaBoostClassifier(n_estimators=100)
>>> scores = cross_val_score(clf, iris.data, iris.target)
>>> scores.mean()
0.9...
The number of weak learners is controlled by the parameter n_estimators. The learning_rate parameter
controls the contribution of the weak learners in the ﬁnal combination. By default, weak learners are decision stumps.
Different weak learners can be speciﬁed through the base_estimator parameter. The main parameters to tune to
obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or
minimum required number of samples at a leaf min_samples_leaf in case of decision trees).
3.1. Supervised learning
231
scikit-learn user guide, Release 0.18.2
Examples:
• Discrete versus Real AdaBoost compares the classiﬁcation error of a decision stump, decision tree, and a
boosted decision stump using AdaBoost-SAMME and AdaBoost-SAMME.R.
• Multi-class AdaBoosted Decision Trees shows the performance of AdaBoost-SAMME and AdaBoost-
SAMME.R on a multi-class problem.
• Two-class AdaBoost shows the decision boundary and decision function values for a non-linearly separable
two-class problem using AdaBoost-SAMME.
• Decision Tree Regression with AdaBoost demonstrates regression with the AdaBoost.R2 algorithm.
References
Gradient Tree Boosting
Gradient Tree Boosting or Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary
differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both
regression and classiﬁcation problems. Gradient Tree Boosting models are used in a variety of areas including Web
search ranking and ecology.
The advantages of GBRT are:
• Natural handling of data of mixed type (= heterogeneous features)
• Predictive power
• Robustness to outliers in output space (via robust loss functions)
The disadvantages of GBRT are:
• Scalability, due to the sequential nature of boosting it can hardly be parallelized.
The module sklearn.ensemble provides methods for both classiﬁcation and regression via gradient boosted
regression trees.
Classiﬁcation
GradientBoostingClassifier supports both binary and multi-class classiﬁcation. The following example
shows how to ﬁt a gradient boosting classiﬁer with 100 decision stumps as weak learners:
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier
>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]
>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...
max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.913...
232
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The number of weak learners (i.e. regression trees) is controlled by the parameter n_estimators; The size of each
tree can be controlled either by setting the tree depth via max_depth or by setting the number of leaf nodes via
max_leaf_nodes. The learning_rate is a hyper-parameter in the range (0.0, 1.0] that controls overﬁtting via
shrinkage .
Note:
Classiﬁcation with more than 2 classes requires the induction of n_classes regression trees at each
iteration, thus, the total number of induced trees equals n_classes * n_estimators.
For datasets with
a large number of classes we strongly recommend to use RandomForestClassifier as an alternative to
GradientBoostingClassifier .
Regression
GradientBoostingRegressor supports a number of different loss functions for regression which can be speci-
ﬁed via the argument loss; the default loss function for regression is least squares ('ls').
>>> import numpy as np
>>> from sklearn.metrics import mean_squared_error
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
>>> X_train, X_test = X[:200], X[200:]
>>> y_train, y_test = y[:200], y[200:]
>>> est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,
...
max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)
>>> mean_squared_error(y_test, est.predict(X_test))
5.00...
The ﬁgure below shows the results of applying GradientBoostingRegressor with least squares loss and 500
base learners to the Boston house price dataset (sklearn.datasets.load_boston). The plot on the left shows
the train and test error at each iteration. The train error at each iteration is stored in the train_score_ attribute
of the gradient boosting model. The test error at each iterations can be obtained via the staged_predict method
which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal
number of trees (i.e. n_estimators) by early stopping. The plot on the right shows the feature importances which
can be obtained via the feature_importances_ property.
3.1. Supervised learning
233
scikit-learn user guide, Release 0.18.2
Examples:
• Gradient Boosting regression
• Gradient Boosting Out-of-Bag estimates
Fitting additional weak-learners
Both
GradientBoostingRegressor
and
GradientBoostingClassifier
support
warm_start=True which allows you to add more estimators to an already ﬁtted model.
>>> _ = est.set_params(n_estimators=200, warm_start=True)
# set warm_start and new
˓→nr of trees
>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est
>>> mean_squared_error(y_test, est.predict(X_test))
3.84...
Controlling the tree size
The size of the regression tree base learners deﬁnes the level of variable interactions that can be captured by the
gradient boosting model. In general, a tree of depth h can capture interactions of order h . There are two ways in
which the size of the individual regression trees can be controlled.
If you specify max_depth=h then complete binary trees of depth h will be grown. Such trees will have (at most)
2**h leaf nodes and 2**h -1 split nodes.
Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter
max_leaf_nodes. In this case, trees will be grown using best-ﬁrst search where nodes with the highest improve-
ment in impurity will be expanded ﬁrst. A tree with max_leaf_nodes=k has k -1 split nodes and thus can model
interactions of up to order max_leaf_nodes -1 .
We found that max_leaf_nodes=k gives comparable results to max_depth=k-1 but is signiﬁcantly faster to
train at the expense of a slightly higher training error. The parameter max_leaf_nodes corresponds to the variable
J in the chapter on gradient boosting in [F2001] and is related to the parameter interaction.depth in R’s gbm
package where max_leaf_nodes == interaction.depth + 1 .
Mathematical formulation
GBRT considers additive models of the following form:
𝐹(𝑥) =
𝑀
∑︁
𝑚=1
𝛾𝑚ℎ𝑚(𝑥)
where ℎ𝑚(𝑥) are the basis functions which are usually called weak learners in the context of boosting. Gradient Tree
Boosting uses decision trees of ﬁxed size as weak learners. Decision trees have a number of abilities that make them
valuable for boosting, namely the ability to handle data of mixed type and the ability to model complex functions.
Similar to other boosting algorithms GBRT builds the additive model in a forward stagewise fashion:
𝐹𝑚(𝑥) = 𝐹𝑚−1(𝑥) + 𝛾𝑚ℎ𝑚(𝑥)
234
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
At each stage the decision tree ℎ𝑚(𝑥) is chosen to minimize the loss function 𝐿given the current model 𝐹𝑚−1 and its
ﬁt 𝐹𝑚−1(𝑥𝑖)
𝐹𝑚(𝑥) = 𝐹𝑚−1(𝑥) + arg min
ℎ
𝑛
∑︁
𝑖=1
𝐿(𝑦𝑖, 𝐹𝑚−1(𝑥𝑖) −ℎ(𝑥))
The initial model 𝐹0 is problem speciﬁc, for least-squares regression one usually chooses the mean of the target values.
Note: The initial model can also be speciﬁed via the init argument. The passed object has to implement fit and
predict.
Gradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent
direction is the negative gradient of the loss function evaluated at the current model 𝐹𝑚−1 which can be calculated for
any differentiable loss function:
𝐹𝑚(𝑥) = 𝐹𝑚−1(𝑥) + 𝛾𝑚
𝑛
∑︁
𝑖=1
∇𝐹𝐿(𝑦𝑖, 𝐹𝑚−1(𝑥𝑖))
Where the step length 𝛾𝑚is chosen using line search:
𝛾𝑚= arg min
𝛾
𝑛
∑︁
𝑖=1
𝐿(𝑦𝑖, 𝐹𝑚−1(𝑥𝑖) −𝛾𝜕𝐿(𝑦𝑖, 𝐹𝑚−1(𝑥𝑖))
𝜕𝐹𝑚−1(𝑥𝑖)
)
The algorithms for regression and classiﬁcation only differ in the concrete loss function used.
Loss Functions
The following loss functions are supported and can be speciﬁed using the parameter loss:
• Regression
– Least squares ('ls'): The natural choice for regression due to its superior computational properties. The
initial model is given by the mean of the target values.
– Least absolute deviation ('lad'): A robust loss function for regression. The initial model is given by the
median of the target values.
– Huber ('huber'): Another robust loss function that combines least squares and least absolute deviation;
use alpha to control the sensitivity with regards to outliers (see [F2001] for more details).
– Quantile ('quantile'): A loss function for quantile regression. Use 0 < alpha < 1 to specify the
quantile. This loss function can be used to create prediction intervals (see Prediction Intervals for Gradient
Boosting Regression).
• Classiﬁcation
– Binomial deviance ('deviance'): The negative binomial log-likelihood loss function for binary classi-
ﬁcation (provides probability estimates). The initial model is given by the log odds-ratio.
3.1. Supervised learning
235
scikit-learn user guide, Release 0.18.2
– Multinomial deviance ('deviance'): The negative multinomial log-likelihood loss function for multi-
class classiﬁcation with n_classes mutually exclusive classes. It provides probability estimates. The
initial model is given by the prior probability of each class. At each iteration n_classes regression trees
have to be constructed which makes GBRT rather inefﬁcient for data sets with a large number of classes.
– Exponential loss ('exponential'): The same loss function as AdaBoostClassifier. Less robust
to mislabeled examples than 'deviance'; can only be used for binary classiﬁcation.
Regularization
Shrinkage
[F2001] proposed a simple regularization strategy that scales the contribution of each weak learner by a factor 𝜈:
𝐹𝑚(𝑥) = 𝐹𝑚−1(𝑥) + 𝜈𝛾𝑚ℎ𝑚(𝑥)
The parameter 𝜈is also called the learning rate because it scales the step length the gradient descent procedure; it can
be set via the learning_rate parameter.
The parameter learning_rate strongly interacts with the parameter n_estimators, the number of weak learn-
ers to ﬁt. Smaller values of learning_rate require larger numbers of weak learners to maintain a constant training
error. Empirical evidence suggests that small values of learning_rate favor better test error. [HTF2009] recom-
mend to set the learning rate to a small constant (e.g. learning_rate <= 0.1) and choose n_estimators by
early stopping. For a more detailed discussion of the interaction between learning_rate and n_estimators
see [R2007].
Subsampling
[F1999] proposed stochastic gradient boosting, which combines gradient boosting with bootstrap averaging (bagging).
At each iteration the base classiﬁer is trained on a fraction subsample of the available training data. The subsample
is drawn without replacement. A typical value of subsample is 0.5.
The ﬁgure below illustrates the effect of shrinkage and subsampling on the goodness-of-ﬁt of the model. We can
clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of
the model. Subsampling without shrinkage, on the other hand, does poorly.
Another strategy to reduce the variance is by subsampling the features analogous to the random splits in
RandomForestClassifier . The number of subsampled features can be controlled via the max_features
parameter.
Note: Using a small max_features value can signiﬁcantly decrease the runtime.
Stochastic gradient boosting allows to compute out-of-bag estimates of the test deviance by computing the improve-
ment in deviance on the examples that are not included in the bootstrap sample (i.e. the out-of-bag examples). The
improvements are stored in the attribute oob_improvement_. oob_improvement_[i] holds the improvement
in terms of the loss on the OOB samples if you add the i-th stage to the current predictions. Out-of-bag estimates can
be used for model selection, for example to determine the optimal number of iterations. OOB estimates are usually
very pessimistic thus we recommend to use cross-validation instead and only use OOB if cross-validation is too time
consuming.
Examples:
236
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• Gradient Boosting regularization
• Gradient Boosting Out-of-Bag estimates
• OOB Errors for Random Forests
Interpretation
Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models,
however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the
individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting
models.
Feature importance
Often features do not contribute equally to predict the target response; in many situations the majority of the features
are in fact irrelevant. When interpreting a model, the ﬁrst question usually is: what are those important features and
how do they contributing in predicting the target response?
Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information
can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split
points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles
by simply averaging the feature importance of each tree (see Feature importance evaluation for more details).
The feature importance scores of a ﬁt gradient boosting model can be accessed via the feature_importances_
property:
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier
3.1. Supervised learning
237
scikit-learn user guide, Release 0.18.2
>>> X, y = make_hastie_10_2(random_state=0)
>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...
max_depth=1, random_state=0).fit(X, y)
>>> clf.feature_importances_
array([ 0.11,
0.1 ,
0.11,
...
Examples:
• Gradient Boosting regression
Partial dependence
Partial dependence plots (PDP) show the dependence between the target response and a set of ‘target’ features,
marginalizing over the values of all other features (the ‘complement’ features). Intuitively, we can interpret the partial
dependence as the expected target response 1 as a function of the ‘target’ features 2.
Due to the limits of human perception the size of the target feature set must be small (usually, one or two) thus the
target features are usually chosen among the most important features.
The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:
One-way PDPs tell us about the interaction between the target response and the target feature (e.g. linear, non-linear).
The upper left plot in the above Figure shows the effect of the median income in a district on the median house price;
we can clearly see a linear relationship among them.
PDPs with two target features show the interactions among the two features. For example, the two-variable PDP in
the above Figure shows the dependence of median house price on joint values of house age and avg. occupants per
1 For classiﬁcation with loss='deviance' the target response is logit(p).
2 More precisely its the expectation of the target response after accounting for the initial model; partial dependence plots do not include the
init model.
238
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
household. We can clearly see an interaction between the two features: For an avg. occupancy greater than two, the
house price is nearly independent of the house age, whereas for values less than two there is a strong dependence on
age.
The module partial_dependence provides a convenience function plot_partial_dependence to cre-
ate one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial
dependence plots: two one-way PDPs for the features 0 and 1 and a two-way PDP between the two features:
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier
>>> from sklearn.ensemble.partial_dependence import plot_partial_dependence
>>> X, y = make_hastie_10_2(random_state=0)
>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...
max_depth=1, random_state=0).fit(X, y)
>>> features = [0, 1, (0, 1)]
>>> fig, axs = plot_partial_dependence(clf, X, features)
For multi-class models, you need to set the class label for which the PDPs should be created via the label argument:
>>> from sklearn.datasets import load_iris
>>> iris = load_iris()
>>> mc_clf = GradientBoostingClassifier(n_estimators=10,
...
max_depth=1).fit(iris.data, iris.target)
>>> features = [3, 2, (3, 2)]
>>> fig, axs = plot_partial_dependence(mc_clf, X, features, label=0)
If you need the raw values of the partial dependence function rather than the plots you can use the
partial_dependence function:
>>> from sklearn.ensemble.partial_dependence import partial_dependence
>>> pdp, axes = partial_dependence(clf, [0], X=X)
>>> pdp
array([[ 2.46643157,
2.46643157, ...
>>> axes
[array([-1.62497054, -1.59201391, ...
The function requires either the argument grid which speciﬁes the values of the target features on which the partial
dependence function should be evaluated or the argument X which is a convenience mode for automatically creating
grid from the training data. If X is given, the axes value returned by the function gives the axis for each target
feature.
For each value of the ‘target’ features in the grid the partial dependence function need to marginalize the predictions
of a tree over all possible values of the ‘complement’ features. In decision trees this function can be evaluated efﬁ-
ciently without reference to the training data. For each grid point a weighted tree traversal is performed: if a split node
involves a ‘target’ feature, the corresponding left or right branch is followed, otherwise both branches are followed,
each branch is weighted by the fraction of training samples that entered that branch. Finally, the partial dependence
is given by a weighted average of all visited leaves. For tree ensembles the results of each individual tree are again
averaged.
Examples:
• Partial Dependence Plots
3.1. Supervised learning
239
scikit-learn user guide, Release 0.18.2
References
VotingClassiﬁer
The idea behind the voting classiﬁer implementation is to combine conceptually different machine learning classiﬁers
and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classiﬁer
can be useful for a set of equally well performing model in order to balance out their individual weaknesses.
Majority Class Labels (Majority/Hard Voting)
In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode)
of the class labels predicted by each individual classiﬁer.
E.g., if the prediction for a given sample is
• classiﬁer 1 -> class 1
• classiﬁer 2 -> class 1
• classiﬁer 3 -> class 2
the VotingClassiﬁer (with voting='hard') would classify the sample as “class 1” based on the majority class label.
In the cases of a tie, the VotingClassiﬁer will select the class based on the ascending sort order. E.g., in the following
scenario
• classiﬁer 1 -> class 2
• classiﬁer 2 -> class 1
the class label 1 will be assigned to the sample.
Usage
The following example shows how to ﬁt the majority rule classiﬁer:
>>> from sklearn import datasets
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.ensemble import VotingClassifier
>>> iris = datasets.load_iris()
>>> X, y = iris.data[:, 1:3], iris.target
>>> clf1 = LogisticRegression(random_state=1)
>>> clf2 = RandomForestClassifier(random_state=1)
>>> clf3 = GaussianNB()
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
˓→voting='hard')
>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random
˓→Forest', 'naive Bayes', 'Ensemble']):
...
scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
240
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
...
print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(),
˓→label))
Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
Accuracy: 0.93 (+/- 0.05) [Random Forest]
Accuracy: 0.91 (+/- 0.04) [naive Bayes]
Accuracy: 0.95 (+/- 0.05) [Ensemble]
Weighted Average Probabilities (Soft Voting)
In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted
probabilities.
Speciﬁc weights can be assigned to each classiﬁer via the weights parameter. When weights are provided, the
predicted class probabilities for each classiﬁer are collected, multiplied by the classiﬁer weight, and averaged. The
ﬁnal class label is then derived from the class label with the highest average probability.
To illustrate this with a simple example, let’s assume we have 3 classiﬁers and a 3-class classiﬁcation problems where
we assign equal weights to all classiﬁers: w1=1, w2=1, w3=1.
The weighted average probabilities for a sample would then be calculated as follows:
classiﬁer
class 1
class 2
class 3
classiﬁer 1
w1 * 0.2
w1 * 0.5
w1 * 0.3
classiﬁer 2
w2 * 0.6
w2 * 0.3
w2 * 0.1
classiﬁer 3
w3 * 0.3
w3 * 0.4
w3 * 0.3
weighted average
0.37
0.4
0.23
Here, the predicted class label is 2, since it has the highest average probability.
The following example illustrates how the decision regions may change when a soft VotingClassiﬁer is used based on
an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classiﬁer:
>>> from sklearn import datasets
>>> from sklearn.tree import DecisionTreeClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> from sklearn.svm import SVC
>>> from itertools import product
>>> from sklearn.ensemble import VotingClassifier
>>> # Loading some example data
>>> iris = datasets.load_iris()
>>> X = iris.data[:, [0,2]]
>>> y = iris.target
>>> # Training classifiers
>>> clf1 = DecisionTreeClassifier(max_depth=4)
>>> clf2 = KNeighborsClassifier(n_neighbors=7)
>>> clf3 = SVC(kernel='rbf', probability=True)
>>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],
˓→voting='soft', weights=[2,1,2])
>>> clf1 = clf1.fit(X,y)
>>> clf2 = clf2.fit(X,y)
>>> clf3 = clf3.fit(X,y)
>>> eclf = eclf.fit(X,y)
3.1. Supervised learning
241
scikit-learn user guide, Release 0.18.2
Using the VotingClassiﬁer with GridSearch
The VotingClassiﬁer can also be used together with GridSearch in order to tune the hyperparameters of the individual
estimators:
>>> from sklearn.model_selection import GridSearchCV
>>> clf1 = LogisticRegression(random_state=1)
>>> clf2 = RandomForestClassifier(random_state=1)
>>> clf3 = GaussianNB()
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
˓→voting='soft')
>>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}
>>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
>>> grid = grid.fit(iris.data, iris.target)
Usage
In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClas-
siﬁer must support predict_proba method):
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
˓→voting='soft')
Optionally, weights can be provided for the individual classiﬁers:
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
˓→voting='soft', weights=[2,5,1])
242
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1.12 Multiclass and multilabel algorithms
Warning:
All classiﬁers in scikit-learn do multiclass classiﬁcation out-of-the-box. You don’t need to use the
sklearn.multiclass module unless you want to experiment with different multiclass strategies.
The sklearn.multiclass module implements meta-estimators to solve multiclass and multilabel clas-
siﬁcation problems by decomposing such problems into binary classiﬁcation problems. Multitarget regression is also
supported.
• Multiclass classiﬁcation means a classiﬁcation task with more than two classes; e.g., classify a set of images of
fruits which may be oranges, apples, or pears. Multiclass classiﬁcation makes the assumption that each sample
is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time.
• Multilabel classiﬁcation assigns to each sample a set of target labels. This can be thought as predicting proper-
ties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might
be about any of religion, politics, ﬁnance or education at the same time or none of these.
• Multioutput regression assigns each sample a set of target values. This can be thought of as predicting several
properties for each data-point, such as wind direction and magnitude at a certain location.
• Multioutput-multiclass classiﬁcation and multi-task classiﬁcation means that a single estimator has to handle
several joint classiﬁcation tasks. This is both a generalization of the multi-label classiﬁcation task, which only
considers binary classiﬁcation, as well as a generalization of the multi-class classiﬁcation task. The output
format is a 2d numpy array or sparse matrix.
The set of labels can be different for each output variable. For instance, a sample could be assigned “pear” for
an output variable that takes possible values in a ﬁnite set of species such as “pear”, “apple”; and “blue” or
“green” for a second output variable that takes possible values in a ﬁnite set of colors such as “green”, “red”,
“blue”, “yellow”...
This means that any classiﬁers handling multi-output multiclass or multi-task classiﬁcation tasks, support the
multi-label classiﬁcation task as a special case. Multi-task classiﬁcation is similar to the multi-output classiﬁca-
tion task with different model formulations. For more information, see the relevant estimator documentation.
All
scikit-learn
classiﬁers
are
capable
of
multiclass
classiﬁcation,
but
the
meta-estimators
offered
by
sklearn.multiclass permit changing the way they handle more than two classes because this may have an
effect on classiﬁer performance (either in terms of generalization error or required computational resources).
Below is a summary of the classiﬁers supported by scikit-learn grouped by strategy; you don’t need the meta-estimators
in this class if you’re using one of these, unless you want custom multiclass behavior:
• Inherently multiclass: Naive Bayes, LDA and QDA, Decision Trees, Random Forests, Nearest Neighbors, setting
multi_class='multinomial' in sklearn.linear_model.LogisticRegression.
• Support multilabel: Decision Trees, Random Forests, Nearest Neighbors.
• One-Vs-One: sklearn.svm.SVC.
• One-Vs-All: all linear models except sklearn.svm.SVC.
Some estimators also support multioutput-multiclass classiﬁcation tasks Decision Trees, Random Forests, Nearest
Neighbors.
3.1. Supervised learning
243
scikit-learn user guide, Release 0.18.2
Warning: At present, no metric in sklearn.metrics supports the multioutput-multiclass classiﬁcation task.
Multilabel classiﬁcation format
In multilabel learning, the joint set of binary classiﬁcation tasks is expressed with label binary indicator array: each
sample is one row of a 2d array of shape (n_samples, n_classes) with binary values: the one, i.e. the non zero elements,
corresponds to the subset of labels. An array such as np.array([[1,0,0],[0,1,1],[0,0,0]]) represents
label 0 in the ﬁrst sample, labels 1 and 2 in the second sample, and no labels in the third sample.
Producing multilabel data as a list of sets of labels may be more intuitive. The MultiLabelBinarizer transformer
can be used to convert between a collection of collections of labels and the indicator format.
>>> from sklearn.preprocessing import MultiLabelBinarizer
>>> y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]
>>> MultiLabelBinarizer().fit_transform(y)
array([[0, 0, 1, 1, 1],
[0, 0, 1, 0, 0],
[1, 1, 0, 1, 0],
[1, 1, 1, 1, 1],
[1, 1, 1, 0, 0]])
One-Vs-The-Rest
This strategy, also known as one-vs-all, is implemented in OneVsRestClassifier. The strategy consists in
ﬁtting one classiﬁer per class. For each classiﬁer, the class is ﬁtted against all the other classes. In addition to its
computational efﬁciency (only n_classes classiﬁers are needed), one advantage of this approach is its interpretability.
Since each class is represented by one and only one classiﬁer, it is possible to gain knowledge about the class by
inspecting its corresponding classiﬁer. This is the most commonly used strategy and is a fair default choice.
Multiclass learning
Below is an example of multiclass learning using OvR:
>>> from sklearn import datasets
>>> from sklearn.multiclass import OneVsRestClassifier
>>> from sklearn.svm import LinearSVC
>>> iris = datasets.load_iris()
>>> X, y = iris.data, iris.target
>>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Multilabel learning
OneVsRestClassifier also supports multilabel classiﬁcation. To use this feature, feed the classiﬁer an indicator
matrix, in which cell [i, j] indicates the presence of label j in sample i.
244
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples:
• Multilabel classiﬁcation
One-Vs-One
OneVsOneClassifier constructs one classiﬁer per pair of classes. At prediction time, the class which received
the most votes is selected. In the event of a tie (among two classes with an equal number of votes), it selects the class
with the highest aggregate classiﬁcation conﬁdence by summing over the pair-wise classiﬁcation conﬁdence levels
computed by the underlying binary classiﬁers.
Since it requires to ﬁt n_classes * (n_classes -1) / 2 classiﬁers, this method is usually slower than one-
vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as
kernel algorithms which don’t scale well with n_samples. This is because each individual learning problem only
involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used n_classes times.
Multiclass learning
Below is an example of multiclass learning using OvO:
>>> from sklearn import datasets
>>> from sklearn.multiclass import OneVsOneClassifier
>>> from sklearn.svm import LinearSVC
>>> iris = datasets.load_iris()
>>> X, y = iris.data, iris.target
>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
3.1. Supervised learning
245
scikit-learn user guide, Release 0.18.2
0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
References:
Error-Correcting Output-Codes
Output-code based strategies are fairly different from one-vs-the-rest and one-vs-one. With these strategies, each class
is represented in a Euclidean space, where each dimension can only be 0 or 1. Another way to put it is that each class
is represented by a binary code (an array of 0 and 1). The matrix which keeps track of the location/code of each class
is called the code book. The code size is the dimensionality of the aforementioned space. Intuitively, each class should
be represented by a code as unique as possible and a good code book should be designed to optimize classiﬁcation
accuracy. In this implementation, we simply use a randomly-generated code book as advocated in 3 although more
elaborate methods may be added in the future.
At ﬁtting time, one binary classiﬁer per bit in the code book is ﬁtted. At prediction time, the classiﬁers are used to
project new points in the class space and the class closest to the points is chosen.
In OutputCodeClassifier, the code_size attribute allows the user to control the number of classiﬁers which
will be used. It is a percentage of the total number of classes.
A number between 0 and 1 will require fewer classiﬁers than one-vs-the-rest. In theory, log2(n_classes) /
n_classes is sufﬁcient to represent each class unambiguously. However, in practice, it may not lead to good
accuracy since log2(n_classes) is much smaller than n_classes.
A number greater than 1 will require more classiﬁers than one-vs-the-rest. In this case, some classiﬁers will in theory
correct for the mistakes made by other classiﬁers, hence the name “error-correcting”. In practice, however, this may
not happen as classiﬁer mistakes will typically be correlated. The error-correcting output codes have a similar effect
to bagging.
Multiclass learning
Below is an example of multiclass learning using Output-Codes:
>>> from sklearn import datasets
>>> from sklearn.multiclass import OutputCodeClassifier
>>> from sklearn.svm import LinearSVC
>>> iris = datasets.load_iris()
>>> X, y = iris.data, iris.target
>>> clf = OutputCodeClassifier(LinearSVC(random_state=0),
...
code_size=2, random_state=0)
>>> clf.fit(X, y).predict(X)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,
1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
3 “The error coding method and PICTs”, James G., Hastie T., Journal of Computational and Graphical statistics 7, 1998.
246
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
References:
Multioutput regression
Multioutput regression support can be added to any regressor with MultiOutputRegressor. This strategy con-
sists of ﬁtting one regressor per target. Since each target is represented by exactly one regressor it is possible to
gain knowledge about the target by inspecting its corresponding regressor. As MultiOutputRegressor ﬁts one
regressor per target it can not take advantage of correlations between targets.
Below is an example of multioutput regression:
>>> from sklearn.datasets import make_regression
>>> from sklearn.multioutput import MultiOutputRegressor
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> X, y = make_regression(n_samples=10, n_targets=3, random_state=1)
>>> MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).
˓→predict(X)
array([[-154.75474165, -147.03498585,
-50.03812219],
[
7.12165031,
5.12914884,
-81.46081961],
[-187.8948621 , -100.44373091,
13.88978285],
[-141.62745778,
95.02891072, -191.48204257],
[
97.03260883,
165.34867495,
139.52003279],
[ 123.92529176,
21.25719016,
-7.84253
],
[-122.25193977,
-85.16443186, -107.12274212],
[ -30.170388
,
-94.80956739,
12.16979946],
[ 140.72667194,
176.50941682,
-17.50447799],
[ 149.37967282,
-81.15699552,
-5.72850319]])
Multioutput classiﬁcation
Multioutput classiﬁcation support can be added to any classiﬁer with MultiOutputClassifier. This strategy
consists of ﬁtting one classiﬁer per target. This allows multiple target variable classiﬁcations. The purpose of this
class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3...,fn) that are trained on a single
X predictor matrix to predict a series of reponses (y1,y2,y3...,yn).
Below is an example of multioutput classiﬁcation:
>>> from sklearn.datasets import make_classification
>>> from sklearn.multioutput import MultiOutputClassifier
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.utils import shuffle
>>> import numpy as np
>>> X, y1 = make_classification(n_samples=10, n_features=100, n_informative=30, n_
˓→classes=3, random_state=1)
>>> y2 = shuffle(y1, random_state=1)
>>> y3 = shuffle(y1, random_state=2)
>>> Y = np.vstack((y1, y2, y3)).T
>>> n_samples, n_features = X.shape # 10,100
>>> n_outputs = Y.shape[1] # 3
>>> n_classes = 3
>>> forest = RandomForestClassifier(n_estimators=100, random_state=1)
>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)
3.1. Supervised learning
247
scikit-learn user guide, Release 0.18.2
>>> multi_target_forest.fit(X, Y).predict(X)
array([[2, 2, 0],
[1, 2, 1],
[2, 1, 0],
[0, 0, 2],
[0, 2, 1],
[0, 0, 2],
[1, 1, 0],
[1, 1, 1],
[0, 0, 2],
[2, 0, 0]])
3.1.13 Feature selection
The classes in the sklearn.feature_selection module can be used for feature selection/dimensionality re-
duction on sample sets, either to improve estimators’ accuracy scores or to boost their performance on very high-
dimensional datasets.
Removing features with low variance
VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance
doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value
in all samples.
As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are
either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and
the variance of such variables is given by
Var[𝑋] = 𝑝(1 −𝑝)
so we can select using the threshold .8 * (1 -.8):
>>> from sklearn.feature_selection import VarianceThreshold
>>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
>>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
>>> sel.fit_transform(X)
array([[0, 1],
[1, 0],
[0, 0],
[1, 1],
[1, 0],
[1, 1]])
As expected, VarianceThreshold has removed the ﬁrst column, which has a probability 𝑝= 5/6 > .8 of
containing a zero.
Univariate feature selection
Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen
as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the
transform method:
• SelectKBest removes all but the 𝑘highest scoring features
248
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• SelectPercentile removes all but a user-speciﬁed highest scoring percentage of features
• using common univariate statistical tests for each feature: false positive rate SelectFpr, false discovery rate
SelectFdr, or family wise error SelectFwe.
• GenericUnivariateSelect allows to perform univariate feature selection with a conﬁgurable strategy.
This allows to select the best univariate selection strategy with hyper-parameter search estimator.
For instance, we can perform a 𝜒2 test to the samples to retrieve only the two best features as follows:
>>> from sklearn.datasets import load_iris
>>> from sklearn.feature_selection import SelectKBest
>>> from sklearn.feature_selection import chi2
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> X.shape
(150, 4)
>>> X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
>>> X_new.shape
(150, 2)
These objects take as input a scoring function that returns univariate scores and p-values (or only scores for
SelectKBest and SelectPercentile):
• For regression: f_regression, mutual_info_regression
• For classiﬁcation: chi2, f_classif, mutual_info_classif
The methods based on F-test estimate the degree of linear dependency between two random variables. On the other
hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they
require more samples for accurate estimation.
Feature selection with sparse data
If you use sparse data (i.e.
data represented as sparse matrices), chi2, mutual_info_regression,
mutual_info_classif will deal with the data without making it dense.
Warning:
Beware not to use a regression scoring function with a classiﬁcation problem, you will get useless
results.
Examples:
• Univariate Feature Selection
• Comparison of F-test and mutual information
Recursive feature elimination
Given an external estimator that assigns weights to features (e.g., the coefﬁcients of a linear model), recursive feature
elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the
estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose
absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on
the pruned set until the desired number of features to select is eventually reached.
3.1. Supervised learning
249
scikit-learn user guide, Release 0.18.2
RFECV performs RFE in a cross-validation loop to ﬁnd the optimal number of features.
Examples:
• Recursive feature elimination: A recursive feature elimination example showing the relevance of pixels in a
digit classiﬁcation task.
• Recursive feature elimination with cross-validation: A recursive feature elimination example with automatic
tuning of the number of features selected with cross-validation.
Feature selection using SelectFromModel
SelectFromModel is a meta-transformer that can be used along with any estimator that has a coef_ or
feature_importances_ attribute after ﬁtting. The features are considered unimportant and removed, if the
corresponding coef_ or feature_importances_ values are below the provided threshold parameter. Apart
from specifying the threshold numerically, there are built-in heuristics for ﬁnding a threshold using a string argument.
Available heuristics are “mean”, “median” and ﬂoat multiples of these like “0.1*mean”.
For examples on how it is to be used refer to the sections below.
Examples
• Feature selection using SelectFromModel and LassoCV: Selecting the two most important features from the
Boston dataset without knowing the threshold beforehand.
L1-based feature selection
Linear models penalized with the L1 norm have sparse solutions:
many of their estimated coefﬁcients are
zero.
When the goal is to reduce the dimensionality of the data to use with another classiﬁer, they can
be used along with feature_selection.SelectFromModel to select the non-zero coefﬁcients.
In
particular, sparse estimators useful for this purpose are the linear_model.Lasso for regression, and of
linear_model.LogisticRegression and svm.LinearSVC for classiﬁcation:
>>> from sklearn.svm import LinearSVC
>>> from sklearn.datasets import load_iris
>>> from sklearn.feature_selection import SelectFromModel
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> X.shape
(150, 4)
>>> lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
>>> model = SelectFromModel(lsvc, prefit=True)
>>> X_new = model.transform(X)
>>> X_new.shape
(150, 3)
With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected.
With Lasso, the higher the alpha parameter, the fewer features selected.
250
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples:
• Classiﬁcation of text documents using sparse features: Comparison of different algorithms for document
classiﬁcation including L1-based feature selection.
L1-recovery and compressive sensing
For a good choice of alpha, the Lasso can fully recover the exact set of non-zero variables using only few obser-
vations, provided certain speciﬁc conditions are met. In particular, the number of samples should be “sufﬁciently
large”, or L1 models will perform at random, where “sufﬁciently large” depends on the number of non-zero co-
efﬁcients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero
coefﬁcients, and the structure of the design matrix X. In addition, the design matrix must display certain speciﬁc
properties, such as not being too correlated.
There is no general rule to select an alpha parameter for recovery of non-zero coefﬁcients. It can by set by cross-
validation (LassoCV or LassoLarsCV), though this may lead to under-penalized models: including a small
number of non-relevant variables is not detrimental to prediction score. BIC (LassoLarsIC) tends, on the oppo-
site, to set high values of alpha.
Reference Richard G. Baraniuk “Compressive Sensing”, IEEE Signal Processing Magazine [120] July 2007 http:
//dsp.rice.edu/sites/dsp.rice.edu/ﬁles/cs/baraniukCSlecture07.pdf
Randomized sparse models
In terms of feature selection, there are some well-known limitations of L1-penalized models for regression and clas-
siﬁcation. For example, it is known that the Lasso will tend to select an individual variable out of a group of highly
correlated features. Furthermore, even when the correlation between features is not too high, the conditions under
which L1-penalized methods consistently select “good” features can be restrictive in general.
To mitigate this problem,
it is possible to use randomization techniques such as those presented in
[B2009] and [M2010].
The latter technique, known as stability selection, is implemented in the module
sklearn.linear_model. In the stability selection method, a subsample of the data is ﬁt to a L1-penalized
model where the penalty of a random subset of coefﬁcients has been scaled. Speciﬁcally, given a subsample of the
data (𝑥𝑖, 𝑦𝑖), 𝑖∈𝐼, where 𝐼⊂{1, 2, . . . , 𝑛} is a random subset of the data of size 𝑛𝐼, the following modiﬁed Lasso
ﬁt is obtained:
ˆ
𝑤𝐼= arg min
𝑤
1
2𝑛𝐼
∑︁
𝑖∈𝐼
(𝑦𝑖−𝑥𝑇
𝑖𝑤)2 + 𝛼
𝑝
∑︁
𝑗=1
|𝑤𝑗|
𝑠𝑗
,
where 𝑠𝑗∈{𝑠, 1} are independent trials of a fair Bernoulli random variable, and 0 < 𝑠< 1 is the scaling factor. By
repeating this procedure across different random subsamples and Bernoulli trials, one can count the fraction of times
the randomized procedure selected each feature, and used these fractions as scores for feature selection.
RandomizedLasso
implements
this
strategy
for
regression
settings,
using
the
Lasso,
while
RandomizedLogisticRegression uses the logistic regression and is suitable for classiﬁcation tasks.
To
get a full path of stability scores you can use lasso_stability_path.
Note that for randomized sparse models to be more powerful than standard F statistics at detecting non-zero features,
the ground truth model should be sparse, in other words, there should be only a small fraction of features non zero.
Examples:
3.1. Supervised learning
251
scikit-learn user guide, Release 0.18.2
• Sparse recovery: feature selection for sparse linear models: An example comparing different feature selection
approaches and discussing in which situation each approach is to be favored.
References:
Tree-based feature selection
Tree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module)
can be used to compute feature importances, which in turn can be used to discard irrelevant features (when coupled
with the sklearn.feature_selection.SelectFromModel meta-transformer):
>>> from sklearn.ensemble import ExtraTreesClassifier
>>> from sklearn.datasets import load_iris
>>> from sklearn.feature_selection import SelectFromModel
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> X.shape
(150, 4)
>>> clf = ExtraTreesClassifier()
>>> clf = clf.fit(X, y)
>>> clf.feature_importances_
array([ 0.04...,
0.05...,
0.4...,
0.4...])
>>> model = SelectFromModel(clf, prefit=True)
>>> X_new = model.transform(X)
>>> X_new.shape
(150, 2)
Examples:
• Feature importances with forests of trees: example on synthetic data showing the recovery of the actually
meaningful features.
252
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• Pixel importances with a parallel forest of trees: example on face recognition data.
Feature selection as part of a pipeline
Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to
do this in scikit-learn is to use a sklearn.pipeline.Pipeline:
clf = Pipeline([
('feature_selection', SelectFromModel(LinearSVC(penalty="l1"))),
('classification', RandomForestClassifier())
])
clf.fit(X, y)
In
this
snippet
we
make
use
of
a
sklearn.svm.LinearSVC
coupled
with
sklearn.feature_selection.SelectFromModel to evaluate feature importances and select the most rele-
vant features. Then, a sklearn.ensemble.RandomForestClassifier is trained on the transformed output,
i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also
classiﬁers that provide a way to evaluate feature importances of course. See the sklearn.pipeline.Pipeline
examples for more details.
3.1.14 Semi-Supervised
Semi-supervised learning is a situation in which in your training data some of the samples are not labeled. The semi-
supervised estimators in sklearn.semi_supervised are able to make use of this additional unlabeled data to
better capture the shape of the underlying data distribution and generalize better to new samples. These algorithms
can perform well when we have a very small amount of labeled points and a large amount of unlabeled points.
Unlabeled entries in y
It is important to assign an identiﬁer to unlabeled points along with the labeled data when training the model with
the fit method. The identiﬁer that this implementation uses is the integer value −1.
Label Propagation
Label propagation denotes a few variations of semi-supervised graph inference algorithms.
A few features available in this model:
• Can be used for classiﬁcation and regression tasks
• Kernel methods to project data into alternate dimensional spaces
scikit-learn provides two label propagation models: LabelPropagation and LabelSpreading. Both work by
constructing a similarity graph over all items in the input dataset.
LabelPropagation and LabelSpreading differ in modiﬁcations to the similarity matrix that graph and the
clamping effect on the label distributions. Clamping allows the algorithm to change the weight of the true ground
labeled data to some degree. The LabelPropagation algorithm performs hard clamping of input labels, which
means 𝛼= 1. This clamping factor can be relaxed, to say 𝛼= 0.8, which means that we will always retain 80 percent
of our original label distribution, but the algorithm gets to change its conﬁdence of the distribution within 20 percent.
LabelPropagation uses the raw similarity matrix constructed from the data with no modiﬁcations. In contrast,
LabelSpreading minimizes a loss function that has regularization properties, as such it is often more robust to
3.1. Supervised learning
253
scikit-learn user guide, Release 0.18.2
Fig. 3.1: An illustration of label-propagation: the structure of unlabeled observations is consistent with the class
structure, and thus the class label can be propagated to the unlabeled observations of the training set.
noise. The algorithm iterates on a modiﬁed version of the original graph and normalizes the edge weights by computing
the normalized graph Laplacian matrix. This procedure is also used in Spectral clustering.
Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance
of the algorithms. The following are available:
• rbf (exp(−𝛾|𝑥−𝑦|2), 𝛾> 0). 𝛾is speciﬁed by keyword gamma.
• knn (1[𝑥′ ∈𝑘𝑁𝑁(𝑥)]). 𝑘is speciﬁed by keyword n_neighbors.
The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix
may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration
of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much
more memory-friendly sparse matrix which can drastically reduce running times.
Examples
• Decision boundary of label propagation versus SVM on the Iris dataset
• Label Propagation learning a complex structure
• Label Propagation digits active learning
References
[1] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised Learning (2006), pp. 193-216
[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efﬁcient Non-Parametric Function Induction in Semi-
Supervised Learning. AISTAT 2005 http://research.microsoft.com/en-us/people/nicolasl/efﬁcient_ssl.pdf
3.1.15 Isotonic regression
The class IsotonicRegression ﬁts a non-decreasing function to data. It solves the following problem:
minimize ∑︀
𝑖𝑤𝑖(𝑦𝑖−ˆ𝑦𝑖)2
subject to ˆ𝑦𝑚𝑖𝑛= ˆ𝑦1 ≤ˆ𝑦2... ≤ˆ𝑦𝑛= ˆ𝑦𝑚𝑎𝑥
254
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
where each 𝑤𝑖is strictly positive and each 𝑦𝑖is an arbitrary real number. It yields the vector which is composed of
non-decreasing elements the closest in terms of mean squared error. In practice this list of elements forms a function
that is piecewise linear.
3.1.16 Probability calibration
When performing classiﬁcation you often want not only to predict the class label, but also obtain a probability of the
respective label. This probability gives you some kind of conﬁdence on the prediction. Some models can give you
poor estimates of the class probabilities and some even do not support probability prediction. The calibration module
allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.
Well calibrated classiﬁers are probabilistic classiﬁers for which the output of the predict_proba method can be directly
interpreted as a conﬁdence level. For instance, a well calibrated (binary) classiﬁer should classify the samples such
that among the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the
positive class. The following plot compares how well the probabilistic predictions of different classiﬁers are calibrated:
LogisticRegression returns well calibrated predictions by default as it directly optimizes log-loss. In contrast,
the other methods return biased probabilities; with different biases per method:
• GaussianNB tends to push probabilties to 0 or 1 (note the counts in the histograms). This is mainly because
it makes the assumption that features are conditionally independent given the class, which is not the case in this
dataset which contains 2 redundant features.
3.1. Supervised learning
255
scikit-learn user guide, Release 0.18.2
256
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• RandomForestClassifier shows the opposite behavior: the histograms show peaks at approximately
0.2 and 0.9 probability, while probabilities close to 0 or 1 are very rare. An explanation for this is given by
Niculescu-Mizil and Caruana [4]: “Methods such as bagging and random forests that average predictions from
a base set of models can have difﬁculty making predictions near 0 and 1 because variance in the underlying base
models will bias predictions that should be near zero or one away from these values. Because predictions are
restricted to the interval [0,1], errors caused by variance tend to be one-sided near zero and one. For example,
if a model should predict p = 0 for a case, the only way bagging can achieve this is if all bagged trees predict
zero. If we add noise to the trees that bagging is averaging over, this noise will cause some trees to predict
values larger than 0 for this case, thus moving the average prediction of the bagged ensemble away from 0. We
observe this effect most strongly with random forests because the base-level trees trained with random forests
have relatively high variance due to feature subseting.” As a result, the calibration curve shows a characteristic
sigmoid shape, indicating that the classiﬁer could trust its “intuition” more and return probabilties closer to 0 or
1 typically.
• Linear Support Vector Classiﬁcation (LinearSVC) shows an even more sigmoid curve as the RandomForest-
Classiﬁer, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [4]), which
focus on hard samples that are close to the decision boundary (the support vectors).
Two approaches for performing calibration of probabilistic predictions are provided: a parametric approach based on
Platt’s sigmoid model and a non-parametric approach based on isotonic regression (sklearn.isotonic). Proba-
bility calibration should be done on new data not used for model ﬁtting. The class CalibratedClassifierCV
uses a cross-validation generator and estimates for each split the model parameter on the train samples and the cali-
bration of the test samples. The probabilities predicted for the folds are then averaged. Already ﬁtted classiﬁers can
be calibrated by CalibratedClassifierCV via the paramter cv=”preﬁt”. In this case, the user has to take care
manually that data for model ﬁtting and calibration are disjoint.
The following images demonstrate the beneﬁt of probability calibration. The ﬁrst image present a dataset with 2
classes and 3 blobs of data. The blob in the middle contains random samples of each class. The probability for the
samples in this blob should be 0.5.
The following image shows on the data above the estimated probability using a Gaussian naive Bayes classiﬁer without
calibration, with a sigmoid calibration and with a non-parametric isotonic calibration. One can observe that the non-
parametric model provides the most accurate probability estimates for samples in the middle, i.e., 0.5.
The following experiment is performed on an artiﬁcial dataset for binary classiﬁcation with 100.000 samples (1.000
of them are used for model ﬁtting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant.
The ﬁgure shows the estimated probabilities obtained with logistic regression, a linear support-vector classiﬁer (SVC),
and linear SVC with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with
Brier score brier_score_loss, reported in the legend (the smaller the better).
One can observe here that logistic regression is well calibrated as its curve is nearly diagonal. Linear SVC’s calibration
curve has a sigmoid curve, which is typical for an under-conﬁdent classiﬁer. In the case of LinearSVC, this is caused
by the margin property of the hinge loss, which lets the model focus on hard samples that are close to the decision
boundary (the support vectors). Both kinds of calibration can ﬁx this issue and yield nearly identical results. The next
ﬁgure shows the calibration curve of Gaussian naive Bayes on the same data, with both kinds of calibration and also
without calibration.
One can see that Gaussian naive Bayes performs very badly but does so in an other way than linear SVC: While linear
SVC exhibited a sigmoid calibration curve, Gaussian naive Bayes’ calibration curve has a transposed-sigmoid shape.
This is typical for an over-conﬁdent classiﬁer. In this case, the classiﬁer’s overconﬁdence is caused by the redundant
features which violate the naive Bayes assumption of feature-independence.
Calibration of the probabilities of Gaussian naive Bayes with isotonic regression can ﬁx this issue as can be seen from
the nearly diagonal calibration curve. Sigmoid calibration also improves the brier score slightly, albeit not as strongly
as the non-parametric isotonic calibration. This is an intrinsic limitation of sigmoid calibration, whose parametric form
assumes a sigmoid rather than a transposed-sigmoid curve. The non-parametric isotonic calibration model, however,
makes no such strong assumptions and can deal with either shape, provided that there is sufﬁcient calibration data. In
general, sigmoid calibration is preferable in cases where the calibration curve is sigmoid and where there is limited
3.1. Supervised learning
257
scikit-learn user guide, Release 0.18.2
258
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
259
scikit-learn user guide, Release 0.18.2
260
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
261
scikit-learn user guide, Release 0.18.2
calibration data, while isotonic calibration is preferable for non-sigmoid calibration curves and in situations where
large amounts of data are available for calibration.
CalibratedClassifierCV can also deal with classiﬁcation tasks that involve more than two classes if the base
estimator can do so. In this case, the classiﬁer is calibrated ﬁrst for each class separately in an one-vs-rest fashion.
When predicting probabilities for unseen data, the calibrated probabilities for each class are predicted separately. As
those probabilities do not necessarily sum to one, a postprocessing is performed to normalize them.
The next image illustrates how sigmoid calibration changes predicted probabilities for a 3-class classiﬁcation problem.
Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the
probability vectors predicted by an uncalibrated classiﬁer to the probability vectors predicted by the same classiﬁer
after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green:
class 2, blue: class 3).
The base classiﬁer is a random forest classiﬁer with 25 base estimators (trees). If this classiﬁer is trained on all 800
training datapoints, it is overly conﬁdent in its predictions and thus incurs a large log-loss. Calibrating an identical
classiﬁer, which was trained on 600 datapoints, with method=’sigmoid’ on the remaining 200 datapoints reduces the
conﬁdence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center:
This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base
estimators which would have resulted in a similar decrease in log-loss.
262
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.1. Supervised learning
263
scikit-learn user guide, Release 0.18.2
References:
3.1.17 Neural network models (supervised)
Warning:
This implementation is not intended for large-scale applications. In particular, scikit-learn offers no
GPU support. For much faster, GPU-based implementations, as well as frameworks offering much more ﬂexibility
to build deep learning architectures, see Related Projects.
Multi-layer Perceptron
Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function 𝑓(·) : 𝑅𝑚→𝑅𝑜by training
on a dataset, where 𝑚is the number of dimensions for input and 𝑜is the number of dimensions for output. Given a set
of features 𝑋= 𝑥1, 𝑥2, ..., 𝑥𝑚and a target 𝑦, it can learn a non-linear function approximator for either classiﬁcation
or regression. It is different from logistic regression, in that between the input and the output layer, there can be one
or more non-linear layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar output.
Fig. 3.2: Figure 1 : One hidden layer MLP.
The leftmost layer, known as the input layer, consists of a set of neurons {𝑥𝑖|𝑥1, 𝑥2, ..., 𝑥𝑚} representing the input
features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear sum-
mation 𝑤1𝑥1 + 𝑤2𝑥2 + ... + 𝑤𝑚𝑥𝑚, followed by a non-linear activation function 𝑔(·) : 𝑅→𝑅- like the hyperbolic
tan function. The output layer receives the values from the last hidden layer and transforms them into output values.
The module contains the public attributes coefs_ and intercepts_. coefs_ is a list of weight matrices, where
weight matrix at index 𝑖represents the weights between layer 𝑖and layer 𝑖+1. intercepts_ is a list of bias vectors,
where the vector at index 𝑖represents the bias values added to layer 𝑖+ 1.
The advantages of Multi-layer Perceptron are:
• Capability to learn non-linear models.
• Capability to learn models in real-time (on-line learning) using partial_fit.
The disadvantages of Multi-layer Perceptron (MLP) include:
264
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• MLP with hidden layers have a non-convex loss function where there exists more than one local minimum.
Therefore different random weight initializations can lead to different validation accuracy.
• MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.
• MLP is sensitive to feature scaling.
Please see Tips on Practical Use section that addresses some of these disadvantages.
Classiﬁcation
Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation.
MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as
ﬂoating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the
training samples:
>>> from sklearn.neural_network import MLPClassifier
>>> X = [[0., 0.], [1., 1.]]
>>> y = [0, 1]
>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
...
hidden_layer_sizes=(5, 2), random_state=1)
...
>>> clf.fit(X, y)
MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
beta_1=0.9, beta_2=0.999, early_stopping=False,
epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate='constant',
learning_rate_init=0.001, max_iter=200, momentum=0.9,
nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,
warm_start=False)
After ﬁtting (training), the model can predict labels for new samples:
>>> clf.predict([[2., 2.], [-1., -2.]])
array([1, 0])
MLP can ﬁt a non-linear model to the training data. clf.coefs_ contains the weight matrices that constitute the
model parameters:
>>> [coef.shape for coef in clf.coefs_]
[(2, 5), (5, 2), (2, 1)]
Currently, MLPClassifier supports only the Cross-Entropy loss function, which allows probability estimates by
running the predict_proba method.
MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients
are calculated using Backpropagation. For classiﬁcation, it minimizes the Cross-Entropy loss function, giving a vector
of probability estimates 𝑃(𝑦|𝑥) per sample 𝑥:
>>> clf.predict_proba([[2., 2.], [1., 2.]])
array([[
1.967...e-04,
9.998...-01],
[
1.967...e-04,
9.998...-01]])
MLPClassifier supports multi-class classiﬁcation by applying Softmax as the output function.
Further, the model supports multi-label classiﬁcation in which a sample can belong to more than one class. For each
class, the raw output passes through the logistic function. Values larger or equal to 0.5 are rounded to 1, otherwise to
0. For a predicted output of a sample, the indices where the value is 1 represents the assigned classes of that sample:
3.1. Supervised learning
265
scikit-learn user guide, Release 0.18.2
>>> X = [[0., 0.], [1., 1.]]
>>> y = [[0, 1], [1, 1]]
>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
...
hidden_layer_sizes=(15,), random_state=1)
...
>>> clf.fit(X, y)
MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
beta_1=0.9, beta_2=0.999, early_stopping=False,
epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate='constant',
learning_rate_init=0.001, max_iter=200, momentum=0.9,
nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,
warm_start=False)
>>> clf.predict([[1., 2.]])
array([[1, 1]])
>>> clf.predict([[0., 0.]])
array([[0, 1]])
See the examples below and the doc string of MLPClassifier.fit for further information.
Examples:
• Compare Stochastic learning strategies for MLPClassiﬁer
• Visualization of MLP weights on MNIST
Regression
Class MLPRegressor implements a multi-layer perceptron (MLP) that trains using backpropagation with no activa-
tion function in the output layer, which can also be seen as using the identity function as activation function. Therefore,
it uses the square error as the loss function, and the output is a set of continuous values.
MLPRegressor also supports multi-output regression, in which a sample can have more than one target.
Regularization
Both MLPRegressor and class:MLPClassiﬁer use parameter alpha for regularization (L2 regularization) term
which helps in avoiding overﬁtting by penalizing weights with large magnitudes. Following plot displays varying
decision function with value of alpha.
See the examples below for further information.
Examples:
• Varying regularization in Multi-layer Perceptron
266
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Algorithms
MLP trains using Stochastic Gradient Descent, Adam, or L-BFGS. Stochastic Gradient Descent (SGD) updates pa-
rameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.
𝑤←𝑤−𝜂(𝛼𝜕𝑅(𝑤)
𝜕𝑤
+ 𝜕𝐿𝑜𝑠𝑠
𝜕𝑤
)
where 𝜂is the learning rate which controls the step-size in the parameter space search. 𝐿𝑜𝑠𝑠is the loss function used
for the network.
More details can be found in the documentation of SGD
Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update
parameters based on adaptive estimates of lower-order moments.
With SGD or Adam, training supports online and mini-batch learning.
L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a
function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation
uses the Scipy version of L-BFGS.
If the selected solver is ‘L-BFGS’, training does not support online nor mini-batch learning.
Complexity
Suppose there are 𝑛training samples, 𝑚features, 𝑘hidden layers, each containing ℎneurons - for simplicity, and 𝑜
output neurons. The time complexity of backpropagation is 𝑂(𝑛· 𝑚· ℎ𝑘· 𝑜· 𝑖), where 𝑖is the number of iterations.
Since backpropagation has a high time complexity, it is advisable to start with smaller number of hidden neurons and
few hidden layers for training.
Mathematical formulation
Given a set of training examples (𝑥1, 𝑦1), (𝑥2, 𝑦2), . . . , (𝑥𝑛, 𝑦𝑛) where 𝑥𝑖∈R𝑛and 𝑦𝑖∈{0, 1}, a one hidden layer
one hidden neuron MLP learns the function 𝑓(𝑥) = 𝑊2𝑔(𝑊𝑇
1 𝑥+ 𝑏1) + 𝑏2 where 𝑊1 ∈R𝑚and 𝑊2, 𝑏1, 𝑏2 ∈R are
model parameters. 𝑊1, 𝑊2 represent the weights of the input layer and hidden layer, resepctively; and 𝑏1, 𝑏2 represent
the bias added to the hidden layer and the output layer, respectively. 𝑔(·) : 𝑅→𝑅is the activation function, set by
3.1. Supervised learning
267
scikit-learn user guide, Release 0.18.2
default as the hyperbolic tan. It is given as,
𝑔(𝑧) = 𝑒𝑧−𝑒−𝑧
𝑒𝑧+ 𝑒−𝑧
For binary classiﬁcation, 𝑓(𝑥) passes through the logistic function 𝑔(𝑧) = 1/(1+𝑒−𝑧) to obtain output values between
zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the
rest to the negative class.
If there are more than two classes, 𝑓(𝑥) itself would be a vector of size (n_classes,). Instead of passing through logistic
function, it passes through the softmax function, which is written as,
softmax(𝑧)𝑖=
exp(𝑧𝑖)
∑︀𝑘
𝑙=1 exp(𝑧𝑙)
where 𝑧𝑖represents the 𝑖th element of the input to softmax, which corresponds to class 𝑖, and 𝐾is the number of
classes. The result is a vector containing the probabilities that sample 𝑥belong to each class. The output is the class
with the highest probability.
In regression, the output remains as 𝑓(𝑥); therefore, output activation function is just the identity function.
MLP uses different loss functions depending on the problem type. The loss function for classiﬁcation is Cross-Entropy,
which in binary case is given as,
𝐿𝑜𝑠𝑠(ˆ𝑦, 𝑦, 𝑊) = −𝑦ln ˆ𝑦−(1 −𝑦) ln (1 −ˆ𝑦) + 𝛼||𝑊||2
2
where 𝛼||𝑊||2
2 is an L2-regularization term (aka penalty) that penalizes complex models; and 𝛼> 0 is a non-negative
hyperparameter that controls the magnitude of the penalty.
For regression, MLP uses the Square Error loss function; written as,
𝐿𝑜𝑠𝑠(ˆ𝑦, 𝑦, 𝑊) = 1
2||ˆ𝑦−𝑦||2
2 + 𝛼||𝑊||2
2
Starting from initial random weights, multi-layer perceptron (MLP) minimizes the loss function by repeatedly updating
these weights. After computing the loss, a backward pass propagates it from the output layer to the previous layers,
providing each weight parameter with an update value meant to decrease the loss.
In gradient descent, the gradient ∇𝐿𝑜𝑠𝑠𝑊of the loss with respect to the weights is computed and deducted from 𝑊.
More formally, this is expressed as,
𝑊𝑖+1 = 𝑊𝑖−𝜖∇𝐿𝑜𝑠𝑠𝑖
𝑊
where 𝑖is the iteration step, and 𝜖is the learning rate with a value larger than 0.
The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below
a certain, small number.
Tips on Practical Use
• Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For
example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and
variance 1. Note that you must apply the same scaling to the test set for meaningful results. You can use
StandardScaler for standardization.
>>> from sklearn.preprocessing import StandardScaler
>>> scaler = StandardScaler()
>>> # Don't cheat - fit only on training data
>>> scaler.fit(X_train)
>>> X_train = scaler.transform(X_train)
>>> # apply same transformation to test data
>>> X_test = scaler.transform(X_test)
268
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
An alternative and recommended approach is to use StandardScaler in a Pipeline
• Finding a reasonable regularization parameter 𝛼is best done using GridSearchCV, usually in the range 10.0
** -np.arange(1,7).
• Empirically, we observed that L-BFGS converges faster and with better solutions on small datasets. For relatively
large datasets, however, Adam is very robust. It usually converges quickly and gives pretty good performance.
SGD with momentum or nesterov’s momentum, on the other hand, can perform better than those two algorithms
if learning rate is correctly tuned.
More control with warm_start
If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using
warm_start=True and max_iter=1 and iterating yourself can be helpful:
>>> X = [[0., 0.], [1., 1.]]
>>> y = [0, 1]
>>> clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_
˓→start=True)
>>> for i in range(10):
...
clf.fit(X, y)
...
# additional monitoring / inspection
MLPClassifier(...
References:
• “Learning representations by back-propagating errors.” Rumelhart, David E., Geoffrey E. Hinton, and Ronald
J. Williams.
• “Stochastic Gradient Descent” L. Bottou - Website, 2010.
• “Backpropagation” Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011.
• “Efﬁcient BackProp” Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998.
• “Adam:
A method for stochastic optimization.”
Kingma, Diederik, and Jimmy Ba.
arXiv preprint
arXiv:1412.6980 (2014).
3.2 Unsupervised learning
3.2.1 Gaussian mixture models
sklearn.mixture is a package which enables one to learn Gaussian Mixture Models (diagonal, spherical, tied
and full covariance matrices supported), sample them, and estimate them from data. Facilities to help determine the
appropriate number of components are also provided.
A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a
ﬁnite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing
k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the
latent Gaussians.
Scikit-learn implements different classes to estimate Gaussian mixture models, that correspond to different estimation
strategies, detailed below.
3.2. Unsupervised learning
269
scikit-learn user guide, Release 0.18.2
Fig. 3.3: Two-component Gaussian mixture model: data points, and equi-probability surfaces of the model.
Gaussian Mixture
The GaussianMixture object implements the expectation-maximization (EM) algorithm for ﬁtting mixture-of-
Gaussian models. It can also draw conﬁdence ellipsoids for multivariate models, and compute the Bayesian Infor-
mation Criterion to assess the number of clusters in the data. A GaussianMixture.fit method is provided that
learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it mostly
probably belong to using the GaussianMixture.predict method.
The GaussianMixture comes with different options to constrain the covariance of the difference classes estimated:
spherical, diagonal, tied or full covariance.
Examples:
• See GMM covariances for an example of using the Gaussian mixture as clustering on the iris dataset.
• See Density Estimation for a Gaussian mixture for an example on plotting the density estimation.
Pros and cons of class GaussianMixture
Pros
Speed It is the fastest algorithm for learning mixture models
Agnostic As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or
bias the cluster sizes to have speciﬁc structures that might or might not apply.
Cons
Singularities When one has insufﬁciently many points per mixture, estimating the covariance matrices
becomes difﬁcult, and the algorithm is known to diverge and ﬁnd solutions with inﬁnite likelihood
unless one regularizes the covariances artiﬁcially.
270
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Number of components This algorithm will always use all the components it has access to, needing
held-out data or information theoretical criteria to decide how many components to use in the ab-
sence of external cues.
Selecting the number of components in a classical Gaussian Mixture Model
The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efﬁcient way. In theory,
it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming
that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a Variational
Bayesian Gaussian mixture avoids the speciﬁcation of the number of components for a Gaussian mixture model.
Examples:
• See Gaussian Mixture Model Selection for an example of model selection performed with classical Gaussian
mixture.
Estimation algorithm Expectation-maximization
The main difﬁculty in learning Gaussian mixture models from unlabeled data is that it is one usually doesn’t know
which points came from which latent component (if one has access to this information it gets very easy to ﬁt a separate
Gaussian distribution to each set of points). Expectation-maximization is a well-founded statistical algorithm to get
around this problem by an iterative process. First one assumes random components (randomly centered on data points,
3.2. Unsupervised learning
271
scikit-learn user guide, Release 0.18.2
learned from k-means, or even just normally distributed around the origin) and computes for each point a probability
of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of
the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum.
Variational Bayesian Gaussian Mixture
The BayesianGaussianMixture object implements a variant of the Gaussian mixture model with variational
inference algorithms. The API is similar as the one deﬁned by GaussianMixture.
Estimation algorithm: variational inference
Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence
(including priors) instead of data likelihood. The principle behind variational methods is the same as expectation-
maximization (that is both are iterative algorithms that alternate between ﬁnding the probabilities for each point to
be generated by each mixture and ﬁtting the mixture to these assigned points), but variational methods add regular-
ization by integrating information from prior distributions. This avoids the singularities often found in expectation-
maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not
usually as much so as to render usage unpractical.
Due to its Bayesian nature, the variational algorithm needs more hyper- parameters than expectation-maximization,
the most important of these being the concentration parameter weight_concentration_prior. Specifying a
low value for the concentration prior will make the model put most of the weight on few components set the remain-
ing components weights very close to zero. High values of the concentration prior will allow a larger number of
components to be active in the mixture.
The parameters implementation of the BayesianGaussianMixture class proposes two types of prior for the
weights distribution: a ﬁnite mixture model with Dirichlet distribution and an inﬁnite mixture model with the Dirichlet
Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a ﬁxed
maximum number of components (called the Stick-breaking representation). The number of components actually used
almost always depends on the data.
The next ﬁgure compares the results obtained for the different type of the weight concentration prior (parameter
weight_concentration_prior_type) for different values of weight_concentration_prior. Here,
we can see the the value of the weight_concentration_prior parameter has a strong impact on the effective
number of active components obtained. We can also notice that large values for the concentration weight prior lead
to more uniform weights when the type of prior is ‘dirichlet_distribution’ while this is not necessarily the case for the
‘dirichlet_process’ type (used by default).
272
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The examples below compare Gaussian mixture models with a ﬁxed number of components, to the variational Gaus-
sian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is ﬁtted with 5 components on
a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is
able to limit itself to only 2 components whereas the Gaussian mixture ﬁts the data with a ﬁxed number of components
that has to be set a priori by the user. In this case the user has selected n_components=5 which does not match the
true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture
models with a Dirichlet process prior can take a conservative stand, and ﬁt only one component.
On the following ﬁgure we are ﬁtting a dataset not well-depicted by a Gaussian mixture.
Adjusting the
weight_concentration_prior, parameter of the class:BayesianGaussianMixture controls the number of com-
ponents used to ﬁt this data. We also present on the last two plots a random sampling generated from the two resulting
mixtures.
Examples:
• See Gaussian Mixture Model Ellipsoids for an example on plotting the conﬁdence ellipsoids for both
GaussianMixture and BayesianGaussianMixture.
• Gaussian
Mixture
Model
Sine
Curve
shows
using
GaussianMixture
and
BayesianGaussianMixture to ﬁt a sine wave.
3.2. Unsupervised learning
273
scikit-learn user guide, Release 0.18.2
• See
Concentration
Prior
Type
Analysis
of
Variation
Bayesian
Gaussian
Mixture
for
an
ex-
ample
plotting
the
conﬁdence
ellipsoids
for
the
BayesianGaussianMixture
with
dif-
ferent
weight_concentration_prior_type
for
different
values
of
the
parameter
weight_concentration_prior.
Pros and cons of variational inference with BayesianGaussianMixture
Pros
Automatic selection when
weight_concentration_prior
is
small
enough
and
n_components is larger than what is found necessary by the model, the Variational Bayesian
mixture model has a natural tendency to set some mixture weights values close to zero. This makes
it possible to let the model choose a suitable number of effective components automatically. Only an
upper bound of this number needs to be provided. Note however that the “ideal” number of active
components is very application speciﬁc and is typically ill-deﬁned in a data exploration setting.
Less sensitivity to the number of parameters unlike ﬁnite models, which will almost always use
all components as much as they can, and hence will produce wildly different solutions for
different numbers of components, the variantional inference with a Dirichlet process prior
(weight_concentration_prior_type='dirichlet_process') won’t change much
with changes to the parameters, leading to more stability and less tuning.
Regularization due to the incorporation of prior information, variational solutions have less pathological
special cases than expectation-maximization solutions.
274
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Cons
Speed the extra parametrization necessary for variational inference make inference slower, although not
by much.
Hyperparameters this algorithm needs an extra hyperparameter that might need experimental tuning via
cross-validation.
Bias there are many implicit biases in the inference algorithms (and also in the Dirichlet process if used),
and whenever there is a mismatch between these biases and the data it might be possible to ﬁt better
models using a ﬁnite mixture.
The Dirichlet Process
Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior
probability distribution on clusterings with an inﬁnite, unbounded, number of partitions. Variational techniques let us
incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a
ﬁnite Gaussian mixture model.
An important question is how can the Dirichlet process use an inﬁnite, unbounded number of clusters and still
be consistent.
While a full explanation doesn’t ﬁt this manual, one can think of its stick breaking process
<https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process> analogy to help understanding it. The
stick breaking process is a generative story for the Dirichlet process. We start with a unit-length stick and in each
step we break off a portion of the remaining stick. Each time, we associate the length of the piece of the stick to
the proportion of points that falls into a group of the mixture. At the end, to represent the inﬁnite mixture, we as-
sociate the last remaining piece of the stick to the proportion of points that don’t fall into all the other groups. The
length of each piece is random variable with probability proportional to the concentration parameter. Smaller value of
the concentration will divide the unit-length into larger pieces of the stick (deﬁning more concentrated distribution).
Larger concentration values will create smaller pieces of the stick (increasing the number of components with non
zero weights).
Variational inference techniques for the Dirichlet process still work with a ﬁnite approximation to this inﬁnite mixture
model, but instead of having to specify a priori how many components one wants to use, one just speciﬁes the concen-
tration parameter and an upper bound on the number of mixture components (this upper bound, assuming it is higher
than the “true” number of components, affects only algorithmic complexity, not the actual number of components
used).
3.2.2 Manifold learning
Look for the bare necessities
The simple bare necessities
Forget about your worries and your strife
I mean the bare necessities
Old Mother Nature’s recipes
That bring the bare necessities of life
– Baloo’s song [The Jungle Book]
Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the
idea that the dimensionality of many data sets is only artiﬁcially high.
3.2. Unsupervised learning
275
scikit-learn user guide, Release 0.18.2
Introduction
High-dimensional datasets can be very difﬁcult to visualize. While data in two or three dimensions can be plotted to
show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization
of the structure of a dataset, the dimension must be reduced in some way.
The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though
this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired.
In a random projection, it is likely that the more interesting structure within the data will be lost.
276
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
To address this concern, a number of supervised and unsupervised linear dimensionality reduction frameworks have
been designed, such as Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant
Analysis, and others. These algorithms deﬁne speciﬁc rubrics to choose an “interesting” linear projection of the data.
These methods can be powerful, but often miss important non-linear structure in the data.
Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-
linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it
learns the high-dimensional structure of the data from the data itself, without the use of predetermined classiﬁcations.
Examples:
• See Manifold learning on handwritten digits: Locally Linear Embedding, Isomap... for an example of di-
mensionality reduction on handwritten digits.
• See Comparison of Manifold Learning methods for an example of dimensionality reduction on a toy “S-
curve” dataset.
The manifold learning implementations available in scikit-learn are summarized below
Isomap
One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping. Isomap can
be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional
3.2. Unsupervised learning
277
scikit-learn user guide, Release 0.18.2
embedding which maintains geodesic distances between all points. Isomap can be performed with the object Isomap.
Complexity
The Isomap algorithm comprises three stages:
1. Nearest neighbor search. Isomap uses sklearn.neighbors.BallTree for efﬁcient neighbor search.
The cost is approximately 𝑂[𝐷log(𝑘)𝑁log(𝑁)], for 𝑘nearest neighbors of 𝑁points in 𝐷dimensions.
2. Shortest-path graph search. The most efﬁcient known algorithms for this are Dijkstra’s Algorithm, which is
approximately 𝑂[𝑁2(𝑘+ log(𝑁))], or the Floyd-Warshall algorithm, which is 𝑂[𝑁3]. The algorithm can be
selected by the user with the path_method keyword of Isomap. If unspeciﬁed, the code attempts to choose
the best algorithm for the input data.
3. Partial eigenvalue decomposition. The embedding is encoded in the eigenvectors corresponding to the 𝑑
largest eigenvalues of the 𝑁× 𝑁isomap kernel. For a dense solver, the cost is approximately 𝑂[𝑑𝑁2]. This
cost can often be improved using the ARPACK solver. The eigensolver can be speciﬁed by the user with the
path_method keyword of Isomap. If unspeciﬁed, the code attempts to choose the best algorithm for the
input data.
The overall complexity of Isomap is 𝑂[𝐷log(𝑘)𝑁log(𝑁)] + 𝑂[𝑁2(𝑘+ log(𝑁))] + 𝑂[𝑑𝑁2].
• 𝑁: number of training data points
• 𝐷: input dimension
• 𝑘: number of nearest neighbors
• 𝑑: output dimension
References:
• “A global geometric framework for nonlinear dimensionality reduction” Tenenbaum, J.B.; De Silva, V.; &
Langford, J.C. Science 290 (5500)
278
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Locally Linear Embedding
Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within
local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally
compared to ﬁnd the best non-linear embedding.
Locally linear embedding can be performed with function locally_linear_embedding or its object-oriented
counterpart LocallyLinearEmbedding.
Complexity
The standard LLE algorithm comprises three stages:
1. Nearest Neighbors Search. See discussion under Isomap above.
2. Weight Matrix Construction. 𝑂[𝐷𝑁𝑘3]. The construction of the LLE weight matrix involves the solution of
a 𝑘× 𝑘linear equation for each of the 𝑁local neighborhoods
3. Partial Eigenvalue Decomposition. See discussion under Isomap above.
The overall complexity of standard LLE is 𝑂[𝐷log(𝑘)𝑁log(𝑁)] + 𝑂[𝐷𝑁𝑘3] + 𝑂[𝑑𝑁2].
• 𝑁: number of training data points
• 𝐷: input dimension
• 𝑘: number of nearest neighbors
• 𝑑: output dimension
References:
• “Nonlinear dimensionality reduction by locally linear embedding” Roweis, S. & Saul, L. Science 290:2323
(2000)
Modiﬁed Locally Linear Embedding
One well-known issue with LLE is the regularization problem. When the number of neighbors is greater than the
number of input dimensions, the matrix deﬁning each local neighborhood is rank-deﬁcient. To address this, standard
3.2. Unsupervised learning
279
scikit-learn user guide, Release 0.18.2
LLE applies an arbitrary regularization parameter 𝑟, which is chosen relative to the trace of the local weight matrix.
Though it can be shown formally that as 𝑟→0, the solution converges to the desired embedding, there is no guarantee
that the optimal solution will be found for 𝑟> 0. This problem manifests itself in embeddings which distort the
underlying geometry of the manifold.
One method to address the regularization problem is to use multiple weight vectors in each neighborhood.
This is the essence of modiﬁed locally linear embedding (MLLE). MLLE can be performed with function
locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the key-
word method = 'modified'. It requires n_neighbors > n_components.
Complexity
The MLLE algorithm comprises three stages:
1. Nearest Neighbors Search. Same as standard LLE
2. Weight Matrix Construction. Approximately 𝑂[𝐷𝑁𝑘3]+𝑂[𝑁(𝑘−𝐷)𝑘2]. The ﬁrst term is exactly equivalent
to that of standard LLE. The second term has to do with constructing the weight matrix from multiple weights.
In practice, the added cost of constructing the MLLE weight matrix is relatively small compared to the cost of
steps 1 and 3.
3. Partial Eigenvalue Decomposition. Same as standard LLE
The overall complexity of MLLE is 𝑂[𝐷log(𝑘)𝑁log(𝑁)] + 𝑂[𝐷𝑁𝑘3] + 𝑂[𝑁(𝑘−𝐷)𝑘2] + 𝑂[𝑑𝑁2].
• 𝑁: number of training data points
• 𝐷: input dimension
• 𝑘: number of nearest neighbors
• 𝑑: output dimension
References:
• “MLLE: Modiﬁed Locally Linear Embedding Using Multiple Weights” Zhang, Z. & Wang, J.
280
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Hessian Eigenmapping
Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization
problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover
the locally linear structure. Though other implementations note its poor scaling with data size, sklearn imple-
ments some algorithmic improvements which make its cost comparable to that of other LLE variants for small output
dimension. HLLE can be performed with function locally_linear_embedding or its object-oriented counter-
part LocallyLinearEmbedding, with the keyword method = 'hessian'. It requires n_neighbors >
n_components * (n_components + 3) / 2.
Complexity
The HLLE algorithm comprises three stages:
1. Nearest Neighbors Search. Same as standard LLE
2. Weight Matrix Construction. Approximately 𝑂[𝐷𝑁𝑘3] + 𝑂[𝑁𝑑6]. The ﬁrst term reﬂects a similar cost to
that of standard LLE. The second term comes from a QR decomposition of the local hessian estimator.
3. Partial Eigenvalue Decomposition. Same as standard LLE
The overall complexity of standard HLLE is 𝑂[𝐷log(𝑘)𝑁log(𝑁)] + 𝑂[𝐷𝑁𝑘3] + 𝑂[𝑁𝑑6] + 𝑂[𝑑𝑁2].
• 𝑁: number of training data points
• 𝐷: input dimension
• 𝑘: number of nearest neighbors
• 𝑑: output dimension
References:
• “Hessian Eigenmaps: Locally linear embedding techniques for high-dimensional data” Donoho, D. &
Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)
3.2. Unsupervised learning
281
scikit-learn user guide, Release 0.18.2
Spectral Embedding
Spectral Embedding (also known as Laplacian Eigenmaps) is one method to calculate non-linear embedding. It ﬁnds
a low dimensional representation of the data using a spectral decomposition of the graph Laplacian. The graph gen-
erated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space.
Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped
close to each other in the low dimensional space, preserving local distances. Spectral embedding can be performed
with the function spectral_embedding or its object-oriented counterpart SpectralEmbedding.
Complexity
The Spectral Embedding algorithm comprises three stages:
1. Weighted Graph Construction. Transform the raw input data into graph representation using afﬁnity (adja-
cency) matrix representation.
2. Graph Laplacian Construction. unnormalized Graph Laplacian is constructed as 𝐿= 𝐷−𝐴for and normal-
ized one as 𝐿= 𝐷−1
2 (𝐷−𝐴)𝐷−1
2 .
3. Partial Eigenvalue Decomposition. Eigenvalue decomposition is done on graph Laplacian
The overall complexity of spectral embedding is 𝑂[𝐷log(𝑘)𝑁log(𝑁)] + 𝑂[𝐷𝑁𝑘3] + 𝑂[𝑑𝑁2].
• 𝑁: number of training data points
• 𝐷: input dimension
• 𝑘: number of nearest neighbors
• 𝑑: output dimension
References:
• “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation” M. Belkin, P. Niyogi, Neural
Computation, June 2003; 15 (6):1373-1396
Local Tangent Space Alignment
Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough
to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE,
LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global
optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function
locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the key-
word method = 'ltsa'.
Complexity
The LTSA algorithm comprises three stages:
1. Nearest Neighbors Search. Same as standard LLE
2. Weight Matrix Construction. Approximately 𝑂[𝐷𝑁𝑘3] + 𝑂[𝑘2𝑑]. The ﬁrst term reﬂects a similar cost to that
of standard LLE.
3. Partial Eigenvalue Decomposition. Same as standard LLE
282
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The overall complexity of standard LTSA is 𝑂[𝐷log(𝑘)𝑁log(𝑁)] + 𝑂[𝐷𝑁𝑘3] + 𝑂[𝑘2𝑑] + 𝑂[𝑑𝑁2].
• 𝑁: number of training data points
• 𝐷: input dimension
• 𝑘: number of nearest neighbors
• 𝑑: output dimension
References:
• “Principal manifolds and nonlinear dimensionality reduction via tangent space alignment” Zhang, Z. & Zha,
H. Journal of Shanghai Univ. 8:406 (2004)
Multi-dimensional Scaling (MDS)
Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well
the distances in the original high-dimensional space.
In general, is a technique used for analyzing similarity or dissimilarity data. MDS attempts to model similarity or
dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction
frequencies of molecules, or trade indices between countries.
There exists two types of MDS algorithm: metric and non metric. In the scikit-learn, the class MDS implements
both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the
distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In
the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic
relationship between the distances in the embedded space and the similarities/dissimilarities.
Let 𝑆be the similarity matrix, and 𝑋the coordinates of the 𝑛input points. Disparities ˆ𝑑𝑖𝑗are transformation of the
similarities chosen in some optimal ways. The objective, called the stress, is then deﬁned by 𝑠𝑢𝑚𝑖<𝑗𝑑𝑖𝑗(𝑋)−ˆ𝑑𝑖𝑗(𝑋)
Metric MDS
The simplest metric MDS model, called absolute MDS, disparities are deﬁned by ˆ𝑑𝑖𝑗= 𝑆𝑖𝑗. With absolute MDS, the
value 𝑆𝑖𝑗should then correspond exactly to the distance between point 𝑖and 𝑗in the embedding point.
3.2. Unsupervised learning
283
scikit-learn user guide, Release 0.18.2
Most commonly, disparities are set to ˆ𝑑𝑖𝑗= 𝑏𝑆𝑖𝑗.
Nonmetric MDS
Non metric MDS focuses on the ordination of the data. If 𝑆𝑖𝑗< 𝑆𝑘𝑙, then the embedding should enforce 𝑑𝑖𝑗< 𝑑𝑗𝑘.
A simple algorithm to enforce that is to use a monotonic regression of 𝑑𝑖𝑗on 𝑆𝑖𝑗, yielding disparities ˆ𝑑𝑖𝑗in the same
order as 𝑆𝑖𝑗.
A trivial solution to this problem is to set all the points on the origin. In order to avoid that, the disparities ˆ𝑑𝑖𝑗are
normalized.
References:
• “Modern Multidimensional Scaling - Theory and Applications” Borg, I.; Groenen P. Springer Series in Statis-
tics (1997)
• “Nonmetric multidimensional scaling: a numerical method” Kruskal, J. Psychometrika, 29 (1964)
284
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• “Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis” Kruskal, J. Psychome-
trika, 29, (1964)
t-distributed Stochastic Neighbor Embedding (t-SNE)
t-SNE (TSNE) converts afﬁnities of data points to probabilities. The afﬁnities in the original space are represented by
Gaussian joint probabilities and the afﬁnities in the embedded space are represented by Student’s t-distributions. This
allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques:
• Revealing the structure at many scales on a single map
• Revealing data that lie in multiple, different, manifolds or clusters
• Reducing the tendency to crowd points together at the center
While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will
focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the
S-curve example. This ability to group samples based on the local structure might be beneﬁcial to visually disentangle
a dataset that comprises several manifolds at once as is the case in the digits dataset.
The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will
be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different
initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds
and select the embedding with the lowest KL divergence.
The disadvantages to using t-SNE are roughly:
• t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will
ﬁnish in seconds or minutes
• The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.
• The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However,
it is perfectly legitimate to pick the embedding with the least error.
• Global structure is not explicitly preserved. This is problem is mitigated by initializing points with PCA (using
init=’pca’).
3.2. Unsupervised learning
285
scikit-learn user guide, Release 0.18.2
Optimizing t-SNE
The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be
embedded on two or three dimensions.
Optimizing the KL divergence can be a little bit tricky sometimes. There are ﬁve parameters that control the optimiza-
tion of t-SNE and therefore possibly the quality of the resulting embedding:
• perplexity
• early exaggeration factor
• learning rate
• maximum number of iterations
• angle (not used in the exact method)
The perplexity is deﬁned as 𝑘= 2(𝑆) where 𝑆is the Shannon entropy of the conditional probability distribution.
The perplexity of a 𝑘-sided die is 𝑘, so that 𝑘is effectively the number of nearest neighbors t-SNE considers when
generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small
structure. Larger datasets tend to require larger perplexities. The maximum number of iterations is usually high
enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the
ﬁnal optimization. During early exaggeration the joint probabilities in the original space will be artiﬁcially increased
by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the
factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical
parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high
the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten’s FAQ (see
references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we
can approximate larger regions by a single point,leading to better speed but less accurate results.
Barnes-Hut t-SNE
The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algo-
rithms. The optimization is quite difﬁcult and the computation of the gradient is 𝑂[𝑑𝑁𝑙𝑜𝑔(𝑁)], where 𝑑is the number
of output dimensions and 𝑁is the number of samples. The Barnes-Hut method improves on the exact method where
t-SNE complexity is 𝑂[𝑑𝑁2], but has several other notable differences:
• The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical
when building visualizations.
• Barnes-Hut only works with dense input data.
Sparse data matrices can only be embedded with
the exact method or can be approximated by a dense low rank projection for instance using
sklearn.decomposition.TruncatedSVD
• Barnes-Hut is an approximation of the exact method. The approximation is parameterized with the angle pa-
rameter, therefore the angle parameter is unused when method=”exact”
• Barnes-Hut is signiﬁcantly more scalable. Barnes-Hut can be used to embed hundred of thousands of data points
while the exact method can handle thousands of samples before becoming computationally intractable
For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recom-
mended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in
higher dimensional space but limit to small datasets due to computational constraints.
Also note that the digits labels roughly match the natural grouping found by t-SNE while the linear 2D projection of
the PCA model yields a representation where label regions largely overlap. This is a strong clue that this data can be
well separated by non linear methods that focus on the local structure (e.g. an SVM with a Gaussian RBF kernel).
However, failing to visualize well separated homogeneously labeled groups with t-SNE in 2D does not necessarily
286
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
implie that the data cannot be correctly classiﬁed by a supervised model. It might be the case that 2 dimensions are
not enough low to accurately represents the internal structure of the data.
References:
• “Visualizing High-Dimensional Data Using t-SNE” van der Maaten, L.J.P.; Hinton, G. Journal of Machine
Learning Research (2008)
• “t-Distributed Stochastic Neighbor Embedding” van der Maaten, L.J.P.
• “Accelerating t-SNE using Tree-Based Algorithms.” L.J.P. van der Maaten. Journal of Machine Learning
Research 15(Oct):3221-3245, 2014.
Tips on practical use
• Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-
neighbor search, the algorithm may perform poorly otherwise. See StandardScaler for convenient ways of
scaling heterogeneous data.
• The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a
𝑑-dimensional manifold embedded in a 𝐷-dimensional parameter space, the reconstruction error will decrease
as n_components is increased until n_components == d.
• Note that noisy data can “short-circuit” the manifold, in essence acting as a bridge between parts of the manifold
that would otherwise be well-separated. Manifold learning on noisy and/or incomplete data is an active area of
research.
• Certain input conﬁgurations can lead to singular weight matrices, for example when more than two points in the
dataset are identical, or when the data is split into disjointed groups. In this case, solver='arpack' will
fail to ﬁnd the null space. The easiest way to address this is to use solver='dense' which will work on a
singular matrix, though it may be very slow depending on the number of input points. Alternatively, one can
attempt to understand the source of the singularity: if it is due to disjoint sets, increasing n_neighbors may
help. If it is due to identical points in the dataset, removing these points may help.
See also:
Totally Random Trees Embedding can also be useful to derive non-linear representations of feature space, also it does
not perform dimensionality reduction.
3.2.3 Clustering
Clustering of unlabeled data can be performed with the module sklearn.cluster.
Each clustering algorithm comes in two variants: a class, that implements the fit method to learn the clusters on train
data, and a function, that, given train data, returns an array of integer labels corresponding to the different clusters. For
the class, the labels over the training data can be found in the labels_ attribute.
Input data
One important thing to note is that the algorithms implemented in this module can take different kinds of
matrix as input.
All the methods accept standard data matrices of shape [n_samples,n_features].
These can be obtained from the classes in the sklearn.feature_extraction module.
For
3.2. Unsupervised learning
287
scikit-learn user guide, Release 0.18.2
AffinityPropagation,
SpectralClustering and DBSCAN
one can also input similarity ma-
trices of shape [n_samples,n_samples].
These can be obtained from the functions in the
sklearn.metrics.pairwise module.
Overview of clustering methods
Fig. 3.4: A comparison of the clustering algorithms in scikit-learn
Method
name
Parameters
Scalability
Usecase
Geometry
(metric used)
K-Means
number of clusters
Very large n_samples,
medium n_clusters
with MiniBatch code
General-purpose, even
cluster size, ﬂat geometry,
not too many clusters
Distances
between points
Afﬁnity
propaga-
tion
damping, sample
preference
Not scalable with
n_samples
Many clusters, uneven
cluster size, non-ﬂat
geometry
Graph distance
(e.g.
nearest-neighbor
graph)
Mean-
shift
bandwidth
Not scalable with
n_samples
Many clusters, uneven
cluster size, non-ﬂat
geometry
Distances
between points
Spectral
clustering
number of clusters
Medium n_samples,
small n_clusters
Few clusters, even cluster
size, non-ﬂat geometry
Graph distance
(e.g.
nearest-neighbor
graph)
Ward hier-
archical
clustering
number of clusters
Large n_samples and
n_clusters
Many clusters, possibly
connectivity constraints
Distances
between points
Agglomer-
ative
clustering
number of clusters,
linkage type,
distance
Large n_samples and
n_clusters
Many clusters, possibly
connectivity constraints,
non Euclidean distances
Any pairwise
distance
DBSCAN
neighborhood size
Very large n_samples,
medium n_clusters
Non-ﬂat geometry, uneven
cluster sizes
Distances
between nearest
points
Gaussian
mixtures
many
Not scalable
Flat geometry, good for
density estimation
Mahalanobis
distances to
centers
Birch
branching factor,
threshold, optional
global clusterer.
Large n_clusters and
n_samples
Large dataset, outlier
removal, data reduction.
Euclidean
distance
between points
Non-ﬂat geometry clustering is useful when the clusters have a speciﬁc shape, i.e. a non-ﬂat manifold, and the standard
288
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
euclidean distance is not the right metric. This case arises in the two top rows of the ﬁgure above.
Gaussian mixture models, useful for clustering, are described in another chapter of the documentation dedicated
to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per
component.
K-means
The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion
known as the inertia or within-cluster sum-of-squares. This algorithm requires the number of clusters to be speciﬁed.
It scales well to large number of samples and has been used across a large range of application areas in many different
ﬁelds.
The k-means algorithm divides a set of 𝑁samples 𝑋into 𝐾disjoint clusters 𝐶, each described by the mean 𝜇𝑗of
the samples in the cluster. The means are commonly called the cluster “centroids”; note that they are not, in general,
points from 𝑋, although they live in the same space. The K-means algorithm aims to choose centroids that minimise
the inertia, or within-cluster sum of squared criterion:
𝑛
∑︁
𝑖=0
min
𝜇𝑗∈𝐶(||𝑥𝑗−𝜇𝑖||2)
Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent
clusters are. It suffers from various drawbacks:
• Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds
poorly to elongated clusters, or manifolds with irregular shapes.
• Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very
high-dimensional spaces, Euclidean distances tend to become inﬂated (this is an instance of the so-called “curse
of dimensionality”). Running a dimensionality reduction algorithm such as PCA prior to k-means clustering
can alleviate this problem and speed up the computations.
K-means is often referred to as Lloyd’s algorithm. In basic
terms, the algorithm has three steps. The ﬁrst step chooses the initial centroids, with the most basic method being to
choose 𝑘samples from the dataset 𝑋. After initialization, K-means consists of looping between the two other steps.
The ﬁrst step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean
value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids
3.2. Unsupervised learning
289
scikit-learn user guide, Release 0.18.2
are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it
repeats until the centroids do not move signiﬁcantly.
K-means is equivalent
to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.
The algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of the points
is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly,
the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is
fulﬁlled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less
than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less
than the tolerance.
Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent
on the initialization of the centroids. As a result, the computation is often done several times, with different initializa-
tions of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been
implemented in scikit-learn (use the init='kmeans++' parameter). This initializes the centroids to be (generally)
distant from each other, leading to provably better results than random initialization, as shown in the reference.
A parameter can be given to allow K-means to be run in parallel, called n_jobs. Giving this parameter a positive
value uses that many processors (default: 1). A value of -1 uses all available processors, with -2 using one less, and so
on. Parallelization generally speeds up computation at the cost of memory (in this case, multiple copies of centroids
need to be stored, one for each job).
Warning: The parallel version of K-Means is broken on OS X when numpy uses the Accelerate Framework. This
is expected behavior: Accelerate can be called after a fork but you need to execv the subprocess with the Python
binary (which multiprocessing does not do under posix).
K-means can be used for vector quantization. This is achieved using the transform method of a trained model of
KMeans.
Examples:
• Demonstration of k-means assumptions: Demonstrating when k-means performs intuitively and when it does
not
• A demo of K-Means clustering on the handwritten digits data: Clustering handwritten digits
References:
• “k-means++: The advantages of careful seeding” Arthur, David, and Sergei Vassilvitskii, Proceedings of
the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Society for Industrial and Applied
Mathematics (2007)
290
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Mini Batch K-Means
The MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation
time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, ran-
domly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required
to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch
k-means produces results that are generally only slightly worse than the standard algorithm.
The algorithm iterates between two major steps, similar to vanilla k-means. In the ﬁrst step, 𝑏samples are drawn
randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step,
the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch,
the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to
that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed
until convergence or a predetermined number of iterations is reached.
MiniBatchKMeans converges faster than KMeans, but the quality of the results is reduced. In practice this differ-
ence in quality can be quite small, as shown in the example and cited reference.
Examples:
• Comparison of the K-Means and MiniBatchKMeans clustering algorithms: Comparison of KMeans and
MiniBatchKMeans
• Clustering text documents using k-means: Document clustering using sparse MiniBatchKMeans
• Online learning of a dictionary of parts of faces
References:
• “Web Scale K-Means clustering” D. Sculley, Proceedings of the 19th international conference on World wide
web (2010)
Afﬁnity Propagation
AffinityPropagation creates clusters by sending messages between pairs of samples until convergence. A
dataset is then described using a small number of exemplars, which are identiﬁed as those most representative of other
3.2. Unsupervised learning
291
scikit-learn user guide, Release 0.18.2
samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other,
which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at
which point the ﬁnal exemplars are chosen, and hence the ﬁnal clustering is given.
Afﬁnity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this
purpose, the two important parameters are the preference, which controls how many exemplars are used, and the
damping factor.
The main drawback of Afﬁnity Propagation is its complexity. The algorithm has a time complexity of the order
𝑂(𝑁2𝑇), where 𝑁is the number of samples and 𝑇is the number of iterations until convergence. Further, the memory
complexity is of the order 𝑂(𝑁2) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is
used. This makes Afﬁnity Propagation most appropriate for small to medium sized datasets.
Examples:
• Demo of afﬁnity propagation clustering algorithm: Afﬁnity Propagation on a synthetic 2D datasets with 3
classes.
• Visualizing the stock market structure Afﬁnity Propagation on Financial time series to ﬁnd groups of compa-
nies
Algorithm description: The messages sent between points belong to one of two categories. The ﬁrst is the responsi-
bility 𝑟(𝑖, 𝑘), which is the accumulated evidence that sample 𝑘should be the exemplar for sample 𝑖. The second is the
availability 𝑎(𝑖, 𝑘) which is the accumulated evidence that sample 𝑖should choose sample 𝑘to be its exemplar, and
considers the values for all other samples that 𝑘should be an exemplar. In this way, exemplars are chosen by samples
if they are (1) similar enough to many samples and (2) chosen by many samples to be representative of themselves.
More formally, the responsibility of a sample 𝑘to be the exemplar of sample 𝑖is given by:
𝑟(𝑖, 𝑘) ←𝑠(𝑖, 𝑘) −𝑚𝑎𝑥[𝑎(𝑖, ´𝑘) + 𝑠(𝑖, ´𝑘)∀´𝑘̸= 𝑘]
Where 𝑠(𝑖, 𝑘) is the similarity between samples 𝑖and 𝑘. The availability of sample 𝑘to be the exemplar of sample 𝑖is
given by:
𝑎(𝑖, 𝑘) ←𝑚𝑖𝑛[0, 𝑟(𝑘, 𝑘) +
∑︁
´𝑖𝑠.𝑡. ´𝑖/∈{𝑖,𝑘}
𝑟(´𝑖, 𝑘)]
To begin with, all values for 𝑟and 𝑎are set to zero, and the calculation of each iterates until convergence.
292
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Mean Shift
MeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which
works by updating candidates for centroids to be the mean of the points within a given region. These candidates are
then ﬁltered in a post-processing stage to eliminate near-duplicates to form the ﬁnal set of centroids.
Given a candidate centroid 𝑥𝑖for iteration 𝑡, the candidate is updated according to the following equation:
𝑥𝑡+1
𝑖
= 𝑥𝑡
𝑖+ 𝑚(𝑥𝑡
𝑖)
Where 𝑁(𝑥𝑖) is the neighborhood of samples within a given distance around 𝑥𝑖and 𝑚is the mean shift vector that
is computed for each centroid that points towards a region of the maximum increase in the density of points. This
is computed using the following equation, effectively updating a centroid to be the mean of the samples within its
neighborhood:
𝑚(𝑥𝑖) =
∑︀
𝑥𝑗∈𝑁(𝑥𝑖) 𝐾(𝑥𝑗−𝑥𝑖)𝑥𝑗
∑︀
𝑥𝑗∈𝑁(𝑥𝑖) 𝐾(𝑥𝑗−𝑥𝑖)
The algorithm automatically sets the number of clusters, instead of relying on a parameter bandwidth, which dictates
the size of the region to search through. This parameter can be set manually, but can be estimated using the provided
estimate_bandwidth function, which is called if the bandwidth is not set.
The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the
algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in
centroids is small.
Labelling a new sample is performed by ﬁnding the nearest centroid for a given sample.
Examples:
• A demo of the mean-shift clustering algorithm: Mean Shift clustering on a synthetic 2D datasets with 3
classes.
References:
• “Mean shift: A robust approach toward feature space analysis.” D. Comaniciu and P. Meer, IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (2002)
3.2. Unsupervised learning
293
scikit-learn user guide, Release 0.18.2
Spectral clustering
SpectralClustering does a low-dimension embedding of the afﬁnity matrix between samples, followed by a
KMeans in the low dimensional space. It is especially efﬁcient if the afﬁnity matrix is sparse and the pyamg module
is installed. SpectralClustering requires the number of clusters to be speciﬁed. It works well for a small number of
clusters but is not advised when using many clusters.
For two clusters, it solves a convex relaxation of the normalised cuts problem on the similarity graph: cutting the
graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This
criteria is especially interesting when working on images: graph vertices are pixels, and edges of the similarity graph
are a function of the gradient of the image.
Warning: Transforming distance to well-behaved similarities
Note that if the values of your similarity matrix are not well distributed, e.g. with negative values or with a distance
matrix rather than a similarity, the spectral problem will be singular and the problem not solvable. In which case
it is advised to apply a transformation to the entries of the matrix. For instance, in the case of a signed distance
matrix, is common to apply a heat kernel:
similarity = np.exp(-beta * distance / distance.std())
See the examples for such an application.
Examples:
• Spectral clustering for image segmentation: Segmenting objects from a noisy background using spectral
clustering.
• Segmenting the picture of a raccoon face in regions: Spectral clustering to split the image of the raccoon face
in regions.
294
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Different label assignment strategies
Different label assignment strategies can be used, corresponding to the assign_labels parameter of
SpectralClustering. The "kmeans" strategy can match ﬁner details of the data, but it can be more unsta-
ble. In particular, unless you control the random_state, it may not be reproducible from run-to-run, as it depends
on a random initialization. On the other hand, the "discretize" strategy is 100% reproducible, but it tends to
create parcels of fairly even and geometrical shape.
assign_labels="kmeans"
assign_labels="discretize"
References:
• “A Tutorial on Spectral Clustering” Ulrike von Luxburg, 2007
• “Normalized cuts and image segmentation” Jianbo Shi, Jitendra Malik, 2000
• “A Random Walks View of Spectral Segmentation” Marina Meila, Jianbo Shi, 2001
• “On Spectral Clustering: Analysis and an algorithm” Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001
Hierarchical clustering
Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting
them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique
cluster that gathers all the samples, the leaves being the clusters with only one sample. See the Wikipedia page for
more details.
The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each
observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the
metric used for the merge strategy:
• Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in
this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.
• Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.
• Average linkage minimizes the average of the distances between all observations of pairs of clusters.
AgglomerativeClustering can also scale to large number of samples when it is used jointly with a connectivity
matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at
each step all the possible merges.
3.2. Unsupervised learning
295
scikit-learn user guide, Release 0.18.2
FeatureAgglomeration
The FeatureAgglomeration uses agglomerative clustering to group together features that look very similar,
thus decreasing the number of features. It is a dimensionality reduction tool, see Unsupervised dimensionality
reduction.
Different linkage type: Ward, complete and average linkage
AgglomerativeClustering
supports
Ward,
average,
and
complete
linkage
strategies.
Agglomerative cluster has a “rich get richer” behavior that leads to uneven cluster sizes. In this regard, complete
linkage is the worst strategy, and Ward gives the most regular sizes. However, the afﬁnity (or distance used in
clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative.
Examples:
• Various Agglomerative Clustering on a 2D embedding of digits: exploration of the different linkage strategies
in a real dataset.
Adding connectivity constraints
An interesting aspect of AgglomerativeClustering is that connectivity constraints can be added to this al-
gorithm (only adjacent clusters can be merged together), through a connectivity matrix that deﬁnes for each sample
the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the
connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming
clusters that extend across overlapping folds of the roll.
296
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when
the number of the samples is high.
The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only
at the intersection of a row and a column with indices of the dataset that should be connected.
This matrix
can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merg-
ing pages with a link pointing from one to another.
It can also be learned from the data, for instance using
sklearn.neighbors.kneighbors_graph to restrict merging to nearest neighbors as in this example, or using
sklearn.feature_extraction.image.grid_to_graph to enable only merging of neighboring pixels on
an image, as in the raccoon face example.
Examples:
• A demo of structured Ward hierarchical clustering on a raccoon face image: Ward clustering to split the
image of a raccoon face in regions.
• Hierarchical clustering: structured vs unstructured ward: Example of Ward algorithm on a swiss-roll, com-
parison of structured approaches versus unstructured approaches.
• Feature agglomeration vs. univariate selection: Example of dimensionality reduction with feature agglomer-
ation based on Ward hierarchical clustering.
• Agglomerative clustering with and without structure
Warning: Connectivity constraints with average and complete linkage
Connectivity constraints and complete or average linkage can enhance the ‘rich getting richer’ aspect of agglom-
erative clustering, particularly so if they are built with sklearn.neighbors.kneighbors_graph. In the
limit of a small number of clusters, they tend to give a few macroscopically occupied clusters and almost empty
ones. (see the discussion in Agglomerative clustering with and without structure).
3.2. Unsupervised learning
297
scikit-learn user guide, Release 0.18.2
Varying the metric
Average and complete linkage can be used with a variety of distances (or afﬁnities), in particular Euclidean distance
(l2), Manhattan distance (or Cityblock, or l1), cosine distance, or any precomputed afﬁnity matrix.
• l1 distance is often good for sparse features, or sparse noise: ie many of the features are zero, as in text mining
using occurences of rare words.
• cosine distance is interesting because it is invariant to global scalings of the signal.
The
guidelines
for
choosing
a
metric
is
to
use
one
that
maximizes
the
dis-
tance
between
samples
in
different
classes,
and
minimizes
that
within
each
class.
Examples:
• Agglomerative clustering with different metrics
DBSCAN
The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather
generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are
convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in
areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance
measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There
are two parameters to the algorithm, min_samples and eps, which deﬁne formally what we mean when we say
dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster.
More formally, we deﬁne a core sample as being a sample in the dataset such that there exist min_samples other
samples within a distance of eps, which are deﬁned as neighbors of the core sample. This tells us that the core sample
is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core
sample, ﬁnding all of its neighbors that are core samples, ﬁnding all of their neighbors that are core samples, and so
on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster
but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.
Any core sample is part of a cluster, by deﬁnition. Any sample that is not a core sample, and is at least eps in distance
from any core sample, is considered an outlier by the algorithm.
In the ﬁgure below, the color indicates cluster membership, with large circles indicating core samples found by the
algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by
black points below.
298
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples:
• Demo of DBSCAN clustering algorithm
Implementation
The algorithm is non-deterministic, but the core samples will always belong to the same clusters (although the
labels may be different). The non-determinism comes from deciding to which cluster a non-core sample belongs.
A non-core sample can have a distance lower than eps to two core samples in different clusters. By the triangular
inequality, those two core samples must be more distant than eps from each other, or they would be in the same
cluster. The non-core sample is assigned to whichever cluster is generated ﬁrst, where the order is determined
randomly. Other than the ordering of the dataset, the algorithm is deterministic, making the results relatively stable
between runs on the same data.
The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids
calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom
metrics is retained; for details, see NearestNeighbors.
Memory consumption for large sample sizes
This implementation is by default not memory efﬁcient because it constructs a full pairwise similarity matrix in the
case where kd-trees or ball-trees cannot be used (e.g. with sparse matrices). This matrix will consume n^2 ﬂoats.
A couple of mechanisms for getting around this are:
• A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precom-
puted in a memory-efﬁcient way and dbscan can be run over this with metric='precomputed'.
• The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using
BIRCH. Then you only have a relatively small number of representatives for a large number of points. You
can then provide a sample_weight when ﬁtting DBSCAN.
References:
• “A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise” Ester, M., H. P.
3.2. Unsupervised learning
299
scikit-learn user guide, Release 0.18.2
Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996
Birch
The Birch builds a tree called the Characteristic Feature Tree (CFT) for the given data. The data is essentially lossy
compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called
Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes
can have CF Nodes as children.
The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data
in memory. This information includes:
• Number of samples in a subcluster.
• Linear Sum - A n-dimensional vector holding the sum of all samples
• Squared Sum - Sum of the squared L2 norm of all samples.
• Centroids - To avoid recalculation linear sum / n_samples.
• Squared norm of the centroids.
The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the
number of subclusters in a node and the threshold limits the distance between the entering sample and the existing
subclusters.
This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of
subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by
feeding it into a global clusterer. This global clusterer can be set by n_clusters. If n_clusters is set to None,
the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into
global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.
Algorithm description:
• A new sample is inserted into the root of the CF Tree which is a CF Node. It is then merged with the subcluster of
the root, that has the smallest radius after merging, constrained by the threshold and branching factor conditions.
If the subcluster has any child node, then this is done repeatedly till it reaches a leaf. After ﬁnding the nearest
subcluster in the leaf, the properties of this subcluster and the parent subclusters are recursively updated.
• If the radius of the subcluster obtained by merging the new sample and the nearest subcluster is greater than
the square of the threshold and if the number of subclusters is greater than the branching factor, then a space is
temporarily allocated to this new sample. The two farthest subclusters are taken and the subclusters are divided
into two groups on the basis of the distance between these subclusters.
• If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two.
If there is no room, then this node is again split into two and the process is continued recursively, till it reaches
the root.
Birch or MiniBatchKMeans?
• Birch does not scale very well to high dimensional data. As a rule of thumb if n_features is greater than
twenty, it is generally better to use MiniBatchKMeans.
• If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a
preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans.
How to use partial_ﬁt?
To avoid the computation of global clustering, for every call of partial_fit the user is advised
300
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
1. To set n_clusters=None initially
2. Train all data by multiple calls to partial_ﬁt.
3. Set n_clusters to a required value using brc.set_params(n_clusters=n_clusters).
4. Call partial_fit ﬁnally with no arguments, i.e brc.partial_fit() which performs the global clus-
tering.
References:
• Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efﬁcient data clustering method for large
databases. http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf
• Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm https://code.google.com/
archive/p/jbirch
Clustering performance evaluation
Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision
and recall of a supervised classiﬁcation algorithm. In particular any evaluation metric should not take the absolute
values of the cluster labels into account but rather if this clustering deﬁne separations of the data similar to some
ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar
that members of different classes according to some similarity metric.
Adjusted Rand index
Given the knowledge of the ground truth class assignments labels_true and our clustering algorithm assignments
of the same samples labels_pred, the adjusted Rand index is a function that measures the similarity of the two
assignments, ignoring permutations and with chance normalization:
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
0.24...
One can permute 0 and 1 in the predicted labels, rename 2 to 3, and get the same score:
3.2. Unsupervised learning
301
scikit-learn user guide, Release 0.18.2
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
0.24...
Furthermore, adjusted_rand_score is symmetric: swapping the argument does not change the score. It can
thus be used as a consensus measure:
>>> metrics.adjusted_rand_score(labels_pred, labels_true)
0.24...
Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
1.0
Bad (e.g. independent labelings) have negative or close to 0.0 scores:
>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
-0.12...
Advantages
• Random (uniform) label assignments have a ARI score close to 0.0 for any value of n_clusters and
n_samples (which is not the case for raw Rand index or the V-measure for instance).
• Bounded range [-1, 1]: negative values are bad (independent labelings), similar clusterings have a positive ARI,
1.0 is the perfect match score.
• No assumption is made on the cluster structure: can be used to compare clustering algorithms such as k-
means which assumes isotropic blob shapes with results of spectral clustering algorithms which can ﬁnd cluster
with “folded” shapes.
Drawbacks
• Contrary to inertia, ARI requires knowledge of the ground truth classes while is almost never available in
practice or requires manual assignment by human annotators (as in the supervised learning setting).
However ARI can also be useful in a purely unsupervised setting as a building block for a Consensus Index that
can be used for clustering model selection (TODO).
Examples:
• Adjustment for chance in clustering performance evaluation: Analysis of the impact of the dataset size on the
value of clustering measures for random assignments.
Mathematical formulation
If C is a ground truth class assignment and K the clustering, let us deﬁne 𝑎and 𝑏as:
302
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• 𝑎, the number of pairs of elements that are in the same set in C and in the same set in K
• 𝑏, the number of pairs of elements that are in different sets in C and in different sets in K
The raw (unadjusted) Rand index is then given by:
RI =
𝑎+ 𝑏
𝐶𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠
2
Where 𝐶𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠
2
is the total number of possible pairs in the dataset (without ordering).
However the RI score does not guarantee that random label assignments will get a value close to zero (esp. if the
number of clusters is in the same order of magnitude as the number of samples).
To counter this effect we can discount the expected RI 𝐸[RI] of random labelings by deﬁning the adjusted Rand index
as follows:
ARI =
RI −𝐸[RI]
max(RI) −𝐸[RI]
References
• Comparing Partitions L. Hubert and P. Arabie, Journal of Classiﬁcation 1985
• Wikipedia entry for the adjusted Rand index
Mutual Information based scores
Given the knowledge of the ground truth class assignments labels_true and our clustering algorithm assignments
of the same samples labels_pred, the Mutual Information is a function that measures the agreement of the two
assignments, ignoring permutations. Two different normalized versions of this measure are available, Normalized
Mutual Information(NMI) and Adjusted Mutual Information(AMI). NMI is often used in the literature while
AMI was proposed more recently and is normalized against chance:
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
0.22504...
One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
0.22504...
All, mutual_info_score, adjusted_mutual_info_score and normalized_mutual_info_score
are symmetric: swapping the argument does not change the score. Thus they can be used as a consensus measure:
>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)
0.22504...
Perfect labeling is scored 1.0:
3.2. Unsupervised learning
303
scikit-learn user guide, Release 0.18.2
>>> labels_pred = labels_true[:]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
1.0
>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)
1.0
This is not true for mutual_info_score, which is therefore harder to judge:
>>> metrics.mutual_info_score(labels_true, labels_pred)
0.69...
Bad (e.g. independent labelings) have non-positive scores:
>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
-0.10526...
Advantages
• Random (uniform) label assignments have a AMI score close to 0.0 for any value of n_clusters and
n_samples (which is not the case for raw Mutual Information or the V-measure for instance).
• Bounded range [0, 1]: Values close to zero indicate two label assignments that are largely independent, while
values close to one indicate signiﬁcant agreement. Further, values of exactly 0 indicate purely independent
label assignments and a AMI of exactly 1 indicates that the two label assignments are equal (with or without
permutation).
• No assumption is made on the cluster structure: can be used to compare clustering algorithms such as k-
means which assumes isotropic blob shapes with results of spectral clustering algorithms which can ﬁnd cluster
with “folded” shapes.
Drawbacks
• Contrary to inertia, MI-based measures require the knowledge of the ground truth classes while almost
never available in practice or requires manual assignment by human annotators (as in the supervised learning
setting).
However MI-based measures can also be useful in purely unsupervised setting as a building block for a Consen-
sus Index that can be used for clustering model selection.
• NMI and MI are not adjusted against chance.
Examples:
• Adjustment for chance in clustering performance evaluation: Analysis of the impact of the dataset size on the
value of clustering measures for random assignments. This example also includes the Adjusted Rand Index.
304
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Mathematical formulation
Assume two label assignments (of the same N objects), 𝑈and 𝑉. Their entropy is the amount of uncertainty for a
partition set, deﬁned by:
𝐻(𝑈) =
|𝑈|
∑︁
𝑖=1
𝑃(𝑖) log(𝑃(𝑖))
where 𝑃(𝑖) = |𝑈𝑖|/𝑁is the probability that an object picked at random from 𝑈falls into class 𝑈𝑖. Likewise for 𝑉:
𝐻(𝑉) =
|𝑉|
∑︁
𝑗=1
𝑃′(𝑗) log(𝑃′(𝑗))
With 𝑃′(𝑗) = |𝑉𝑗|/𝑁. The mutual information (MI) between 𝑈and 𝑉is calculated by:
MI(𝑈, 𝑉) =
|𝑈|
∑︁
𝑖=1
|𝑉|
∑︁
𝑗=1
𝑃(𝑖, 𝑗) log
(︂
𝑃(𝑖, 𝑗)
𝑃(𝑖)𝑃′(𝑗)
)︂
where 𝑃(𝑖, 𝑗) = |𝑈𝑖∩𝑉𝑗|/𝑁is the probability that an object picked at random falls into both classes 𝑈𝑖and 𝑉𝑗.
The normalized mutual information is deﬁned as
NMI(𝑈, 𝑉) =
MI(𝑈, 𝑉)
√︀
𝐻(𝑈)𝐻(𝑉)
This value of the mutual information and also the normalized variant is not adjusted for chance and will tend to increase
as the number of different labels (clusters) increases, regardless of the actual amount of “mutual information” between
the label assignments.
The expected value for the mutual information can be calculated using the following equation, from Vinh, Epps, and
Bailey, (2009). In this equation, 𝑎𝑖= |𝑈𝑖| (the number of elements in 𝑈𝑖) and 𝑏𝑗= |𝑉𝑗| (the number of elements in
𝑉𝑗).
𝐸[MI(𝑈, 𝑉)] =
|
∑︁
𝑖=1
𝑈|
|
∑︁
𝑗=1
𝑉|
min(𝑎𝑖,𝑏𝑗)
∑︁
𝑛𝑖𝑗=(𝑎𝑖+𝑏𝑗−𝑁)+
𝑛𝑖𝑗
𝑁log
(︂𝑁.𝑛𝑖𝑗
𝑎𝑖𝑏𝑗
)︂
𝑎𝑖!𝑏𝑗!(𝑁−𝑎𝑖)!(𝑁−𝑏𝑗)!
𝑁!𝑛𝑖𝑗!(𝑎𝑖−𝑛𝑖𝑗)!(𝑏𝑗−𝑛𝑖𝑗)!(𝑁−𝑎𝑖−𝑏𝑗+ 𝑛𝑖𝑗)!
Using the expected value, the adjusted mutual information can then be calculated using a similar form to that of the
adjusted Rand index:
AMI =
MI −𝐸[MI]
max(𝐻(𝑈), 𝐻(𝑉)) −𝐸[MI]
References
• Strehl, Alexander, and Joydeep Ghosh (2002).
“Cluster ensembles – a knowledge reuse frame-
work for combining multiple partitions”.
Journal of Machine Learning Research 3:
583–617.
doi:10.1162/153244303321897735.
• Vinh, Epps, and Bailey, (2009). “Information theoretic measures for clusterings comparison”. Proceedings of
the 26th Annual International Conference on Machine Learning - ICML ‘09. doi:10.1145/1553374.1553511.
ISBN 9781605585161.
• Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for Clusterings Comparison: Variants, Prop-
erties, Normalization and Correction for Chance, JMLR http://jmlr.csail.mit.edu/papers/volume11/vinh10a/
vinh10a.pdf
3.2. Unsupervised learning
305
scikit-learn user guide, Release 0.18.2
• Wikipedia entry for the (normalized) Mutual Information
• Wikipedia entry for the Adjusted Mutual Information
Homogeneity, completeness and V-measure
Given the knowledge of the ground truth class assignments of the samples, it is possible to deﬁne some intuitive metric
using conditional entropy analysis.
In particular Rosenberg and Hirschberg (2007) deﬁne the following two desirable objectives for any cluster assign-
ment:
• homogeneity: each cluster contains only members of a single class.
• completeness: all members of a given class are assigned to the same cluster.
We can turn those concept as scores homogeneity_score and completeness_score. Both are bounded
below by 0.0 and above by 1.0 (higher is better):
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.homogeneity_score(labels_true, labels_pred)
0.66...
>>> metrics.completeness_score(labels_true, labels_pred)
0.42...
Their harmonic mean called V-measure is computed by v_measure_score:
>>> metrics.v_measure_score(labels_true, labels_pred)
0.51...
The V-measure is actually equivalent to the mutual information (NMI) discussed above normalized by the sum of the
label entropies [B2011].
Homogeneity,
completeness
and
V-measure
can
be
computed
at
once
using
homogeneity_completeness_v_measure as follows:
>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
...
(0.66..., 0.42..., 0.51...)
The following clustering assignment is slightly better, since it is homogeneous but not complete:
>>> labels_pred = [0, 0, 0, 1, 2, 2]
>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
...
(1.0, 0.68..., 0.81...)
Note: v_measure_score is symmetric: it can be used to evaluate the agreement of two independent assignments
on the same dataset.
This is not the case for completeness_score and homogeneity_score: both are bound by the relationship:
306
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
homogeneity_score(a, b) == completeness_score(b, a)
Advantages
• Bounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score.
• Intuitive interpretation: clustering with bad V-measure can be qualitatively analyzed in terms of homogeneity
and completeness to better feel what ‘kind’ of mistakes is done by the assignment.
• No assumption is made on the cluster structure: can be used to compare clustering algorithms such as k-
means which assumes isotropic blob shapes with results of spectral clustering algorithms which can ﬁnd cluster
with “folded” shapes.
Drawbacks
• The previously introduced metrics are not normalized with regards to random labeling: this means that
depending on the number of samples, clusters and ground truth classes, a completely random labeling will
not always yield the same values for homogeneity, completeness and hence v-measure. In particular random
labeling won’t yield zero scores especially when the number of clusters is large.
This problem can safely be ignored when the number of samples is more than a thousand and the number of
clusters is less than 10. For smaller sample sizes or larger number of clusters it is safer to use an adjusted
index such as the Adjusted Rand Index (ARI).
• These metrics require the knowledge of the ground truth classes while almost never available in practice or
requires manual assignment by human annotators (as in the supervised learning setting).
Examples:
• Adjustment for chance in clustering performance evaluation: Analysis of the impact of the dataset size on the
value of clustering measures for random assignments.
Mathematical formulation
Homogeneity and completeness scores are formally given by:
ℎ= 1 −𝐻(𝐶|𝐾)
𝐻(𝐶)
𝑐= 1 −𝐻(𝐾|𝐶)
𝐻(𝐾)
where 𝐻(𝐶|𝐾) is the conditional entropy of the classes given the cluster assignments and is given by:
𝐻(𝐶|𝐾) = −
|𝐶|
∑︁
𝑐=1
|𝐾|
∑︁
𝑘=1
𝑛𝑐,𝑘
𝑛
· log
(︂𝑛𝑐,𝑘
𝑛𝑘
)︂
and 𝐻(𝐶) is the entropy of the classes and is given by:
𝐻(𝐶) = −
|𝐶|
∑︁
𝑐=1
𝑛𝑐
𝑛· log
(︁𝑛𝑐
𝑛
)︁
3.2. Unsupervised learning
307
scikit-learn user guide, Release 0.18.2
308
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
with 𝑛the total number of samples, 𝑛𝑐and 𝑛𝑘the number of samples respectively belonging to class 𝑐and cluster 𝑘,
and ﬁnally 𝑛𝑐,𝑘the number of samples from class 𝑐assigned to cluster 𝑘.
The conditional entropy of clusters given class 𝐻(𝐾|𝐶) and the entropy of clusters 𝐻(𝐾) are deﬁned in a sym-
metric manner.
Rosenberg and Hirschberg further deﬁne V-measure as the harmonic mean of homogeneity and completeness:
𝑣= 2 · ℎ· 𝑐
ℎ+ 𝑐
References
Fowlkes-Mallows scores
The Fowlkes-Mallows index (sklearn.metrics.fowlkes_mallows_score) can be used when the ground
truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is deﬁned as the geometric mean
of the pairwise precision and recall:
FMI =
TP
√︀
(TP + FP)(TP + FN)
Where TP is the number of True Positive (i.e. the number of pair of points that belong to the same clusters in both the
true labels and the predicted labels), FP is the number of False Positive (i.e. the number of pair of points that belong
to the same clusters in the true labels and not in the predicted labels) and FN is the number of False Negative (i.e the
number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels).
The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.47140...
One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.47140...
Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
1.0
Bad (e.g. independent labelings) have zero scores:
>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.0
3.2. Unsupervised learning
309
scikit-learn user guide, Release 0.18.2
Advantages
• Random (uniform) label assignments have a FMI score close to 0.0 for any value of n_clusters and
n_samples (which is not the case for raw Mutual Information or the V-measure for instance).
• Bounded range [0, 1]: Values close to zero indicate two label assignments that are largely independent, while
values close to one indicate signiﬁcant agreement. Further, values of exactly 0 indicate purely independent
label assignments and a AMI of exactly 1 indicates that the two label assignments are equal (with or without
permutation).
• No assumption is made on the cluster structure: can be used to compare clustering algorithms such as k-
means which assumes isotropic blob shapes with results of spectral clustering algorithms which can ﬁnd cluster
with “folded” shapes.
Drawbacks
• Contrary to inertia, FMI-based measures require the knowledge of the ground truth classes while almost
never available in practice or requires manual assignment by human annotators (as in the supervised learning
setting).
References
• E. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two hierarchical clusterings”. Journal of
the American Statistical Association. http://wildﬁre.stat.ucla.edu/pdﬂibrary/fowlkes.pdf
• Wikipedia entry for the Fowlkes-Mallows Index
Silhouette Coefﬁcient
If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefﬁ-
cient (sklearn.metrics.silhouette_score) is an example of such an evaluation, where a higher Silhouette
Coefﬁcient score relates to a model with better deﬁned clusters. The Silhouette Coefﬁcient is deﬁned for each sample
and is composed of two scores:
• a: The mean distance between a sample and all other points in the same class.
• b: The mean distance between a sample and all other points in the next nearest cluster.
The Silhouette Coefﬁcient s for a single sample is then given as:
𝑠=
𝑏−𝑎
𝑚𝑎𝑥(𝑎, 𝑏)
The Silhouette Coefﬁcient for a set of samples is given as the mean of the Silhouette Coefﬁcient for each sample.
>>> from sklearn import metrics
>>> from sklearn.metrics import pairwise_distances
>>> from sklearn import datasets
>>> dataset = datasets.load_iris()
>>> X = dataset.data
>>> y = dataset.target
In normal usage, the Silhouette Coefﬁcient is applied to the results of a cluster analysis.
310
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> import numpy as np
>>> from sklearn.cluster import KMeans
>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans_model.labels_
>>> metrics.silhouette_score(X, labels, metric='euclidean')
...
0.55...
References
• Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster
Analysis”. Computational and Applied Mathematics 20: 53–65. doi:10.1016/0377-0427(87)90125-7.
Advantages
• The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero
indicate overlapping clusters.
• The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.
Drawbacks
• The Silhouette Coefﬁcient is generally higher for convex clusters than other concepts of clusters, such as density
based clusters like those obtained through DBSCAN.
Examples:
• Selecting the number of clusters with silhouette analysis on KMeans clustering : In this example the silhouette
analysis is used to choose an optimal value for n_clusters.
Calinski-Harabaz Index
If
the
ground
truth
labels
are
not
known,
the
Calinski-Harabaz
index
(sklearn.metrics.calinski_harabaz_score) can be used to evaluate the model, where a higher
Calinski-Harabaz score relates to a model with better deﬁned clusters.
For 𝑘clusters, the Calinski-Harabaz score 𝑠is given as the ratio of the between-clusters dispersion mean and the
within-cluster dispersion:
𝑠(𝑘) = Tr(𝐵𝑘)
Tr(𝑊𝑘) × 𝑁−𝑘
𝑘−1
where 𝐵𝐾is the between group dispersion matrix and 𝑊𝐾is the within-cluster dispersion matrix deﬁned by:
𝑊𝑘=
𝑘
∑︁
𝑞=1
∑︁
𝑥∈𝐶𝑞
(𝑥−𝑐𝑞)(𝑥−𝑐𝑞)𝑇
𝐵𝑘=
∑︁
𝑞
𝑛𝑞(𝑐𝑞−𝑐)(𝑐𝑞−𝑐)𝑇
3.2. Unsupervised learning
311
scikit-learn user guide, Release 0.18.2
with 𝑁be the number of points in our data, 𝐶𝑞be the set of points in cluster 𝑞, 𝑐𝑞be the center of cluster 𝑞, 𝑐be the
center of 𝐸, 𝑛𝑞be the number of points in cluster 𝑞.
>>> from sklearn import metrics
>>> from sklearn.metrics import pairwise_distances
>>> from sklearn import datasets
>>> dataset = datasets.load_iris()
>>> X = dataset.data
>>> y = dataset.target
In normal usage, the Calinski-Harabaz index is applied to the results of a cluster analysis.
>>> import numpy as np
>>> from sklearn.cluster import KMeans
>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans_model.labels_
>>> metrics.calinski_harabaz_score(X, labels)
560.39...
Advantages
• The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.
• The score is fast to compute
Drawbacks
• The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as
density based clusters like those obtained through DBSCAN.
References
• Cali´nski, T., & Harabasz, J. (1974). “A dendrite method for cluster analysis”. Communications in Statistics-
theory and Methods 3: 1-27. doi:10.1080/03610926.2011.560741.
3.2.4 Biclustering
Biclustering can be performed with the module sklearn.cluster.bicluster. Biclustering algorithms simul-
taneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters.
Each determines a submatrix of the original data matrix with some desired properties.
For instance, given a matrix of shape (10,10), one possible bicluster with three rows and two columns induces a
submatrix of shape (3,2):
>>> import numpy as np
>>> data = np.arange(100).reshape(10, 10)
>>> rows = np.array([0, 2, 3])[:, np.newaxis]
>>> columns = np.array([1, 2])
>>> data[rows, columns]
array([[ 1,
2],
[21, 22],
[31, 32]])
312
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the
bicluster contiguous.
Algorithms differ in how they deﬁne biclusters. Some of the common types include:
• constant values, constant rows, or constant columns
• unusually high or low values
• submatrices with low variance
• correlated rows or columns
Algorithms also differ in how rows and columns may be assigned to biclusters, which leads to different bicluster
structures. Block diagonal or checkerboard structures occur when rows and columns are divided into partitions.
If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix
reveals the biclusters on the diagonal. Here is an example of this structure where biclusters have higher average values
than the other rows and columns:
Fig. 3.5: An example of biclusters formed by partitioning rows and columns.
In the checkerboard case, each row belongs to all column clusters, and each column belongs to all row clusters. Here
is an example of this structure where the variance of the values within each bicluster is small:
After ﬁtting a model, row and column cluster membership can be found in the rows_ and columns_ attributes.
rows_[i] is a binary vector with nonzero entries corresponding to rows that belong to bicluster i. Similarly,
columns_[i] indicates which columns belong to bicluster i.
Some models also have row_labels_ and column_labels_ attributes. These models partition the rows and
columns, such as in the block diagonal and checkerboard bicluster structures.
Note: Biclustering has many other names in different ﬁelds including co-clustering, two-mode clustering, two-way
clustering, block clustering, coupled two-way clustering, etc. The names of some algorithms, such as the Spectral
Co-Clustering algorithm, reﬂect these alternate names.
3.2. Unsupervised learning
313
scikit-learn user guide, Release 0.18.2
Fig. 3.6: An example of checkerboard biclusters.
Spectral Co-Clustering
The SpectralCoclustering algorithm ﬁnds biclusters with values higher than those in the corresponding other
rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns
to make partitions contiguous reveals these high values along the diagonal:
Note: The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond
to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm
approximates the normalized cut of this graph to ﬁnd heavy subgraphs.
Mathematical formulation
An approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of
the Laplacian of the graph. Usually this would mean working directly with the Laplacian matrix. If the original data
matrix 𝐴has shape 𝑚× 𝑛, the Laplacian matrix for the corresponding bipartite graph has shape (𝑚+ 𝑛) × (𝑚+ 𝑛).
However, in this case it is possible to work directly with 𝐴, which is smaller and more efﬁcient.
The input matrix 𝐴is preprocessed as follows:
𝐴𝑛= 𝑅−1/2𝐴𝐶−1/2
Where 𝑅is the diagonal matrix with entry 𝑖equal to ∑︀
𝑗𝐴𝑖𝑗and 𝐶is the diagonal matrix with entry 𝑗equal to
∑︀
𝑖𝐴𝑖𝑗.
The singular value decomposition, 𝐴𝑛= 𝑈Σ𝑉⊤, provides the partitions of the rows and columns of 𝐴. A subset of
the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.
The ℓ= ⌈log2 𝑘⌉singular vectors, starting from the second, provide the desired partitioning information. They are
used to form the matrix 𝑍:
𝑍=
⎡
⎣
𝑅−1/2𝑈
𝐶−1/2𝑉
⎤
⎦
314
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
where the columns of 𝑈are 𝑢2, . . . , 𝑢ℓ+1, and similarly for 𝑉.
Then the rows of 𝑍are clustered using k-means. The ﬁrst n_rows labels provide the row partitioning, and the
remaining n_columns labels provide the column partitioning.
Examples:
• A demo of the Spectral Co-Clustering algorithm: A simple example showing how to generate a data matrix
with biclusters and apply this method to it.
• Biclustering documents with the Spectral Co-clustering algorithm: An example of ﬁnding biclusters in the
twenty newsgroup dataset.
References:
• Dhillon, Inderjit S, 2001. Co-clustering documents and words using bipartite spectral graph partitioning.
Spectral Biclustering
The SpectralBiclustering algorithm assumes that the input data matrix has a hidden checkerboard structure.
The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the
Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row
partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two
biclusters.
The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard
matrix provides a good approximation to the original matrix.
Mathematical formulation
The input matrix 𝐴is ﬁrst normalized to make the checkerboard pattern more obvious. There are three possible
methods:
1. Independent row and column normalization, as in Spectral Co-Clustering. This method makes the rows sum to
a constant and the columns sum to a different constant.
2. Bistochastization: repeated row and column normalization until convergence. This method makes both rows
and columns sum to the same constant.
3. Log normalization: the log of the data matrix is computed: 𝐿= log 𝐴. Then the column mean 𝐿𝑖·, row mean
𝐿·𝑗, and overall mean 𝐿·· of 𝐿are computed. The ﬁnal matrix is computed according to the formula
𝐾𝑖𝑗= 𝐿𝑖𝑗−𝐿𝑖· −𝐿·𝑗+ 𝐿··
After normalizing, the ﬁrst few singular vectors are computed, just as in the Spectral Co-Clustering algorithm.
If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or
bistochastization were used, the ﬁrst singular vectors, 𝑢1 and 𝑣1. are discarded. From now on, the “ﬁrst” singular
vectors refers to 𝑢2 . . . 𝑢𝑝+1 and 𝑣2 . . . 𝑣𝑝+1 except in the case of log normalization.
Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant
vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean
distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best
subset of singular vectors and clustered.
3.2. Unsupervised learning
315
scikit-learn user guide, Release 0.18.2
For instance, if 𝑝singular vectors were calculated, the 𝑞best are found as described, where 𝑞< 𝑝. Let 𝑈be the matrix
with columns the 𝑞best left singular vectors, and similarly 𝑉for the right. To partition the rows, the rows of 𝐴are
projected to a 𝑞dimensional space: 𝐴* 𝑉. Treating the 𝑚rows of this 𝑚× 𝑞matrix as samples and clustering using
k-means yields the row labels. Similarly, projecting the columns to 𝐴⊤* 𝑈and clustering this 𝑛× 𝑞matrix yields the
column labels.
Examples:
• A demo of the Spectral Biclustering algorithm: a simple example showing how to generate a checkerboard
matrix and bicluster it.
References:
• Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray data: coclustering genes and conditions.
Biclustering evaluation
There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster
stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-
learn. External measures refer to an external source of information, such as the true solution. When working with
real data the true solution is usually unknown, but biclustering artiﬁcial data may be useful for evaluating algorithms
precisely because the true solution is known.
To compare a set of found biclusters to the set of true biclusters, two similarity measures are needed: a similarity
measure for individual biclusters, and a way to combine these individual similarities into an overall score.
To compare individual biclusters, several measures have been used. For now, only the Jaccard index is implemented:
𝐽(𝐴, 𝐵) =
|𝐴∩𝐵|
|𝐴| + |𝐵| −|𝐴∩𝐵|
where 𝐴and 𝐵are biclusters, |𝐴∩𝐵| is the number of elements in their intersection. The Jaccard index achieves its
minimum of 0 when the biclusters to not overlap at all and its maximum of 1 when they are identical.
Several methods have been developed to compare two sets of biclusters. For now, only consensus_score (Hochre-
iter et. al., 2010) is available:
1. Compute bicluster similarities for pairs of biclusters, one in each set, using the Jaccard index or a similar
measure.
2. Assign biclusters from one set to another in a one-to-one fashion to maximize the sum of their similarities. This
step is performed using the Hungarian algorithm.
3. The ﬁnal sum of similarities is divided by the size of the larger set.
The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1,
occurs when both sets are identical.
References:
• Hochreiter, Bodenhofer, et. al., 2010. FABIA: factor analysis for bicluster acquisition.
316
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.2.5 Decomposing signals in components (matrix factorization problems)
Principal component analysis (PCA)
Exact PCA and probabilistic interpretation
PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum
amount of the variance. In scikit-learn, PCA is implemented as a transformer object that learns 𝑛components in its
fit method, and can be used on new data to project it on these components.
The optional parameter whiten=True makes it possible to project the data onto the singular space while scaling
each component to unit variance. This is often useful if the models down-stream make strong assumptions on the
isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means
clustering algorithm.
Below is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain
most variance:
The PCA object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the
amount of variance it explains. As such it implements a score method that can be used in cross-validation:
Examples:
• Comparison of LDA and PCA 2D projection of Iris dataset
• Model selection with Probabilistic PCA and Factor Analysis (FA)
3.2. Unsupervised learning
317
scikit-learn user guide, Release 0.18.2
Incremental PCA
The PCA object is very useful, but has certain limitations for large datasets. The biggest limitation is that PCA only sup-
ports batch processing, which means all of the data to be processed must ﬁt in main memory. The IncrementalPCA
object uses a different form of processing and allows for partial computations which almost exactly match the results
of PCA while processing the data in a minibatch fashion. IncrementalPCA makes it possible to implement out-of-
core Principal Component Analysis either by:
• Using its partial_fit method on chunks of data fetched sequentially from the local hard drive or a network
database.
• Calling its ﬁt method on a memory mapped ﬁle using numpy.memmap.
IncrementalPCA
only
stores
estimates
of
component
and
noise
variances,
in
order
update
explained_variance_ratio_ incrementally.
This is why memory usage depends on the number of
samples per batch, rather than the number of samples to be processed in the dataset.
Examples:
• Incremental PCA
PCA using randomized SVD
It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the
singular vector of components associated with lower singular values.
For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is
4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic
dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The
samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to
318
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.2. Unsupervised learning
319
scikit-learn user guide, Release 0.18.2
320
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the
same time.
The class PCA used with the optional parameter svd_solver='randomized' is very useful in that case: since
we are going to drop most of the singular vectors it is much more efﬁcient to limit the computation to an approximated
estimate of the singular vectors we will keep to actually perform the transform.
For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right
hand side are the ﬁrst 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a
dataset with size 𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠= 400 and 𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠= 64 × 64 = 4096, the computation time is less than 1s:
Note: with the optional parameter svd_solver='randomized', we also need to give PCA the size of the lower-
dimensional space n_components as a mandatory input parameter.
If we note 𝑛𝑚𝑎𝑥= 𝑚𝑎𝑥(𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠, 𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠) and 𝑛𝑚𝑖𝑛= 𝑚𝑖𝑛(𝑛𝑠𝑎𝑚𝑝𝑙𝑒𝑠, 𝑛𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠), the time complexity of the
randomized PCA is 𝑂(𝑛2
𝑚𝑎𝑥· 𝑛𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡𝑠) instead of 𝑂(𝑛2
𝑚𝑎𝑥· 𝑛𝑚𝑖𝑛) for the exact method implemented in PCA.
The memory footprint of randomized PCA is also proportional to 2 · 𝑛𝑚𝑎𝑥· 𝑛𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡𝑠instead of 𝑛𝑚𝑎𝑥· 𝑛𝑚𝑖𝑛for
the exact method.
Note: the implementation of inverse_transform in PCA with svd_solver='randomized' is not the exact
inverse transform of transform even when whiten=False (default).
3.2. Unsupervised learning
321
scikit-learn user guide, Release 0.18.2
Examples:
• Faces recognition example using eigenfaces and SVMs
• Faces dataset decompositions
References:
• “Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decomposi-
tions” Halko, et al., 2009
Kernel PCA
KernelPCA is an extension of PCA which achieves non-linear dimensionality reduction through the use of
kernels (see Pairwise metrics, Afﬁnities and Kernels).
It has many applications including denoising, compres-
sion and structured prediction (kernel dependency estimation).
KernelPCA supports both transform and
inverse_transform.
Examples:
• Kernel PCA
Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)
SparsePCA is a variant of PCA, with the goal of extracting the set of sparse components that best reconstruct the
322
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
data.
Mini-batch sparse PCA (MiniBatchSparsePCA) is a variant of SparsePCA that is faster but less accurate. The
increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.
Principal component analysis (PCA) has the disadvantage that the components extracted by this method have exclu-
sively dense expressions, i.e. they have non-zero coefﬁcients when expressed as linear combinations of the original
variables. This can make interpretation difﬁcult. In many cases, the real underlying components can be more naturally
imagined as sparse vectors; for example in face recognition, components might naturally map to parts of faces.
Sparse principal components yields a more parsimonious, interpretable representation, clearly emphasizing which of
the original features contribute to the differences between samples.
The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can
be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the
non-zero coefﬁcients to be vertically adjacent. The model does not enforce this mathematically: each component is
a vector ℎ∈R4096, and there is no notion of vertical adjacency except during the human-friendly visualization as
64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of
the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take
into account adjacency and different kinds of structure; see [Jen09] for a review of such methods. For more details on
how to use Sparse PCA, see the Examples section, below.
Note that there are many different formulations for the Sparse PCA problem. The one implemented here is based
3.2. Unsupervised learning
323
scikit-learn user guide, Release 0.18.2
on [Mrl09] . The optimization problem solved is a PCA problem (dictionary learning) with an ℓ1 penalty on the
components:
(𝑈*, 𝑉*) = arg min
𝑈,𝑉
1
2||𝑋−𝑈𝑉||2
2 + 𝛼||𝑉||1
subject to ||𝑈𝑘||2 = 1 for all 0 ≤𝑘< 𝑛𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡𝑠
The sparsity-inducing ℓ1 norm also prevents learning components from noise when few training samples are available.
The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter alpha. Small values lead
to a gently regularized factorization, while larger values shrink many coefﬁcients to zero.
Note:
While in the spirit of an online algorithm, the class MiniBatchSparsePCA does not implement
partial_fit because the algorithm is online along the features direction, not the samples direction.
Examples:
• Faces dataset decompositions
References:
Truncated singular value decomposition and latent semantic analysis
TruncatedSVD implements a variant of singular value decomposition (SVD) that only computes the 𝑘largest
singular values, where 𝑘is a user-speciﬁed parameter.
When
truncated
SVD
is
applied
to
term-document
matrices
(as
returned
by
CountVectorizer
or
TfidfVectorizer), this transformation is known as latent semantic analysis (LSA), because it transforms such
matrices to a “semantic” space of low dimensionality. In particular, LSA is known to combat the effects of synonymy
and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document ma-
trices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.
Note: LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes
for information retrieval purposes.
Mathematically, truncated SVD applied to training samples 𝑋produces a low-rank approximation 𝑋:
𝑋≈𝑋𝑘= 𝑈𝑘Σ𝑘𝑉⊤
𝑘
After this operation, 𝑈𝑘Σ⊤
𝑘is the transformed training set with 𝑘features (called n_components in the API).
To also transform a test set 𝑋, we multiply it with 𝑉𝑘:
𝑋′ = 𝑋𝑉𝑘
Note:
Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature
swap the axes of the matrix 𝑋so that it has shape n_features × n_samples. We present LSA in a different way
that matches the scikit-learn API better, but the singular values found are the same.
324
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
TruncatedSVD is very similar to PCA, but differs in that it works on sample matrices 𝑋directly instead of their
covariance matrices. When the columnwise (per-feature) means of 𝑋are subtracted from the feature values, truncated
SVD on the resulting matrix is equivalent to PCA. In practical terms, this means that the TruncatedSVD transformer
accepts scipy.sparse matrices without the need to densify them, as densifying may ﬁll up memory even for
medium-sized document collections.
While the TruncatedSVD transformer works with any (sparse) feature matrix, using it on tf–idf matrices is rec-
ommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and
inverse document frequency should be turned on (sublinear_tf=True,use_idf=True) to bring the feature
values closer to a Gaussian distribution, compensating for LSA’s erroneous assumptions about textual data.
Examples:
• Clustering text documents using k-means
References:
• Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008), Introduction to Information Re-
trieval, Cambridge University Press, chapter 18: Matrix decompositions & latent semantic indexing
Dictionary Learning
Sparse coding with a precomputed dictionary
The SparseCoder object is an estimator that can be used to transform signals into sparse linear combination of
atoms from a ﬁxed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement
a fit method. The transformation amounts to a sparse coding problem: ﬁnding a representation of the data as a linear
combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following
transform methods, controllable via the transform_method initialization parameter:
• Orthogonal matching pursuit (Orthogonal Matching Pursuit (OMP))
• Least-angle regression (Least Angle Regression)
• Lasso computed by least-angle regression
• Lasso using coordinate descent (Lasso)
• Thresholding
Thresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for
classiﬁcation tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased
reconstruction.
The dictionary learning objects offer, via the split_code parameter, the possibility to separate the positive and
negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features
that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative
loadings of a particular atom, from to the corresponding positive loading.
The split code for a single sample has length 2 * n_components and is constructed using the following rule:
First, the regular code of length n_components is computed. Then, the ﬁrst n_components entries of the
split_code are ﬁlled with the positive part of the regular code vector. The second half of the split code is ﬁlled
with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative.
3.2. Unsupervised learning
325
scikit-learn user guide, Release 0.18.2
Examples:
• Sparse coding with a precomputed dictionary
Generic dictionary learning
Dictionary learning (DictionaryLearning) is a matrix factorization problem that amounts to ﬁnding a (usually
overcomplete) dictionary that will perform good at sparsely encoding the ﬁtted data.
Representing data as sparse combinations of atoms from an overcomplete dictionary is suggested to be the way the
mammal primary visual cortex works. Consequently, dictionary learning applied on image patches has been shown
to give good results in image processing tasks such as image completion, inpainting and denoising, as well as for
supervised recognition tasks.
Dictionary learning is an optimization problem solved by alternatively updating the sparse code, as a solution to
multiple Lasso problems, considering the dictionary ﬁxed, and then updating the dictionary to best ﬁt the sparse code.
(𝑈*, 𝑉*) = arg min
𝑈,𝑉
1
2||𝑋−𝑈𝑉||2
2 + 𝛼||𝑈||1
subject to ||𝑉𝑘||2 = 1 for all 0 ≤𝑘< 𝑛𝑎𝑡𝑜𝑚𝑠
326
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
After using such a procedure to ﬁt the dictionary, the transform is simply a sparse coding step that shares the same
implementation with all dictionary learning objects (see Sparse coding with a precomputed dictionary).
The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image
of a raccoon face looks like.
Examples:
• Image denoising using dictionary learning
References:
• “Online dictionary learning for sparse coding” J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009
Mini-batch dictionary learning
MiniBatchDictionaryLearning implements a faster, but less accurate version of the dictionary learning algo-
rithm that is better suited for large datasets.
By default, MiniBatchDictionaryLearning divides the data into mini-batches and optimizes in an online
manner by cycling over the mini-batches for the speciﬁed number of iterations. However, at the moment it does not
implement a stopping condition.
The estimator also implements partial_fit, which updates the dictionary by iterating only once over a mini-batch.
This can be used for online learning when the data is not readily available from the start, or for when the data does not
ﬁt into the memory.
3.2. Unsupervised learning
327
scikit-learn user guide, Release 0.18.2
Clustering for dictionary learning
Note that when using dictionary learning to extract a representation (e.g. for sparse coding) clustering can be a
good proxy to learn the dictionary. For instance the MiniBatchKMeans estimator is computationally efﬁcient
and implements on-line learning with a partial_fit method.
Example: Online learning of a dictionary of parts of faces
Factor Analysis
In unsupervised learning we only have a dataset 𝑋= {𝑥1, 𝑥2, . . . , 𝑥𝑛}. How can this dataset be described mathemat-
ically? A very simple continuous latent variable model for 𝑋is
𝑥𝑖= 𝑊ℎ𝑖+ 𝜇+ 𝜖
The vector ℎ𝑖is called “latent” because it is unobserved. 𝜖is considered a noise term distributed according to a
Gaussian with mean 0 and covariance Ψ (i.e. 𝜖∼𝒩(0, Ψ)), 𝜇is some arbitrary offset vector. Such a model is called
“generative” as it describes how 𝑥𝑖is generated from ℎ𝑖. If we use all the 𝑥𝑖‘s as columns to form a matrix X and all
the ℎ𝑖‘s as columns of a matrix H then we can write (with suitably deﬁned M and E):
X = 𝑊H + M + E
In other words, we decomposed matrix X.
If ℎ𝑖is given, the above equation automatically implies the following probabilistic interpretation:
𝑝(𝑥𝑖|ℎ𝑖) = 𝒩(𝑊ℎ𝑖+ 𝜇, Ψ)
For a complete probabilistic model we also need a prior distribution for the latent variable ℎ. The most straightforward
assumption (based on the nice properties of the Gaussian distribution) is ℎ∼𝒩(0, I). This yields a Gaussian as the
marginal distribution of 𝑥:
𝑝(𝑥) = 𝒩(𝜇, 𝑊𝑊𝑇+ Ψ)
Now, without any further assumptions the idea of having a latent variable ℎwould be superﬂuous – 𝑥can be com-
pletely modelled with a mean and a covariance. We need to impose some more speciﬁc structure on one of these two
parameters. A simple additional assumption regards the structure of the error covariance Ψ:
• Ψ = 𝜎2I: This assumption leads to the probabilistic model of PCA.
• Ψ = 𝑑𝑖𝑎𝑔(𝜓1, 𝜓2, . . . , 𝜓𝑛): This model is called FactorAnalysis, a classical statistical model. The matrix
W is sometimes called the “factor loading matrix”.
Both models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic
they can be integrated in more complex models, e.g. Mixture of Factor Analysers. One gets very different models (e.g.
FastICA) if non-Gaussian priors on the latent variables are assumed.
Factor analysis can produce similar components (the columns of its loading matrix) to PCA. However, one can not
make any general statements about these components (e.g. whether they are orthogonal):
328
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The main advantage for Factor Analysis (over PCA is that it can model the variance in every direction of the input
space independently (heteroscedastic noise):
This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise:
Examples:
• Model selection with Probabilistic PCA and Factor Analysis (FA)
3.2. Unsupervised learning
329
scikit-learn user guide, Release 0.18.2
Independent component analysis (ICA)
Independent component analysis separates a multivariate signal into additive subcomponents that are maximally in-
dependent. It is implemented in scikit-learn using the Fast ICA algorithm. Typically, ICA is not used for reducing
dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the
model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually
using one of the PCA variants.
It is classically used to separate mixed signals (a problem known as blind source separation), as in the example below:
ICA can also be used as yet another non linear decomposition that ﬁnds components with some sparsity:
330
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples:
• Blind source separation using FastICA
• FastICA on 2D point clouds
• Faces dataset decompositions
Non-negative matrix factorization (NMF or NNMF)
NMF is an alternative approach to decomposition that assumes that the data and the components are non-negative. NMF
can be plugged in instead of PCA or its variants, in the cases where the data matrix does not contain negative values.
It ﬁnds a decomposition of samples 𝑋into two matrices 𝑊and 𝐻of non-negative elements, by optimizing for the
squared Frobenius norm:
arg min
𝑊,𝐻
1
2||𝑋−𝑊𝐻||2
𝐹𝑟𝑜= 1
2
∑︁
𝑖,𝑗
(𝑋𝑖𝑗−𝑊𝐻𝑖𝑗)2
This norm is an obvious extension of the Euclidean norm to matrices. (Other optimization objectives have been
suggested in the NMF literature, in particular Kullback-Leibler divergence, but these are not currently implemented.)
3.2. Unsupervised learning
331
scikit-learn user guide, Release 0.18.2
Unlike PCA, the representation of a vector is obtained in an additive fashion, by superimposing the components,
without subtracting. Such additive models are efﬁcient for representing images and text.
It has been observed in [Hoyer, 04] that, when carefully constrained, NMF can produce a parts-based representation of
the dataset, resulting in interpretable models. The following example displays 16 sparse components found by NMF
from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.
The init attribute determines the initialization method applied, which has a great impact on the performance of the
method. NMF implements the method Nonnegative Double Singular Value Decomposition. NNDSVD is based on two
SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial
SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better ﬁt for sparse
factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and
NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are
recommended in the dense case.
NMF can also be initialized with correctly scaled random non-negative matrices by setting init="random". An
integer seed or a RandomState can also be passed to random_state to control reproducibility.
In NMF, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the
Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in ElasticNet, we control the combination
of L1 and L2 with the l1_ratio (𝜌) parameter, and the intensity of the regularization with the alpha (𝛼) parameter.
332
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Then the priors terms are:
𝛼𝜌||𝑊||1 + 𝛼𝜌||𝐻||1 + 𝛼(1 −𝜌)
2
||𝑊||2
𝐹𝑟𝑜+ 𝛼(1 −𝜌)
2
||𝐻||2
𝐹𝑟𝑜
and the regularized objective function is:
1
2||𝑋−𝑊𝐻||2
𝐹𝑟𝑜+ 𝛼𝜌||𝑊||1 + 𝛼𝜌||𝐻||1 + 𝛼(1 −𝜌)
2
||𝑊||2
𝐹𝑟𝑜+ 𝛼(1 −𝜌)
2
||𝐻||2
𝐹𝑟𝑜
NMF regularizes both W and H. The public function non_negative_factorization allows a ﬁner control
through the regularization attribute, and may regularize only W, only H, or both.
Examples:
• Faces dataset decompositions
• Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
References:
• “Learning the parts of objects by non-negative matrix factorization” D. Lee, S. Seung, 1999
• “Non-negative Matrix Factorization with Sparseness Constraints” P. Hoyer, 2004
• “Projected gradient methods for non-negative matrix factorization” C.-J. Lin, 2007
• “SVD based initialization: A head start for nonnegative matrix factorization” C. Boutsidis, E. Gallopoulos,
2008
• “Fast local algorithms for large scale nonnegative matrix and tensor factorizations.” A. Cichocki, P. Anh-Huy,
2009
Latent Dirichlet Allocation (LDA)
Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora.
It is also a topic model that is used for discovering abstract topics from a collection of documents.
The graphical model of LDA is a three-level Bayesian model:
3.2. Unsupervised learning
333
scikit-learn user guide, Release 0.18.2
When modeling text corpora, the model assumes the following generative process for a corpus with 𝐷documents and
𝐾topics:
1. For each topic 𝑘, draw 𝛽𝑘∼𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝜂), 𝑘= 1...𝐾
2. For each document 𝑑, draw 𝜃𝑑∼𝐷𝑖𝑟𝑖𝑐ℎ𝑙𝑒𝑡(𝛼), 𝑑= 1...𝐷
3. For each word 𝑖in document 𝑑:
1. Draw a topic index 𝑧𝑑𝑖∼𝑀𝑢𝑙𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝜃𝑑)
2. Draw the observed word 𝑤𝑖𝑗∼𝑀𝑢𝑙𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝑏𝑒𝑡𝑎𝑧𝑑𝑖.)
For parameter estimation, the posterior distribution is:
𝑝(𝑧, 𝜃, 𝛽|𝑤, 𝛼, 𝜂) = 𝑝(𝑧, 𝜃, 𝛽|𝛼, 𝜂)
𝑝(𝑤|𝛼, 𝜂)
Since the posterior is intractable, variational Bayesian method uses a simpler distribution 𝑞(𝑧, 𝜃, 𝛽|𝜆, 𝜑, 𝛾) to approx-
imate it, and those variational parameters 𝜆, 𝜑, 𝛾are optimized to maximize the Evidence Lower Bound (ELBO):
𝑙𝑜𝑔𝑃(𝑤|𝛼, 𝜂) ≥𝐿(𝑤, 𝜑, 𝛾, 𝜆)
△= 𝐸𝑞[𝑙𝑜𝑔𝑝(𝑤, 𝑧, 𝜃, 𝛽|𝛼, 𝜂)] −𝐸𝑞[𝑙𝑜𝑔𝑞(𝑧, 𝜃, 𝛽)]
Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between 𝑞(𝑧, 𝜃, 𝛽) and the true
posterior 𝑝(𝑧, 𝜃, 𝛽|𝑤, 𝛼, 𝜂).
LatentDirichletAllocation implements online variational Bayes algorithm and supports both online and
batch update method. While batch method updates variational variables after each full pass through the data, online
method updates variational variables from mini-batch data points.
Note: Although online method is guaranteed to converge to a local optimum point, the quality of the optimum point
and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting.
When LatentDirichletAllocation is applied on a “document-term” matrix, the matrix will be decomposed
into a “topic-term” matrix and a “document-topic” matrix. While “topic-term” matrix is stored as components_ in
the model, “document-topic” matrix can be calculated from transform method.
LatentDirichletAllocation also implements partial_fit method. This is used when data can be fetched
sequentially.
334
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples:
• Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
References:
• “Latent Dirichlet Allocation” D. Blei, A. Ng, M. Jordan, 2003
• “Online Learning for Latent Dirichlet Allocation” M. Hoffman, D. Blei, F. Bach, 2010
• “Stochastic Variational Inference” M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013
3.2.6 Covariance estimation
Many statistical problems require at some point the estimation of a population’s covariance matrix, which can be seen
as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose
properties (size, structure, homogeneity) has a large inﬂuence on the estimation’s quality. The sklearn.covariance
package aims at providing tools affording an accurate estimation of a population’s covariance matrix under various
settings.
We assume that the observations are independent and identically distributed (i.i.d.).
Empirical covariance
The covariance matrix of a data set is known to be well approximated with the classical maximum likelihood estimator
(or “empirical covariance”), provided the number of observations is large enough compared to the number of features
(the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an
unbiased estimator of the corresponding population covariance matrix.
The empirical covariance matrix of a sample can be computed using the empirical_covariance func-
tion of the package,
or by ﬁtting an EmpiricalCovariance object to the data sample with the
EmpiricalCovariance.fit method. Be careful that depending whether the data are centered or not, the re-
sult will be different, so one may want to use the assume_centered parameter accurately. More precisely if one
uses assume_centered=False, then the test set is supposed to have the same mean vector as the training set. If
not so, both should be centered by the user, and assume_centered=True should be used.
Examples:
• See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for an example on how to ﬁt an
EmpiricalCovariance object to data.
Shrunk Covariance
Basic shrinkage
Despite being an unbiased estimator of the covariance matrix, the Maximum Likelihood Estimator is not a good esti-
mator of the eigenvalues of the covariance matrix, so the precision matrix obtained from its inversion is not accurate.
3.2. Unsupervised learning
335
scikit-learn user guide, Release 0.18.2
Sometimes, it even occurs that the empirical covariance matrix cannot be inverted for numerical reasons. To avoid
such an inversion problem, a transformation of the empirical covariance matrix has been introduced: the shrinkage.
In the scikit-learn, this transformation (with a user-deﬁned shrinkage coefﬁcient) can be directly applied to a pre-
computed covariance with the shrunk_covariance method. Also, a shrunk estimator of the covariance can be
ﬁtted to data with a ShrunkCovariance object and its ShrunkCovariance.fit method. Again, depending
whether the data are centered or not, the result will be different, so one may want to use the assume_centered
parameter accurately.
Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalue of the
empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is
equivalent of ﬁnding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage
boils down to a simple a convex transformation : Σshrunk = (1 −𝛼)ˆΣ + 𝛼Tr^Σ
𝑝Id.
Choosing the amount of shrinkage, 𝛼amounts to setting a bias/variance trade-off, and is discussed below.
Examples:
• See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for an example on how to ﬁt a
ShrunkCovariance object to data.
Ledoit-Wolf shrinkage
In their 2004 paper [1], O. Ledoit and M. Wolf propose a formula so as to compute the optimal shrinkage coefﬁcient
𝛼that minimizes the Mean Squared Error between the estimated and the real covariance matrix.
The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the ledoit_wolf function of
the sklearn.covariance package, or it can be otherwise obtained by ﬁtting a LedoitWolf object to the same sample.
Examples:
• See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for an example on how to ﬁt a
LedoitWolf object to data and for visualizing the performances of the Ledoit-Wolf estimator in terms of
likelihood.
[1] O. Ledoit and M. Wolf, “A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices”, Jour-
nal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.
Oracle Approximating Shrinkage
Under the assumption that the data are Gaussian distributed, Chen et al. [2] derived a formula aimed at choosing a
shrinkage coefﬁcient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s formula. The
resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance.
The OAS estimator of the covariance matrix can be computed on a sample with the oas function of the
sklearn.covariance package, or it can be otherwise obtained by ﬁtting an OAS object to the same sample.
[2] Chen et al., “Shrinkage Algorithms for MMSE Covariance Estimation”, IEEE Trans. on Sign. Proc., Volume
58, Issue 10, October 2010.
336
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Fig. 3.7: Bias-variance trade-off when setting the shrinkage: comparing the choices of Ledoit-Wolf and OAS estima-
tors
Examples:
• See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for an example on how to ﬁt an
OAS object to data.
• See Ledoit-Wolf vs OAS estimation to visualize the Mean Squared Error difference between a LedoitWolf
and an OAS estimator of the covariance.
Sparse inverse covariance
The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation
matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on
the others, the corresponding coefﬁcient in the precision matrix will be zero. This is why it makes sense to estimate a
sparse precision matrix: by learning independence relations from the data, the estimation of the covariance matrix is
better conditioned. This is known as covariance selection.
In the small-samples situation, in which n_samples is on the order of n_features or smaller, sparse inverse
covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for
very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are
able to recover off-diagonal structure.
The GraphLasso estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its alpha
parameter, the more sparse the precision matrix. The corresponding GraphLassoCV object uses cross-validation to
automatically set the alpha parameter.
Note: Structure recovery
Recovering a graphical structure from correlations in the data is a challenging thing. If you are interested in such
recovery keep in mind that:
3.2. Unsupervised learning
337
scikit-learn user guide, Release 0.18.2
Fig. 3.8: A comparison of maximum likelihood, shrinkage and sparse estimates of the covariance and precision matrix
in the very small samples settings.
338
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• Recovery is easier from a correlation matrix than a covariance matrix: standardize your observations before
running GraphLasso
• If the underlying graph has nodes with much more connections than the average node, the algorithm will miss
some of these connections.
• If your number of observations is not large compared to the number of edges in your underlying graph, you will
not recover it.
• Even if you are in favorable recovery conditions, the alpha parameter chosen by cross-validation (e.g. using the
GraphLassoCV object) will lead to selecting too many edges. However, the relevant edges will have heavier
weights than the irrelevant ones.
The mathematical formulation is the following:
ˆ𝐾= argmin𝐾
(︀
tr𝑆𝐾−logdet𝐾+ 𝛼‖𝐾‖1
)︀
Where 𝐾is the precision matrix to be estimated, and 𝑆is the sample covariance matrix. ‖𝐾‖1 is the sum of the abso-
lute values of off-diagonal coefﬁcients of 𝐾. The algorithm employed to solve this problem is the GLasso algorithm,
from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R glasso package.
Examples:
• Sparse inverse covariance estimation: example on synthetic data showing some recovery of a structure, and
comparing to other covariance estimators.
• Visualizing the stock market structure: example on real stock market data, ﬁnding which symbols are most
linked.
References:
• Friedman et al, “Sparse inverse covariance estimation with the graphical lasso”, Biostatistics 9, pp 432, 2008
Robust Covariance Estimation
Real data set are often subjects to measurement or recording errors. Regular but uncommon observations may also
appear for a variety of reason. Every observation which is very uncommon is called an outlier. The empirical covari-
ance estimator and the shrunk covariance estimators presented above are very sensitive to the presence of outlying
observations in the data. Therefore, one should use robust covariance estimators to estimate the covariance of its real
data sets. Alternatively, robust covariance estimators can be used to perform outlier detection and discard/downweight
some observations according to further processing of the data.
The sklearn.covariance package implements a robust estimator of covariance, the Minimum Covariance De-
terminant [3].
Minimum Covariance Determinant
The Minimum Covariance Determinant estimator is a robust estimator of a data set’s covariance introduced by P.J.
Rousseeuw in [3]. The idea is to ﬁnd a given proportion (h) of “good” observations which are not outliers and compute
their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed
selection of observations (“consistency step”). Having computed the Minimum Covariance Determinant estimator,
3.2. Unsupervised learning
339
scikit-learn user guide, Release 0.18.2
one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the
covariance matrix of the data set (“reweighting step”).
Rousseeuw and Van Driessen [4] developed the FastMCD algorithm in order to compute the Minimum Covariance
Determinant. This algorithm is used in scikit-learn when ﬁtting an MCD object to data. The FastMCD algorithm also
computes a robust estimate of the data set location at the same time.
Raw estimates can be accessed as raw_location_ and raw_covariance_ attributes of a MinCovDet robust
covariance estimator object.
[3] P. J. Rousseeuw. Least median of squares regression.
10. Am Stat Ass, 79:871, 1984.
[4] A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical Associa-
tion and the American Society for Quality, TECHNOMETRICS.
Examples:
• See Robust vs Empirical covariance estimate for an example on how to ﬁt a MinCovDet object to data and
see how the estimate remains accurate despite the presence of outliers.
• See Robust covariance estimation and Mahalanobis distances relevance to visualize the difference between
EmpiricalCovariance and MinCovDet covariance estimators in terms of Mahalanobis distance (so
we get a better estimate of the precision matrix too).
Inﬂuence of outliers on location and covariance
estimates
Separating inliers from outliers using a Mahalanobis
distance
3.2.7 Novelty and Outlier Detection
Many applications require being able to decide whether a new observation belongs to the same distribution as existing
observations (it is an inlier), or should be considered as different (it is an outlier). Often, this ability is used to clean
real data sets. Two important distinction must be made:
novelty detection The training data is not polluted by outliers, and we are interested in detecting anoma-
lies in new observations.
outlier detection The training data contains outliers, and we need to ﬁt the central mode of the training
data, ignoring the deviant observations.
The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outliers detection.
This strategy is implemented with objects learning in an unsupervised way from the data:
estimator.fit(X_train)
new observations can then be sorted as inliers or outliers with a predict method:
340
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
estimator.predict(X_test)
Inliers are labeled 1, while outliers are labeled -1.
Novelty Detection
Consider a data set of 𝑛observations from the same distribution described by 𝑝features. Consider now that we add one
more observation to that data set. Is the new observation so different from the others that we can doubt it is regular?
(i.e. does it come from the same distribution?) Or on the contrary, is it so similar to the other that we cannot distinguish
it from the original observations? This is the question addressed by the novelty detection tools and methods.
In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution,
plotted in embedding 𝑝-dimensional space. Then, if further observations lay within the frontier-delimited subspace,
they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside
the frontier, we can say that they are abnormal with a given conﬁdence in our assessment.
The One-Class SVM has been introduced by Schölkopf et al. for that purpose and implemented in the Support Vector
Machines module in the svm.OneClassSVM object. It requires the choice of a kernel and a scalar parameter to
deﬁne a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its
bandwidth parameter. This is the default in the scikit-learn implementation. The 𝜈parameter, also known as the
margin of the One-Class SVM, corresponds to the probability of ﬁnding a new, but regular, observation outside the
frontier.
References:
• Estimating the support of a high-dimensional distribution Schölkopf, Bernhard, et al. Neural computation
13.7 (2001): 1443-1471.
Examples:
• See One-class SVM with non-linear kernel (RBF) for visualizing the frontier learned around some data by a
svm.OneClassSVM object.
Outlier Detection
Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations
from some polluting ones, called “outliers”. Yet, in the case of outlier detection, we don’t have a clean data set
representing the population of regular observations that can be used to train any tool.
Fitting an elliptic envelope
One common way of performing outlier detection is to assume that the regular data come from a known distribution
(e.g. data are Gaussian distributed). From this assumption, we generally try to deﬁne the “shape” of the data, and can
deﬁne outlying observations as observations which stand far enough from the ﬁt shape.
The scikit-learn provides an object covariance.EllipticEnvelope that ﬁts a robust covariance estimate to
the data, and thus ﬁts an ellipse to the central data points, ignoring points outside the central mode.
3.2. Unsupervised learning
341
scikit-learn user guide, Release 0.18.2
For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance
in a robust way (i.e. whithout being inﬂuenced by outliers). The Mahalanobis distances obtained from this estimate is
used to derive a measure of outlyingness. This strategy is illustrated below.
Examples:
• See Robust covariance estimation and Mahalanobis distances relevance for an illustration of the dif-
ference between using a standard (covariance.EmpiricalCovariance) or a robust estimate
(covariance.MinCovDet) of location and covariance to assess the degree of outlyingness of an ob-
servation.
References:
Isolation Forest
One efﬁcient way of performing outlier detection in high-dimensional datasets is to use random forests.
The
ensemble.IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly se-
lecting a split value between the maximum and minimum values of the selected feature.
Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample
is equivalent to the path length from the root node to the terminating node.
This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.
Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collec-
tively produce shorter path lengths for particular samples, they are highly likely to be anomalies.
This strategy is illustrated below.
342
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.2. Unsupervised learning
343
scikit-learn user guide, Release 0.18.2
Examples:
• See IsolationForest example for an illustration of the use of IsolationForest.
• See Outlier detection with several methods. for a comparison of ensemble.IsolationForest with
svm.OneClassSVM (tuned to perform like an outlier detection method) and a covariance-based outlier
detection with covariance.MinCovDet.
References:
One-class SVM versus Elliptic Envelope versus Isolation Forest
Strictly-speaking, the One-class SVM is not an outlier-detection method, but a novelty-detection method: its training
set should not be contaminated by outliers as it may ﬁt them. That said, outlier detection in high-dimension, or without
any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM gives useful results
in these situations.
The examples below illustrate how the performance of the covariance.EllipticEnvelope degrades as
the data is less and less unimodal.
The svm.OneClassSVM works better on data with multiple modes and
ensemble.IsolationForest performs well in every cases.
Table 3.1: Comparing One-class SVM approach, and elliptic envelope
For a inlier mode well-centered and elliptic, the svm.OneClassSVM is not able
to beneﬁt from the rotational symmetry of the inlier population. In addition, it ﬁts
a bit the outliers present in the training set. On the opposite, the decision rule
based on ﬁtting an covariance.EllipticEnvelope learns an ellipse,
which ﬁts well the inlier distribution. The ensemble.IsolationForest
performs as well.
As the inlier distribution becomes bimodal, the
covariance.EllipticEnvelope does not ﬁt well the inliers. However, we
can see that both ensemble.IsolationForest and svm.OneClassSVM
have difﬁculties to detect the two modes, and that the svm.OneClassSVM tends
to overﬁt: because it has not model of inliers, it interprets a region where, by
chance some outliers are clustered, as inliers.
If the inlier distribution is strongly non Gaussian, the svm.OneClassSVM is
able to recover a reasonable approximation as well as
ensemble.IsolationForest, whereas the
covariance.EllipticEnvelope completely fails.
Examples:
• See Outlier detection with several methods. for a comparison of the svm.OneClassSVM (tuned to perform
like an outlier detection method), the ensemble.IsolationForest and a covariance-based outlier de-
tection with covariance.MinCovDet.
344
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.2.8 Density Estimation
Density estimation walks the line between unsupervised learning, feature engineering, and data modeling. Some
of the most popular and useful density estimation techniques are mixture models such as Gaussian Mixtures
(sklearn.mixture.GaussianMixture), and neighbor-based approaches such as the kernel density estimate
(sklearn.neighbors.KernelDensity). Gaussian Mixtures are discussed more fully in the context of clus-
tering, because the technique is also useful as an unsupervised clustering scheme.
Density estimation is a very simple concept, and most people are already familiar with one common density estimation
technique: the histogram.
Density Estimation: Histograms
A histogram is a simple visualization of data where bins are deﬁned, and the number of data points within each bin is
tallied. An example of a histogram can be seen in the upper-left panel of the following ﬁgure:
A major problem with histograms, however, is that the choice of binning can have a disproportionate effect on the
resulting visualization. Consider the upper-right panel of the above ﬁgure. It shows a histogram over the same data,
with the bins shifted right. The results of the two visualizations look entirely different, and might lead to different
interpretations of the data.
Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the
appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we
center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left
visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it
is a much better representation of the underlying data.
This visualization is an example of a kernel density estimation, in this case with a top-hat kernel (i.e. a square block
at each point). We can recover a smoother distribution by using a smoother kernel. The bottom-right plot shows a
Gaussian kernel density estimate, in which each point contributes a Gaussian curve to the total. The result is a smooth
3.2. Unsupervised learning
345
scikit-learn user guide, Release 0.18.2
density estimate which is derived from the data, and functions as a powerful non-parametric model of the distribution
of points.
Kernel Density Estimation
Kernel density estimation in scikit-learn is implemented in the sklearn.neighbors.KernelDensity esti-
mator, which uses the Ball Tree or KD Tree for efﬁcient queries (see Nearest Neighbors for a discussion of these).
Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number
of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions.
In the following ﬁgure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown
for three choices of kernels:
It’s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density
estimator can be used as follows:
>>> from sklearn.neighbors.kde import KernelDensity
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)
>>> kde.score_samples(X)
array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,
-0.41076071])
Here we have used kernel='gaussian', as seen above. Mathematically, a kernel is a positive function 𝐾(𝑥; ℎ)
which is controlled by the bandwidth parameter ℎ. Given this kernel form, the density estimate at a point 𝑦within a
group of points 𝑥𝑖; 𝑖= 1 · · · 𝑁is given by:
𝜌𝐾(𝑦) =
𝑁
∑︁
𝑖=1
𝐾((𝑦−𝑥𝑖)/ℎ)
346
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A
large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth
(i.e. high-variance) density distribution.
sklearn.neighbors.KernelDensity implements several common kernel forms, which are shown in the
following ﬁgure:
The form of these kernels is as follows:
• Gaussian kernel (kernel = 'gaussian')
𝐾(𝑥; ℎ) ∝exp(−𝑥2
2ℎ2 )
• Tophat kernel (kernel = 'tophat')
𝐾(𝑥; ℎ) ∝1 if 𝑥< ℎ
• Epanechnikov kernel (kernel = 'epanechnikov')
𝐾(𝑥; ℎ) ∝1 −𝑥2
ℎ2
• Exponential kernel (kernel = 'exponential')
𝐾(𝑥; ℎ) ∝exp(−𝑥/ℎ)
• Linear kernel (kernel = 'linear')
𝐾(𝑥; ℎ) ∝1 −𝑥/ℎif 𝑥< ℎ
• Cosine kernel (kernel = 'cosine')
𝐾(𝑥; ℎ) ∝cos( 𝜋𝑥
2ℎ) if 𝑥< ℎ
The
kernel
density
estimator
can
be
used
with
any
of
the
valid
distance
metrics
(see
sklearn.neighbors.DistanceMetric for a list of available metrics), though the results are properly
normalized only for the Euclidean metric. One particularly useful metric is the Haversine distance which measures the
angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization
3.2. Unsupervised learning
347
scikit-learn user guide, Release 0.18.2
of geospatial data, in this case the distribution of observations of two different species on the South American
continent:
One other useful application of kernel density estimation is to learn a non-parametric generative model of a dataset in
order to efﬁciently draw new samples from this generative model. Here is an example of using this process to create a
new set of hand-written digits, using a Gaussian kernel learned on a PCA projection of the data:
348
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The “new” data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE
model.
Examples:
• Simple 1D Kernel Density Estimation: computation of simple kernel density estimates in one dimension.
• Kernel Density Estimation: an example of using Kernel Density estimation to learn a generative model of the
hand-written digits data, and drawing new samples from this model.
• Kernel Density Estimate of Species Distributions: an example of Kernel Density estimation using the Haver-
sine distance metric to visualize geospatial data
3.2.9 Neural network models (unsupervised)
Restricted Boltzmann machines
Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model.
The features extracted by an RBM or a hierarchy of RBMs often give good results when fed into a linear classiﬁer
such as a linear SVM or a perceptron.
The model makes assumptions regarding the distribution of inputs.
At the moment, scikit-learn only provides
BernoulliRBM, which assumes the inputs are either binary values or values between 0 and 1, each encoding the
probability that the speciﬁc feature would be turned on.
The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning
algorithm used (Stochastic Maximum Likelihood) prevents the representations from straying far from the input data,
which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not
useful for density estimation.
The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This
method is known as unsupervised pre-training.
Examples:
• Restricted Boltzmann Machine features for digit classiﬁcation
Graphical model and parametrization
The graphical model of an RBM is a fully-connected bipartite graph.
3.2. Unsupervised learning
349
scikit-learn user guide, Release 0.18.2
The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model
is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and
hidden unit, ommited from the image for simplicity.
The energy function measures the quality of a joint assignment:
𝐸(v, h) =
∑︁
𝑖
∑︁
𝑗
𝑤𝑖𝑗𝑣𝑖ℎ𝑗+
∑︁
𝑖
𝑏𝑖𝑣𝑖+
∑︁
𝑗
𝑐𝑗ℎ𝑗
In the formula above, b and c are the intercept vectors for the visible and hidden layers, respectively. The joint
350
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
probability of the model is deﬁned in terms of the energy:
𝑃(v, h) = 𝑒−𝐸(v,h)
𝑍
The word restricted refers to the bipartite structure of the model, which prohibits direct interaction between hidden
units, or between visible units. This means that the following conditional independencies are assumed:
ℎ𝑖⊥ℎ𝑗|v
𝑣𝑖⊥𝑣𝑗|h
The bipartite structure allows for the use of efﬁcient block Gibbs sampling for inference.
Bernoulli Restricted Boltzmann machines
In the BernoulliRBM, all units are binary stochastic units. This means that the input data should either be binary, or
real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model
for character recognition, where the interest is on which pixels are active and which aren’t. For images of natural
scenes it no longer ﬁts because of background, depth and the tendency of neighbouring pixels to take the same values.
The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it
receives:
𝑃(𝑣𝑖= 1|h) = 𝜎(
∑︁
𝑗
𝑤𝑖𝑗ℎ𝑗+ 𝑏𝑖)
𝑃(ℎ𝑖= 1|v) = 𝜎(
∑︁
𝑖
𝑤𝑖𝑗𝑣𝑖+ 𝑐𝑗)
where 𝜎is the logistic sigmoid function:
𝜎(𝑥) =
1
1 + 𝑒−𝑥
Stochastic Maximum Likelihood learning
The training algorithm implemented in BernoulliRBM is known as Stochastic Maximum Likelihood (SML) or
Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form
of the data likelihood:
log 𝑃(𝑣) = log
∑︁
ℎ
𝑒−𝐸(𝑣,ℎ) −log
∑︁
𝑥,𝑦
𝑒−𝐸(𝑥,𝑦)
For simplicity the equation above is written for a single training example. The gradient with respect to the weights is
formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative
gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of
samples.
In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with
the observed training data. Because of the bipartite structure of RBMs, it can be computed efﬁciently. The negative
gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making
it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively
sampling each of 𝑣and ℎgiven the other, until the chain mixes. Samples generated in this way are sometimes refered
as fantasy particles. This is inefﬁcient and it is difﬁcult to determine whether the Markov chain mixes.
The Contrastive Divergence method suggests to stop the chain after a small number of iterations, 𝑘, usually even 1.
This method is fast and has low variance, but the samples are far from the model distribution.
3.2. Unsupervised learning
351
scikit-learn user guide, Release 0.18.2
Persistent Contrastive Divergence addresses this. Instead of starting a new chain each time the gradient is needed, and
performing only one Gibbs sampling step, in PCD we keep a number of chains (fantasy particles) that are updated 𝑘
Gibbs steps after each weight update. This allows the particles to explore the space more thoroughly.
References:
• “A fast learning algorithm for deep belief nets” G. Hinton, S. Osindero, Y.-W. Teh, 2006
• “Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient” T. Tieleman,
2008
3.3 Model selection and evaluation
3.3.1 Cross-validation: evaluating estimator performance
Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model
that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to
predict anything useful on yet-unseen data. This situation is called overﬁtting. To avoid it, it is common practice
when performing a (supervised) machine learning experiment to hold out part of the available data as a test set
X_test,y_test. Note that the word “experiment” is not intended to denote academic use only, because even in
commercial settings machine learning usually starts out experimentally.
In scikit-learn a random split into training and test sets can be quickly computed with the train_test_split
helper function. Let’s load the iris data set to ﬁt a linear support vector machine on it:
>>> import numpy as np
>>> from sklearn.model_selection import train_test_split
>>> from sklearn import datasets
>>> from sklearn import svm
>>> iris = datasets.load_iris()
>>> iris.data.shape, iris.target.shape
((150, 4), (150,))
We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classiﬁer:
>>> X_train, X_test, y_train, y_test = train_test_split(
...
iris.data, iris.target, test_size=0.4, random_state=0)
>>> X_train.shape, y_train.shape
((90, 4), (90,))
>>> X_test.shape, y_test.shape
((60, 4), (60,))
>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.96...
When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set
for an SVM, there is still a risk of overﬁtting on the test set because the parameters can be tweaked until the estimator
performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer
report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-
called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and
when the experiment seems to be successful, ﬁnal evaluation can be done on the test set.
352
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be
used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation)
sets.
A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for
ﬁnal evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the
training set is split into k smaller sets (other approaches are described below, but generally follow the same principles).
The following procedure is followed for each of the k “folds”:
• A model is trained using 𝑘−1 of the folds as training data;
• the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a
performance measure such as accuracy).
The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop.
This approach can be computationally expensive, but does not waste too much data (as it is the case when ﬁxing an
arbitrary test set), which is a major advantage in problem such as inverse inference where the number of samples is
very small.
Computing cross-validated metrics
The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the
dataset.
The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the
iris dataset by splitting the data, ﬁtting a model and computing the score 5 consecutive times (with different splits each
time):
>>> from sklearn.model_selection import cross_val_score
>>> clf = svm.SVC(kernel='linear', C=1)
>>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)
>>> scores
array([ 0.96...,
1.
...,
0.96...,
0.96...,
1.
])
The mean score and the 95% conﬁdence interval of the score estimate are hence given by:
>>> print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
Accuracy: 0.98 (+/- 0.03)
By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change
this by using the scoring parameter:
>>> from sklearn import metrics
>>> scores = cross_val_score(
...
clf, iris.data, iris.target, cv=5, scoring='f1_macro')
>>> scores
array([ 0.96...,
1.
...,
0.96...,
0.96...,
1.
])
See The scoring parameter: deﬁning model evaluation rules for details. In the case of the Iris dataset, the samples are
balanced across target classes hence the accuracy and the F1-score are almost equal.
When the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by
default, the latter being used if the estimator derives from ClassifierMixin.
It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:
>>> from sklearn.model_selection import ShuffleSplit
>>> n_samples = iris.data.shape[0]
>>> cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)
3.3. Model selection and evaluation
353
scikit-learn user guide, Release 0.18.2
>>> cross_val_score(clf, iris.data, iris.target, cv=cv)
...
array([ 0.97...,
0.97...,
1.
])
Data transformation with held out data
Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization,
feature selection, etc.) and similar data transformations similarly should be learnt from a training set and applied
to held-out data for prediction:
>>> from sklearn import preprocessing
>>> X_train, X_test, y_train, y_test = train_test_split(
...
iris.data, iris.target, test_size=0.4, random_state=0)
>>> scaler = preprocessing.StandardScaler().fit(X_train)
>>> X_train_transformed = scaler.transform(X_train)
>>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)
>>> X_test_transformed = scaler.transform(X_test)
>>> clf.score(X_test_transformed, y_test)
0.9333...
A Pipeline makes it easier to compose estimators, providing this behavior under cross-validation:
>>> from sklearn.pipeline import make_pipeline
>>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))
>>> cross_val_score(clf, iris.data, iris.target, cv=cv)
...
array([ 0.97...,
0.93...,
0.95...])
See Pipeline and FeatureUnion: combining estimators.
Obtaining predictions by cross-validation
The function cross_val_predict has a similar interface to cross_val_score, but returns, for each element
in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation
strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).
These prediction can then be used to evaluate the classiﬁer:
>>> from sklearn.model_selection import cross_val_predict
>>> predicted = cross_val_predict(clf, iris.data, iris.target, cv=10)
>>> metrics.accuracy_score(iris.target, predicted)
0.966...
Note that the result of this computation may be slightly different from those obtained using cross_val_score as
the elements are grouped in different ways.
The available cross validation iterators are introduced in the following section.
Examples
• Receiver Operating Characteristic (ROC) with cross validation,
• Recursive feature elimination with cross-validation,
• Parameter estimation using grid search with cross-validation,
354
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• Sample pipeline for text feature extraction and evaluation,
• Plotting Cross-Validated Predictions,
• Nested versus non-nested cross-validation.
Cross validation iterators
The following sections list utilities to generate indices that can be used to generate dataset splits according to different
cross validation strategies.
Cross-validation iterators for i.i.d. data
Assuming that some data is Independent Identically Distributed (i.i.d.) is making the assumption that all samples stem
from the same generative process and that the generative process is assumed to have no memory of past generated
samples.
The following cross-validators can be used in such cases.
NOTE
While i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows
that the samples have been generated using a time-dependent process, it’s safer to use a time-series aware cross-
validation scheme <time_series_cv> Similarly if we know that the generative process has a group structure (samples
from collected from different subjects, experiments, measurement devices) it safer to use group-wise cross-validation
<group_cv>.
K-fold
KFold divides all the samples in 𝑘groups of samples, called folds (if 𝑘= 𝑛, this is equivalent to the Leave One Out
strategy), of equal sizes (if possible). The prediction function is learned using 𝑘−1 folds, and the fold left out is used
for test.
Example of 2-fold cross-validation on a dataset with 4 samples:
>>> import numpy as np
>>> from sklearn.model_selection import KFold
>>> X = ["a", "b", "c", "d"]
>>> kf = KFold(n_splits=2)
>>> for train, test in kf.split(X):
...
print("%s %s" % (train, test))
[2 3] [0 1]
[0 1] [2 3]
Each fold is constituted by two arrays: the ﬁrst one is related to the training set, and the second one to the test set.
Thus, one can create the training/test sets using numpy indexing:
>>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])
>>> y = np.array([0, 1, 0, 1])
>>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]
3.3. Model selection and evaluation
355
scikit-learn user guide, Release 0.18.2
Leave One Out (LOO)
LeaveOneOut (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except
one, the test set being the sample left out. Thus, for 𝑛samples, we have 𝑛different training sets and 𝑛different tests
set. This cross-validation procedure does not waste much data as only one sample is removed from the training set:
>>> from sklearn.model_selection import LeaveOneOut
>>> X = [1, 2, 3, 4]
>>> loo = LeaveOneOut()
>>> for train, test in loo.split(X):
...
print("%s %s" % (train, test))
[1 2 3] [0]
[0 2 3] [1]
[0 1 3] [2]
[0 1 2] [3]
Potential users of LOO for model selection should weigh a few known caveats. When compared with 𝑘-fold cross
validation, one builds 𝑛models from 𝑛samples instead of 𝑘models, where 𝑛> 𝑘. Moreover, each is trained on 𝑛−1
samples rather than (𝑘−1)𝑛/𝑘. In both ways, assuming 𝑘is not too large and 𝑘< 𝑛, LOO is more computationally
expensive than 𝑘-fold cross validation.
In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since 𝑛−1 of
the 𝑛samples are used to build each model, models constructed from folds are virtually identical to each other and to
the model built from the entire training set.
However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can
overestimate the generalization error.
As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred
to LOO.
References:
• http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html;
• T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning, Springer 2009
• L. Breiman, P. Spector Submodel selection and evaluation in regression: The X-random case, International
Statistical Review 1992;
• R. Kohavi, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Intl.
Jnt. Conf. AI
• R. Bharat Rao, G. Fung, R. Rosales, On the Dangers of Cross-Validation. An Experimental Evaluation, SIAM
2008;
• G. James, D. Witten, T. Hastie, R Tibshirani, An Introduction to Statistical Learning, Springer 2013.
Leave P Out (LPO)
LeavePOut is very similar to LeaveOneOut as it creates all the possible training/test sets by removing 𝑝samples
from the complete set. For 𝑛samples, this produces
(︀𝑛
𝑝
)︀
train-test pairs. Unlike LeaveOneOut and KFold, the test
sets will overlap for 𝑝> 1.
Example of Leave-2-Out on a dataset with 4 samples:
356
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> from sklearn.model_selection import LeavePOut
>>> X = np.ones(4)
>>> lpo = LeavePOut(p=2)
>>> for train, test in lpo.split(X):
...
print("%s %s" % (train, test))
[2 3] [0 1]
[1 3] [0 2]
[1 2] [0 3]
[0 3] [1 2]
[0 2] [1 3]
[0 1] [2 3]
Random permutations cross-validation a.k.a. Shufﬂe & Split
ShuffleSplit
The ShuffleSplit iterator will generate a user deﬁned number of independent train / test dataset splits. Samples
are ﬁrst shufﬂed and then split into a pair of train and test sets.
It is possible to control the randomness for reproducibility of the results by explicitly seeding the random_state
pseudo random number generator.
Here is a usage example:
>>> from sklearn.model_selection import ShuffleSplit
>>> X = np.arange(5)
>>> ss = ShuffleSplit(n_splits=3, test_size=0.25,
...
random_state=0)
>>> for train_index, test_index in ss.split(X):
...
print("%s %s" % (train_index, test_index))
...
[1 3 4] [2 0]
[1 4 3] [0 2]
[4 0 2] [1 3]
ShuffleSplit is thus a good alternative to KFold cross validation that allows a ﬁner control on the number of
iterations and the proportion of samples on each side of the train / test split.
Cross-validation iterators with stratiﬁcation based on class labels.
Some classiﬁcation problems can exhibit a large imbalance in the distribution of the target classes: for instance there
could be several times more negative samples than positive samples. In such cases it is recommended to use stratiﬁed
sampling as implemented in StratifiedKFold and StratifiedShuffleSplit to ensure that relative class
frequencies is approximately preserved in each train and validation fold.
Stratiﬁed k-fold
StratifiedKFold is a variation of k-fold which returns stratiﬁed folds: each set contains approximately the same
percentage of samples of each target class as the complete set.
Example of stratiﬁed 3-fold cross-validation on a dataset with 10 samples from two slightly unbalanced classes:
3.3. Model selection and evaluation
357
scikit-learn user guide, Release 0.18.2
>>> from sklearn.model_selection import StratifiedKFold
>>> X = np.ones(10)
>>> y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
>>> skf = StratifiedKFold(n_splits=3)
>>> for train, test in skf.split(X, y):
...
print("%s %s" % (train, test))
[2 3 6 7 8 9] [0 1 4 5]
[0 1 3 4 5 8 9] [2 6 7]
[0 1 2 4 5 6 7] [3 8 9]
Stratiﬁed Shufﬂe Split
StratifiedShuffleSplit is a variation of ShufﬂeSplit, which returns stratiﬁed splits, i.e which creates splits
by preserving the same percentage for each target class as in the complete set.
Cross-validation iterators for grouped data.
The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.
Such a grouping of data is domain speciﬁc. An example would be when there is medical data collected from multiple
patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual
group. In our example, the patient id for each sample will be its group identiﬁer.
In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen
groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not
represented at all in the paired training fold.
The following cross-validation splitters can be used to do that. The grouping identiﬁer for the samples is speciﬁed via
the groups parameter.
Group k-fold
class:GroupKFold is a variation of k-fold which ensures that the same group is not represented in both testing and
training sets. For example if the data is obtained from different subjects with several samples per-subject and if the
model is ﬂexible enough to learn from highly person speciﬁc features it could fail to generalize to new subjects.
class:GroupKFold makes it possible to detect this kind of overﬁtting situations.
Imagine you have three subjects, each with an associated number from 1 to 3:
>>> from sklearn.model_selection import GroupKFold
>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]
>>> y = ["a", "b", "b", "b", "c", "c", "c", "d", "d", "d"]
>>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]
>>> gkf = GroupKFold(n_splits=3)
>>> for train, test in gkf.split(X, y, groups=groups):
...
print("%s %s" % (train, test))
[0 1 2 3 4 5] [6 7 8 9]
[0 1 2 6 7 8 9] [3 4 5]
[3 4 5 6 7 8 9] [0 1 2]
358
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Each subject is in a different testing fold, and the same subject is never in both testing and training. Notice that the
folds do not have exactly the same size due to the imbalance in the data.
Leave One Group Out
LeaveOneGroupOut is a cross-validation scheme which holds out the samples according to a third-party provided
array of integer groups. This group information can be used to encode arbitrary domain speciﬁc pre-deﬁned cross-
validation folds.
Each training set is thus constituted by all the samples except the ones related to a speciﬁc group.
For example, in the cases of multiple experiments, LeaveOneGroupOut can be used to create a cross-validation
based on the different experiments: we create a training set using the samples of all the experiments except one:
>>> from sklearn.model_selection import LeaveOneGroupOut
>>> X = [1, 5, 10, 50, 60, 70, 80]
>>> y = [0, 1, 1, 2, 2, 2, 2]
>>> groups = [1, 1, 2, 2, 3, 3, 3]
>>> logo = LeaveOneGroupOut()
>>> for train, test in logo.split(X, y, groups=groups):
...
print("%s %s" % (train, test))
[2 3 4 5 6] [0 1]
[0 1 4 5 6] [2 3]
[0 1 2 3] [4 5 6]
Another common application is to use time information: for instance the groups could be the year of collection of the
samples and thus allow for cross-validation against time-based splits.
Leave P Groups Out
LeavePGroupsOut is similar as LeaveOneGroupOut, but removes samples related to 𝑃groups for each train-
ing/test set.
Example of Leave-2-Group Out:
>>> from sklearn.model_selection import LeavePGroupsOut
>>> X = np.arange(6)
>>> y = [1, 1, 1, 2, 2, 2]
>>> groups = [1, 1, 2, 2, 3, 3]
>>> lpgo = LeavePGroupsOut(n_groups=2)
>>> for train, test in lpgo.split(X, y, groups=groups):
...
print("%s %s" % (train, test))
[4 5] [0 1 2 3]
[2 3] [0 1 4 5]
[0 1] [2 3 4 5]
Group Shufﬂe Split
GroupShuffleSplit
The GroupShuffleSplit iterator behaves as a combination of ShuffleSplit and LeavePGroupsOut, and
generates a sequence of randomized partitions in which a subset of groups are held out for each split.
3.3. Model selection and evaluation
359
scikit-learn user guide, Release 0.18.2
Here is a usage example:
>>> from sklearn.model_selection import GroupShuffleSplit
>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]
>>> y = ["a", "b", "b", "b", "c", "c", "c", "a"]
>>> groups = [1, 1, 2, 2, 3, 3, 4, 4]
>>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)
>>> for train, test in gss.split(X, y, groups=groups):
...
print("%s %s" % (train, test))
...
[0 1 2 3] [4 5 6 7]
[2 3 6 7] [0 1 4 5]
[2 3 4 5] [0 1 6 7]
[4 5 6 7] [0 1 2 3]
This class is useful when the behavior of LeavePGroupsOut is desired, but the number of groups is large enough
that generating all possible partitions with 𝑃groups withheld would be prohibitively expensive. In such a sce-
nario, GroupShuffleSplit provides a random sample (with replacement) of the train / test splits generated by
LeavePGroupsOut.
Predeﬁned Fold-Splits / Validation-Sets
For some datasets, a pre-deﬁned split of the data into training- and validation fold or into several cross-validation folds
already exists. Using PredefinedSplit it is possible to use these folds e.g. when searching for hyperparameters.
For example, when using a validation set, set the test_fold to 0 for all samples that are part of the validation set,
and to -1 for all other samples.
Cross validation of time series data
Time series data is characterised by the correlation between observations that are near in time (autocorrelation). How-
ever, classical cross-validation techniques such as KFold and ShuffleSplit assume the samples are independent
and identically distributed, and would result in unreasonable correlation between training and testing instances (yield-
ing poor estimates of generalisation error) on time series data. Therefore, it is very important to evaluate our model
for time series data on the “future” observations least like those that are used to train the model. To achieve this, one
solution is provided by TimeSeriesSplit.
Time Series Split
TimeSeriesSplit is a variation of k-fold which returns ﬁrst 𝑘folds as train set and the (𝑘+ 1) th fold as test set.
Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before
them. Also, it adds all surplus data to the ﬁrst training partition, which is always used to train the model.
This class can be used to cross-validate time series data samples that are observed at ﬁxed time intervals.
Example of 3-split time series cross-validation on a dataset with 6 samples:
>>> from sklearn.model_selection import TimeSeriesSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> tscv = TimeSeriesSplit(n_splits=3)
>>> print(tscv)
TimeSeriesSplit(n_splits=3)
360
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> for train, test in tscv.split(X):
...
print("%s %s" % (train, test))
[0 1 2] [3]
[0 1 2 3] [4]
[0 1 2 3 4] [5]
A note on shufﬂing
If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shufﬂing it ﬁrst may
be essential to get a meaningful cross- validation result. However, the opposite may be true if the samples are not
independently and identically distributed. For example, if samples correspond to news articles, and are ordered by
their time of publication, then shufﬂing the data will likely lead to a model that is overﬁt and an inﬂated validation
score: it will be tested on samples that are artiﬁcially similar (close in time) to training samples.
Some cross validation iterators, such as KFold, have an inbuilt option to shufﬂe the data indices before splitting them.
Note that:
• This consumes less memory than shufﬂing the data directly.
• By default no shufﬂing occurs, including for the (stratiﬁed) K fold cross- validation performed by specifying
cv=some_integer to cross_val_score, grid search, etc. Keep in mind that train_test_split
still returns a random split.
• The random_state parameter defaults to None, meaning that the shufﬂing will be different every time
KFold(...,shuffle=True) is iterated. However, GridSearchCV will use the same shufﬂing for each
set of parameters validated by a single call to its fit method.
• To ensure results are repeatable (on the same platform), use a ﬁxed value for random_state.
Cross validation and model selection
Cross validation iterators can also be used to directly perform model selection using Grid Search for the optimal
hyperparameters of the model. This is the topic of the next section: Tuning the hyper-parameters of an estimator.
3.3.2 Tuning the hyper-parameters of an estimator
Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as
arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support
Vector Classiﬁer, alpha for Lasso, etc.
It is possible and recommended to search the hyper-parameter space for the best Cross-validation: evaluating estimator
performance score.
Any parameter provided when constructing an estimator may be optimized in this manner. Speciﬁcally, to ﬁnd the
names and current values for all parameters for a given estimator, use:
estimator.get_params()
A search consists of:
• an estimator (regressor or classiﬁer such as sklearn.svm.SVC());
• a parameter space;
• a method for searching or sampling candidates;
• a cross-validation scheme; and
3.3. Model selection and evaluation
361
scikit-learn user guide, Release 0.18.2
• a score function.
Some models allow for specialized, efﬁcient parameter search strategies, outlined below. Two generic approaches to
sampling search candidates are provided in scikit-learn: for given values, GridSearchCV exhaustively considers all
parameter combinations, while RandomizedSearchCV can sample a given number of candidates from a parameter
space with a speciﬁed distribution. After describing these tools we detail best practice applicable to both approaches.
Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation
performance of the model while others can be left to their default values. It is recommend to read the docstring of the
estimator class to get a ﬁner understanding of their expected behavior, possibly by reading the enclosed reference to
the literature.
Exhaustive Grid Search
The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values
speciﬁed with the param_grid parameter. For instance, the following param_grid:
param_grid = [
{'C': [1, 10, 100, 1000], 'kernel': ['linear']},
{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
]
speciﬁes that two grids should be explored: one with a linear kernel and C values in [1, 10, 100, 1000], and the second
one with an RBF kernel, and the cross-product of C values ranging in [1, 10, 100, 1000] and gamma values in [0.001,
0.0001].
The GridSearchCV instance implements the usual estimator API: when “ﬁtting” it on a dataset all the possible
combinations of parameter values are evaluated and the best combination is retained.
Examples:
• See Parameter estimation using grid search with cross-validation for an example of Grid Search computation
on the digits dataset.
• See Sample pipeline for text feature extraction and evaluation for an example of Grid Search coupling
parameters from a text documents feature extractor (n-gram count vectorizer and TF-IDF transformer)
with a classiﬁer (here a linear SVM trained with SGD with either elastic net or L2 penalty) using a
pipeline.Pipeline instance.
• See Nested versus non-nested cross-validation for an example of Grid Search within a cross validation loop
on the iris dataset. This is the best practice for evaluating the performance of a model with grid search.
Randomized Parameter Optimization
While using a grid of parameter settings is currently the most widely used method for parameter optimization, other
search methods have more favourable properties. RandomizedSearchCV implements a randomized search over
parameters, where each setting is sampled from a distribution over possible parameter values. This has two main
beneﬁts over an exhaustive search:
• A budget can be chosen independent of the number of parameters and possible values.
• Adding parameters that do not inﬂuence the performance does not decrease efﬁciency.
Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for
GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling itera-
362
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
tions, is speciﬁed using the n_iter parameter. For each parameter, either a distribution over possible values or a list
of discrete choices (which will be sampled uniformly) can be speciﬁed:
{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),
'kernel': ['rbf'], 'class_weight':['balanced', None]}
This example uses the scipy.stats module, which contains many useful distributions for sampling parameters,
such as expon, gamma, uniform or randint. In principle, any function can be passed that provides a rvs
(random variate sample) method to sample a value. A call to the rvs function should provide independent random
samples from possible parameter values on consecutive calls.
Warning:
The distributions in scipy.stats prior to version scipy 0.16 do not allow speci-
fying a random state.
Instead, they use the global numpy random state, that can be seeded via
np.random.seed or set using np.random.set_state. However, beginning scikit-learn 0.18,
the sklearn.model_selection module sets the random state provided by the user if scipy >=
0.16 is also available.
For continuous parameters, such as C above, it is important to specify a continuous distribution to take full advantage
of the randomization. This way, increasing n_iter will always lead to a ﬁner search.
Examples:
• Comparing randomized search and grid search for hyperparameter estimation compares the usage and efﬁ-
ciency of randomized search and grid search.
References:
• Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learn-
ing Research (2012)
Tips for parameter search
Specifying an objective metric
By default, parameter search uses the score function of the estimator to evaluate a parameter setting. These are
the sklearn.metrics.accuracy_score for classiﬁcation and sklearn.metrics.r2_score for regres-
sion. For some applications, other scoring functions are better suited (for example in unbalanced classiﬁcation, the
accuracy score is often uninformative). An alternative scoring function can be speciﬁed via the scoring parameter
to GridSearchCV, RandomizedSearchCV and many of the specialized cross-validation tools described below.
See The scoring parameter: deﬁning model evaluation rules for more details.
Composite estimators and parameter spaces
Pipeline: chaining estimators describes building composite estimators whose parameter space can be searched with
these tools.
3.3. Model selection and evaluation
363
scikit-learn user guide, Release 0.18.2
Model selection: development and evaluation
Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to “train” the
parameters of the grid.
When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid
search process: it is recommended to split the data into a development set (to be fed to the GridSearchCV instance)
and an evaluation set to compute performance metrics.
This can be done by using the train_test_split utility function.
Parallelism
GridSearchCV and RandomizedSearchCV evaluate each parameter setting independently. Computations can
be run in parallel if your OS supports it, by using the keyword n_jobs=-1. See function signature for more details.
Robustness to failure
Some parameter settings may result in a failure to fit one or more folds of the data. By default, this will cause
the entire search to fail, even if some parameter settings could be fully evaluated. Setting error_score=0 (or
=np.NaN) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to 0 (or
NaN), but completing the search.
Alternatives to brute force parameter search
Model speciﬁc cross-validation
Some models can ﬁt data for a range of values of some parameter almost as efﬁciently as ﬁtting the estimator for
a single value of the parameter. This feature can be leveraged to perform a more efﬁcient cross-validation used for
model selection of this parameter.
The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In
this case we say that we compute the regularization path of the estimator.
Here is the list of such models:
linear_model.ElasticNetCV([l1_ratio, eps, ...])
Elastic Net model with iterative ﬁtting along a regulariza-
tion path
linear_model.LarsCV([ﬁt_intercept, ...])
Cross-validated Least Angle Regression model
linear_model.LassoCV([eps, n_alphas, ...])
Lasso linear model with iterative ﬁtting along a regulariza-
tion path
linear_model.LassoLarsCV([ﬁt_intercept, ...])
Cross-validated Lasso, using the LARS algorithm
linear_model.LogisticRegressionCV([Cs,
...])
Logistic Regression CV (aka logit, MaxEnt) classiﬁer.
linear_model.MultiTaskElasticNetCV([...])
Multi-task L1/L2 ElasticNet with built-in cross-validation.
linear_model.MultiTaskLassoCV([eps, ...])
Multi-task L1/L2 Lasso with built-in cross-validation.
linear_model.OrthogonalMatchingPursuitCV([...])
Cross-validated
Orthogonal
Matching
Pursuit
model
(OMP)
linear_model.RidgeCV([alphas, ...])
Ridge regression with built-in cross-validation.
linear_model.RidgeClassifierCV([alphas, ...])
Ridge classiﬁer with built-in cross-validation.
364
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
sklearn.linear_model.ElasticNetCV
class sklearn.linear_model.ElasticNetCV(l1_ratio=0.5,
eps=0.001,
n_alphas=100,
al-
phas=None,
ﬁt_intercept=True,
normalize=False,
precompute=’auto’,
max_iter=1000,
tol=0.0001,
cv=None, copy_X=True, verbose=0, n_jobs=1, posi-
tive=False, random_state=None, selection=’cyclic’)
Elastic Net model with iterative ﬁtting along a regularization path
The best model is selected by cross-validation.
Read more in the User Guide.
Parametersl1_ratio : ﬂoat or array of ﬂoats, optional
ﬂoat between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For
l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty.
For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2 This parameter
can be a list, in which case the different values are tested by cross-validation and the
one giving the best prediction score is used. Note that a good choice of list of values
for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e.
Ridge), as in [.1,.5,.7,.9,.95,.99,1]
eps : ﬂoat, optional
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3.
n_alphas : int, optional
Number of alphas along the regularization path, used for each l1_ratio.
alphas : numpy array, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
max_iter : int, optional
The maximum number of iterations
tol : ﬂoat, optional
The tolerance for the optimization: if the updates are smaller than tol, the optimization
code checks the dual gap for optimality and continues until it is smaller than tol.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
verbose : bool or integer
3.3. Model selection and evaluation
365
scikit-learn user guide, Release 0.18.2
Amount of verbosity.
n_jobs : integer, optional
Number of CPUs to use during the cross validation. If -1, use all the CPUs.
positive : bool, optional
When set to True, forces the coefﬁcients to be positive.
selection : str, default ‘cyclic’
If set to ‘random’, a random coefﬁcient is updated every iteration rather than looping
over features sequentially by default. This (setting to ‘random’) often leads to signiﬁ-
cantly faster convergence especially when tol is higher than 1e-4.
random_state : int, RandomState instance, or None (default)
The seed of the pseudo random number generator that selects a random feature to up-
date. Useful only when selection is set to ‘random’.
ﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is
ignored when fit_intercept is set to False. When the regressors are normalized,
note that this makes the hyperparameters learnt more robust and almost independent of
the number of samples. The same property is not valid for standardized data. However,
if you wish to standardize, please use preprocessing.StandardScaler before
calling fit on an estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
Attributesalpha_ : ﬂoat
The amount of penalization chosen by cross validation
l1_ratio_ : ﬂoat
The compromise between l1 and l2 penalization chosen by cross validation
coef_ : array, shape (n_features,) | (n_targets, n_features)
Parameter vector (w in the cost function formula),
intercept_ : ﬂoat | array, shape (n_targets, n_features)
Independent term in the decision function.
mse_path_ : array, shape (n_l1_ratio, n_alpha, n_folds)
Mean square error for the test set on each fold, varying l1_ratio and alpha.
alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)
The grid of alphas used for ﬁtting, for each l1_ratio.
n_iter_ : int
number of iterations run by the coordinate descent solver to reach the speciﬁed tolerance
for the optimal alpha.
366
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
See also:
enet_path, ElasticNet
Notes
See examples/linear_model/plot_lasso_model_selection.py for an example.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda
parameter in glmnet. More speciﬁcally, the optimization objective is:
1 / (2 * n_samples) * ||y - Xw||^2_2
+ alpha * l1_ratio * ||w||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2
If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to:
a * L1 + b * L2
for:
alpha = a + b and l1_ratio = a / (a + b).
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y)
Fit linear model with coordinate descent
get_params([deep])
Get parameters for this estimator.
path(X, y[, l1_ratio, eps, n_alphas, ...])
Compute elastic net path with coordinate descent
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(l1_ratio=0.5,
eps=0.001,
n_alphas=100,
alphas=None,
ﬁt_intercept=True,
normal-
ize=False, precompute=’auto’, max_iter=1000, tol=0.0001, cv=None, copy_X=True, ver-
bose=0, n_jobs=1, positive=False, random_state=None, selection=’cyclic’)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit linear model with coordinate descent
3.3. Model selection and evaluation
367
scikit-learn user guide, Release 0.18.2
Fit is on grid of alphas and best alpha estimated by cross-validation.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as ﬂoat64, Fortran-contiguous data to avoid unnecessary
memory duplication. If y is mono-output, X can be sparse.
y : array-like, shape (n_samples,) or (n_samples, n_targets)
Target values
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
static path(X, y,
l1_ratio=0.5,
eps=0.001,
n_alphas=100,
alphas=None,
precompute=’auto’,
Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, posi-
tive=False, check_input=True, **params)
Compute elastic net path with coordinate descent
The elastic net optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
1 / (2 * n_samples) * ||y - Xw||^2_2
+ alpha * l1_ratio * ||w||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2
For multi-output tasks it is:
(1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+ alpha * l1_ratio * ||W||_21
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory
duplication. If y is mono-output then X can be sparse.
y : ndarray, shape (n_samples,) or (n_samples, n_outputs)
Target values
l1_ratio : ﬂoat, optional
ﬂoat between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties).
l1_ratio=1 corresponds to the Lasso
eps : ﬂoat
368
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3
n_alphas : int, optional
Number of alphas along the regularization path
alphas : ndarray, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
Xy : array-like, optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
coef_init : array, shape (n_features, ) | None
The initial values of the coefﬁcients.
verbose : bool or integer
Amount of verbosity.
params : kwargs
keyword arguments passed to the coordinate descent solver.
return_n_iter : bool
whether to return the number of iterations or not.
positive : bool, default False
If set to True, forces coefﬁcients to be positive.
check_input : bool, default True
Skip input validation checks, including the Gram matrix when provided assuming there
are handled by the caller when check_input=False.
Returnsalphas : array, shape (n_alphas,)
The alphas along the path where models are computed.
coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
Coefﬁcients along the path.
dual_gaps : array, shape (n_alphas,)
The dual gaps at the end of the optimization for each alpha.
n_iters : array-like, shape (n_alphas,)
The number of iterations taken by the coordinate descent optimizer to reach the speciﬁed
tolerance for each alpha. (Is returned when return_n_iter is set to True).
See also:
MultiTaskElasticNet, MultiTaskElasticNetCV, ElasticNet, ElasticNetCV
3.3. Model selection and evaluation
369
scikit-learn user guide, Release 0.18.2
Notes
See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sklearn.linear_model.LarsCV
class sklearn.linear_model.LarsCV(ﬁt_intercept=True,
verbose=False,
max_iter=500,
normal-
ize=True, precompute=’auto’, cv=None, max_n_alphas=1000,
n_jobs=1, eps=2.2204460492503131e-16, copy_X=True, posi-
tive=False)
Cross-validated Least Angle Regression model
Read more in the User Guide.
Parametersﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
positive : boolean (default=False)
370
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Restrict coefﬁcients to be >= 0. Be aware that you might want to remove ﬁt_intercept
which is set True by default.
verbose : boolean or integer, optional
Sets the verbosity amount
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
max_iter: integer, optional :
Maximum number of iterations to perform.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
max_n_alphas : integer, optional
The maximum number of points on the path used to compute the residuals in the cross-
validation
n_jobs : integer, optional
Number of CPUs to use during the cross validation. If -1, use all the CPUs
eps : ﬂoat, optional
The machine-precision regularization in the computation of the Cholesky diagonal fac-
tors. Increase this for very ill-conditioned systems.
Attributescoef_ : array, shape (n_features,)
parameter vector (w in the formulation formula)
intercept_ : ﬂoat
independent term in decision function
coef_path_ : array, shape (n_features, n_alphas)
3.3. Model selection and evaluation
371
scikit-learn user guide, Release 0.18.2
the varying values of the coefﬁcients along the path
alpha_ : ﬂoat
the estimated regularization parameter alpha
alphas_ : array, shape (n_alphas,)
the different values of alpha along the path
cv_alphas_ : array, shape (n_cv_alphas,)
all the values of alpha along the path for the different folds
cv_mse_path_ : array, shape (n_folds, n_cv_alphas)
the mean square error on left-out for each fold along the path (alpha values given by
cv_alphas)
n_iter_ : array-like or int
the number of iterations run by Lars with the optimal alpha.
See also:
lars_path, LassoLars, LassoLarsCV
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y)
Fit the model using X, y as training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(ﬁt_intercept=True, verbose=False, max_iter=500, normalize=True, precompute=’auto’,
cv=None, max_n_alphas=1000, n_jobs=1, eps=2.2204460492503131e-16, copy_X=True,
positive=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit the model using X, y as training data.
ParametersX : array-like, shape (n_samples, n_features)
Training data.
y : array-like, shape (n_samples,)
372
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Target values.
Returnsself : object
returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
3.3. Model selection and evaluation
373
scikit-learn user guide, Release 0.18.2
sklearn.linear_model.LassoCV
class sklearn.linear_model.LassoCV(eps=0.001, n_alphas=100, alphas=None, ﬁt_intercept=True,
normalize=False,
precompute=’auto’,
max_iter=1000,
tol=0.0001,
copy_X=True,
cv=None,
verbose=False,
n_jobs=1,
positive=False,
random_state=None,
selec-
tion=’cyclic’)
Lasso linear model with iterative ﬁtting along a regularization path
The best model is selected by cross-validation.
The optimization objective for Lasso is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
Read more in the User Guide.
Parameterseps : ﬂoat, optional
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3.
n_alphas : int, optional
Number of alphas along the regularization path
alphas : numpy array, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
max_iter : int, optional
The maximum number of iterations
tol : ﬂoat, optional
The tolerance for the optimization: if the updates are smaller than tol, the optimization
code checks the dual gap for optimality and continues until it is smaller than tol.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
verbose : bool or integer
Amount of verbosity.
n_jobs : integer, optional
Number of CPUs to use during the cross validation. If -1, use all the CPUs.
positive : bool, optional
374
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
If positive, restrict regression coefﬁcients to be positive
selection : str, default ‘cyclic’
If set to ‘random’, a random coefﬁcient is updated every iteration rather than looping
over features sequentially by default. This (setting to ‘random’) often leads to signiﬁ-
cantly faster convergence especially when tol is higher than 1e-4.
random_state : int, RandomState instance, or None (default)
The seed of the pseudo random number generator that selects a random feature to up-
date. Useful only when selection is set to ‘random’.
ﬁt_intercept : boolean, default True
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is
ignored when fit_intercept is set to False. When the regressors are normalized,
note that this makes the hyperparameters learnt more robust and almost independent of
the number of samples. The same property is not valid for standardized data. However,
if you wish to standardize, please use preprocessing.StandardScaler before
calling fit on an estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
Attributesalpha_ : ﬂoat
The amount of penalization chosen by cross validation
coef_ : array, shape (n_features,) | (n_targets, n_features)
parameter vector (w in the cost function formula)
intercept_ : ﬂoat | array, shape (n_targets,)
independent term in decision function.
mse_path_ : array, shape (n_alphas, n_folds)
mean square error for the test set on each fold, varying alpha
alphas_ : numpy array, shape (n_alphas,)
The grid of alphas used for ﬁtting
dual_gap_ : ndarray, shape ()
The dual gap at the end of the optimization for the optimal alpha (alpha_).
n_iter_ : int
number of iterations run by the coordinate descent solver to reach the speciﬁed tolerance
for the optimal alpha.
See also:
lars_path, lasso_path, LassoLars, Lasso, LassoLarsCV
3.3. Model selection and evaluation
375
scikit-learn user guide, Release 0.18.2
Notes
See examples/linear_model/plot_lasso_model_selection.py for an example.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y)
Fit linear model with coordinate descent
get_params([deep])
Get parameters for this estimator.
path(X, y[, eps, n_alphas, alphas, ...])
Compute Lasso path with coordinate descent
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(eps=0.001,
n_alphas=100,
alphas=None,
ﬁt_intercept=True,
normalize=False,
pre-
compute=’auto’, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False,
n_jobs=1, positive=False, random_state=None, selection=’cyclic’)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit linear model with coordinate descent
Fit is on grid of alphas and best alpha estimated by cross-validation.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as ﬂoat64, Fortran-contiguous data to avoid unnecessary
memory duplication. If y is mono-output, X can be sparse.
y : array-like, shape (n_samples,) or (n_samples, n_targets)
Target values
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
376
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
static path(X,
y,
eps=0.001,
n_alphas=100,
alphas=None,
precompute=’auto’,
Xy=None,
copy_X=True,
coef_init=None,
verbose=False,
return_n_iter=False,
positive=False,
**params)
Compute Lasso path with coordinate descent
The Lasso optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
For multi-output tasks it is:
(1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory
duplication. If y is mono-output then X can be sparse.
y : ndarray, shape (n_samples,), or (n_samples, n_outputs)
Target values
eps : ﬂoat, optional
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3
n_alphas : int, optional
Number of alphas along the regularization path
alphas : ndarray, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
Xy : array-like, optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
coef_init : array, shape (n_features, ) | None
The initial values of the coefﬁcients.
verbose : bool or integer
Amount of verbosity.
params : kwargs
3.3. Model selection and evaluation
377
scikit-learn user guide, Release 0.18.2
keyword arguments passed to the coordinate descent solver.
positive : bool, default False
If set to True, forces coefﬁcients to be positive.
return_n_iter : bool
whether to return the number of iterations or not.
Returnsalphas : array, shape (n_alphas,)
The alphas along the path where models are computed.
coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
Coefﬁcients along the path.
dual_gaps : array, shape (n_alphas,)
The dual gaps at the end of the optimization for each alpha.
n_iters : array-like, shape (n_alphas,)
The number of iterations taken by the coordinate descent optimizer to reach the speciﬁed
tolerance for each alpha.
See also:
lars_path,
Lasso,
LassoLars,
LassoCV,
LassoLarsCV,
sklearn.decomposition.sparse_encode
Notes
See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
Note that in certain cases, the Lars solver may be signiﬁcantly faster to implement this functionality. In
particular, linear interpolation can be used to retrieve model coefﬁcients between the values output by
lars_path
Examples
Comparing lasso_path and lars_path with interpolation:
>>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T
>>> y = np.array([1, 2, 3.1])
>>> # Use lasso_path to compute a coefficient path
>>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])
>>> print(coef_path)
[[ 0.
0.
0.46874778]
[ 0.2159048
0.4425765
0.23689075]]
>>> # Now use lars_path and 1D linear interpolation to compute the
>>> # same path
>>> from sklearn.linear_model import lars_path
>>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')
>>> from scipy import interpolate
>>> coef_path_continuous = interpolate.interp1d(alphas[::-1],
378
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
...
coef_path_lars[:, ::-1])
>>> print(coef_path_continuous([5., 1., .5]))
[[ 0.
0.
0.46915237]
[ 0.2159048
0.4425765
0.23668876]]
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.LassoCV
• Cross-validation on diabetes Dataset Exercise
• Feature selection using SelectFromModel and LassoCV
• Lasso model selection: Cross-Validation / AIC / BIC
3.3. Model selection and evaluation
379
scikit-learn user guide, Release 0.18.2
sklearn.linear_model.LassoLarsCV
class sklearn.linear_model.LassoLarsCV(ﬁt_intercept=True,
verbose=False,
max_iter=500,
normalize=True,
precompute=’auto’,
cv=None,
max_n_alphas=1000,
n_jobs=1,
eps=2.2204460492503131e-16,
copy_X=True,
posi-
tive=False)
Cross-validated Lasso, using the LARS algorithm
The optimization objective for Lasso is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
Read more in the User Guide.
Parametersﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
positive : boolean (default=False)
Restrict coefﬁcients to be >= 0. Be aware that you might want to remove ﬁt_intercept
which is set True by default. Under the positive restriction the model coefﬁcients do
not converge to the ordinary-least-squares solution for small values of alpha. Only co-
efﬁcients up to the smallest alpha value (alphas_[alphas_ > 0.].min() when
ﬁt_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congru-
ence with the solution of the coordinate descent Lasso estimator. As a consequence
using LassoLarsCV only makes sense for problems where a sparse solution is expected
and/or reached.
verbose : boolean or integer, optional
Sets the verbosity amount
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
max_iter : integer, optional
Maximum number of iterations to perform.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
380
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
For integer/None inputs, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
max_n_alphas : integer, optional
The maximum number of points on the path used to compute the residuals in the cross-
validation
n_jobs : integer, optional
Number of CPUs to use during the cross validation. If -1, use all the CPUs
eps : ﬂoat, optional
The machine-precision regularization in the computation of the Cholesky diagonal fac-
tors. Increase this for very ill-conditioned systems.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
Attributescoef_ : array, shape (n_features,)
parameter vector (w in the formulation formula)
intercept_ : ﬂoat
independent term in decision function.
coef_path_ : array, shape (n_features, n_alphas)
the varying values of the coefﬁcients along the path
alpha_ : ﬂoat
the estimated regularization parameter alpha
alphas_ : array, shape (n_alphas,)
the different values of alpha along the path
cv_alphas_ : array, shape (n_cv_alphas,)
all the values of alpha along the path for the different folds
cv_mse_path_ : array, shape (n_folds, n_cv_alphas)
the mean square error on left-out for each fold along the path (alpha values given by
cv_alphas)
n_iter_ : array-like or int
the number of iterations run by Lars with the optimal alpha.
See also:
lars_path, LassoLars, LarsCV, LassoCV
Notes
The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it ﬁnd the relevant
alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to
heavily multicollinear datasets.
It is more efﬁcient than the LassoCV if only a small number of features are selected compared to the total
number, for instance if there are very few samples compared to the number of features.
3.3. Model selection and evaluation
381
scikit-learn user guide, Release 0.18.2
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y)
Fit the model using X, y as training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(ﬁt_intercept=True, verbose=False, max_iter=500, normalize=True, precompute=’auto’,
cv=None, max_n_alphas=1000, n_jobs=1, eps=2.2204460492503131e-16, copy_X=True,
positive=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit the model using X, y as training data.
ParametersX : array-like, shape (n_samples, n_features)
Training data.
y : array-like, shape (n_samples,)
Target values.
Returnsself : object
returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
382
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.LassoLarsCV
• Lasso model selection: Cross-Validation / AIC / BIC
• Sparse recovery: feature selection for sparse linear models
sklearn.linear_model.LogisticRegressionCV
class sklearn.linear_model.LogisticRegressionCV(Cs=10,
ﬁt_intercept=True,
cv=None,
dual=False,
penalty=’l2’,
scor-
ing=None,
solver=’lbfgs’,
tol=0.0001,
max_iter=100,
class_weight=None,
n_jobs=1,
verbose=0,
reﬁt=True,
in-
tercept_scaling=1.0,
multi_class=’ovr’,
random_state=None)
Logistic Regression CV (aka logit, MaxEnt) classiﬁer.
This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag
and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1
and L2 regularization, with a dual formulation only for the L2 penalty.
For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and
1e4), the best hyperparameter is selected by the cross-validator StratiﬁedKFold, but it can be changed using
the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial
coefﬁcients of the present ﬁt to be the coefﬁcients got after convergence in the previous ﬁt, so it is supposed to
be faster for high-dimensional dense data.
3.3. Model selection and evaluation
383
scikit-learn user guide, Release 0.18.2
For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a
one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.
Read more in the User Guide.
ParametersCs : list of ﬂoats | int
Each of the values in Cs describes the inverse of regularization strength. If Cs is as an
int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4.
Like in support vector machines, smaller values specify stronger regularization.
ﬁt_intercept : bool, default: True
Speciﬁes if a constant (a.k.a. bias or intercept) should be added to the decision function.
class_weight : dict or ‘balanced’, optional
Weights associated with classes in the form {class_label:
weight}. If not
given, all classes are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y)).
Note that these weights will be multiplied with sample_weight (passed through the ﬁt
method) if sample_weight is speciﬁed.
New in version 0.17: class_weight == ‘balanced’
cv : integer or cross-validation generator
The default cross-validation generator used is Stratiﬁed K-Folds.
If an in-
teger is provided,
then it is the number of folds used.
See the module
sklearn.model_selection module for the list of possible cross-validation ob-
jects.
penalty : str, ‘l1’ or ‘l2’
Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’
solvers support only l2 penalties.
dual : bool
Dual or primal formulation. Dual formulation is only implemented for l2 penalty with
liblinear solver. Prefer dual=False when n_samples > n_features.
scoring : callabale
Scoring function to use as cross-validation criteria. For a list of scoring functions that
can be used, look at sklearn.metrics. The default scoring option used is accu-
racy_score.
solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}
Algorithm to use in the optimization problem.
•For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ isfaster
for
large
ones.
•For multiclass problems, only ‘newton-cg’, ‘sag’ and ‘lbfgs’ handlemultinomial
loss; ‘liblinear’ is limited to one-versus-rest schemes.
•‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty.
•‘liblinear’ might be slower in LogisticRegressionCV because it doesnot
handle
warm-starting.
384
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Note that ‘sag’ fast convergence is only guaranteed on features with approximately the
same scale. You can preprocess the data with a scaler from sklearn.preprocessing.
New in version 0.17: Stochastic Average Gradient descent solver.
tol : ﬂoat, optional
Tolerance for stopping criteria.
max_iter : int, optional
Maximum number of iterations of the optimization algorithm.
n_jobs : int, optional
Number of CPU cores used during the cross-validation loop. If given a value of -1, all
cores are used.
verbose : int
For the ‘liblinear’, ‘sag’ and ‘lbfgs’ solvers set verbose to any positive number for ver-
bosity.
reﬁt : bool
If set to True, the scores are averaged across all folds, and the coefs and the C that
corresponds to the best score is taken, and a ﬁnal reﬁt is done using these parameters.
Otherwise the coefs, intercepts and C that correspond to the best scores across folds are
averaged.
multi_class : str, {‘ovr’, ‘multinomial’}
Multiclass option can be either ‘ovr’ or ‘multinomial’. If the option chosen is ‘ovr’,
then a binary problem is ﬁt for each label. Else the loss minimised is the multinomial
loss ﬁt across the entire probability distribution. Works only for the ‘newton-cg’, ‘sag’
and ‘lbfgs’ solver.
New in version 0.18: Stochastic Average Gradient descent solver for ‘multinomial’
case.
intercept_scaling : ﬂoat, default 1.
Useful only when the solver ‘liblinear’ is used and self.ﬁt_intercept is set to True. In this
case, x becomes [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value
equal to intercept_scaling is appended to the instance vector. The intercept becomes
intercept_scaling * synthetic_feature_weight.
Note! the synthetic feature weight is subject to l1/l2 regularization as all other features.
To lessen the effect of regularization on synthetic feature weight (and therefore on the
intercept) intercept_scaling has to be increased.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data.
Attributescoef_ : array, shape (1, n_features) or (n_classes, n_features)
Coefﬁcient of the features in the decision function.
coef_ is of shape (1, n_features) when the given problem is binary. coef_ is readonly
property derived from raw_coef_ that follows the internal memory layout of liblinear.
intercept_ : array, shape (1,) or (n_classes,)
Intercept (a.k.a. bias) added to the decision function. It is available only when parameter
intercept is set to True and is of shape(1,) when the problem is binary.
3.3. Model selection and evaluation
385
scikit-learn user guide, Release 0.18.2
Cs_ : array
Array of C i.e. inverse of regularization parameter values used for cross-validation.
coefs_paths_
:
array,
shape
(n_folds,len(Cs_),n_features)
or
(n_folds,len(Cs_),n_features + 1)
dict with classes as the keys,
and the path of coefﬁcients obtained during
cross-validating across each fold and then across each Cs after doing an OvR
for the corresponding class as values.
If the ‘multi_class’ option is set to
‘multinomial’, then the coefs_paths are the coefﬁcients corresponding to each
class.
Each dict value has shape (n_folds,len(Cs_),n_features) or
(n_folds,len(Cs_),n_features + 1) depending on whether the intercept
is ﬁt or not.
scores_ : dict
dict with classes as the keys, and the values as the grid of scores obtained during cross-
validating each fold, after doing an OvR for the corresponding class. If the ‘multi_class’
option given is ‘multinomial’ then the same scores are repeated across all classes, since
this is the multinomial class. Each dict value has shape (n_folds, len(Cs))
C_ : array, shape (n_classes,) or (n_classes - 1,)
Array of C that maps to the best scores across every class. If reﬁt is set to False, then
for each class, the best C is the average of the C’s that correspond to the best scores for
each fold.
n_iter_ : array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)
Actual number of iterations for all classes, folds and Cs. In the binary or multinomial
cases, the ﬁrst dimension is equal to 1.
See also:
LogisticRegression
Methods
decision_function(X)
Predict conﬁdence scores for samples.
densify()
Convert coefﬁcient matrix to dense array format.
fit(X, y[, sample_weight])
Fit the model according to the given training data.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class labels for samples in X.
predict_log_proba(X)
Log of probability estimates.
predict_proba(X)
Probability estimates.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
sparsify()
Convert coefﬁcient matrix to sparse format.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(Cs=10,
ﬁt_intercept=True,
cv=None,
dual=False,
penalty=’l2’,
scoring=None,
solver=’lbfgs’, tol=0.0001, max_iter=100, class_weight=None, n_jobs=1, verbose=0,
reﬁt=True, intercept_scaling=1.0, multi_class=’ovr’, random_state=None)
386
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
decision_function(X)
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
densify()
Convert coefﬁcient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is
required for ﬁtting, so calling this method is only required on models that have previously been sparsiﬁed;
otherwise, it is a no-op.
Returnsself: estimator :
fit(X, y, sample_weight=None)
Fit the model according to the given training data.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training vector, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples,)
Target vector relative to X.
sample_weight : array-like, shape (n_samples,) optional
Array of weights that are assigned to individual samples. If not provided, then each
sample is given unit weight.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
3.3. Model selection and evaluation
387
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class labels for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
predict_log_proba(X)
Log of probability estimates.
The returned estimates for all classes are ordered by the label of classes.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsT : array-like, shape = [n_samples, n_classes]
Returns the log-probability of the sample for each class in the model, where classes are
ordered as they are in self.classes_.
predict_proba(X)
Probability estimates.
The returned estimates for all classes are ordered by the label of classes.
For a multi_class problem, if multi_class is set to be “multinomial” the softmax function is used to ﬁnd
the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of
each class assuming it to be positive using the logistic function. and normalize these values across all the
classes.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsT : array-like, shape = [n_samples, n_classes]
Returns the probability of the sample for each class in the model, where classes are
ordered as they are in self.classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
388
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sparsify()
Convert coefﬁcient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more
memory- and storage-efﬁcient than the usual numpy.ndarray representation.
The intercept_ member is not converted.
Returnsself: estimator :
Notes
For non-sparse models, i.e. when there are not many zeros in coef_, this may actually increase memory
usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be
computed with (coef_ == 0).sum(), must be more than 50% for this to provide signiﬁcant beneﬁts.
After calling this method, further ﬁtting with the partial_ﬁt method (if any) will not work until you call
densify.
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
3.3. Model selection and evaluation
389
scikit-learn user guide, Release 0.18.2
sklearn.linear_model.MultiTaskElasticNetCV
class sklearn.linear_model.MultiTaskElasticNetCV(l1_ratio=0.5, eps=0.001, n_alphas=100,
alphas=None,
ﬁt_intercept=True,
normalize=False,
max_iter=1000,
tol=0.0001,
cv=None,
copy_X=True,
verbose=0,
n_jobs=1,
ran-
dom_state=None, selection=’cyclic’)
Multi-task L1/L2 ElasticNet with built-in cross-validation.
The optimization objective for MultiTaskElasticNet is:
(1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+ alpha * l1_ratio * ||W||_21
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
Parameterseps : ﬂoat, optional
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3.
alphas : array-like, optional
List of alphas where to compute the models. If not provided, set automatically.
n_alphas : int, optional
Number of alphas along the regularization path
l1_ratio : ﬂoat or array of ﬂoats
The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 0 the penalty
is an L1/L2 penalty. For l1_ratio = 1 it is an L1 penalty. For 0 < l1_ratio <
1, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in
which case the different values are tested by cross-validation and the one giving the
best prediction score is used. Note that a good choice of list of values for l1_ratio is
often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in
[.1,.5,.7,.9,.95,.99,1]
ﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is
ignored when fit_intercept is set to False. When the regressors are normalized,
note that this makes the hyperparameters learnt more robust and almost independent of
the number of samples. The same property is not valid for standardized data. However,
if you wish to standardize, please use preprocessing.StandardScaler before
calling fit on an estimator with normalize=False.
copy_X : boolean, optional, default True
390
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
If True, X will be copied; else, it may be overwritten.
max_iter : int, optional
The maximum number of iterations
tol : ﬂoat, optional
The tolerance for the optimization: if the updates are smaller than tol, the optimization
code checks the dual gap for optimality and continues until it is smaller than tol.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
verbose : bool or integer
Amount of verbosity.
n_jobs : integer, optional
Number of CPUs to use during the cross validation. If -1, use all the CPUs. Note that
this is used only if multiple values for l1_ratio are given.
selection : str, default ‘cyclic’
If set to ‘random’, a random coefﬁcient is updated every iteration rather than looping
over features sequentially by default. This (setting to ‘random’) often leads to signiﬁ-
cantly faster convergence especially when tol is higher than 1e-4.
random_state : int, RandomState instance, or None (default)
The seed of the pseudo random number generator that selects a random feature to up-
date. Useful only when selection is set to ‘random’.
Attributesintercept_ : array, shape (n_tasks,)
Independent term in decision function.
coef_ : array, shape (n_tasks, n_features)
Parameter vector (W in the cost function formula).
alpha_ : ﬂoat
The amount of penalization chosen by cross validation
mse_path_ : array, shape (n_alphas, n_folds) or (n_l1_ratio, n_alphas, n_folds)
mean square error for the test set on each fold, varying alpha
alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)
The grid of alphas used for ﬁtting, for each l1_ratio
l1_ratio_ : ﬂoat
best l1_ratio obtained by cross-validation.
3.3. Model selection and evaluation
391
scikit-learn user guide, Release 0.18.2
n_iter_ : int
number of iterations run by the coordinate descent solver to reach the speciﬁed tolerance
for the optimal alpha.
See also:
MultiTaskElasticNet, ElasticNetCV, MultiTaskLassoCV
Notes
The algorithm used to ﬁt the model is coordinate descent.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
Examples
>>> from sklearn import linear_model
>>> clf = linear_model.MultiTaskElasticNetCV()
>>> clf.fit([[0,0], [1, 1], [2, 2]],
...
[[0, 0], [1, 1], [2, 2]])
...
MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001,
fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,
n_jobs=1, normalize=False, random_state=None, selection='cyclic',
tol=0.0001, verbose=0)
>>> print(clf.coef_)
[[ 0.52875032
0.46958558]
[ 0.52875032
0.46958558]]
>>> print(clf.intercept_)
[ 0.00166409
0.00166409]
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y)
Fit linear model with coordinate descent
get_params([deep])
Get parameters for this estimator.
path(X, y[, l1_ratio, eps, n_alphas, ...])
Compute elastic net path with coordinate descent
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(l1_ratio=0.5,
eps=0.001,
n_alphas=100,
alphas=None,
ﬁt_intercept=True,
normal-
ize=False, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=1, ran-
dom_state=None, selection=’cyclic’)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
392
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit linear model with coordinate descent
Fit is on grid of alphas and best alpha estimated by cross-validation.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as ﬂoat64, Fortran-contiguous data to avoid unnecessary
memory duplication. If y is mono-output, X can be sparse.
y : array-like, shape (n_samples,) or (n_samples, n_targets)
Target values
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
static path(X, y,
l1_ratio=0.5,
eps=0.001,
n_alphas=100,
alphas=None,
precompute=’auto’,
Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, posi-
tive=False, check_input=True, **params)
Compute elastic net path with coordinate descent
The elastic net optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
1 / (2 * n_samples) * ||y - Xw||^2_2
+ alpha * l1_ratio * ||w||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2
For multi-output tasks it is:
(1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+ alpha * l1_ratio * ||W||_21
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory
duplication. If y is mono-output then X can be sparse.
3.3. Model selection and evaluation
393
scikit-learn user guide, Release 0.18.2
y : ndarray, shape (n_samples,) or (n_samples, n_outputs)
Target values
l1_ratio : ﬂoat, optional
ﬂoat between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties).
l1_ratio=1 corresponds to the Lasso
eps : ﬂoat
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3
n_alphas : int, optional
Number of alphas along the regularization path
alphas : ndarray, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
Xy : array-like, optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
coef_init : array, shape (n_features, ) | None
The initial values of the coefﬁcients.
verbose : bool or integer
Amount of verbosity.
params : kwargs
keyword arguments passed to the coordinate descent solver.
return_n_iter : bool
whether to return the number of iterations or not.
positive : bool, default False
If set to True, forces coefﬁcients to be positive.
check_input : bool, default True
Skip input validation checks, including the Gram matrix when provided assuming there
are handled by the caller when check_input=False.
Returnsalphas : array, shape (n_alphas,)
The alphas along the path where models are computed.
coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
Coefﬁcients along the path.
dual_gaps : array, shape (n_alphas,)
394
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The dual gaps at the end of the optimization for each alpha.
n_iters : array-like, shape (n_alphas,)
The number of iterations taken by the coordinate descent optimizer to reach the speciﬁed
tolerance for each alpha. (Is returned when return_n_iter is set to True).
See also:
MultiTaskElasticNet, MultiTaskElasticNetCV, ElasticNet, ElasticNetCV
Notes
See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
3.3. Model selection and evaluation
395
scikit-learn user guide, Release 0.18.2
sklearn.linear_model.MultiTaskLassoCV
class sklearn.linear_model.MultiTaskLassoCV(eps=0.001,
n_alphas=100,
alphas=None,
ﬁt_intercept=True,
normalize=False,
max_iter=1000,
tol=0.0001,
copy_X=True,
cv=None,
verbose=False,
n_jobs=1,
ran-
dom_state=None, selection=’cyclic’)
Multi-task L1/L2 Lasso with built-in cross-validation.
The optimization objective for MultiTaskLasso is:
(1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
Parameterseps : ﬂoat, optional
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3.
alphas : array-like, optional
List of alphas where to compute the models. If not provided, set automatically.
n_alphas : int, optional
Number of alphas along the regularization path
ﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is
ignored when fit_intercept is set to False. When the regressors are normalized,
note that this makes the hyperparameters learnt more robust and almost independent of
the number of samples. The same property is not valid for standardized data. However,
if you wish to standardize, please use preprocessing.StandardScaler before
calling fit on an estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
max_iter : int, optional
The maximum number of iterations.
tol : ﬂoat, optional
The tolerance for the optimization: if the updates are smaller than tol, the optimization
code checks the dual gap for optimality and continues until it is smaller than tol.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
396
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
verbose : bool or integer
Amount of verbosity.
n_jobs : integer, optional
Number of CPUs to use during the cross validation. If -1, use all the CPUs. Note that
this is used only if multiple values for l1_ratio are given.
selection : str, default ‘cyclic’
If set to ‘random’, a random coefﬁcient is updated every iteration rather than looping
over features sequentially by default. This (setting to ‘random’) often leads to signiﬁ-
cantly faster convergence especially when tol is higher than 1e-4.
random_state : int, RandomState instance, or None (default)
The seed of the pseudo random number generator that selects a random feature to up-
date. Useful only when selection is set to ‘random’.
Attributesintercept_ : array, shape (n_tasks,)
Independent term in decision function.
coef_ : array, shape (n_tasks, n_features)
Parameter vector (W in the cost function formula).
alpha_ : ﬂoat
The amount of penalization chosen by cross validation
mse_path_ : array, shape (n_alphas, n_folds)
mean square error for the test set on each fold, varying alpha
alphas_ : numpy array, shape (n_alphas,)
The grid of alphas used for ﬁtting.
n_iter_ : int
number of iterations run by the coordinate descent solver to reach the speciﬁed tolerance
for the optimal alpha.
See also:
MultiTaskElasticNet, ElasticNetCV, MultiTaskElasticNetCV
Notes
The algorithm used to ﬁt the model is coordinate descent.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
3.3. Model selection and evaluation
397
scikit-learn user guide, Release 0.18.2
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y)
Fit linear model with coordinate descent
get_params([deep])
Get parameters for this estimator.
path(X, y[, eps, n_alphas, alphas, ...])
Compute Lasso path with coordinate descent
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(eps=0.001,
n_alphas=100,
alphas=None,
ﬁt_intercept=True,
normalize=False,
max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, ran-
dom_state=None, selection=’cyclic’)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit linear model with coordinate descent
Fit is on grid of alphas and best alpha estimated by cross-validation.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as ﬂoat64, Fortran-contiguous data to avoid unnecessary
memory duplication. If y is mono-output, X can be sparse.
y : array-like, shape (n_samples,) or (n_samples, n_targets)
Target values
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
static path(X,
y,
eps=0.001,
n_alphas=100,
alphas=None,
precompute=’auto’,
Xy=None,
copy_X=True,
coef_init=None,
verbose=False,
return_n_iter=False,
positive=False,
**params)
Compute Lasso path with coordinate descent
The Lasso optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
398
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
For multi-output tasks it is:
(1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory
duplication. If y is mono-output then X can be sparse.
y : ndarray, shape (n_samples,), or (n_samples, n_outputs)
Target values
eps : ﬂoat, optional
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3
n_alphas : int, optional
Number of alphas along the regularization path
alphas : ndarray, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
Xy : array-like, optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
coef_init : array, shape (n_features, ) | None
The initial values of the coefﬁcients.
verbose : bool or integer
Amount of verbosity.
params : kwargs
keyword arguments passed to the coordinate descent solver.
positive : bool, default False
If set to True, forces coefﬁcients to be positive.
return_n_iter : bool
3.3. Model selection and evaluation
399
scikit-learn user guide, Release 0.18.2
whether to return the number of iterations or not.
Returnsalphas : array, shape (n_alphas,)
The alphas along the path where models are computed.
coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
Coefﬁcients along the path.
dual_gaps : array, shape (n_alphas,)
The dual gaps at the end of the optimization for each alpha.
n_iters : array-like, shape (n_alphas,)
The number of iterations taken by the coordinate descent optimizer to reach the speciﬁed
tolerance for each alpha.
See also:
lars_path,
Lasso,
LassoLars,
LassoCV,
LassoLarsCV,
sklearn.decomposition.sparse_encode
Notes
See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
Note that in certain cases, the Lars solver may be signiﬁcantly faster to implement this functionality. In
particular, linear interpolation can be used to retrieve model coefﬁcients between the values output by
lars_path
Examples
Comparing lasso_path and lars_path with interpolation:
>>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T
>>> y = np.array([1, 2, 3.1])
>>> # Use lasso_path to compute a coefficient path
>>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])
>>> print(coef_path)
[[ 0.
0.
0.46874778]
[ 0.2159048
0.4425765
0.23689075]]
>>> # Now use lars_path and 1D linear interpolation to compute the
>>> # same path
>>> from sklearn.linear_model import lars_path
>>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')
>>> from scipy import interpolate
>>> coef_path_continuous = interpolate.interp1d(alphas[::-1],
...
coef_path_lars[:, ::-1])
>>> print(coef_path_continuous([5., 1., .5]))
[[ 0.
0.
0.46915237]
[ 0.2159048
0.4425765
0.23668876]]
400
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sklearn.linear_model.OrthogonalMatchingPursuitCV
class sklearn.linear_model.OrthogonalMatchingPursuitCV(copy=True, ﬁt_intercept=True,
normalize=True,
max_iter=None,
cv=None,
n_jobs=1, verbose=False)
Cross-validated Orthogonal Matching Pursuit model (OMP)
Parameterscopy : bool, optional
Whether the design matrix X must be copied by the algorithm. A false value is only
helpful if X is already Fortran-ordered, otherwise a copy is made anyway.
ﬁt_intercept : boolean, optional
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
3.3. Model selection and evaluation
401
scikit-learn user guide, Release 0.18.2
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
max_iter : integer, optional
Maximum numbers of iterations to perform, therefore maximum features to include.
10% of n_features but at least 5 if available.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
n_jobs : integer, optional
Number of CPUs to use during the cross validation. If -1, use all the CPUs
verbose : boolean or integer, optional
Sets the verbosity amount
Read more in the :ref:‘User Guide <omp>‘. :
Attributesintercept_ : ﬂoat or array, shape (n_targets,)
Independent term in decision function.
coef_ : array, shape (n_features,) or (n_features, n_targets)
Parameter vector (w in the problem formulation).
n_nonzero_coefs_ : int
Estimated number of non-zero coefﬁcients giving the best mean squared error over the
cross-validation folds.
n_iter_ : int or array-like
Number of active features across every target for the model reﬁt with the best hyperpa-
rameters got by cross-validating across all folds.
See also:
orthogonal_mp,
orthogonal_mp_gram,
lars_path,
Lars,
LassoLars,
OrthogonalMatchingPursuit, LarsCV, LassoLarsCV, decomposition.sparse_encode
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
Continued on next page
402
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Table 3.10 – continued from previous page
fit(X, y)
Fit the model using X, y as training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(copy=True, ﬁt_intercept=True, normalize=True, max_iter=None, cv=None, n_jobs=1, ver-
bose=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit the model using X, y as training data.
ParametersX : array-like, shape [n_samples, n_features]
Training data.
y : array-like, shape [n_samples]
Target values.
Returnsself : object
returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
3.3. Model selection and evaluation
403
scikit-learn user guide, Release 0.18.2
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.OrthogonalMatchingPursuitCV
• Orthogonal Matching Pursuit
sklearn.linear_model.RidgeCV
class sklearn.linear_model.RidgeCV(alphas=(0.1,
1.0,
10.0),
ﬁt_intercept=True,
normal-
ize=False,
scoring=None,
cv=None,
gcv_mode=None,
store_cv_values=False)
Ridge regression with built-in cross-validation.
By default, it performs Generalized Cross-Validation, which is a form of efﬁcient Leave-One-Out cross-
validation.
Read more in the User Guide.
Parametersalphas : numpy array of shape [n_alphas]
Array of alpha values to try. Regularization strength; must be a positive ﬂoat. Reg-
ularization improves the conditioning of the problem and reduces the variance of the
estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in
other linear models such as LogisticRegression or LinearSVC.
ﬁt_intercept : boolean
Whether to calculate the intercept for this model. If set to false, no intercept will be
used in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
404
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
scoring : string, callable or None, optional, default: None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the efﬁcient Leave-One-Out cross-validation
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For
integer/None
inputs,
if
y
is
binary
or
multiclass,
sklearn.model_selection.StratifiedKFold
is
used,
else,
sklearn.model_selection.KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
gcv_mode : {None, ‘auto’, ‘svd’, eigen’}, optional
Flag indicating which strategy to use when performing Generalized Cross-Validation.
Options are:
'auto' : use svd if n_samples > n_features or when X is a sparse
matrix, otherwise use eigen
'svd' : force computation via singular value decomposition of X
(does not work for sparse matrices)
'eigen' : force computation via eigendecomposition of X^T X
The ‘auto’ mode is the default and is intended to pick the cheaper option of the two
depending upon the shape and format of the training data.
store_cv_values : boolean, default=False
Flag indicating if the cross-validation values corresponding to each alpha should be
stored in the cv_values_ attribute (see below).
This ﬂag is only compatible with
cv=None (i.e. using Generalized Cross-Validation).
Attributescv_values_ : array, shape = [n_samples, n_alphas] or shape = [n_samples, n_targets,
n_alphas], optional
Cross-validation values for each alpha (if store_cv_values=True and cv=None). After
ﬁt() has been called, this attribute will contain the mean squared errors (by default) or
the values of the {loss,score}_func function (if provided in the constructor).
coef_ : array, shape = [n_features] or [n_targets, n_features]
Weight vector(s).
intercept_ : ﬂoat | array, shape = (n_targets,)
Independent term in decision function. Set to 0.0 if fit_intercept = False.
3.3. Model selection and evaluation
405
scikit-learn user guide, Release 0.18.2
alpha_ : ﬂoat
Estimated regularization parameter.
See also:
RidgeRidge regression
RidgeClassifierRidge classiﬁer
RidgeClassifierCVRidge classiﬁer with built-in cross validation
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y[, sample_weight])
Fit Ridge regression model
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alphas=(0.1, 1.0, 10.0), ﬁt_intercept=True, normalize=False, scoring=None, cv=None,
gcv_mode=None, store_cv_values=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y, sample_weight=None)
Fit Ridge regression model
ParametersX : array-like, shape = [n_samples, n_features]
Training data
y : array-like, shape = [n_samples] or [n_samples, n_targets]
Target values
sample_weight : ﬂoat or array-like of shape [n_samples]
Sample weight
Returnsself : Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
406
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.RidgeCV
• Face completion with a multi-output estimators
sklearn.linear_model.RidgeClassiﬁerCV
class sklearn.linear_model.RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), ﬁt_intercept=True,
normalize=False,
scoring=None,
cv=None,
class_weight=None)
Ridge classiﬁer with built-in cross-validation.
By default, it performs Generalized Cross-Validation, which is a form of efﬁcient Leave-One-Out cross-
validation. Currently, only the n_features > n_samples case is handled efﬁciently.
Read more in the User Guide.
3.3. Model selection and evaluation
407
scikit-learn user guide, Release 0.18.2
Parametersalphas : numpy array of shape [n_alphas]
Array of alpha values to try. Regularization strength; must be a positive ﬂoat. Reg-
ularization improves the conditioning of the problem and reduces the variance of the
estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in
other linear models such as LogisticRegression or LinearSVC.
ﬁt_intercept : boolean
Whether to calculate the intercept for this model. If set to false, no intercept will be
used in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
scoring : string, callable or None, optional, default: None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the efﬁcient Leave-One-Out cross-validation
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
Refer User Guide for the various cross-validation strategies that can be used here.
class_weight : dict or ‘balanced’, optional
Weights associated with classes in the form {class_label:
weight}. If not
given, all classes are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y))
Attributescv_values_ : array, shape = [n_samples, n_alphas] or shape = [n_samples, n_responses,
n_alphas], optional
Cross-validation values for each alpha (if store_cv_values=True and
‘cv=None‘). After ‘ﬁt()‘ has been called, this attribute will contain the mean squared errors
(by default) or the values of the ‘{loss,score}_func‘ function (if provided in the constructor).
:
coef_ : array, shape = [n_features] or [n_targets, n_features]
Weight vector(s).
intercept_ : ﬂoat | array, shape = (n_targets,)
Independent term in decision function. Set to 0.0 if fit_intercept = False.
408
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
alpha_ : ﬂoat
Estimated regularization parameter
See also:
RidgeRidge regression
RidgeClassifierRidge classiﬁer
RidgeCVRidge regression with built-in cross validation
Notes
For multi-class classiﬁcation, n_class classiﬁers are trained in a one-versus-all approach. Concretely, this is
implemented by taking advantage of the multi-variate response support in Ridge.
Methods
decision_function(X)
Predict conﬁdence scores for samples.
fit(X, y[, sample_weight])
Fit the ridge classiﬁer.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class labels for samples in X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alphas=(0.1, 1.0, 10.0), ﬁt_intercept=True, normalize=False, scoring=None, cv=None,
class_weight=None)
decision_function(X)
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
fit(X, y, sample_weight=None)
Fit the ridge classiﬁer.
ParametersX : array-like, shape (n_samples, n_features)
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape (n_samples,)
Target values.
sample_weight : ﬂoat or numpy array of shape (n_samples,)
Sample weight.
3.3. Model selection and evaluation
409
scikit-learn user guide, Release 0.18.2
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class labels for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Information Criterion
Some models can offer an information-theoretic closed-form formula of the optimal estimate of the regularization
parameter by computing a single regularization path (instead of several when using cross-validation).
Here is the list of models beneﬁtting from the Aikike Information Criterion (AIC) or the Bayesian Information Crite-
rion (BIC) for automated model selection:
410
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
linear_model.LassoLarsIC([criterion, ...])
Lasso model ﬁt with Lars using BIC or AIC for model se-
lection
sklearn.linear_model.LassoLarsIC
class sklearn.linear_model.LassoLarsIC(criterion=’aic’,
ﬁt_intercept=True,
verbose=False,
normalize=True,
precompute=’auto’,
max_iter=500,
eps=2.2204460492503131e-16,
copy_X=True,
posi-
tive=False)
Lasso model ﬁt with Lars using BIC or AIC for model selection
The optimization objective for Lasso is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful
to select the value of the regularization parameter by making a trade-off between the goodness of ﬁt and the
complexity of the model. A good model should explain well the data while being simple.
Read more in the User Guide.
Parameterscriterion : ‘bic’ | ‘aic’
The type of criterion to use.
ﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
positive : boolean (default=False)
Restrict coefﬁcients to be >= 0. Be aware that you might want to remove ﬁt_intercept
which is set True by default. Under the positive restriction the model coefﬁcients do
not converge to the ordinary-least-squares solution for small values of alpha. Only co-
efﬁcients up to the smallest alpha value (alphas_[alphas_ > 0.].min() when
ﬁt_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congru-
ence with the solution of the coordinate descent Lasso estimator. As a consequence
using LassoLarsIC only makes sense for problems where a sparse solution is expected
and/or reached.
verbose : boolean or integer, optional
Sets the verbosity amount
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
precompute : True | False | ‘auto’ | array-like
3.3. Model selection and evaluation
411
scikit-learn user guide, Release 0.18.2
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
max_iter : integer, optional
Maximum number of iterations to perform. Can be used for early stopping.
eps : ﬂoat, optional
The machine-precision regularization in the computation of the Cholesky diagonal fac-
tors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some
iterative optimization-based algorithms, this parameter does not control the tolerance of
the optimization.
Attributescoef_ : array, shape (n_features,)
parameter vector (w in the formulation formula)
intercept_ : ﬂoat
independent term in decision function.
alpha_ : ﬂoat
the alpha parameter chosen by the information criterion
n_iter_ : int
number of iterations run by lars_path to ﬁnd the grid of alphas.
criterion_ : array, shape (n_alphas,)
The value of the information criteria (‘aic’, ‘bic’) across all alphas. The alpha which
has the smallest information criteria is chosen.
See also:
lars_path, LassoLars, LassoLarsCV
Notes
The estimation of the number of degrees of freedom is given by:
“On the degrees of freedom of the lasso” Hui Zou, Trevor Hastie, and Robert Tibshirani Ann. Statist. Volume
35, Number 5 (2007), 2173-2192.
https://en.wikipedia.org/wiki/Akaike_information_criterion
https://en.wikipedia.org/wiki/Bayesian_
information_criterion
Examples
>>> from sklearn import linear_model
>>> clf = linear_model.LassoLarsIC(criterion='bic')
>>> clf.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])
...
LassoLarsIC(copy_X=True, criterion='bic', eps=..., fit_intercept=True,
max_iter=500, normalize=True, positive=False, precompute='auto',
verbose=False)
>>> print(clf.coef_)
[ 0.
-1.11...]
412
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y[, copy_X])
Fit the model using X, y as training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(criterion=’aic’, ﬁt_intercept=True, verbose=False, normalize=True, precompute=’auto’,
max_iter=500, eps=2.2204460492503131e-16, copy_X=True, positive=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y, copy_X=True)
Fit the model using X, y as training data.
ParametersX : array-like, shape (n_samples, n_features)
training data.
y : array-like, shape (n_samples,)
target values.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
Returnsself : object
returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
3.3. Model selection and evaluation
413
scikit-learn user guide, Release 0.18.2
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.LassoLarsIC
• Lasso model selection: Cross-Validation / AIC / BIC
Out of Bag Estimates
When using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement,
part of the training set remains unused. For each classiﬁer in the ensemble, a different part of the training set is left
out.
This left out portion can be used to estimate the generalization error without having to rely on a separate validation
set. This estimate comes “for free” as no additional data is needed and can be used for model selection.
This is currently implemented in the following classes:
ensemble.RandomForestClassifier([...])
A random forest classiﬁer.
ensemble.RandomForestRegressor([...])
A random forest regressor.
ensemble.ExtraTreesClassifier([...])
An extra-trees classiﬁer.
ensemble.ExtraTreesRegressor([n_estimators,
...])
An extra-trees regressor.
ensemble.GradientBoostingClassifier([loss,
...])
Gradient Boosting for classiﬁcation.
Continued on next page
414
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Table 3.15 – continued from previous page
ensemble.GradientBoostingRegressor([loss,
...])
Gradient Boosting for regression.
sklearn.ensemble.RandomForestClassiﬁer
class sklearn.ensemble.RandomForestClassifier(n_estimators=10,
crite-
rion=’gini’,
max_depth=None,
min_samples_split=2, min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
max_leaf_nodes=None,
min_impurity_split=1e-07,
boot-
strap=True,
oob_score=False,
n_jobs=1,
random_state=None,
verbose=0,
warm_start=False, class_weight=None)
A random forest classiﬁer.
A random forest is a meta estimator that ﬁts a number of decision tree classiﬁers on various sub-samples of the
dataset and use averaging to improve the predictive accuracy and control over-ﬁtting. The sub-sample size is
always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True
(default).
Read more in the User Guide.
Parametersn_estimators : integer, optional (default=10)
The number of trees in the forest.
criterion : string, optional (default=”gini”)
The function to measure the quality of a split. Supported criteria are “gini” for the Gini
impurity and “entropy” for the information gain. Note: this parameter is tree-speciﬁc.
max_features : int, ﬂoat, string or None, optional (default=”auto”)
The number of features to consider when looking for the best split:
•If int, then consider max_features features at each split.
•If ﬂoat, then max_features is a percentage and int(max_features * n_features) features
are considered at each split.
•If “auto”, then max_features=sqrt(n_features).
•If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).
•If “log2”, then max_features=log2(n_features).
•If None, then max_features=n_features.
Note: the search for a split does not stop until at least one valid partition of the node
samples is found, even if it requires to effectively inspect more than max_features
features.
max_depth : integer or None, optional (default=None)
The maximum depth of the tree. If None, then nodes are expanded until all leaves are
pure or until all leaves contain less than min_samples_split samples.
min_samples_split : int, ﬂoat, optional (default=2)
3.3. Model selection and evaluation
415
scikit-learn user guide, Release 0.18.2
The minimum number of samples required to split an internal node:
•If int, then consider min_samples_split as the minimum number.
•If ﬂoat, then min_samples_split is a percentage and ceil(min_samples_split *
n_samples) are the minimum number of samples for each split.
Changed in version 0.18: Added ﬂoat values for percentages.
min_samples_leaf : int, ﬂoat, optional (default=1)
The minimum number of samples required to be at a leaf node:
•If int, then consider min_samples_leaf as the minimum number.
•If ﬂoat, then min_samples_leaf is a percentage and ceil(min_samples_leaf *
n_samples) are the minimum number of samples for each node.
Changed in version 0.18: Added ﬂoat values for percentages.
min_weight_fraction_leaf : ﬂoat, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples)
required to be at a leaf node. Samples have equal weight when sample_weight is not
provided.
max_leaf_nodes : int or None, optional (default=None)
Grow trees with max_leaf_nodes in best-ﬁrst fashion. Best nodes are deﬁned as
relative reduction in impurity. If None then unlimited number of leaf nodes.
min_impurity_split : ﬂoat, optional (default=1e-7)
Threshold for early stopping in tree growth. A node will split if its impurity is above
the threshold, otherwise it is a leaf.
New in version 0.18.
bootstrap : boolean, optional (default=True)
Whether bootstrap samples are used when building trees.
oob_score : bool (default=False)
Whether to use out-of-bag samples to estimate the generalization accuracy.
n_jobs : integer, optional (default=1)
The number of jobs to run in parallel for both ﬁt and predict. If -1, then the number of
jobs is set to the number of cores.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
verbose : int, optional (default=0)
Controls the verbosity of the tree building process.
warm_start : bool, optional (default=False)
When set to True, reuse the solution of the previous call to ﬁt and add more estimators
to the ensemble, otherwise, just ﬁt a whole new forest.
class_weight : dict, list of dicts, “balanced”,
416
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
“balanced_subsample” or None, optional (default=None) Weights associated with
classes in the form {class_label:
weight}. If not given, all classes are sup-
posed to have weight one. For multi-output problems, a list of dicts can be provided in
the same order as the columns of y.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y))
The “balanced_subsample” mode is the same as “balanced” except that weights are
computed based on the bootstrap sample for every tree grown.
For multi-output, the weights of each column of y will be multiplied.
Note that these weights will be multiplied with sample_weight (passed through the ﬁt
method) if sample_weight is speciﬁed.
Attributesestimators_ : list of DecisionTreeClassiﬁer
The collection of ﬁtted sub-estimators.
classes_ : array of shape = [n_classes] or a list of such arrays
The classes labels (single output problem), or a list of arrays of class labels (multi-output
problem).
n_classes_ : int or list
The number of classes (single output problem), or a list containing the number of classes
for each output (multi-output problem).
n_features_ : int
The number of features when fit is performed.
n_outputs_ : int
The number of outputs when fit is performed.
feature_importances_ : array of shape = [n_features]
The feature importances (the higher, the more important the feature).
oob_score_ : ﬂoat
Score of the training dataset obtained using an out-of-bag estimate.
oob_decision_function_ : array of shape = [n_samples, n_classes]
Decision function computed with out-of-bag estimate on the training set.
If
n_estimators is small it might be possible that a data point was never left out during
the bootstrap. In this case, oob_decision_function_ might contain NaN.
See also:
DecisionTreeClassifier, ExtraTreesClassifier
References
[R23]
Methods
3.3. Model selection and evaluation
417
scikit-learn user guide, Release 0.18.2
apply(X)
Apply trees in the forest to X, return leaf indices.
decision_path(X)
Return the decision path in the forest
fit(X, y[, sample_weight])
Build a forest of trees from the training set (X, y).
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class for X.
predict_log_proba(X)
Predict class log-probabilities for X.
predict_proba(X)
Predict class probabilities for X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(n_estimators=10,
criterion=’gini’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
max_leaf_nodes=None,
min_impurity_split=1e-07,
bootstrap=True,
oob_score=False,
n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)
apply(X)
Apply trees in the forest to X, return leaf indices.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
ReturnsX_leaves : array_like, shape = [n_samples, n_estimators]
For each datapoint x in X and for each tree in the forest, return the index of the leaf x
ends up in.
decision_path(X)
Return the decision path in the forest
New in version 0.18.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsindicator : sparse csr array, shape = [n_samples, n_nodes]
Return a node indicator matrix where non zero elements indicates that the samples goes
through the nodes.
n_nodes_ptr : array of size (n_estimators + 1, )
The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value
for the i-th estimator.
feature_importances_
Return the feature importances (the higher, the more important thefeature).
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None)
Build a forest of trees from the training set (X, y).
418
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The
training
input
samples.
Internally,
its
dtype
will
be
converted
to
dtype=np.float32. If a sparse matrix is provided, it will be converted into a sparse
csc_matrix.
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
The target values (class labels in classiﬁcation, real numbers in regression).
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node. In the case of classiﬁcation, splits are also ignored if they would result in
any single class carrying a negative weight in either child node.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class for X.
The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability
estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsy : array of shape = [n_samples] or [n_samples, n_outputs]
The predicted classes.
3.3. Model selection and evaluation
419
scikit-learn user guide, Release 0.18.2
predict_log_proba(X)
Predict class log-probabilities for X.
The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class
probabilities of the trees in the forest.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsp : array of shape = [n_samples, n_classes], or a list of n_outputs
such arrays if n_outputs > 1. The class probabilities of the input samples. The order of
the classes corresponds to that in the attribute classes_.
predict_proba(X)
Predict class probabilities for X.
The predicted class probabilities of an input sample are computed as the mean predicted class probabilities
of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class
in a leaf.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsp : array of shape = [n_samples, n_classes], or a list of n_outputs
such arrays if n_outputs > 1. The class probabilities of the input samples. The order of
the classes corresponds to that in the attribute classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
420
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.ensemble.RandomForestClassifier
• Probability Calibration for 3-class classiﬁcation
• Comparison of Calibration of Classiﬁers
• Classiﬁer comparison
• OOB Errors for Random Forests
• Feature transformations with ensembles of trees
• Plot the decision surfaces of ensembles of trees on the iris dataset
• Plot class probabilities calculated by the VotingClassiﬁer
• Comparing randomized search and grid search for hyperparameter estimation
• Classiﬁcation of text documents using sparse features
sklearn.ensemble.RandomForestRegressor
class sklearn.ensemble.RandomForestRegressor(n_estimators=10,
crite-
rion=’mse’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’, max_leaf_nodes=None,
min_impurity_split=1e-07,
boot-
strap=True,
oob_score=False,
n_jobs=1,
random_state=None,
verbose=0,
warm_start=False)
A random forest regressor.
A random forest is a meta estimator that ﬁts a number of classifying decision trees on various sub-samples of
the dataset and use averaging to improve the predictive accuracy and control over-ﬁtting. The sub-sample size is
3.3. Model selection and evaluation
421
scikit-learn user guide, Release 0.18.2
always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True
(default).
Read more in the User Guide.
Parametersn_estimators : integer, optional (default=10)
The number of trees in the forest.
criterion : string, optional (default=”mse”)
The function to measure the quality of a split. Supported criteria are “mse” for the
mean squared error, which is equal to variance reduction as feature selection criterion,
and “mae” for the mean absolute error.
New in version 0.18: Mean Absolute Error (MAE) criterion.
max_features : int, ﬂoat, string or None, optional (default=”auto”)
The number of features to consider when looking for the best split:
•If int, then consider max_features features at each split.
•If ﬂoat, then max_features is a percentage and int(max_features * n_features) features
are considered at each split.
•If “auto”, then max_features=n_features.
•If “sqrt”, then max_features=sqrt(n_features).
•If “log2”, then max_features=log2(n_features).
•If None, then max_features=n_features.
Note: the search for a split does not stop until at least one valid partition of the node
samples is found, even if it requires to effectively inspect more than max_features
features.
max_depth : integer or None, optional (default=None)
The maximum depth of the tree. If None, then nodes are expanded until all leaves are
pure or until all leaves contain less than min_samples_split samples.
min_samples_split : int, ﬂoat, optional (default=2)
The minimum number of samples required to split an internal node:
•If int, then consider min_samples_split as the minimum number.
•If ﬂoat, then min_samples_split is a percentage and ceil(min_samples_split *
n_samples) are the minimum number of samples for each split.
Changed in version 0.18: Added ﬂoat values for percentages.
min_samples_leaf : int, ﬂoat, optional (default=1)
The minimum number of samples required to be at a leaf node:
•If int, then consider min_samples_leaf as the minimum number.
•If ﬂoat, then min_samples_leaf is a percentage and ceil(min_samples_leaf *
n_samples) are the minimum number of samples for each node.
Changed in version 0.18: Added ﬂoat values for percentages.
min_weight_fraction_leaf : ﬂoat, optional (default=0.)
422
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The minimum weighted fraction of the sum total of weights (of all the input samples)
required to be at a leaf node. Samples have equal weight when sample_weight is not
provided.
max_leaf_nodes : int or None, optional (default=None)
Grow trees with max_leaf_nodes in best-ﬁrst fashion. Best nodes are deﬁned as
relative reduction in impurity. If None then unlimited number of leaf nodes.
min_impurity_split : ﬂoat, optional (default=1e-7)
Threshold for early stopping in tree growth. A node will split if its impurity is above
the threshold, otherwise it is a leaf.
New in version 0.18.
bootstrap : boolean, optional (default=True)
Whether bootstrap samples are used when building trees.
oob_score : bool, optional (default=False)
whether to use out-of-bag samples to estimate the R^2 on unseen data.
n_jobs : integer, optional (default=1)
The number of jobs to run in parallel for both ﬁt and predict. If -1, then the number of
jobs is set to the number of cores.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
verbose : int, optional (default=0)
Controls the verbosity of the tree building process.
warm_start : bool, optional (default=False)
When set to True, reuse the solution of the previous call to ﬁt and add more estimators
to the ensemble, otherwise, just ﬁt a whole new forest.
Attributesestimators_ : list of DecisionTreeRegressor
The collection of ﬁtted sub-estimators.
feature_importances_ : array of shape = [n_features]
The feature importances (the higher, the more important the feature).
n_features_ : int
The number of features when fit is performed.
n_outputs_ : int
The number of outputs when fit is performed.
oob_score_ : ﬂoat
Score of the training dataset obtained using an out-of-bag estimate.
oob_prediction_ : array of shape = [n_samples]
Prediction computed with out-of-bag estimate on the training set.
3.3. Model selection and evaluation
423
scikit-learn user guide, Release 0.18.2
See also:
DecisionTreeRegressor, ExtraTreesRegressor
References
[R24]
Methods
apply(X)
Apply trees in the forest to X, return leaf indices.
decision_path(X)
Return the decision path in the forest
fit(X, y[, sample_weight])
Build a forest of trees from the training set (X, y).
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict regression target for X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(n_estimators=10,
criterion=’mse’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
max_leaf_nodes=None,
min_impurity_split=1e-07,
bootstrap=True,
oob_score=False,
n_jobs=1, random_state=None, verbose=0, warm_start=False)
apply(X)
Apply trees in the forest to X, return leaf indices.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
ReturnsX_leaves : array_like, shape = [n_samples, n_estimators]
For each datapoint x in X and for each tree in the forest, return the index of the leaf x
ends up in.
decision_path(X)
Return the decision path in the forest
New in version 0.18.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsindicator : sparse csr array, shape = [n_samples, n_nodes]
Return a node indicator matrix where non zero elements indicates that the samples goes
through the nodes.
n_nodes_ptr : array of size (n_estimators + 1, )
424
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value
for the i-th estimator.
feature_importances_
Return the feature importances (the higher, the more important thefeature).
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None)
Build a forest of trees from the training set (X, y).
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The
training
input
samples.
Internally,
its
dtype
will
be
converted
to
dtype=np.float32. If a sparse matrix is provided, it will be converted into a sparse
csc_matrix.
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
The target values (class labels in classiﬁcation, real numbers in regression).
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node. In the case of classiﬁcation, splits are also ignored if they would result in
any single class carrying a negative weight in either child node.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict regression target for X.
3.3. Model selection and evaluation
425
scikit-learn user guide, Release 0.18.2
The predicted regression target of an input sample is computed as the mean predicted regression targets of
the trees in the forest.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsy : array of shape = [n_samples] or [n_samples, n_outputs]
The predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
426
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.ensemble.RandomForestRegressor
• Imputing missing values before building an estimator
• Prediction Latency
• Comparing random forests and the multi-output meta estimator
sklearn.ensemble.ExtraTreesClassiﬁer
class sklearn.ensemble.ExtraTreesClassifier(n_estimators=10,
crite-
rion=’gini’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
max_leaf_nodes=None,
min_impurity_split=1e-07,
boot-
strap=False,
oob_score=False,
n_jobs=1,
random_state=None,
verbose=0,
warm_start=False, class_weight=None)
An extra-trees classiﬁer.
This class implements a meta estimator that ﬁts a number of randomized decision trees (a.k.a. extra-trees) on
various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-ﬁtting.
Read more in the User Guide.
Parametersn_estimators : integer, optional (default=10)
The number of trees in the forest.
criterion : string, optional (default=”gini”)
The function to measure the quality of a split. Supported criteria are “gini” for the Gini
impurity and “entropy” for the information gain.
max_features : int, ﬂoat, string or None, optional (default=”auto”)
The number of features to consider when looking for the best split:
•If int, then consider max_features features at each split.
•If ﬂoat, then max_features is a percentage and int(max_features * n_features) features
are considered at each split.
•If “auto”, then max_features=sqrt(n_features).
•If “sqrt”, then max_features=sqrt(n_features).
•If “log2”, then max_features=log2(n_features).
•If None, then max_features=n_features.
Note: the search for a split does not stop until at least one valid partition of the node
samples is found, even if it requires to effectively inspect more than max_features
features.
max_depth : integer or None, optional (default=None)
3.3. Model selection and evaluation
427
scikit-learn user guide, Release 0.18.2
The maximum depth of the tree. If None, then nodes are expanded until all leaves are
pure or until all leaves contain less than min_samples_split samples.
min_samples_split : int, ﬂoat, optional (default=2)
The minimum number of samples required to split an internal node:
•If int, then consider min_samples_split as the minimum number.
•If ﬂoat, then min_samples_split is a percentage and ceil(min_samples_split *
n_samples) are the minimum number of samples for each split.
Changed in version 0.18: Added ﬂoat values for percentages.
min_samples_leaf : int, ﬂoat, optional (default=1)
The minimum number of samples required to be at a leaf node:
•If int, then consider min_samples_leaf as the minimum number.
•If ﬂoat, then min_samples_leaf is a percentage and ceil(min_samples_leaf *
n_samples) are the minimum number of samples for each node.
Changed in version 0.18: Added ﬂoat values for percentages.
min_weight_fraction_leaf : ﬂoat, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples)
required to be at a leaf node. Samples have equal weight when sample_weight is not
provided.
max_leaf_nodes : int or None, optional (default=None)
Grow trees with max_leaf_nodes in best-ﬁrst fashion. Best nodes are deﬁned as
relative reduction in impurity. If None then unlimited number of leaf nodes.
min_impurity_split : ﬂoat, optional (default=1e-7)
Threshold for early stopping in tree growth. A node will split if its impurity is above
the threshold, otherwise it is a leaf.
New in version 0.18.
bootstrap : boolean, optional (default=False)
Whether bootstrap samples are used when building trees.
oob_score : bool, optional (default=False)
Whether to use out-of-bag samples to estimate the generalization accuracy.
n_jobs : integer, optional (default=1)
The number of jobs to run in parallel for both ﬁt and predict. If -1, then the number of
jobs is set to the number of cores.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
verbose : int, optional (default=0)
Controls the verbosity of the tree building process.
warm_start : bool, optional (default=False)
428
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
When set to True, reuse the solution of the previous call to ﬁt and add more estimators
to the ensemble, otherwise, just ﬁt a whole new forest.
class_weight : dict, list of dicts, “balanced”, “balanced_subsample” or None, optional (de-
fault=None)
Weights associated with classes in the form {class_label:
weight}. If not
given, all classes are supposed to have weight one. For multi-output problems, a list of
dicts can be provided in the same order as the columns of y.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y))
The “balanced_subsample” mode is the same as “balanced” except that weights are
computed based on the bootstrap sample for every tree grown.
For multi-output, the weights of each column of y will be multiplied.
Note that these weights will be multiplied with sample_weight (passed through the ﬁt
method) if sample_weight is speciﬁed.
Attributesestimators_ : list of DecisionTreeClassiﬁer
The collection of ﬁtted sub-estimators.
classes_ : array of shape = [n_classes] or a list of such arrays
The classes labels (single output problem), or a list of arrays of class labels (multi-output
problem).
n_classes_ : int or list
The number of classes (single output problem), or a list containing the number of classes
for each output (multi-output problem).
feature_importances_ : array of shape = [n_features]
The feature importances (the higher, the more important the feature).
n_features_ : int
The number of features when fit is performed.
n_outputs_ : int
The number of outputs when fit is performed.
oob_score_ : ﬂoat
Score of the training dataset obtained using an out-of-bag estimate.
oob_decision_function_ : array of shape = [n_samples, n_classes]
Decision function computed with out-of-bag estimate on the training set.
If
n_estimators is small it might be possible that a data point was never left out during
the bootstrap. In this case, oob_decision_function_ might contain NaN.
See also:
sklearn.tree.ExtraTreeClassifierBase classiﬁer for this ensemble.
RandomForestClassifierEnsemble Classiﬁer based on trees with optimal splits.
3.3. Model selection and evaluation
429
scikit-learn user guide, Release 0.18.2
References
[R19]
Methods
apply(X)
Apply trees in the forest to X, return leaf indices.
decision_path(X)
Return the decision path in the forest
fit(X, y[, sample_weight])
Build a forest of trees from the training set (X, y).
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class for X.
predict_log_proba(X)
Predict class log-probabilities for X.
predict_proba(X)
Predict class probabilities for X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(n_estimators=10,
criterion=’gini’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
max_leaf_nodes=None,
min_impurity_split=1e-07,
bootstrap=False,
oob_score=False,
n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)
apply(X)
Apply trees in the forest to X, return leaf indices.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
ReturnsX_leaves : array_like, shape = [n_samples, n_estimators]
For each datapoint x in X and for each tree in the forest, return the index of the leaf x
ends up in.
decision_path(X)
Return the decision path in the forest
New in version 0.18.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsindicator : sparse csr array, shape = [n_samples, n_nodes]
Return a node indicator matrix where non zero elements indicates that the samples goes
through the nodes.
n_nodes_ptr : array of size (n_estimators + 1, )
The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value
for the i-th estimator.
430
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
feature_importances_
Return the feature importances (the higher, the more important thefeature).
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None)
Build a forest of trees from the training set (X, y).
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The
training
input
samples.
Internally,
its
dtype
will
be
converted
to
dtype=np.float32. If a sparse matrix is provided, it will be converted into a sparse
csc_matrix.
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
The target values (class labels in classiﬁcation, real numbers in regression).
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node. In the case of classiﬁcation, splits are also ignored if they would result in
any single class carrying a negative weight in either child node.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class for X.
The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability
estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
3.3. Model selection and evaluation
431
scikit-learn user guide, Release 0.18.2
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsy : array of shape = [n_samples] or [n_samples, n_outputs]
The predicted classes.
predict_log_proba(X)
Predict class log-probabilities for X.
The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class
probabilities of the trees in the forest.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsp : array of shape = [n_samples, n_classes], or a list of n_outputs
such arrays if n_outputs > 1. The class probabilities of the input samples. The order of
the classes corresponds to that in the attribute classes_.
predict_proba(X)
Predict class probabilities for X.
The predicted class probabilities of an input sample are computed as the mean predicted class probabilities
of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class
in a leaf.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsp : array of shape = [n_samples, n_classes], or a list of n_outputs
such arrays if n_outputs > 1. The class probabilities of the input samples. The order of
the classes corresponds to that in the attribute classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
432
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.ensemble.ExtraTreesClassifier
• Feature importances with forests of trees
• Pixel importances with a parallel forest of trees
• Plot the decision surfaces of ensembles of trees on the iris dataset
• Hashing feature transformation using Totally Random Trees
sklearn.ensemble.ExtraTreesRegressor
class sklearn.ensemble.ExtraTreesRegressor(n_estimators=10,
crite-
rion=’mse’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
max_leaf_nodes=None,
min_impurity_split=1e-07,
boot-
strap=False,
oob_score=False,
n_jobs=1,
random_state=None,
verbose=0,
warm_start=False)
An extra-trees regressor.
This class implements a meta estimator that ﬁts a number of randomized decision trees (a.k.a. extra-trees) on
various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-ﬁtting.
Read more in the User Guide.
3.3. Model selection and evaluation
433
scikit-learn user guide, Release 0.18.2
Parametersn_estimators : integer, optional (default=10)
The number of trees in the forest.
criterion : string, optional (default=”mse”)
The function to measure the quality of a split. Supported criteria are “mse” for the
mean squared error, which is equal to variance reduction as feature selection criterion,
and “mae” for the mean absolute error.
New in version 0.18: Mean Absolute Error (MAE) criterion.
max_features : int, ﬂoat, string or None, optional (default=”auto”)
The number of features to consider when looking for the best split:
•If int, then consider max_features features at each split.
•If ﬂoat, then max_features is a percentage and int(max_features * n_features) features
are considered at each split.
•If “auto”, then max_features=n_features.
•If “sqrt”, then max_features=sqrt(n_features).
•If “log2”, then max_features=log2(n_features).
•If None, then max_features=n_features.
Note: the search for a split does not stop until at least one valid partition of the node
samples is found, even if it requires to effectively inspect more than max_features
features.
max_depth : integer or None, optional (default=None)
The maximum depth of the tree. If None, then nodes are expanded until all leaves are
pure or until all leaves contain less than min_samples_split samples.
min_samples_split : int, ﬂoat, optional (default=2)
The minimum number of samples required to split an internal node:
•If int, then consider min_samples_split as the minimum number.
•If ﬂoat, then min_samples_split is a percentage and ceil(min_samples_split *
n_samples) are the minimum number of samples for each split.
Changed in version 0.18: Added ﬂoat values for percentages.
min_samples_leaf : int, ﬂoat, optional (default=1)
The minimum number of samples required to be at a leaf node:
•If int, then consider min_samples_leaf as the minimum number.
•If ﬂoat, then min_samples_leaf is a percentage and ceil(min_samples_leaf *
n_samples) are the minimum number of samples for each node.
Changed in version 0.18: Added ﬂoat values for percentages.
min_weight_fraction_leaf : ﬂoat, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples)
required to be at a leaf node. Samples have equal weight when sample_weight is not
provided.
max_leaf_nodes : int or None, optional (default=None)
434
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Grow trees with max_leaf_nodes in best-ﬁrst fashion. Best nodes are deﬁned as
relative reduction in impurity. If None then unlimited number of leaf nodes.
min_impurity_split : ﬂoat, optional (default=1e-7)
Threshold for early stopping in tree growth. A node will split if its impurity is above
the threshold, otherwise it is a leaf.
New in version 0.18.
bootstrap : boolean, optional (default=False)
Whether bootstrap samples are used when building trees.
oob_score : bool, optional (default=False)
Whether to use out-of-bag samples to estimate the R^2 on unseen data.
n_jobs : integer, optional (default=1)
The number of jobs to run in parallel for both ﬁt and predict. If -1, then the number of
jobs is set to the number of cores.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
verbose : int, optional (default=0)
Controls the verbosity of the tree building process.
warm_start : bool, optional (default=False)
When set to True, reuse the solution of the previous call to ﬁt and add more estimators
to the ensemble, otherwise, just ﬁt a whole new forest.
Attributesestimators_ : list of DecisionTreeRegressor
The collection of ﬁtted sub-estimators.
feature_importances_ : array of shape = [n_features]
The feature importances (the higher, the more important the feature).
n_features_ : int
The number of features.
n_outputs_ : int
The number of outputs.
oob_score_ : ﬂoat
Score of the training dataset obtained using an out-of-bag estimate.
oob_prediction_ : array of shape = [n_samples]
Prediction computed with out-of-bag estimate on the training set.
See also:
sklearn.tree.ExtraTreeRegressorBase estimator for this ensemble.
RandomForestRegressorEnsemble regressor using trees with optimal splits.
3.3. Model selection and evaluation
435
scikit-learn user guide, Release 0.18.2
References
[R20]
Methods
apply(X)
Apply trees in the forest to X, return leaf indices.
decision_path(X)
Return the decision path in the forest
fit(X, y[, sample_weight])
Build a forest of trees from the training set (X, y).
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict regression target for X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(n_estimators=10,
criterion=’mse’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
max_leaf_nodes=None,
min_impurity_split=1e-07,
bootstrap=False,
oob_score=False,
n_jobs=1, random_state=None, verbose=0, warm_start=False)
apply(X)
Apply trees in the forest to X, return leaf indices.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
ReturnsX_leaves : array_like, shape = [n_samples, n_estimators]
For each datapoint x in X and for each tree in the forest, return the index of the leaf x
ends up in.
decision_path(X)
Return the decision path in the forest
New in version 0.18.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsindicator : sparse csr array, shape = [n_samples, n_nodes]
Return a node indicator matrix where non zero elements indicates that the samples goes
through the nodes.
n_nodes_ptr : array of size (n_estimators + 1, )
The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value
for the i-th estimator.
feature_importances_
Return the feature importances (the higher, the more important thefeature).
436
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None)
Build a forest of trees from the training set (X, y).
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The
training
input
samples.
Internally,
its
dtype
will
be
converted
to
dtype=np.float32. If a sparse matrix is provided, it will be converted into a sparse
csc_matrix.
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
The target values (class labels in classiﬁcation, real numbers in regression).
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node. In the case of classiﬁcation, splits are also ignored if they would result in
any single class carrying a negative weight in either child node.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict regression target for X.
The predicted regression target of an input sample is computed as the mean predicted regression targets of
the trees in the forest.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsy : array of shape = [n_samples] or [n_samples, n_outputs]
3.3. Model selection and evaluation
437
scikit-learn user guide, Release 0.18.2
The predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
438
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples using sklearn.ensemble.ExtraTreesRegressor
• Face completion with a multi-output estimators
• Sparse recovery: feature selection for sparse linear models
sklearn.ensemble.GradientBoostingClassiﬁer
class sklearn.ensemble.GradientBoostingClassifier(loss=’deviance’,
learning_rate=0.1,
n_estimators=100,
subsam-
ple=1.0,
criterion=’friedman_mse’,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_depth=3, min_impurity_split=1e-
07,
init=None,
random_state=None,
max_features=None,
ver-
bose=0,
max_leaf_nodes=None,
warm_start=False, presort=’auto’)
Gradient Boosting for classiﬁcation.
GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differen-
tiable loss functions. In each stage n_classes_ regression trees are ﬁt on the negative gradient of the binomial
or multinomial deviance loss function. Binary classiﬁcation is a special case where only a single regression tree
is induced.
Read more in the User Guide.
Parametersloss : {‘deviance’, ‘exponential’}, optional (default=’deviance’)
loss function to be optimized. ‘deviance’ refers to deviance (= logistic regression) for
classiﬁcation with probabilistic outputs. For loss ‘exponential’ gradient boosting recov-
ers the AdaBoost algorithm.
learning_rate : ﬂoat, optional (default=0.1)
learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off
between learning_rate and n_estimators.
n_estimators : int (default=100)
The number of boosting stages to perform. Gradient boosting is fairly robust to over-
ﬁtting so a large number usually results in better performance.
max_depth : integer, optional (default=3)
maximum depth of the individual regression estimators. The maximum depth limits the
number of nodes in the tree. Tune this parameter for best performance; the best value
depends on the interaction of the input variables.
criterion : string, optional (default=”friedman_mse”)
The function to measure the quality of a split. Supported criteria are “friedman_mse” for
the mean squared error with improvement score by Friedman, “mse” for mean squared
error, and “mae” for the mean absolute error. The default value of “friedman_mse” is
generally the best as it can provide a better approximation in some cases.
New in version 0.18.
min_samples_split : int, ﬂoat, optional (default=2)
3.3. Model selection and evaluation
439
scikit-learn user guide, Release 0.18.2
The minimum number of samples required to split an internal node:
•If int, then consider min_samples_split as the minimum number.
•If ﬂoat, then min_samples_split is a percentage and ceil(min_samples_split *
n_samples) are the minimum number of samples for each split.
Changed in version 0.18: Added ﬂoat values for percentages.
min_samples_leaf : int, ﬂoat, optional (default=1)
The minimum number of samples required to be at a leaf node:
•If int, then consider min_samples_leaf as the minimum number.
•If ﬂoat, then min_samples_leaf is a percentage and ceil(min_samples_leaf *
n_samples) are the minimum number of samples for each node.
Changed in version 0.18: Added ﬂoat values for percentages.
min_weight_fraction_leaf : ﬂoat, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples)
required to be at a leaf node. Samples have equal weight when sample_weight is not
provided.
subsample : ﬂoat, optional (default=1.0)
The fraction of samples to be used for ﬁtting the individual base learners. If smaller
than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the pa-
rameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and
an increase in bias.
max_features : int, ﬂoat, string or None, optional (default=None)
The number of features to consider when looking for the best split:
•If int, then consider max_features features at each split.
•If ﬂoat, then max_features is a percentage and int(max_features * n_features) features
are considered at each split.
•If “auto”, then max_features=sqrt(n_features).
•If “sqrt”, then max_features=sqrt(n_features).
•If “log2”, then max_features=log2(n_features).
•If None, then max_features=n_features.
Choosing max_features < n_features leads to a reduction of variance and an increase in
bias.
Note: the search for a split does not stop until at least one valid partition of the node
samples is found, even if it requires to effectively inspect more than max_features
features.
max_leaf_nodes : int or None, optional (default=None)
Grow trees with max_leaf_nodes in best-ﬁrst fashion. Best nodes are deﬁned as
relative reduction in impurity. If None then unlimited number of leaf nodes.
min_impurity_split : ﬂoat, optional (default=1e-7)
Threshold for early stopping in tree growth. A node will split if its impurity is above
the threshold, otherwise it is a leaf.
440
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
New in version 0.18.
init : BaseEstimator, None, optional (default=None)
An estimator object that is used to compute the initial predictions. init has to provide
fit and predict. If None it uses loss.init_estimator.
verbose : int, default: 0
Enable verbose output. If 1 then it prints progress and performance once in a while
(the more trees the lower the frequency). If greater than 1 then it prints progress and
performance for every tree.
warm_start : bool, default: False
When set to True, reuse the solution of the previous call to ﬁt and add more estimators
to the ensemble, otherwise, just erase the previous solution.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
presort : bool or ‘auto’, optional (default=’auto’)
Whether to presort the data to speed up the ﬁnding of best splits in ﬁtting. Auto mode
by default will use presorting on dense data and default to normal sorting on sparse data.
Setting presort to true on sparse data will raise an error.
New in version 0.17: presort parameter.
Attributesfeature_importances_ : array, shape = [n_features]
The feature importances (the higher, the more important the feature).
oob_improvement_ : array, shape = [n_estimators]
The improvement in loss (= deviance) on the out-of-bag samples relative to the previous
iteration. oob_improvement_[0] is the improvement in loss of the ﬁrst stage over
the init estimator.
train_score_ : array, shape = [n_estimators]
The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i
on the in-bag sample. If subsample == 1 this is the deviance on the training data.
loss_ : LossFunction
The concrete LossFunction object.
init : BaseEstimator
The estimator that provides the initial predictions.
Set via the init argument or
loss.init_estimator.
estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, loss_.K]
The collection of ﬁtted sub-estimators. loss_.K is 1 for binary classiﬁcation, other-
wise n_classes.
See also:
sklearn.tree.DecisionTreeClassifier,
RandomForestClassifier,
AdaBoostClassifier
3.3. Model selection and evaluation
441
scikit-learn user guide, Release 0.18.2
References
J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29,
No. 5, 2001.
10.Friedman, Stochastic Gradient Boosting, 1999
T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009.
Methods
apply(X)
Apply trees in the ensemble to X, return leaf indices.
decision_function(X)
Compute the decision function of X.
fit(X, y[, sample_weight, monitor])
Fit the gradient boosting model.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class for X.
predict_log_proba(X)
Predict class log-probabilities for X.
predict_proba(X)
Predict class probabilities for X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
staged_decision_function(X)
Compute decision function of X for each iteration.
staged_predict(X)
Predict class at each stage for X.
staged_predict_proba(X)
Predict class probabilities at each stage for X.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(loss=’deviance’,
learning_rate=0.1,
n_estimators=100,
subsample=1.0,
criterion=’friedman_mse’,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_depth=3,
min_impurity_split=1e-07,
init=None,
random_state=None,
max_features=None,
verbose=0,
max_leaf_nodes=None,
warm_start=False, presort=’auto’)
apply(X)
Apply trees in the ensemble to X, return leaf indices.
New in version 0.17.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted to a sparse csr_matrix.
ReturnsX_leaves : array_like, shape = [n_samples, n_estimators, n_classes]
For each datapoint x in X and for each tree in the ensemble, return the index of the leaf
x ends up in each estimator. In the case of binary classiﬁcation n_classes is 1.
decision_function(X)
Compute the decision function of X.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsscore : array, shape = [n_samples, n_classes] or [n_samples]
442
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The decision function of the input samples. The order of the classes corresponds to that
in the attribute classes_. Regression and binary classiﬁcation produce an array of shape
[n_samples].
feature_importances_
Return the feature importances (the higher, the more important thefeature).
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None, monitor=None)
Fit the gradient boosting model.
ParametersX : array-like, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples]
Target values (integers in classiﬁcation, real numbers in regression) For classiﬁcation,
labels must correspond to classes.
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node. In the case of classiﬁcation, splits are also ignored if they would result in
any single class carrying a negative weight in either child node.
monitor : callable, optional
The monitor is called after each iteration with the current iteration, a reference
to the estimator and the local variables of _fit_stages as keyword arguments
callable(i,self,locals()). If the callable returns True the ﬁtting proce-
dure is stopped. The monitor can be used for various things such as computing held-out
estimates, early stopping, model introspect, and snapshoting.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
3.3. Model selection and evaluation
443
scikit-learn user guide, Release 0.18.2
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class for X.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsy: array of shape = [”n_samples] :
The predicted values.
predict_log_proba(X)
Predict class log-probabilities for X.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsp : array of shape = [n_samples]
The class log-probabilities of the input samples. The order of the classes corresponds to
that in the attribute classes_.
RaisesAttributeError :
If the loss does not support probabilities.
predict_proba(X)
Predict class probabilities for X.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsp : array of shape = [n_samples]
The class probabilities of the input samples. The order of the classes corresponds to that
in the attribute classes_.
RaisesAttributeError :
If the loss does not support probabilities.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
444
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
staged_decision_function(X)
Compute decision function of X for each iteration.
This method allows monitoring (i.e. determine error on testing set) after each stage.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsscore : generator of array, shape = [n_samples, k]
The decision function of the input samples. The order of the classes corresponds to that
in the attribute classes_. Regression and binary classiﬁcation are special cases with k
== 1, otherwise k==n_classes.
staged_predict(X)
Predict class at each stage for X.
This method allows monitoring (i.e. determine error on testing set) after each stage.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsy : generator of array of shape = [n_samples]
The predicted value of the input samples.
staged_predict_proba(X)
Predict class probabilities at each stage for X.
This method allows monitoring (i.e. determine error on testing set) after each stage.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsy : generator of array of shape = [n_samples]
The predicted value of the input samples.
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
3.3. Model selection and evaluation
445
scikit-learn user guide, Release 0.18.2
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.ensemble.GradientBoostingClassifier
• Feature transformations with ensembles of trees
• Gradient Boosting Out-of-Bag estimates
• Gradient Boosting regularization
sklearn.ensemble.GradientBoostingRegressor
class sklearn.ensemble.GradientBoostingRegressor(loss=’ls’,
learning_rate=0.1,
n_estimators=100,
subsam-
ple=1.0,
criterion=’friedman_mse’,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_depth=3,
min_impurity_split=1e-
07,
init=None,
random_state=None,
max_features=None,
alpha=0.9,
verbose=0,
max_leaf_nodes=None,
warm_start=False, presort=’auto’)
Gradient Boosting for regression.
GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differ-
entiable loss functions. In each stage a regression tree is ﬁt on the negative gradient of the given loss function.
Read more in the User Guide.
Parametersloss : {‘ls’, ‘lad’, ‘huber’, ‘quantile’}, optional (default=’ls’)
loss function to be optimized. ‘ls’ refers to least squares regression. ‘lad’ (least absolute
deviation) is a highly robust loss function solely based on order information of the input
variables. ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression
(use alpha to specify the quantile).
learning_rate : ﬂoat, optional (default=0.1)
learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off
between learning_rate and n_estimators.
n_estimators : int (default=100)
The number of boosting stages to perform. Gradient boosting is fairly robust to over-
ﬁtting so a large number usually results in better performance.
max_depth : integer, optional (default=3)
446
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
maximum depth of the individual regression estimators. The maximum depth limits the
number of nodes in the tree. Tune this parameter for best performance; the best value
depends on the interaction of the input variables.
criterion : string, optional (default=”friedman_mse”)
The function to measure the quality of a split. Supported criteria are “friedman_mse” for
the mean squared error with improvement score by Friedman, “mse” for mean squared
error, and “mae” for the mean absolute error. The default value of “friedman_mse” is
generally the best as it can provide a better approximation in some cases.
New in version 0.18.
min_samples_split : int, ﬂoat, optional (default=2)
The minimum number of samples required to split an internal node:
•If int, then consider min_samples_split as the minimum number.
•If ﬂoat, then min_samples_split is a percentage and ceil(min_samples_split *
n_samples) are the minimum number of samples for each split.
Changed in version 0.18: Added ﬂoat values for percentages.
min_samples_leaf : int, ﬂoat, optional (default=1)
The minimum number of samples required to be at a leaf node:
•If int, then consider min_samples_leaf as the minimum number.
•If ﬂoat, then min_samples_leaf is a percentage and ceil(min_samples_leaf *
n_samples) are the minimum number of samples for each node.
Changed in version 0.18: Added ﬂoat values for percentages.
min_weight_fraction_leaf : ﬂoat, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples)
required to be at a leaf node. Samples have equal weight when sample_weight is not
provided.
subsample : ﬂoat, optional (default=1.0)
The fraction of samples to be used for ﬁtting the individual base learners. If smaller
than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the pa-
rameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and
an increase in bias.
max_features : int, ﬂoat, string or None, optional (default=None)
The number of features to consider when looking for the best split:
•If int, then consider max_features features at each split.
•If ﬂoat, then max_features is a percentage and int(max_features * n_features) features
are considered at each split.
•If “auto”, then max_features=n_features.
•If “sqrt”, then max_features=sqrt(n_features).
•If “log2”, then max_features=log2(n_features).
•If None, then max_features=n_features.
3.3. Model selection and evaluation
447
scikit-learn user guide, Release 0.18.2
Choosing max_features < n_features leads to a reduction of variance and an increase in
bias.
Note: the search for a split does not stop until at least one valid partition of the node
samples is found, even if it requires to effectively inspect more than max_features
features.
max_leaf_nodes : int or None, optional (default=None)
Grow trees with max_leaf_nodes in best-ﬁrst fashion. Best nodes are deﬁned as
relative reduction in impurity. If None then unlimited number of leaf nodes.
min_impurity_split : ﬂoat, optional (default=1e-7)
Threshold for early stopping in tree growth. A node will split if its impurity is above
the threshold, otherwise it is a leaf.
New in version 0.18.
alpha : ﬂoat (default=0.9)
The alpha-quantile of the huber loss function and the quantile loss function. Only if
loss='huber' or loss='quantile'.
init : BaseEstimator, None, optional (default=None)
An estimator object that is used to compute the initial predictions. init has to provide
fit and predict. If None it uses loss.init_estimator.
verbose : int, default: 0
Enable verbose output. If 1 then it prints progress and performance once in a while
(the more trees the lower the frequency). If greater than 1 then it prints progress and
performance for every tree.
warm_start : bool, default: False
When set to True, reuse the solution of the previous call to ﬁt and add more estimators
to the ensemble, otherwise, just erase the previous solution.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
presort : bool or ‘auto’, optional (default=’auto’)
Whether to presort the data to speed up the ﬁnding of best splits in ﬁtting. Auto mode
by default will use presorting on dense data and default to normal sorting on sparse data.
Setting presort to true on sparse data will raise an error.
New in version 0.17: optional parameter presort.
Attributesfeature_importances_ : array, shape = [n_features]
The feature importances (the higher, the more important the feature).
oob_improvement_ : array, shape = [n_estimators]
The improvement in loss (= deviance) on the out-of-bag samples relative to the previous
iteration. oob_improvement_[0] is the improvement in loss of the ﬁrst stage over
the init estimator.
train_score_ : array, shape = [n_estimators]
448
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i
on the in-bag sample. If subsample == 1 this is the deviance on the training data.
loss_ : LossFunction
The concrete LossFunction object.
‘init‘ : BaseEstimator
The estimator that provides the initial predictions.
Set via the init argument or
loss.init_estimator.
estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]
The collection of ﬁtted sub-estimators.
See also:
DecisionTreeRegressor, RandomForestRegressor
References
J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29,
No. 5, 2001.
10.Friedman, Stochastic Gradient Boosting, 1999
T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009.
Methods
apply(X)
Apply trees in the ensemble to X, return leaf indices.
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19
fit(X, y[, sample_weight, monitor])
Fit the gradient boosting model.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict regression target for X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
staged_decision_function(\*args,
\*\*kwargs)
DEPRECATED: and will be removed in 0.19
staged_predict(X)
Predict regression target at each stage for X.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(loss=’ls’,
learning_rate=0.1,
n_estimators=100,
subsample=1.0,
cri-
terion=’friedman_mse’,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_depth=3,
min_impurity_split=1e-07,
init=None,
random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,
warm_start=False, presort=’auto’)
apply(X)
Apply trees in the ensemble to X, return leaf indices.
New in version 0.17.
3.3. Model selection and evaluation
449
scikit-learn user guide, Release 0.18.2
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted to a sparse csr_matrix.
ReturnsX_leaves : array_like, shape = [n_samples, n_estimators]
For each datapoint x in X and for each tree in the ensemble, return the index of the leaf
x ends up in each estimator.
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19
Compute the decision function of X.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsscore : array, shape = [n_samples, n_classes] or [n_samples]
The decision function of the input samples. The order of the classes corresponds to that
in the attribute classes_. Regression and binary classiﬁcation produce an array of shape
[n_samples].
feature_importances_
Return the feature importances (the higher, the more important thefeature).
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None, monitor=None)
Fit the gradient boosting model.
ParametersX : array-like, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples]
Target values (integers in classiﬁcation, real numbers in regression) For classiﬁcation,
labels must correspond to classes.
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node. In the case of classiﬁcation, splits are also ignored if they would result in
any single class carrying a negative weight in either child node.
monitor : callable, optional
The monitor is called after each iteration with the current iteration, a reference
to the estimator and the local variables of _fit_stages as keyword arguments
callable(i,self,locals()). If the callable returns True the ﬁtting proce-
dure is stopped. The monitor can be used for various things such as computing held-out
estimates, early stopping, model introspect, and snapshoting.
Returnsself : object
Returns self.
450
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict regression target for X.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsy : array of shape = [n_samples]
The predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
3.3. Model selection and evaluation
451
scikit-learn user guide, Release 0.18.2
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
staged_decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19
Compute decision function of X for each iteration.
This method allows monitoring (i.e. determine error on testing set) after each stage.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsscore : generator of array, shape = [n_samples, k]
The decision function of the input samples. The order of the classes corresponds to that
in the attribute classes_. Regression and binary classiﬁcation are special cases with k
== 1, otherwise k==n_classes.
staged_predict(X)
Predict regression target at each stage for X.
This method allows monitoring (i.e. determine error on testing set) after each stage.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsy : generator of array of shape = [n_samples]
The predicted value of the input samples.
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
452
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples using sklearn.ensemble.GradientBoostingRegressor
• Model Complexity Inﬂuence
• Prediction Intervals for Gradient Boosting Regression
• Gradient Boosting regression
• Partial Dependence Plots
3.3.3 Model evaluation: quantifying the quality of predictions
There are 3 different approaches to evaluate the quality of predictions of a model:
• Estimator score method: Estimators have a score method providing a default evaluation criterion for the
problem they are designed to solve. This is not discussed on this page, but in each estimator’s documentation.
• Scoring
parameter:
Model-evaluation
tools
using
cross-validation
(such
as
model_selection.cross_val_score and model_selection.GridSearchCV) rely on an
internal scoring strategy. This is discussed in the section The scoring parameter: deﬁning model evaluation
rules.
• Metric functions: The metrics module implements functions assessing prediction error for speciﬁc purposes.
These metrics are detailed in sections on Classiﬁcation metrics, Multilabel ranking metrics, Regression metrics
and Clustering metrics.
Finally, Dummy estimators are useful to get a baseline value of those metrics for random predictions.
See also:
For “pairwise” metrics, between samples and not estimators or predictions, see the Pairwise metrics, Afﬁnities and
Kernels section.
The scoring parameter: deﬁning model evaluation rules
Model
selection
and
evaluation
using
tools,
such
as
model_selection.GridSearchCV
and
model_selection.cross_val_score, take a scoring parameter that controls what metric they ap-
ply to the estimators evaluated.
Common cases: predeﬁned values
For the most common use cases, you can designate a scorer object with the scoring parameter; the table be-
low shows all possible values.
All scorer objects follow the convention that higher return values are bet-
ter than lower return values. Thus metrics which measure the distance between the model and the data, like
metrics.mean_squared_error, are available as neg_mean_squared_error which return the negated value of
the metric.
3.3. Model selection and evaluation
453
scikit-learn user guide, Release 0.18.2
Scoring
Function
Comment
Classiﬁcation
‘accuracy’
metrics.accuracy_score
‘average_precision’
metrics.average_precision_score
‘f1’
metrics.f1_score
for binary targets
‘f1_micro’
metrics.f1_score
micro-averaged
‘f1_macro’
metrics.f1_score
macro-averaged
‘f1_weighted’
metrics.f1_score
weighted average
‘f1_samples’
metrics.f1_score
by multilabel sample
‘neg_log_loss’
metrics.log_loss
requires predict_proba
support
‘precision’ etc.
metrics.precision_score
sufﬁxes apply as with ‘f1’
‘recall’ etc.
metrics.recall_score
sufﬁxes apply as with ‘f1’
‘roc_auc’
metrics.roc_auc_score
Clustering
‘adjusted_rand_score’
metrics.adjusted_rand_score
Regression
‘neg_mean_absolute_error’
metrics.mean_absolute_error
‘neg_mean_squared_error’
metrics.mean_squared_error
‘neg_median_absolute_error’
metrics.median_absolute_error
‘r2’
metrics.r2_score
Usage examples:
>>> from sklearn import svm, datasets
>>> from sklearn.model_selection import cross_val_score
>>> iris = datasets.load_iris()
>>> X, y = iris.data, iris.target
>>> clf = svm.SVC(probability=True, random_state=0)
>>> cross_val_score(clf, X, y, scoring='neg_log_loss')
array([-0.07..., -0.16..., -0.06...])
>>> model = svm.SVC()
>>> cross_val_score(model, X, y, scoring='wrong_choice')
Traceback (most recent call last):
ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy
˓→', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_
˓→samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_
˓→squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro',
˓→'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall',
˓→'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']
Note:
The values listed by the ValueError exception correspond to the functions measuring prediction accu-
racy described in the following sections.
The scorer objects for those functions are stored in the dictionary
sklearn.metrics.SCORERS.
Deﬁning your scoring strategy from metric functions
The module sklearn.metric also exposes a set of simple functions measuring a prediction error given ground
truth and prediction:
• functions ending with _score return a value to maximize, the higher the better.
• functions ending with _error or _loss return a value to minimize, the lower the better. When converting into
454
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
a scorer object using make_scorer, set the greater_is_better parameter to False (True by default; see
the parameter description below).
Metrics available for various machine learning tasks are detailed in sections below.
Many metrics are not given names to be used as scoring values, sometimes because they require additional param-
eters, such as fbeta_score. In such cases, you need to generate an appropriate scoring object. The simplest way
to generate a callable object for scoring is by using make_scorer. That function converts metrics into callables that
can be used for model evaluation.
One typical use case is to wrap an existing metric function from the library with non-default values for its parameters,
such as the beta parameter for the fbeta_score function:
>>> from sklearn.metrics import fbeta_score, make_scorer
>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.svm import LinearSVC
>>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)
The second use case is to build a completely custom scorer object from a simple python function using
make_scorer, which can take several parameters:
• the python function you want to use (my_custom_loss_func in the example below)
• whether the python function returns a score (greater_is_better=True, the default) or a loss
(greater_is_better=False). If a loss, the output of the python function is negated by the scorer ob-
ject, conforming to the cross validation convention that scorers return higher values for better models.
• for classiﬁcation metrics only: whether the python function you provided requires continuous decision certain-
ties (needs_threshold=True). The default value is False.
• any additional parameters, such as beta or labels in f1_score.
Here is an example of building custom scorers, and of using the greater_is_better parameter:
>>> import numpy as np
>>> def my_custom_loss_func(ground_truth, predictions):
...
diff = np.abs(ground_truth - predictions).max()
...
return np.log(1 + diff)
...
>>> # loss_func will negate the return value of my_custom_loss_func,
>>> #
which will be np.log(2), 0.693, given the values for ground_truth
>>> #
and predictions defined below.
>>> loss
= make_scorer(my_custom_loss_func, greater_is_better=False)
>>> score = make_scorer(my_custom_loss_func, greater_is_better=True)
>>> ground_truth = [[1, 1]]
>>> predictions
= [0, 1]
>>> from sklearn.dummy import DummyClassifier
>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)
>>> clf = clf.fit(ground_truth, predictions)
>>> loss(clf,ground_truth, predictions)
-0.69...
>>> score(clf,ground_truth, predictions)
0.69...
Implementing your own scoring object
You can generate even more ﬂexible model scorers by constructing your own scoring object from scratch, without using
the make_scorer factory. For a callable to be a scorer, it needs to meet the protocol speciﬁed by the following two
3.3. Model selection and evaluation
455
scikit-learn user guide, Release 0.18.2
rules:
• It can be called with parameters (estimator,X,y), where estimator is the model that should be eval-
uated, X is validation data, and y is the ground truth target for X (in the supervised case) or None (in the
unsupervised case).
• It returns a ﬂoating point number that quantiﬁes the estimator prediction quality on X, with reference to y.
Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.
Classiﬁcation metrics
The sklearn.metrics module implements several loss, score, and utility functions to measure classiﬁcation per-
formance. Some metrics might require probability estimates of the positive class, conﬁdence values, or binary deci-
sions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through
the sample_weight parameter.
Some of these are restricted to the binary classiﬁcation case:
matthews_corrcoef(y_true, y_pred[, ...])
Compute the Matthews correlation coefﬁcient (MCC) for
binary classes
precision_recall_curve(y_true, probas_pred)
Compute precision-recall pairs for different probability
thresholds
roc_curve(y_true, y_score[, pos_label, ...])
Compute Receiver operating characteristic (ROC)
Others also work in the multiclass case:
cohen_kappa_score(y1, y2[, labels, weights])
Cohen’s kappa: a statistic that measures inter-annotator
agreement.
confusion_matrix(y_true, y_pred[, labels, ...])
Compute confusion matrix to evaluate the accuracy of a
classiﬁcation
hinge_loss(y_true, pred_decision[, labels, ...])
Average hinge loss (non-regularized)
Some also work in the multilabel case:
accuracy_score(y_true, y_pred[, normalize, ...])
Accuracy classiﬁcation score.
classification_report(y_true, y_pred[, ...])
Build a text report showing the main classiﬁcation metrics
f1_score(y_true, y_pred[, labels, ...])
Compute the F1 score, also known as balanced F-score or
F-measure
fbeta_score(y_true, y_pred, beta[, labels, ...])
Compute the F-beta score
hamming_loss(y_true, y_pred[, labels, ...])
Compute the average Hamming loss.
jaccard_similarity_score(y_true, y_pred[, ...])
Jaccard similarity coefﬁcient score
log_loss(y_true, y_pred[, eps, normalize, ...])
Log loss, aka logistic loss or cross-entropy loss.
precision_recall_fscore_support(y_true,
y_pred)
Compute precision, recall, F-measure and support for each
class
precision_score(y_true, y_pred[, labels, ...])
Compute the precision
recall_score(y_true, y_pred[, labels, ...])
Compute the recall
zero_one_loss(y_true, y_pred[, normalize, ...])
Zero-one classiﬁcation loss.
And some work with binary and multilabel (but not multiclass) problems:
average_precision_score(y_true, y_score[, ...])
Compute average precision (AP) from prediction scores
Continued on next page
456
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Table 3.25 – continued from previous page
roc_auc_score(y_true, y_score[, average, ...])
Compute Area Under the Curve (AUC) from prediction
scores
In the following sub-sections, we will describe each of those functions, preceded by some notes on common API and
metric deﬁnition.
From binary to multiclass and multilabel
Some metrics are essentially deﬁned for binary classiﬁcation tasks (e.g. f1_score, roc_auc_score). In these
cases, by default only the positive label is evaluated, assuming by default that the positive class is labelled 1 (though
this may be conﬁgurable through the pos_label parameter). In extending a binary metric to multiclass or multilabel
problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways
to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where
available, you should select among these using the average parameter.
• "macro" simply calculates the mean of the binary metrics, giving equal weight to each class. In problems
where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their
performance. On the other hand, the assumption that all classes are equally important is often untrue, such that
macro-averaging will over-emphasize the typically low performance on an infrequent class.
• "weighted" accounts for class imbalance by computing the average of binary metrics in which each class’s
score is weighted by its presence in the true data sample.
• "micro" gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-
weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the
per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings,
including multiclass classiﬁcation where a majority class is to be ignored.
• "samples" applies only to multilabel problems. It does not calculate a per-class measure, instead calculat-
ing the metric over the true and predicted classes for each sample in the evaluation data, and returning their
(sample_weight-weighted) average.
• Selecting average=None will return an array with the score for each class.
While multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is
speciﬁed as an indicator matrix, in which cell [i,j] has value 1 if sample i has label j and value 0 otherwise.
Accuracy score
The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False)
of correct predictions.
In multilabel classiﬁcation, the function returns the subset accuracy. If the entire set of predicted labels for a sample
strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.
If ˆ𝑦𝑖is the predicted value of the 𝑖-th sample and 𝑦𝑖is the corresponding true value, then the fraction of correct
predictions over 𝑛samples is deﬁned as
accuracy(𝑦, ˆ𝑦) =
1
𝑛samples
𝑛samples−1
∑︁
𝑖=0
1(ˆ𝑦𝑖= 𝑦𝑖)
where 1(𝑥) is the indicator function.
3.3. Model selection and evaluation
457
scikit-learn user guide, Release 0.18.2
>>> import numpy as np
>>> from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)
2
In the multilabel case with binary label indicators:
>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5
Example:
• See Test with permutations the signiﬁcance of a classiﬁcation score for an example of accuracy score usage
using permutations of the dataset.
Cohen’s kappa
The function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare label-
ings by different human annotators, not a classiﬁer versus a ground truth.
The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agree-
ment; zero or lower means no agreement (practically random labels).
Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually
computing a per-label score) and not for more than two annotators.
>>> from sklearn.metrics import cohen_kappa_score
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> cohen_kappa_score(y_true, y_pred)
0.4285714285714286
Confusion matrix
The confusion_matrix function evaluates classiﬁcation accuracy by computing the confusion matrix.
By deﬁnition, entry 𝑖, 𝑗in a confusion matrix is the number of observations actually in group 𝑖, but predicted to be in
group 𝑗. Here is an example:
>>> from sklearn.metrics import confusion_matrix
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
[0, 0, 1],
[1, 0, 2]])
458
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Here is a visual representation of such a confusion matrix (this ﬁgure comes from the Confusion matrix example):
For binary problems, we can
get counts of true negatives, false positives, false negatives and true positives as follows:
>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]
>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
>>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
>>> tn, fp, fn, tp
(2, 1, 2, 3)
Example:
• See Confusion matrix for an example of using a confusion matrix to evaluate classiﬁer output quality.
• See Recognizing hand-written digits for an example of using a confusion matrix to classify hand-written
digits.
• See Classiﬁcation of text documents using sparse features for an example of using a confusion matrix to
classify text documents.
Classiﬁcation report
The classification_report function builds a text report showing the main classiﬁcation metrics. Here is a
small example with custom target_names and inferred labels:
>>> from sklearn.metrics import classification_report
>>> y_true = [0, 1, 2, 2, 0]
>>> y_pred = [0, 0, 2, 1, 0]
>>> target_names = ['class 0', 'class 1', 'class 2']
>>> print(classification_report(y_true, y_pred, target_names=target_names))
precision
recall
f1-score
support
3.3. Model selection and evaluation
459
scikit-learn user guide, Release 0.18.2
class 0
0.67
1.00
0.80
2
class 1
0.00
0.00
0.00
1
class 2
1.00
0.50
0.67
2
avg / total
0.67
0.60
0.59
5
Example:
• See Recognizing hand-written digits for an example of classiﬁcation report usage for hand-written digits.
• See Classiﬁcation of text documents using sparse features for an example of classiﬁcation report usage for
text documents.
• See Parameter estimation using grid search with cross-validation for an example of classiﬁcation report usage
for grid search with nested cross-validation.
Hamming loss
The hamming_loss computes the average Hamming loss or Hamming distance between two sets of samples.
If ˆ𝑦𝑗is the predicted value for the 𝑗-th label of a given sample, 𝑦𝑗is the corresponding true value, and 𝑛labels is the
number of classes or labels, then the Hamming loss 𝐿𝐻𝑎𝑚𝑚𝑖𝑛𝑔between two samples is deﬁned as:
𝐿𝐻𝑎𝑚𝑚𝑖𝑛𝑔(𝑦, ˆ𝑦) =
1
𝑛labels
𝑛labels−1
∑︁
𝑗=0
1(ˆ𝑦𝑗̸= 𝑦𝑗)
where 1(𝑥) is the indicator function.
>>> from sklearn.metrics import hamming_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> hamming_loss(y_true, y_pred)
0.25
In the multilabel case with binary label indicators:
>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
0.75
Note:
In multiclass classiﬁcation, the Hamming loss corresponds to the Hamming distance between y_true and
y_pred which is similar to the Zero one loss function. However, while zero-one loss penalizes prediction sets that
do not strictly match true sets, the Hamming loss penalizes individual labels. Thus the Hamming loss, upper bounded
by the zero-one loss, is always between zero and one, inclusive; and predicting a proper subset or superset of the true
labels will give a Hamming loss between zero and one, exclusive.
Jaccard similarity coefﬁcient score
The jaccard_similarity_score function computes the average (default) or sum of Jaccard similarity coefﬁ-
cients, also called the Jaccard index, between pairs of label sets.
460
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
The Jaccard similarity coefﬁcient of the 𝑖-th samples, with a ground truth label set 𝑦𝑖and predicted label set ˆ𝑦𝑖, is
deﬁned as
𝐽(𝑦𝑖, ˆ𝑦𝑖) = |𝑦𝑖∩ˆ𝑦𝑖|
|𝑦𝑖∪ˆ𝑦𝑖|.
In binary and multiclass classiﬁcation, the Jaccard similarity coefﬁcient score is equal to the classiﬁcation accuracy.
>>> import numpy as np
>>> from sklearn.metrics import jaccard_similarity_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> jaccard_similarity_score(y_true, y_pred)
0.5
>>> jaccard_similarity_score(y_true, y_pred, normalize=False)
2
In the multilabel case with binary label indicators:
>>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.75
Precision, recall and F-measures
Intuitively, precision is the ability of the classiﬁer not to label as positive a sample that is negative, and recall is the
ability of the classiﬁer to ﬁnd all the positive samples.
The F-measure (𝐹𝛽and 𝐹1 measures) can be interpreted as a weighted harmonic mean of the precision and recall. A
𝐹𝛽measure reaches its best value at 1 and its worst score at 0. With 𝛽= 1, 𝐹𝛽and 𝐹1 are equivalent, and the recall
and the precision are equally important.
The precision_recall_curve computes a precision-recall curve from the ground truth label and a score given
by the classiﬁer by varying a decision threshold.
The average_precision_score function computes the average precision (AP) from prediction scores. This
score corresponds to the area under the precision-recall curve. The value is between 0 and 1 and higher is better. With
random predictions, the AP is the fraction of positive samples.
Several functions allow you to analyze the precision, recall and F-measures score:
average_precision_score(y_true, y_score[, ...])
Compute average precision (AP) from prediction scores
f1_score(y_true, y_pred[, labels, ...])
Compute the F1 score, also known as balanced F-score or
F-measure
fbeta_score(y_true, y_pred, beta[, labels, ...])
Compute the F-beta score
precision_recall_curve(y_true, probas_pred)
Compute precision-recall pairs for different probability
thresholds
precision_recall_fscore_support(y_true,
y_pred)
Compute precision, recall, F-measure and support for each
class
precision_score(y_true, y_pred[, labels, ...])
Compute the precision
recall_score(y_true, y_pred[, labels, ...])
Compute the recall
Note
that
the
precision_recall_curve
function
is
restricted
to
the
binary
case.
The
average_precision_score function works only in binary classiﬁcation and multilabel indicator format.
3.3. Model selection and evaluation
461
scikit-learn user guide, Release 0.18.2
Examples:
• See Classiﬁcation of text documents using sparse features for an example of f1_score usage to classify
text documents.
• See Parameter estimation using grid search with cross-validation for an example of precision_score
and recall_score usage to estimate parameters using grid search with nested cross-validation.
• See Precision-Recall for an example of precision_recall_curve usage to evaluate classiﬁer output
quality.
• See
Sparse
recovery:
feature
selection
for
sparse
linear
models
for
an
example
of
precision_recall_curve usage to select features for sparse linear models.
Binary classiﬁcation
In a binary classiﬁcation task, the terms ‘’positive” and ‘’negative” refer to the classiﬁer’s prediction, and the terms
‘’true” and ‘’false” refer to whether that prediction corresponds to the external judgment (sometimes known as the
‘’observation’‘). Given these deﬁnitions, we can formulate the following table:
Actual class (observation)
Predicted class (expectation)
tp (true positive) Correct result
fp (false positive) Unexpected result
fn (false negative) Missing result
tn (true negative) Correct absence of result
In this context, we can deﬁne the notions of precision, recall and F-measure:
precision =
𝑡𝑝
𝑡𝑝+ 𝑓𝑝,
recall =
𝑡𝑝
𝑡𝑝+ 𝑓𝑛,
𝐹𝛽= (1 + 𝛽2) precision × recall
𝛽2precision + recall.
Here are some small examples in binary classiﬁcation:
>>> from sklearn import metrics
>>> y_pred = [0, 1, 0, 0]
>>> y_true = [0, 1, 0, 1]
>>> metrics.precision_score(y_true, y_pred)
1.0
>>> metrics.recall_score(y_true, y_pred)
0.5
>>> metrics.f1_score(y_true, y_pred)
0.66...
>>> metrics.fbeta_score(y_true, y_pred, beta=0.5)
0.83...
>>> metrics.fbeta_score(y_true, y_pred, beta=1)
0.66...
>>> metrics.fbeta_score(y_true, y_pred, beta=2)
0.55...
>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)
(array([ 0.66...,
1.
]), array([ 1. ,
0.5]), array([ 0.71...,
0.83...]),
˓→array([2, 2]...))
>>> import numpy as np
462
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> from sklearn.metrics import precision_recall_curve
>>> from sklearn.metrics import average_precision_score
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> precision, recall, threshold = precision_recall_curve(y_true, y_scores)
>>> precision
array([ 0.66...,
0.5
,
1.
,
1.
])
>>> recall
array([ 1. ,
0.5,
0.5,
0. ])
>>> threshold
array([ 0.35,
0.4 ,
0.8 ])
>>> average_precision_score(y_true, y_scores)
0.79...
Multiclass and multilabel classiﬁcation
In multiclass and multilabel classiﬁcation task, the notions of precision, recall, and F-measures can be ap-
plied to each label independently.
There are a few ways to combine results across labels, speciﬁed by the
average argument to the average_precision_score (multilabel only), f1_score, fbeta_score,
precision_recall_fscore_support, precision_score and recall_score functions, as described
above. Note that for “micro”-averaging in a multiclass setting with all labels included will produce equal precision,
recall and 𝐹, while “weighted” averaging may produce an F-score that is not between precision and recall.
To make this more explicit, consider the following notation:
• 𝑦the set of predicted (𝑠𝑎𝑚𝑝𝑙𝑒, 𝑙𝑎𝑏𝑒𝑙) pairs
• ˆ𝑦the set of true (𝑠𝑎𝑚𝑝𝑙𝑒, 𝑙𝑎𝑏𝑒𝑙) pairs
• 𝐿the set of labels
• 𝑆the set of samples
• 𝑦𝑠the subset of 𝑦with sample 𝑠, i.e. 𝑦𝑠:= {(𝑠′, 𝑙) ∈𝑦|𝑠′ = 𝑠}
• 𝑦𝑙the subset of 𝑦with label 𝑙
• similarly, ˆ𝑦𝑠and ˆ𝑦𝑙are subsets of ˆ𝑦
• 𝑃(𝐴, 𝐵) := |𝐴∩𝐵|
|𝐴|
• 𝑅(𝐴, 𝐵) := |𝐴∩𝐵|
|𝐵|
(Conventions vary on handling 𝐵= ∅; this implementation uses 𝑅(𝐴, 𝐵) := 0, and similar
for 𝑃.)
• 𝐹𝛽(𝐴, 𝐵) :=
(︀
1 + 𝛽2)︀
𝑃(𝐴,𝐵)×𝑅(𝐴,𝐵)
𝛽2𝑃(𝐴,𝐵)+𝑅(𝐴,𝐵)
Then the metrics are deﬁned as:
average
Precision
Recall
F_beta
"micro"
𝑃(𝑦, ˆ𝑦)
𝑅(𝑦, ˆ𝑦)
𝐹𝛽(𝑦, ˆ𝑦)
"samples"
1
|𝑆|
∑︀
𝑠∈𝑆𝑃(𝑦𝑠, ˆ𝑦𝑠)
1
|𝑆|
∑︀
𝑠∈𝑆𝑅(𝑦𝑠, ˆ𝑦𝑠)
1
|𝑆|
∑︀
𝑠∈𝑆𝐹𝛽(𝑦𝑠, ˆ𝑦𝑠)
"macro"
1
|𝐿|
∑︀
𝑙∈𝐿𝑃(𝑦𝑙, ˆ𝑦𝑙)
1
|𝐿|
∑︀
𝑙∈𝐿𝑅(𝑦𝑙, ˆ𝑦𝑙)
1
|𝐿|
∑︀
𝑙∈𝐿𝐹𝛽(𝑦𝑙, ˆ𝑦𝑙)
"weighted"
1
∑︀
𝑙∈𝐿|^𝑦𝑙|
∑︀
𝑙∈𝐿|ˆ𝑦𝑙| 𝑃(𝑦𝑙, ˆ𝑦𝑙)
1
∑︀
𝑙∈𝐿|^𝑦𝑙|
∑︀
𝑙∈𝐿|ˆ𝑦𝑙| 𝑅(𝑦𝑙, ˆ𝑦𝑙)
1
∑︀
𝑙∈𝐿|^𝑦𝑙|
∑︀
𝑙∈𝐿|ˆ𝑦𝑙| 𝐹𝛽(𝑦𝑙, ˆ𝑦𝑙)
None
⟨𝑃(𝑦𝑙, ˆ𝑦𝑙)|𝑙∈𝐿⟩
⟨𝑅(𝑦𝑙, ˆ𝑦𝑙)|𝑙∈𝐿⟩
⟨𝐹𝛽(𝑦𝑙, ˆ𝑦𝑙)|𝑙∈𝐿⟩
>>> from sklearn import metrics
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> metrics.precision_score(y_true, y_pred, average='macro')
3.3. Model selection and evaluation
463
scikit-learn user guide, Release 0.18.2
0.22...
>>> metrics.recall_score(y_true, y_pred, average='micro')
...
0.33...
>>> metrics.f1_score(y_true, y_pred, average='weighted')
0.26...
>>> metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)
0.23...
>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)
...
(array([ 0.66...,
0.
,
0.
]), array([ 1.,
0.,
0.]), array([ 0.71...,
˓→
0.
,
0.
]), array([2, 2, 2]...))
For multiclass classiﬁcation with a “negative class”, it is possible to exclude some labels:
>>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')
... # excluding 0, no labels were correctly recalled
0.0
Similarly, labels not present in the data sample may be accounted for in macro-averaging.
>>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')
...
0.166...
Hinge loss
The hinge_loss function computes the average distance between the model and the data using hinge loss, a one-
sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classiﬁers such as support
vector machines.)
If the labels are encoded with +1 and -1, 𝑦: is the true value, and 𝑤is the predicted decisions as output by
decision_function, then the hinge loss is deﬁned as:
𝐿Hinge(𝑦, 𝑤) = max {1 −𝑤𝑦, 0} = |1 −𝑤𝑦|+
If there are more than two labels, hinge_loss uses a multiclass variant due to Crammer & Singer. Here is the paper
describing it.
If 𝑦𝑤is the predicted decision for true label and 𝑦𝑡is the maximum of the predicted decisions for all other labels,
where predicted decisions are output by decision function, then multiclass hinge loss is deﬁned by:
𝐿Hinge(𝑦𝑤, 𝑦𝑡) = max {1 + 𝑦𝑡−𝑦𝑤, 0}
Here a small example demonstrating the use of the hinge_loss function with a svm classiﬁer in a binary class
problem:
>>> from sklearn import svm
>>> from sklearn.metrics import hinge_loss
>>> X = [[0], [1]]
>>> y = [-1, 1]
>>> est = svm.LinearSVC(random_state=0)
>>> est.fit(X, y)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
intercept_scaling=1, loss='squared_hinge', max_iter=1000,
multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
464
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
verbose=0)
>>> pred_decision = est.decision_function([[-2], [3], [0.5]])
>>> pred_decision
array([-2.18...,
2.36...,
0.09...])
>>> hinge_loss([-1, 1, 1], pred_decision)
0.3...
Here is an example demonstrating the use of the hinge_loss function with a svm classiﬁer in a multiclass problem:
>>> X = np.array([[0], [1], [2], [3]])
>>> Y = np.array([0, 1, 2, 3])
>>> labels = np.array([0, 1, 2, 3])
>>> est = svm.LinearSVC()
>>> est.fit(X, Y)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
intercept_scaling=1, loss='squared_hinge', max_iter=1000,
multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
verbose=0)
>>> pred_decision = est.decision_function([[-1], [2], [3]])
>>> y_true = [0, 2, 3]
>>> hinge_loss(y_true, pred_decision, labels)
0.56...
Log loss
Log loss, also called logistic regression loss or cross-entropy loss, is deﬁned on probability estimates. It is commonly
used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization,
and can be used to evaluate the probability outputs (predict_proba) of a classiﬁer instead of its discrete predic-
tions.
For binary classiﬁcation with a true label 𝑦∈{0, 1} and a probability estimate 𝑝= Pr(𝑦= 1), the log loss per sample
is the negative log-likelihood of the classiﬁer given the true label:
𝐿log(𝑦, 𝑝) = −log Pr(𝑦|𝑝) = −(𝑦log(𝑝) + (1 −𝑦) log(1 −𝑝))
This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary
indicator matrix 𝑌, i.e., 𝑦𝑖,𝑘= 1 if sample 𝑖has label 𝑘taken from a set of 𝐾labels. Let 𝑃be a matrix of probability
estimates, with 𝑝𝑖,𝑘= Pr(𝑡𝑖,𝑘= 1). Then the log loss of the whole set is
𝐿log(𝑌, 𝑃) = −log Pr(𝑌|𝑃) = −1
𝑁
𝑁−1
∑︁
𝑖=0
𝐾−1
∑︁
𝑘=0
𝑦𝑖,𝑘log 𝑝𝑖,𝑘
To see how this generalizes the binary log loss given above, note that in the binary case, 𝑝𝑖,0 = 1 −𝑝𝑖,1 and 𝑦𝑖,0 =
1 −𝑦𝑖,1, so expanding the inner sum over 𝑦𝑖,𝑘∈{0, 1} gives the binary log loss.
The log_loss function computes log loss given a list of ground-truth labels and a probability matrix, as returned by
an estimator’s predict_proba method.
>>> from sklearn.metrics import log_loss
>>> y_true = [0, 0, 1, 1]
>>> y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]
>>> log_loss(y_true, y_pred)
0.1738...
The ﬁrst [.9,.1] in y_pred denotes 90% probability that the ﬁrst sample has label 0. The log loss is non-negative.
3.3. Model selection and evaluation
465
scikit-learn user guide, Release 0.18.2
Matthews correlation coefﬁcient
The matthews_corrcoef function computes the Matthew’s correlation coefﬁcient (MCC) for binary classes.
Quoting Wikipedia:
“The Matthews correlation coefﬁcient is used in machine learning as a measure of the quality of binary
(two-class) classiﬁcations. It takes into account true and false positives and negatives and is generally
regarded as a balanced measure which can be used even if the classes are of very different sizes. The
MCC is in essence a correlation coefﬁcient value between -1 and +1. A coefﬁcient of +1 represents
a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also
known as the phi coefﬁcient.”
If 𝑡𝑝, 𝑡𝑛, 𝑓𝑝and 𝑓𝑛are respectively the number of true positives, true negatives, false positives and false negatives,
the MCC coefﬁcient is deﬁned as
𝑀𝐶𝐶=
𝑡𝑝× 𝑡𝑛−𝑓𝑝× 𝑓𝑛
√︀
(𝑡𝑝+ 𝑓𝑝)(𝑡𝑝+ 𝑓𝑛)(𝑡𝑛+ 𝑓𝑝)(𝑡𝑛+ 𝑓𝑛)
.
Here is a small example illustrating the usage of the matthews_corrcoef function:
>>> from sklearn.metrics import matthews_corrcoef
>>> y_true = [+1, +1, +1, -1]
>>> y_pred = [+1, -1, +1, +1]
>>> matthews_corrcoef(y_true, y_pred)
-0.33...
Receiver operating characteristic (ROC)
The function roc_curve computes the receiver operating characteristic curve, or ROC curve. Quoting Wikipedia :
“A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates
the performance of a binary classiﬁer system as its discrimination threshold is varied. It is created by
plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false
positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known
as sensitivity, and FPR is one minus the speciﬁcity or true negative rate.”
This function requires the true binary value and the target scores, which can either be probability estimates of the
positive class, conﬁdence values, or binary decisions. Here is a small example of how to use the roc_curve function:
>>> import numpy as np
>>> from sklearn.metrics import roc_curve
>>> y = np.array([1, 1, 2, 2])
>>> scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)
>>> fpr
array([ 0. ,
0.5,
0.5,
1. ])
>>> tpr
array([ 0.5,
0.5,
1. ,
1. ])
>>> thresholds
array([ 0.8 ,
0.4 ,
0.35,
0.1 ])
466
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
This ﬁgure shows an example of such an ROC curve:
The roc_auc_score function computes the area under the receiver operating characteristic (ROC) curve, which is
also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in
one number. For more information see the Wikipedia article on AUC.
>>> import numpy as np
>>> from sklearn.metrics import roc_auc_score
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> roc_auc_score(y_true, y_scores)
0.75
In multi-label classiﬁcation, the roc_auc_score function is extended by averaging over the labels as above.
Compared to metrics such as the subset accuracy, the Hamming loss, or the F1 score, ROC doesn’t require optimizing a
threshold for each label. The roc_auc_score function can also be used in multi-class classiﬁcation, if the predicted
3.3. Model selection and evaluation
467
scikit-learn user guide, Release 0.18.2
outputs have been binarized.
Examples:
• See Receiver Operating Characteristic (ROC) for an example of using ROC to evaluate the quality of the
output of a classiﬁer.
• See Receiver Operating Characteristic (ROC) with cross validation for an example of using ROC to evaluate
classiﬁer output quality, using cross-validation.
• See Species distribution modeling for an example of using ROC to model species distribution.
Zero one loss
The zero_one_loss function computes the sum or the average of the 0-1 classiﬁcation loss (𝐿0−1) over 𝑛samples.
By default, the function normalizes over the sample. To get the sum of the 𝐿0−1, set normalize to False.
In multilabel classiﬁcation, the zero_one_loss scores a subset as one if its labels strictly match the predictions,
and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets.
To get the count of such subsets instead, set normalize to False
If ˆ𝑦𝑖is the predicted value of the 𝑖-th sample and 𝑦𝑖is the corresponding true value, then the 0-1 loss 𝐿0−1 is deﬁned
as:
𝐿0−1(𝑦𝑖, ˆ𝑦𝑖) = 1(ˆ𝑦𝑖̸= 𝑦𝑖)
where 1(𝑥) is the indicator function.
>>> from sklearn.metrics import zero_one_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> zero_one_loss(y_true, y_pred)
0.25
468
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> zero_one_loss(y_true, y_pred, normalize=False)
1
In the multilabel case with binary label indicators, where the ﬁrst label set [0,1] has an error:
>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5
>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),
normalize=False)
1
Example:
• See Recursive feature elimination with cross-validation for an example of zero one loss usage to perform
recursive feature elimination with cross-validation.
Brier score loss
The brier_score_loss function computes the Brier score for binary classes. Quoting Wikipedia:
“The Brier score is a proper score function that measures the accuracy of probabilistic predictions. It is
applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete
outcomes.”
This function returns a score of the mean square difference between the actual outcome and the predicted probability
of the possible outcome. The actual outcome has to be 1 or 0 (true or false), while the predicted probability of the
actual outcome can be a value between 0 and 1.
The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more
accurate the prediction is. It can be thought of as a measure of the “calibration” of a set of probabilistic predictions.
𝐵𝑆= 1
𝑁
𝑁
∑︁
𝑡=1
(𝑓𝑡−𝑜𝑡)2
where : 𝑁is the total number of predictions, 𝑓𝑡is the predicted probablity of the actual outcome 𝑜𝑡.
Here is a small example of usage of this function::
>>> import numpy as np
>>> from sklearn.metrics import brier_score_loss
>>> y_true = np.array([0, 1, 1, 0])
>>> y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
>>> y_prob = np.array([0.1, 0.9, 0.8, 0.4])
>>> y_pred = np.array([0, 1, 1, 0])
>>> brier_score_loss(y_true, y_prob)
0.055
>>> brier_score_loss(y_true, 1-y_prob, pos_label=0)
0.055
>>> brier_score_loss(y_true_categorical, y_prob, pos_label="ham")
0.055
>>> brier_score_loss(y_true, y_prob > 0.5)
0.0
3.3. Model selection and evaluation
469
scikit-learn user guide, Release 0.18.2
Example:
• See Probability calibration of classiﬁers for an example of Brier score loss usage to perform probability
calibration of classiﬁers.
References:
• G. Brier, Veriﬁcation of forecasts expressed in terms of probability, Monthly weather review 78.1 (1950)
Multilabel ranking metrics
In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give
high scores and better rank to the ground truth labels.
Coverage error
The coverage_error function computes the average number of labels that have to be included in the ﬁnal predic-
tion such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have
to predict in average without missing any true one. The best value of this metrics is thus the average number of true
labels.
Formally, given a binary indicator matrix of the ground truth labels 𝑦∈{0, 1}𝑛samples×𝑛labels and the score associated
with each label ˆ𝑓∈R𝑛samples×𝑛labels, the coverage is deﬁned as
𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒(𝑦, ˆ𝑓) =
1
𝑛samples
𝑛samples−1
∑︁
𝑖=0
max
𝑗:𝑦𝑖𝑗=1 rank𝑖𝑗
with rank𝑖𝑗=
⃒⃒⃒
{︁
𝑘: ˆ𝑓𝑖𝑘≥ˆ𝑓𝑖𝑗
}︁⃒⃒⃒. Given the rank deﬁnition, ties in y_scores are broken by giving the maximal rank
that would have been assigned to all tied values.
Here is a small example of usage of this function:
>>> import numpy as np
>>> from sklearn.metrics import coverage_error
>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> coverage_error(y_true, y_score)
2.5
Label ranking average precision
The label_ranking_average_precision_score function implements label ranking average precision
(LRAP). This metric is linked to the average_precision_score function, but is based on the notion of la-
bel ranking instead of precision and recall.
Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the
ratio of true vs. total labels with lower score. This metric will yield better scores if you are able to give better rank to
the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1.
If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal
rank.
470
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Formally, given a binary indicator matrix of the ground truth labels 𝑦∈ℛ𝑛samples×𝑛labels and the score associated with
each label ˆ𝑓∈ℛ𝑛samples×𝑛labels, the average precision is deﬁned as
𝐿𝑅𝐴𝑃(𝑦, ˆ𝑓) =
1
𝑛samples
𝑛samples−1
∑︁
𝑖=0
1
|𝑦𝑖|
∑︁
𝑗:𝑦𝑖𝑗=1
|ℒ𝑖𝑗|
rank𝑖𝑗
with ℒ𝑖𝑗=
{︁
𝑘: 𝑦𝑖𝑘= 1, ˆ𝑓𝑖𝑘≥ˆ𝑓𝑖𝑗
}︁
, rank𝑖𝑗=
⃒⃒⃒
{︁
𝑘: ˆ𝑓𝑖𝑘≥ˆ𝑓𝑖𝑗
}︁⃒⃒⃒and | · | is the l0 norm or the cardinality of the set.
Here is a small example of usage of this function:
>>> import numpy as np
>>> from sklearn.metrics import label_ranking_average_precision_score
>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> label_ranking_average_precision_score(y_true, y_score)
0.416...
Ranking loss
The label_ranking_loss function computes the ranking loss which averages over the samples the number of
label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse
number of false and true labels. The lowest achievable ranking loss is zero.
Formally, given a binary indicator matrix of the ground truth labels 𝑦∈{0, 1}𝑛samples×𝑛labels and the score associated
with each label ˆ𝑓∈R𝑛samples×𝑛labels, the ranking loss is deﬁned as
ranking_loss(𝑦, ˆ𝑓) =
1
𝑛samples
𝑛samples−1
∑︁
𝑖=0
1
|𝑦𝑖|(𝑛labels −|𝑦𝑖|)
⃒⃒⃒
{︁
(𝑘, 𝑙) : ˆ𝑓𝑖𝑘< ˆ𝑓𝑖𝑙, 𝑦𝑖𝑘= 1, 𝑦𝑖𝑙= 0
}︁⃒⃒⃒
where | · | is the ℓ0 norm or the cardinality of the set.
Here is a small example of usage of this function:
>>> import numpy as np
>>> from sklearn.metrics import label_ranking_loss
>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> label_ranking_loss(y_true, y_score)
0.75...
>>> # With the following prediction, we have perfect and minimal loss
>>> y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])
>>> label_ranking_loss(y_true, y_score)
0.0
Regression metrics
The sklearn.metrics module implements several loss, score, and utility functions to measure regression
performance.
Some of those have been enhanced to handle the multioutput case: mean_squared_error,
mean_absolute_error, explained_variance_score and r2_score.
These functions have an multioutput keyword argument which speciﬁes the way the scores or losses for each
individual target should be averaged. The default is 'uniform_average', which speciﬁes a uniformly weighted
mean over outputs. If an ndarray of shape (n_outputs,) is passed, then its entries are interpreted as weights
3.3. Model selection and evaluation
471
scikit-learn user guide, Release 0.18.2
and an according weighted average is returned. If multioutput is 'raw_values' is speciﬁed, then all unaltered
individual scores or losses will be returned in an array of shape (n_outputs,).
The r2_score and explained_variance_score accept an additional value 'variance_weighted' for
the multioutput parameter. This option leads to a weighting of each individual score by the variance of the
corresponding target variable. This setting quantiﬁes the globally captured unscaled variance. If the target vari-
ables are of different scale, then this score puts more importance on well explaining the higher variance variables.
multioutput='variance_weighted' is the default value for r2_score for backward compatibility. This
will be changed to uniform_average in the future.
Explained variance score
The explained_variance_score computes the explained variance regression score.
If ˆ𝑦is the estimated target output, 𝑦the corresponding (correct) target output, and 𝑉𝑎𝑟is Variance, the square of the
standard deviation, then the explained variance is estimated as follow:
explained_variance(𝑦, ˆ𝑦) = 1 −𝑉𝑎𝑟{𝑦−ˆ𝑦}
𝑉𝑎𝑟{𝑦}
The best possible score is 1.0, lower values are worse.
Here is a small example of usage of the explained_variance_score function:
>>> from sklearn.metrics import explained_variance_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> explained_variance_score(y_true, y_pred)
0.957...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> explained_variance_score(y_true, y_pred, multioutput='raw_values')
...
array([ 0.967...,
1.
])
>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])
...
0.990...
Mean absolute error
The mean_absolute_error function computes mean absolute error, a risk metric corresponding to the expected
value of the absolute error loss or 𝑙1-norm loss.
If ˆ𝑦𝑖is the predicted value of the 𝑖-th sample, and 𝑦𝑖is the corresponding true value, then the mean absolute error
(MAE) estimated over 𝑛samples is deﬁned as
MAE(𝑦, ˆ𝑦) =
1
𝑛samples
𝑛samples−1
∑︁
𝑖=0
|𝑦𝑖−ˆ𝑦𝑖| .
Here is a small example of usage of the mean_absolute_error function:
>>> from sklearn.metrics import mean_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_error(y_true, y_pred)
0.5
472
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_absolute_error(y_true, y_pred)
0.75
>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')
array([ 0.5,
1. ])
>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
...
0.849...
Mean squared error
The mean_squared_error function computes mean square error, a risk metric corresponding to the expected
value of the squared (quadratic) error loss or loss.
If ˆ𝑦𝑖is the predicted value of the 𝑖-th sample, and 𝑦𝑖is the corresponding true value, then the mean squared error
(MSE) estimated over 𝑛samples is deﬁned as
MSE(𝑦, ˆ𝑦) =
1
𝑛samples
𝑛samples−1
∑︁
𝑖=0
(𝑦𝑖−ˆ𝑦𝑖)2.
Here is a small example of usage of the mean_squared_error function:
>>> from sklearn.metrics import mean_squared_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_squared_error(y_true, y_pred)
0.375
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_squared_error(y_true, y_pred)
0.7083...
Examples:
• See Gradient Boosting regression for an example of mean squared error usage to evaluate gradient boosting
regression.
Median absolute error
The median_absolute_error is particularly interesting because it is robust to outliers. The loss is calculated by
taking the median of all absolute differences between the target and the prediction.
If ˆ𝑦𝑖is the predicted value of the 𝑖-th sample and 𝑦𝑖is the corresponding true value, then the median absolute error
(MedAE) estimated over 𝑛samples is deﬁned as
MedAE(𝑦, ˆ𝑦) = median(| 𝑦1 −ˆ𝑦1 |, . . . , | 𝑦𝑛−ˆ𝑦𝑛|).
The median_absolute_error does not support multioutput.
Here is a small example of usage of the median_absolute_error function:
3.3. Model selection and evaluation
473
scikit-learn user guide, Release 0.18.2
>>> from sklearn.metrics import median_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> median_absolute_error(y_true, y_pred)
0.5
R2 score, the coefﬁcient of determination
The r2_score function computes R2, the coefﬁcient of determination. It provides a measure of how well future
samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model
can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.
If ˆ𝑦𝑖is the predicted value of the 𝑖-th sample and 𝑦𝑖is the corresponding true value, then the score R2 estimated over
𝑛samples is deﬁned as
𝑅2(𝑦, ˆ𝑦) = 1 −
∑︀𝑛samples−1
𝑖=0
(𝑦𝑖−ˆ𝑦𝑖)2
∑︀𝑛samples−1
𝑖=0
(𝑦𝑖−¯𝑦)2
where ¯𝑦=
1
𝑛samples
∑︀𝑛samples−1
𝑖=0
𝑦𝑖.
Here is a small example of usage of the r2_score function:
>>> from sklearn.metrics import r2_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> r2_score(y_true, y_pred)
0.948...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred, multioutput='variance_weighted')
...
0.938...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred, multioutput='uniform_average')
...
0.936...
>>> r2_score(y_true, y_pred, multioutput='raw_values')
...
array([ 0.965...,
0.908...])
>>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])
...
0.925...
Example:
• See Lasso and Elastic Net for Sparse Signals for an example of R2 score usage to evaluate Lasso and Elastic
Net on sparse signals.
474
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Clustering metrics
The sklearn.metrics module implements several loss, score, and utility functions. For more information see the
Clustering performance evaluation section for instance clustering, and Biclustering evaluation for biclustering.
Dummy estimators
When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of
thumb. DummyClassifier implements several such simple strategies for classiﬁcation:
• stratified generates random predictions by respecting the training set class distribution.
• most_frequent always predicts the most frequent label in the training set.
• prior always predicts the class that maximizes the class prior (like most_frequent`) and
``predict_proba returns the class prior.
• uniform generates predictions uniformly at random.
• constant always predicts a constant label that is provided by the user. A
major
motivation
of
this
method is F1-scoring, when the positive class is in the minority.
Note that with all these strategies, the predict method completely ignores the input data!
To illustrate DummyClassifier, ﬁrst let’s create an imbalanced dataset:
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> y[y != 1] = -1
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
Next, let’s compare the accuracy of SVC and most_frequent:
>>> from sklearn.dummy import DummyClassifier
>>> from sklearn.svm import SVC
>>> clf = SVC(kernel='linear', C=1).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.63...
>>> clf = DummyClassifier(strategy='most_frequent',random_state=0)
>>> clf.fit(X_train, y_train)
DummyClassifier(constant=None, random_state=0, strategy='most_frequent')
>>> clf.score(X_test, y_test)
0.57...
We see that SVC doesn’t do much better than a dummy classiﬁer. Now, let’s change the kernel:
>>> clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.97...
We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better
estimate of the accuracy, if it is not too CPU costly. For more information see the Cross-validation: evaluating
estimator performance section. Moreover if you want to optimize over the parameter space, it is highly recommended
to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator section for details.
More generally, when the accuracy of a classiﬁer is too close to random, it probably means that something went wrong:
features are not helpful, a hyperparameter is not correctly tuned, the classiﬁer is suffering from class imbalance, etc...
3.3. Model selection and evaluation
475
scikit-learn user guide, Release 0.18.2
DummyRegressor also implements four simple rules of thumb for regression:
• mean always predicts the mean of the training targets.
• median always predicts the median of the training targets.
• quantile always predicts a user provided quantile of the training targets.
• constant always predicts a constant value that is provided by the user.
In all these strategies, the predict method completely ignores the input data.
3.3.4 Model persistence
After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to
retrain. The following section gives you an example of how to persist a model with pickle. We’ll also review a few
security and maintainability issues when working with pickle serialization.
Persistence example
It is possible to save a model in the scikit by using Python’s built-in persistence model, namely pickle:
>>> from sklearn import svm
>>> from sklearn import datasets
>>> clf = svm.SVC()
>>> iris = datasets.load_iris()
>>> X, y = iris.data, iris.target
>>> clf.fit(X, y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
>>> import pickle
>>> s = pickle.dumps(clf)
>>> clf2 = pickle.loads(s)
>>> clf2.predict(X[0:1])
array([0])
>>> y[0]
0
In the speciﬁc case of the scikit, it may be more interesting to use joblib’s replacement of pickle (joblib.dump &
joblib.load), which is more efﬁcient on objects that carry large numpy arrays internally as is often the case for
ﬁtted scikit-learn estimators, but can only pickle to the disk and not to a string:
>>> from sklearn.externals import joblib
>>> joblib.dump(clf, 'filename.pkl')
Later you can load back the pickled model (possibly in another Python process) with:
>>> clf = joblib.load('filename.pkl')
Note: joblib.dump and joblib.load functions also accept ﬁle-like object instead of ﬁlenames. More infor-
mation on data persistence with Joblib is available here.
476
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Security & maintainability limitations
pickle (and joblib by extension), has some issues regarding maintainability and security. Because of this,
• Never unpickle untrusted data as it could lead to malicious code being executed upon loading.
• While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported
and inadvisable. It should also be kept in mind that operations performed on such data could give different and
unexpected results.
In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the
pickled model:
• The training data, e.g. a reference to a immutable snapshot
• The python source code used to generate the model
• The versions of scikit-learn and its dependencies
• The cross validation score obtained on the training data
This should make it possible to check that the cross-validation score is in the same range as before.
If you want to know more about these issues and explore other possible serialization methods, please refer to this talk
by Alex Gaynor.
3.3.5 Validation curves: plotting scores to evaluate models
Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias,
variance and noise. The bias of an estimator is its average error for different training sets. The variance of an
estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.
In the following plot, we see a function 𝑓(𝑥) = cos( 3
2𝜋𝑥) and some noisy samples from that function. We use three
different estimators to ﬁt the function: linear regression with polynomial features of degree 1, 4 and 15. We see that
the ﬁrst estimator can at best provide only a poor ﬁt to the samples and the true function because it is too simple
(high bias), the second estimator approximates it almost perfectly and the last estimator approximates the training data
perfectly but does not ﬁt the true function very well, i.e. it is very sensitive to varying training data (high variance).
Bias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyper-
parameters so that both bias and variance are as low as possible (see Bias-variance dilemma). Another way to reduce
the variance of a model is to use more training data. However, you should only collect more training data if the true
function is too complex to be approximated by an estimator with a lower variance.
In the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers
from bias or variance. However, in high-dimensional spaces, models can become very difﬁcult to visualize. For this
reason, it is often helpful to use the tools described below.
Examples:
3.3. Model selection and evaluation
477
scikit-learn user guide, Release 0.18.2
• Underﬁtting vs. Overﬁtting
• Plotting Validation Curves
• Plotting Learning Curves
Validation curve
To validate a model we need a scoring function (see Model evaluation: quantifying the quality of predictions), for
example accuracy for classiﬁers. The proper way of choosing multiple hyperparameters of an estimator are of course
grid search or similar methods (see Tuning the hyper-parameters of an estimator) that select the hyperparameter with
the maximum score on a validation set or multiple validation sets. Note that if we optimized the hyperparameters
based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To
get a proper estimate of the generalization we have to compute the score on another test set.
However, it is sometimes helpful to plot the inﬂuence of a single hyperparameter on the training score and the valida-
tion score to ﬁnd out whether the estimator is overﬁtting or underﬁtting for some hyperparameter values.
The function validation_curve can help in this case:
>>> import numpy as np
>>> from sklearn.model_selection import validation_curve
>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import Ridge
>>> np.random.seed(0)
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> indices = np.arange(y.shape[0])
>>> np.random.shuffle(indices)
>>> X, y = X[indices], y[indices]
>>> train_scores, valid_scores = validation_curve(Ridge(), X, y, "alpha",
...
np.logspace(-7, 3, 3))
>>> train_scores
array([[ 0.94...,
0.92...,
0.92...],
[ 0.94...,
0.92...,
0.92...],
[ 0.47...,
0.45...,
0.42...]])
>>> valid_scores
array([[ 0.90...,
0.92...,
0.94...],
[ 0.90...,
0.92...,
0.94...],
[ 0.44...,
0.39...,
0.45...]])
If the training score and the validation score are both low, the estimator will be underﬁtting. If the training score is
high and the validation score is low, the estimator is overﬁtting and otherwise it is working very well. A low training
score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary
the parameter 𝛾of an SVM on the digits dataset.
Learning curve
A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It
is a tool to ﬁnd out how much we beneﬁt from adding more training data and whether the estimator suffers more from
a variance error or a bias error. If both the validation score and the training score converge to a value that is too low
with increasing size of the training set, we will not beneﬁt much from more training data. In the following plot you
can see an example: naive Bayes roughly converges to a low score.
478
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.3. Model selection and evaluation
479
scikit-learn user guide, Release 0.18.2
We will probably have to use an estimator or a parametrization of the current estimator that can learn more complex
concepts (i.e. has a lower bias). If the training score is much greater than the validation score for the maximum number
of training samples, adding more training samples will most likely increase generalization. In the following plot you
can see that the SVM could beneﬁt from more training examples.
We can use the function learning_curve to generate the values that are required to plot such a learning curve
(number of samples that have been used, the average scores on the training sets and the average scores on the validation
sets):
>>> from sklearn.model_selection import learning_curve
>>> from sklearn.svm import SVC
>>> train_sizes, train_scores, valid_scores = learning_curve(
...
SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)
>>> train_sizes
array([ 50, 80, 110])
>>> train_scores
array([[ 0.98...,
0.98 ,
0.98...,
0.98...,
0.98...],
[ 0.98...,
1.
,
0.98...,
0.98...,
0.98...],
[ 0.98...,
1.
,
0.98...,
0.98...,
0.99...]])
>>> valid_scores
array([[ 1. ,
0.93...,
1. ,
1. ,
0.96...],
[ 1. ,
0.96...,
1. ,
1. ,
0.96...],
[ 1. ,
0.96...,
1. ,
1. ,
0.96...]])
3.4 Dataset transformations
scikit-learn provides a library of transformers, which may clean (see Preprocessing data), reduce (see Unsupervised
dimensionality reduction), expand (see Kernel Approximation) or generate (see Feature extraction) feature representa-
tions.
Like other estimators, these are represented by classes with a fit method, which learns model parameters (e.g.
mean and standard deviation for normalization) from a training set, and a transform method which applies this
transformation model to unseen data. fit_transform may be more convenient and efﬁcient for modelling and
transforming the training data simultaneously.
Combining such transformers, either in parallel or series is covered in Pipeline and FeatureUnion: combining es-
timators. Pairwise metrics, Afﬁnities and Kernels covers transforming feature spaces into afﬁnity matrices, while
480
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Transforming the prediction target (y) considers transformations of the target space (e.g. categorical labels) for use in
scikit-learn.
3.4.1 Pipeline and FeatureUnion: combining estimators
Pipeline: chaining estimators
Pipeline can be used to chain multiple estimators into one. This is useful as there is often a ﬁxed sequence of
steps in processing the data, for example feature selection, normalization and classiﬁcation. Pipeline serves two
purposes here:
Convenience: You only have to call fit and predict once on your data to ﬁt a whole sequence of
estimators.
Joint parameter selection: You can grid search over parameters of all estimators in the pipeline at once.
All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The
last estimator may be any type (transformer, classiﬁer, etc.).
Usage
The Pipeline is built using a list of (key,value) pairs, where the key is a string containing the name you want
to give this step and value is an estimator object:
>>> from sklearn.pipeline import Pipeline
>>> from sklearn.svm import SVC
>>> from sklearn.decomposition import PCA
>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]
>>> pipe = Pipeline(estimators)
>>> pipe
Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',
n_components=None, random_state=None, svd_solver='auto', tol=0.0,
whiten=False)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None,
coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',
kernel='rbf', max_iter=-1, probability=False, random_state=None,
shrinking=True, tol=0.001, verbose=False))])
The utility function make_pipeline is a shorthand for constructing pipelines; it takes a variable number of estima-
tors and returns a pipeline, ﬁlling in the names automatically:
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn.preprocessing import Binarizer
>>> make_pipeline(Binarizer(), MultinomialNB())
Pipeline(steps=[('binarizer', Binarizer(copy=True, threshold=0.0)),
('multinomialnb', MultinomialNB(alpha=1.0,
class_prior=None,
fit_prior=True))])
The estimators of a pipeline are stored as a list in the steps attribute:
>>> pipe.steps[0]
('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_
˓→state=None,
svd_solver='auto', tol=0.0, whiten=False))
3.4. Dataset transformations
481
scikit-learn user guide, Release 0.18.2
and as a dict in named_steps:
>>> pipe.named_steps['reduce_dim']
PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
svd_solver='auto', tol=0.0, whiten=False)
Parameters of the estimators in the pipeline can be accessed using the <estimator>__<parameter> syntax:
>>> pipe.set_params(clf__C=10)
Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',
n_components=None, random_state=None, svd_solver='auto', tol=0.0,
whiten=False)), ('clf', SVC(C=10, cache_size=200, class_weight=None,
coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',
kernel='rbf', max_iter=-1, probability=False, random_state=None,
shrinking=True, tol=0.001, verbose=False))])
This is particularly important for doing grid searches:
>>> from sklearn.model_selection import GridSearchCV
>>> params = dict(reduce_dim__n_components=[2, 5, 10],
...
clf__C=[0.1, 10, 100])
>>> grid_search = GridSearchCV(pipe, param_grid=params)
Individual steps may also be replaced as parameters, and non-ﬁnal steps may be ignored by setting them to None:
>>> from sklearn.linear_model import LogisticRegression
>>> params = dict(reduce_dim=[None, PCA(5), PCA(10)],
...
clf=[SVC(), LogisticRegression()],
...
clf__C=[0.1, 10, 100])
>>> grid_search = GridSearchCV(pipe, param_grid=params)
Examples:
• Pipeline Anova SVM
• Sample pipeline for text feature extraction and evaluation
• Pipelining: chaining a PCA and a logistic regression
• Explicit feature map approximation for RBF kernels
• SVM-Anova: SVM with univariate feature selection
See also:
• Tuning the hyper-parameters of an estimator
Notes
Calling fit on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it
on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator
is a classiﬁer, the Pipeline can be used as a classiﬁer. If the last estimator is a transformer, again, so is the pipeline.
482
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
FeatureUnion: composite feature spaces
FeatureUnion combines several transformer objects into a new transformer that combines their output.
A
FeatureUnion takes a list of transformer objects. During ﬁtting, each of these is ﬁt to the data independently.
For transforming data, the transformers are applied in parallel, and the sample vectors they output are concatenated
end-to-end into larger vectors.
FeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and valida-
tion.
FeatureUnion and Pipeline can be combined to create complex models.
(A FeatureUnion has no way of checking whether two transformers might produce identical features. It only
produces a union when the feature sets are disjoint, and making sure they are the caller’s responsibility.)
Usage
A FeatureUnion is built using a list of (key,value) pairs, where the key is the name you want to give to a
given transformation (an arbitrary string; it only serves as an identiﬁer) and value is an estimator object:
>>> from sklearn.pipeline import FeatureUnion
>>> from sklearn.decomposition import PCA
>>> from sklearn.decomposition import KernelPCA
>>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]
>>> combined = FeatureUnion(estimators)
>>> combined
FeatureUnion(n_jobs=1, transformer_list=[('linear_pca', PCA(copy=True,
iterated_power='auto', n_components=None, random_state=None,
svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca',
KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3,
eigen_solver='auto', fit_inverse_transform=False, gamma=None,
kernel='linear', kernel_params=None, max_iter=None, n_components=None,
n_jobs=1, random_state=None, remove_zero_eig=False, tol=0))],
transformer_weights=None)
Like pipelines, feature unions have a shorthand constructor called make_union that does not require explicit naming
of the components.
Like Pipeline, individual steps may be replaced using set_params, and ignored by setting to None:
>>> combined.set_params(kernel_pca=None)
FeatureUnion(n_jobs=1, transformer_list=[('linear_pca', PCA(copy=True,
iterated_power='auto', n_components=None, random_state=None,
svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca', None)],
transformer_weights=None)
Examples:
• Concatenating multiple feature extraction methods
• Feature Union with Heterogeneous Data Sources
3.4. Dataset transformations
483
scikit-learn user guide, Release 0.18.2
3.4.2 Feature extraction
The sklearn.feature_extraction module can be used to extract features in a format supported by machine
learning algorithms from datasets consisting of formats such as text and image.
Note: Feature extraction is very different from Feature selection: the former consists in transforming arbitrary data,
such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique
applied on these features.
Loading features from dicts
The class DictVectorizer can be used to convert feature arrays represented as lists of standard Python dict
objects to the NumPy/SciPy representation used by scikit-learn estimators.
While not particularly fast to process, Python’s dict has the advantages of being convenient to use, being sparse
(absent features need not be stored) and storing feature names in addition to values.
DictVectorizer implements what is called one-of-K or “one-hot” coding for categorical (aka nominal, discrete)
features. Categorical features are “attribute-value” pairs where the value is restricted to a list of discrete of possibilities
without ordering (e.g. topic identiﬁers, types of objects, tags, names...).
In the following, “city” is a categorical attribute while “temperature” is a traditional numerical feature:
>>> measurements = [
...
{'city': 'Dubai', 'temperature': 33.},
...
{'city': 'London', 'temperature': 12.},
...
{'city': 'San Fransisco', 'temperature': 18.},
... ]
>>> from sklearn.feature_extraction import DictVectorizer
>>> vec = DictVectorizer()
>>> vec.fit_transform(measurements).toarray()
array([[
1.,
0.,
0.,
33.],
[
0.,
1.,
0.,
12.],
[
0.,
0.,
1.,
18.]])
>>> vec.get_feature_names()
['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']
DictVectorizer is also a useful representation transformation for training sequence classiﬁers in Natural Lan-
guage Processing models that typically work by extracting feature windows around a particular word of interest.
For example, suppose that we have a ﬁrst algorithm that extracts Part of Speech (PoS) tags that we want to use as
complementary tags for training a sequence classiﬁer (e.g. a chunker). The following dict could be such a window of
features extracted around the word ‘sat’ in the sentence ‘The cat sat on the mat.’:
>>> pos_window = [
...
{
...
'word-2': 'the',
...
'pos-2': 'DT',
...
'word-1': 'cat',
...
'pos-1': 'NN',
...
'word+1': 'on',
...
'pos+1': 'PP',
...
},
484
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
...
# in a real application one would extract many such dictionaries
... ]
This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classiﬁer (maybe
after being piped into a text.TfidfTransformer for normalization):
>>> vec = DictVectorizer()
>>> pos_vectorized = vec.fit_transform(pos_window)
>>> pos_vectorized
<1x6 sparse matrix of type '<... 'numpy.float64'>'
with 6 stored elements in Compressed Sparse ... format>
>>> pos_vectorized.toarray()
array([[ 1.,
1.,
1.,
1.,
1.,
1.]])
>>> vec.get_feature_names()
['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']
As you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting
matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to
make the resulting data structure able to ﬁt in memory the DictVectorizer class uses a scipy.sparse matrix
by default instead of a numpy.ndarray.
Feature hashing
The class FeatureHasher is a high-speed, low-memory vectorizer that uses a technique known as feature hashing,
or the “hashing trick”. Instead of building a hash table of the features encountered in training, as the vectorizers
do, instances of FeatureHasher apply a hash function to the features to determine their column index in sample
matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher
does not remember what the input features looked like and has no inverse_transform method.
Since the hash function might cause collisions between (unrelated) features, a signed hash function is used and the
sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions
are likely to cancel out rather than accumulate error, and the expected mean of any output feature’s value is zero.
If non_negative=True is passed to the constructor, the absolute value is taken. This undoes some of the collision
handling, but allows the output to be passed to estimators like sklearn.naive_bayes.MultinomialNB or
sklearn.feature_selection.chi2 feature selectors that expect non-negative inputs.
FeatureHasher accepts either mappings (like Python’s dict and its variants in the collections mod-
ule), (feature,value) pairs, or strings, depending on the constructor parameter input_type.
Map-
ping are treated as lists of (feature,value) pairs, while single strings have an implicit value of 1, so
['feat1','feat2','feat3'] is interpreted as [('feat1',1),('feat2',1),('feat3',1)]. If a
single feature occurs multiple times in a sample, the associated values will be summed (so ('feat',2) and
('feat',3.5) become ('feat',5.5)). The output from FeatureHasher is always a scipy.sparse
matrix in the CSR format.
Feature
hashing
can
be
employed
in
document
classiﬁcation,
but
unlike
text.CountVectorizer,
FeatureHasher does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see
Vectorizing a large text corpus with the hashing trick, below, for a combined tokenizer/hasher.
As an example, consider a word-level natural language processing task that needs features extracted from
(token,part_of_speech) pairs. One could use a Python generator function to extract features:
def token_features(token, part_of_speech):
if token.isdigit():
yield "numeric"
else:
yield "token={}".format(token.lower())
3.4. Dataset transformations
485
scikit-learn user guide, Release 0.18.2
yield "token,pos={},{}".format(token, part_of_speech)
if token[0].isupper():
yield "uppercase_initial"
if token.isupper():
yield "all_uppercase"
yield "pos={}".format(part_of_speech)
Then, the raw_X to be fed to FeatureHasher.transform can be constructed using:
raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)
and fed to a hasher with:
hasher = FeatureHasher(input_type='string')
X = hasher.transform(raw_X)
to get a scipy.sparse matrix X.
Note the use of a generator comprehension, which introduces laziness into the feature extraction: tokens are only
processed on demand from the hasher.
Implementation details
FeatureHasher uses the signed 32-bit variant of MurmurHash3.
As a result (and because of limitations in
scipy.sparse), the maximum number of features supported is currently 231 −1.
The original formulation of the hashing trick by Weinberger et al. used two separate hash functions ℎand 𝜉to deter-
mine the column index and sign of a feature, respectively. The present implementation works under the assumption
that the sign bit of MurmurHash3 is independent of its other bits.
Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two
as the n_features parameter; otherwise the features will not be mapped evenly to the columns.
References:
• Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). Feature hash-
ing for large scale multitask learning. Proc. ICML.
• MurmurHash3.
Text feature extraction
The Bag of Words representation
Text Analysis is a major application ﬁeld for machine learning algorithms. However the raw data, a sequence of
symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a
ﬁxed size rather than the raw text documents with variable length.
In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from
text content, namely:
• tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and
punctuation as token separators.
• counting the occurrences of tokens in each document.
486
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• normalizing and weighting with diminishing importance tokens that occur in the majority of samples / docu-
ments.
In this scheme, features and samples are deﬁned as follows:
• each individual token occurrence frequency (normalized or not) is treated as a feature.
• the vector of all the token frequencies for a given document is considered a multivariate sample.
A corpus of documents can thus be represented by a matrix with one row per document and one column per token
(e.g. word) occurring in the corpus.
We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This
speciﬁc strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” represen-
tation. Documents are described by word occurrences while completely ignoring the relative position information of
the words in the document.
Sparsity
As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will
have many feature values that are zeros (typically more than 99% of them).
For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order
of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.
In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, imple-
mentations will typically use a sparse representation such as the implementations available in the scipy.sparse
package.
Common Vectorizer usage
CountVectorizer implements both tokenization and occurrence counting in a single class:
>>> from sklearn.feature_extraction.text import CountVectorizer
This model has many parameters, however the default values are quite reasonable (please see the reference documen-
tation for the details):
>>> vectorizer = CountVectorizer(min_df=1)
>>> vectorizer
CountVectorizer(analyzer=...'word', binary=False, decode_error=...'strict',
dtype=<... 'numpy.int64'>, encoding=...'utf-8', input=...'content',
lowercase=True, max_df=1.0, max_features=None, min_df=1,
ngram_range=(1, 1), preprocessor=None, stop_words=None,
strip_accents=None, token_pattern=...'(?u)\\b\\w\\w+\\b',
tokenizer=None, vocabulary=None)
Let’s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:
>>> corpus = [
...
'This is the first document.',
...
'This is the second second document.',
...
'And the third one.',
...
'Is this the first document?',
... ]
>>> X = vectorizer.fit_transform(corpus)
>>> X
3.4. Dataset transformations
487
scikit-learn user guide, Release 0.18.2
<4x9 sparse matrix of type '<... 'numpy.int64'>'
with 19 stored elements in Compressed Sparse ... format>
The default conﬁguration tokenizes the string by extracting words of at least 2 letters. The speciﬁc function that does
this step can be requested explicitly:
>>> analyze = vectorizer.build_analyzer()
>>> analyze("This is a text document to analyze.") == (
...
['this', 'is', 'text', 'document', 'to', 'analyze'])
True
Each term found by the analyzer during the ﬁt is assigned a unique integer index corresponding to a column in the
resulting matrix. This interpretation of the columns can be retrieved as follows:
>>> vectorizer.get_feature_names() == (
...
['and', 'document', 'first', 'is', 'one',
...
'second', 'the', 'third', 'this'])
True
>>> X.toarray()
array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
[0, 1, 0, 1, 0, 2, 1, 0, 1],
[1, 0, 0, 0, 1, 0, 1, 1, 0],
[0, 1, 1, 1, 0, 0, 1, 0, 1]]...)
The converse mapping from feature name to column index is stored in the vocabulary_ attribute of the vectorizer:
>>> vectorizer.vocabulary_.get('document')
1
Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform
method:
>>> vectorizer.transform(['Something completely new.']).toarray()
...
array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)
Note that in the previous corpus, the ﬁrst and the last documents have exactly the same words hence are encoded in
equal vectors. In particular we lose the information that the last document is an interrogative form. To preserve some
of the local ordering information we can extract 2-grams of words in addition to the 1-grams (individual words):
>>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),
...
token_pattern=r'\b\w+\b', min_df=1)
>>> analyze = bigram_vectorizer.build_analyzer()
>>> analyze('Bi-grams are cool!') == (
...
['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])
True
The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local
positioning patterns:
>>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()
>>> X_2
...
array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],
[0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
488
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],
[0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)
In particular the interrogative form “Is this” is only present in the last document:
>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')
>>> X_2[:, feature_index]
array([0, 0, 0, 1]...)
Tf–idf term weighting
In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little
meaningful information about the actual contents of the document. If we were to feed the direct count data directly to
a classiﬁer those very frequent terms would shadow the frequencies of rarer yet more interesting terms.
In order to re-weight the count features into ﬂoating point values suitable for usage by a classiﬁer it is very common
to use the tf–idf transform.
Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency: tf-idf(t,d) =
tf(t,d) × idf(t).
Using the TfidfTransformer‘s default settings, TfidfTransformer(norm='l2',use_idf=True,smooth_idf=True,
the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which
is computed as
idf(𝑡) = 𝑙𝑜𝑔
1+𝑛𝑑
1+df(𝑑,𝑡) + 1,
where 𝑛𝑑is the total number of documents, and df(𝑑, 𝑡) is the number of documents that contain term 𝑡. The resulting
tf-idf vectors are then normalized by the Euclidean norm:
𝑣𝑛𝑜𝑟𝑚=
𝑣
||𝑣||2 =
𝑣
√
𝑣12+𝑣22+···+𝑣𝑛2 .
This was originally a term weighting scheme developed for information retrieval (as a ranking function for search
engines results) that has also found good use in document classiﬁcation and clustering.
The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly
and how the tf-idfs computed in scikit-learn’s TfidfTransformer and TfidfVectorizer differ slightly from
the standard textbook notation that deﬁnes the idf as
idf(𝑡) = 𝑙𝑜𝑔
𝑛𝑑
1+df(𝑑,𝑡).
In the TfidfTransformer and TfidfVectorizer with smooth_idf=False, the “1” count is added to the
idf instead of the idf’s denominator:
idf(𝑡) = 𝑙𝑜𝑔
𝑛𝑑
df(𝑑,𝑡) + 1
This normalization is implemented by the TfidfTransformer class:
>>> from sklearn.feature_extraction.text import TfidfTransformer
>>> transformer = TfidfTransformer(smooth_idf=False)
>>> transformer
TfidfTransformer(norm=...'l2', smooth_idf=False, sublinear_tf=False,
use_idf=True)
Again please see the reference documentation for the details on all the parameters.
Let’s take an example with the following counts. The ﬁrst term is present 100% of the time hence not very interesting.
The two other features only in less than 50% of the time hence probably more representative of the content of the
documents:
3.4. Dataset transformations
489
scikit-learn user guide, Release 0.18.2
>>> counts = [[3, 0, 1],
...
[2, 0, 0],
...
[3, 0, 0],
...
[4, 0, 0],
...
[3, 2, 0],
...
[3, 0, 2]]
...
>>> tfidf = transformer.fit_transform(counts)
>>> tfidf
<6x3 sparse matrix of type '<... 'numpy.float64'>'
with 9 stored elements in Compressed Sparse ... format>
>>> tfidf.toarray()
array([[ 0.81940995,
0.
,
0.57320793],
[ 1.
,
0.
,
0.
],
[ 1.
,
0.
,
0.
],
[ 1.
,
0.
,
0.
],
[ 0.47330339,
0.88089948,
0.
],
[ 0.58149261,
0.
,
0.81355169]])
Each row is normalized to have unit Euclidean norm:
𝑣𝑛𝑜𝑟𝑚=
𝑣
||𝑣||2 =
𝑣
√
𝑣12+𝑣22+···+𝑣𝑛2
For example, we can compute the tf-idf of the ﬁrst term in the ﬁrst document in the counts array as follows:
𝑛𝑑,term1 = 6
df(𝑑, 𝑡)term1 = 6
idf(𝑑, 𝑡)term1 = 𝑙𝑜𝑔
𝑛𝑑
df(𝑑,𝑡) + 1 = 𝑙𝑜𝑔(1) + 1 = 1
tf-idfterm1 = tf × idf = 3 × 1 = 3
Now, if we repeat this computation for the remaining 2 terms in the document, we get
tf-idfterm2 = 0 × 𝑙𝑜𝑔(6/1) + 1 = 0
tf-idfterm3 = 1 × 𝑙𝑜𝑔(6/2) + 1 ≈2.0986
and the vector of raw tf-idfs:
tf-idf𝑟𝑎𝑤= [3, 0, 2.0986].
Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:
[3,0,2.0986]
√︂(︀
32+02+2.09862)︀= [0.819, 0, 0.573].
Furthermore, the default parameter smooth_idf=True adds “1” to the numerator and denominator as if an extra
document was seen containing every term in the collection exactly once, which prevents zero divisions:
idf(𝑡) = 𝑙𝑜𝑔
1+𝑛𝑑
1+df(𝑑,𝑡) + 1
Using this modiﬁcation, the tf-idf of the third term in document 1 changes to 1.8473:
tf-idfterm3 = 1 × 𝑙𝑜𝑔(7/3) + 1 ≈1.8473
And the L2-normalized tf-idf changes to
[3,0,1.8473]
√︂(︀
32+02+1.84732)︀= [0.8515, 0, 0.5243]:
490
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> transformer = TfidfTransformer()
>>> transformer.fit_transform(counts).toarray()
array([[ 0.85151335,
0.
,
0.52433293],
[ 1.
,
0.
,
0.
],
[ 1.
,
0.
,
0.
],
[ 1.
,
0.
,
0.
],
[ 0.55422893,
0.83236428,
0.
],
[ 0.63035731,
0.
,
0.77630514]])
The weights of each feature computed by the fit method call are stored in a model attribute:
>>> transformer.idf_
array([ 1. ...,
2.25...,
1.84...])
As tf–idf is very often used for text features, there is also another class called TfidfVectorizer that combines all
the options of CountVectorizer and TfidfTransformer in a single model:
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> vectorizer = TfidfVectorizer(min_df=1)
>>> vectorizer.fit_transform(corpus)
...
<4x9 sparse matrix of type '<... 'numpy.float64'>'
with 19 stored elements in Compressed Sparse ... format>
While the tf–idf normalization is often very useful, there might be cases where the binary occurrence markers might
offer better features. This can be achieved by using the binary parameter of CountVectorizer. In particular,
some estimators such as Bernoulli Naive Bayes explicitly model discrete boolean random variables. Also, very short
texts are likely to have noisy tf–idf values while the binary occurrence info is more stable.
As usual the best way to adjust the feature extraction parameters is to use a cross-validated grid search, for instance by
pipelining the feature extractor with a classiﬁer:
• Sample pipeline for text feature extraction and evaluation
Decoding text ﬁles
Text is made of characters, but ﬁles are made of bytes. These bytes represent characters according to some encoding.
To work with text ﬁles in Python, their bytes must be decoded to a character set called Unicode. Common encodings
are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the universal encodings UTF-8 and UTF-16. Many
others exist.
Note: An encoding can also be called a ‘character set’, but this term is less accurate: several encodings can exist for
a single character set.
The text feature extractors in scikit-learn know how to decode text ﬁles, but only if you tell them what encoding the
ﬁles are in. The CountVectorizer takes an encoding parameter for this purpose. For modern text ﬁles, the
correct encoding is probably UTF-8, which is therefore the default (encoding="utf-8").
If the text you are loading is not actually encoded with UTF-8, however, you will get a UnicodeDecodeError.
The vectorizers can be told to be silent about decoding errors by setting the decode_error parameter to either
"ignore" or "replace". See the documentation for the Python function bytes.decode for more details (type
help(bytes.decode) at the Python prompt).
If you are having trouble decoding text, here are some things to try:
3.4. Dataset transformations
491
scikit-learn user guide, Release 0.18.2
• Find out what the actual encoding of the text is. The ﬁle might come with a header or README that tells you
the encoding, or there might be some standard encoding you can assume based on where the text comes from.
• You may be able to ﬁnd out what kind of encoding it is in general using the UNIX command file. The Python
chardet module comes with a script called chardetect.py that will guess the speciﬁc encoding, though
you cannot rely on its guess being correct.
• You
could
try
UTF-8
and
disregard
the
errors.
You
can
decode
byte
strings
with
bytes.decode(errors='replace') to replace all decoding errors with a meaningless character,
or set decode_error='replace' in the vectorizer. This may damage the usefulness of your features.
• Real text may come from a variety of sources that may have used different encodings, or even be sloppily
decoded in a different encoding than the one it was encoded with. This is common in text retrieved from the
Web. The Python package ftfy can automatically sort out some classes of decoding errors, so you could try
decoding the unknown text as latin-1 and then using ftfy to ﬁx errors.
• If the text is in a mish-mash of encodings that is simply too hard to sort out (which is the case for the 20
Newsgroups dataset), you can fall back on a simple single-byte encoding such as latin-1. Some text may
display incorrectly, but at least the same sequence of bytes will always represent the same feature.
For example, the following snippet uses chardet (not shipped with scikit-learn, must be installed separately) to
ﬁgure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not
shown here.
>>> import chardet
>>> text1 = b"Sei mir gegr\xc3\xbc\xc3\x9ft mein Sauerkraut"
>>> text2 = b"holdselig sind deine Ger\xfcche"
>>> text3 = b"\xff\xfeA\x00u\x00f\x00 \x00F\x00l\x00\xfc\x00g\x00e\x00l\x00n\x00
˓→\x00d\x00e\x00s\x00 \x00G\x00e\x00s\x00a\x00n\x00g\x00e\x00s\x00,\x00
˓→\x00H\x00e\x00r\x00z\x00l\x00i\x00e\x00b\x00c\x00h\x00e\x00n\x00,\x00
˓→\x00t\x00r\x00a\x00g\x00 \x00i\x00c\x00h\x00 \x00d\x00i\x00c\x00h\x00
˓→\x00f\x00o\x00r\x00t\x00"
>>> decoded = [x.decode(chardet.detect(x)['encoding'])
...
for x in (text1, text2, text3)]
>>> v = CountVectorizer().fit(decoded).vocabulary_
>>> for term in v: print(v)
(Depending on the version of chardet, it might get the ﬁrst one wrong.)
For an introduction to Unicode and character encodings in general, see Joel Spolsky’s Absolute Minimum Every
Software Developer Must Know About Unicode.
Applications and examples
The bag of words representation is quite simplistic but surprisingly useful in practice.
In particular in a supervised setting it can be successfully combined with fast and scalable linear models to train
document classiﬁers, for instance:
• Classiﬁcation of text documents using sparse features
In an unsupervised setting it can be used to group similar documents together by applying clustering algorithms such
as K-means:
• Clustering text documents using k-means
Finally it is possible to discover the main topics of a corpus by relaxing the hard assignment constraint of clustering,
for instance by using Non-negative matrix factorization (NMF or NNMF):
• Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
492
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Limitations of the Bag of Words representation
A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disre-
garding any word order dependence. Additionally, the bag of words model doesn’t account for potential misspellings
or word derivations.
N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of
bigrams (n=2), where occurrences of pairs of consecutive words are counted.
One might alternatively consider a collection of character n-grams, a representation resilient against misspellings and
derivations.
For example, let’s say we’re dealing with a corpus of two documents: ['words','wprds']. The second document
contains a misspelling of the word ‘words’. A simple bag of words representation would consider these two as very
distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would
ﬁnd the documents matching in 4 out of 8 features, which may help the preferred classiﬁer decide better:
>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2), min_
˓→df=1)
>>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])
>>> ngram_vectorizer.get_feature_names() == (
...
[' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'])
True
>>> counts.toarray().astype(int)
array([[1, 1, 1, 0, 1, 1, 1, 0],
[1, 1, 0, 1, 1, 1, 0, 1]])
In the above example, 'char_wb analyzer is used, which creates n-grams only from characters inside word bound-
aries (padded with space on each side). The 'char' analyzer, alternatively, creates n-grams that span across words:
>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5), min_
˓→df=1)
>>> ngram_vectorizer.fit_transform(['jumpy fox'])
...
<1x4 sparse matrix of type '<... 'numpy.int64'>'
with 4 stored elements in Compressed Sparse ... format>
>>> ngram_vectorizer.get_feature_names() == (
...
[' fox ', ' jump', 'jumpy', 'umpy '])
True
>>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5), min_df=1)
>>> ngram_vectorizer.fit_transform(['jumpy fox'])
...
<1x5 sparse matrix of type '<... 'numpy.int64'>'
with 5 stored elements in Compressed Sparse ... format>
>>> ngram_vectorizer.get_feature_names() == (
...
['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'])
True
The word boundaries-aware variant char_wb is especially interesting for languages that use white-spaces for word
separation as it generates signiﬁcantly less noisy features than the raw char variant in that case. For such languages
it can increase both the predictive accuracy and convergence speed of classiﬁers trained using such features while
retaining the robustness with regards to misspellings and word derivations.
While some local positioning information can be preserved by extracting n-grams instead of individual words, bag of
words and bag of n-grams destroy most of the inner structure of the document and hence most of the meaning carried
by that internal structure.
In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs
3.4. Dataset transformations
493
scikit-learn user guide, Release 0.18.2
should thus be taken into account. Many such models will thus be casted as “Structured output” problems which are
currently outside of the scope of scikit-learn.
Vectorizing a large text corpus with the hashing trick
The above vectorization scheme is simple but the fact that it holds an in- memory mapping from the string tokens
to the integer feature indices (the vocabulary_ attribute) causes several problems when dealing with large
datasets:
• the larger the corpus, the larger the vocabulary will grow and hence the memory use too,
• ﬁtting requires the allocation of intermediate data structures of size proportional to that of the original dataset.
• building the word-mapping requires a full pass over the dataset hence it is not possible to ﬁt text classiﬁers in a
strictly online manner.
• pickling and un-pickling vectorizers with a large vocabulary_ can be very slow (typically much slower than
pickling / un-pickling ﬂat data structures such as a NumPy array of the same size),
• it is not easily possible to split the vectorization work into concurrent sub tasks as the vocabulary_ attribute
would have to be a shared state with a ﬁne grained synchronization barrier: the mapping from token string
to feature index is dependent on ordering of the ﬁrst occurrence of each token hence would have to be shared,
potentially harming the concurrent workers’ performance to the point of making them slower than the sequential
variant.
It is possible to overcome those limitations by combining the “hashing trick” (Feature hashing) implemented by the
sklearn.feature_extraction.FeatureHasher class and the text preprocessing and tokenization features
of the CountVectorizer.
This combination is implementing in HashingVectorizer, a transformer class that is mostly API compatible with
CountVectorizer. HashingVectorizer is stateless, meaning that you don’t have to call fit on it:
>>> from sklearn.feature_extraction.text import HashingVectorizer
>>> hv = HashingVectorizer(n_features=10)
>>> hv.transform(corpus)
...
<4x10 sparse matrix of type '<... 'numpy.float64'>'
with 16 stored elements in Compressed Sparse ... format>
You can see that 16 non-zero feature tokens were extracted in the vector output: this is less than the 19 non-zeros
extracted previously by the CountVectorizer on the same toy corpus. The discrepancy comes from hash function
collisions because of the low value of the n_features parameter.
In a real world setting, the n_features parameter can be left to its default value of 2 ** 20 (roughly one million
possible features). If memory or downstream models size is an issue selecting a lower value such as 2 ** 18 might
help without introducing too many additional collisions on typical text classiﬁcation tasks.
Note that the dimensionality does not affect the CPU training time of algorithms which operate on CSR matrices
(LinearSVC(dual=True), Perceptron, SGDClassifier, PassiveAggressive) but it does for algo-
rithms that work with CSC matrices (LinearSVC(dual=False), Lasso(), etc).
Let’s try again with the default setting:
>>> hv = HashingVectorizer()
>>> hv.transform(corpus)
...
<4x1048576 sparse matrix of type '<... 'numpy.float64'>'
with 19 stored elements in Compressed Sparse ... format>
494
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
We no longer get the collisions, but this comes at the expense of a much larger dimensionality of the output space. Of
course, other terms than the 19 used here might still collide with each other.
The HashingVectorizer also comes with the following limitations:
• it is not possible to invert the model (no inverse_transform method), nor to access the original string
representation of the features, because of the one-way nature of the hash function that performs the mapping.
• it does not provide IDF weighting as that would introduce statefulness in the model. A TfidfTransformer
can be appended to it in a pipeline if required.
Performing out-of-core scaling with HashingVectorizer
An interesting development of using a HashingVectorizer is the ability to perform out-of-core scaling. This
means that we can learn from data that does not ﬁt into the computer’s main memory.
A strategy to implement out-of-core scaling is to stream data to the estimator in mini-batches. Each mini-batch is
vectorized using HashingVectorizer so as to guarantee that the input space of the estimator has always the same
dimensionality. The amount of memory used at any time is thus bounded by the size of a mini-batch. Although there is
no limit to the amount of data that can be ingested using such an approach, from a practical point of view the learning
time is often limited by the CPU time one wants to spend on the task.
For a full-ﬂedged example of out-of-core scaling in a text classiﬁcation task see Out-of-core classiﬁcation of text
documents.
Customizing the vectorizer classes
It is possible to customize the behavior by passing a callable to the vectorizer constructor:
>>> def my_tokenizer(s):
...
return s.split()
...
>>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)
>>> vectorizer.build_analyzer()(u"Some... punctuation!") == (
...
['some...', 'punctuation!'])
True
In particular we name:
• preprocessor: a callable that takes an entire document as input (as a single string), and returns a possibly
transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase
the entire document, etc.
• tokenizer: a callable that takes the output from the preprocessor and splits it into tokens, then returns a list
of these.
• analyzer: a callable that replaces the preprocessor and tokenizer. The default analyzers all call the prepro-
cessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word ﬁltering take place
at the analyzer level, so a custom analyzer may have to reproduce these steps.
(Lucene users might recognize these names, but be aware that scikit-learn concepts may not map one-to-one onto
Lucene concepts.)
To make the preprocessor, tokenizer and analyzers aware of the model parameters it is possible to derive from the
class and override the build_preprocessor, build_tokenizer` and build_analyzer factory methods
instead of passing custom functions.
Some tips and tricks:
3.4. Dataset transformations
495
scikit-learn user guide, Release 0.18.2
• If documents are pre-tokenized by an external package, then store them in ﬁles (or strings) with the tokens
separated by whitespace and pass analyzer=str.split
• Fancy token-level analysis such as stemming, lemmatizing, compound splitting, ﬁltering based on part-of-
speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer
or the analyzer. Here’s a CountVectorizer with a tokenizer and lemmatizer using NLTK:
>>> from nltk import word_tokenize
>>> from nltk.stem import WordNetLemmatizer
>>> class LemmaTokenizer(object):
...
def __init__(self):
...
self.wnl = WordNetLemmatizer()
...
def __call__(self, doc):
...
return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]
...
>>> vect = CountVectorizer(tokenizer=LemmaTokenizer())
(Note that this will not ﬁlter out punctuation.)
Customizing the vectorizer can also be useful when handling Asian languages that do not use an explicit word separator
such as whitespace.
Image feature extraction
Patch extraction
The extract_patches_2d function extracts patches from an image stored as a two-dimensional array, or
three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use
reconstruct_from_patches_2d. For example let use generate a 4x4 pixel picture with 3 color channels (e.g.
in RGB format):
>>> import numpy as np
>>> from sklearn.feature_extraction import image
>>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))
>>> one_image[:, :, 0]
# R channel of a fake RGB picture
array([[ 0,
3,
6,
9],
[12, 15, 18, 21],
[24, 27, 30, 33],
[36, 39, 42, 45]])
>>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,
...
random_state=0)
>>> patches.shape
(2, 2, 2, 3)
>>> patches[:, :, :, 0]
array([[[ 0,
3],
[12, 15]],
[[15, 18],
[27, 30]]])
>>> patches = image.extract_patches_2d(one_image, (2, 2))
>>> patches.shape
(9, 2, 2, 3)
>>> patches[4, :, :, 0]
array([[15, 18],
[27, 30]])
496
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Let us now try to reconstruct the original image from the patches by averaging on overlapping areas:
>>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))
>>> np.testing.assert_array_equal(one_image, reconstructed)
The PatchExtractor class works in the same way as extract_patches_2d, only it supports multiple images
as input. It is implemented as an estimator, so it can be used in pipelines. See:
>>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)
>>> patches = image.PatchExtractor((2, 2)).transform(five_images)
>>> patches.shape
(45, 2, 2, 3)
Connectivity graph of an image
Several estimators in the scikit-learn can use connectivity information between features or samples. For instance Ward
clustering (Hierarchical clustering) can cluster together only neighboring pixels of an image, thus forming contiguous
patches:
For this purpose, the estimators use a ‘connectivity’ matrix, giving which samples are connected.
The function img_to_graph returns such a matrix from a 2D or 3D image. Similarly, grid_to_graph build a
connectivity matrix for images given the shape of these image.
These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward
clustering (Hierarchical clustering), but also to build precomputed kernels, or similarity matrices.
Note: Examples
• A demo of structured Ward hierarchical clustering on a raccoon face image
• Spectral clustering for image segmentation
• Feature agglomeration vs. univariate selection
3.4.3 Preprocessing data
The sklearn.preprocessing package provides several common utility functions and transformer classes to
change raw feature vectors into a representation that is more suitable for the downstream estimators.
3.4. Dataset transformations
497
scikit-learn user guide, Release 0.18.2
Standardization, or mean removal and variance scaling
Standardization of datasets is a common requirement for many machine learning estimators implemented in
scikit-learn; they might behave badly if the individual features do not more or less look like standard normally dis-
tributed data: Gaussian with zero mean and unit variance.
In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean
value of each feature, then scale it by dividing non-constant features by their standard deviation.
For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support
Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and
have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might
dominate the objective function and make the estimator unable to learn from other features correctly as expected.
The function scale provides a quick and easy way to perform this operation on a single array-like dataset:
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X = np.array([[ 1., -1.,
2.],
...
[ 2.,
0.,
0.],
...
[ 0.,
1., -1.]])
>>> X_scaled = preprocessing.scale(X)
>>> X_scaled
array([[ 0.
..., -1.22...,
1.33...],
[ 1.22...,
0.
..., -0.26...],
[-1.22...,
1.22..., -1.06...]])
Scaled data has zero mean and unit variance:
>>> X_scaled.mean(axis=0)
array([ 0.,
0.,
0.])
>>> X_scaled.std(axis=0)
array([ 1.,
1.,
1.])
The preprocessing module further provides a utility class StandardScaler that implements the
Transformer API to compute the mean and standard deviation on a training set so as to be able to later
reapply the same transformation on the testing set.
This class is hence suitable for use in the early steps of a
sklearn.pipeline.Pipeline:
>>> scaler = preprocessing.StandardScaler().fit(X)
>>> scaler
StandardScaler(copy=True, with_mean=True, with_std=True)
>>> scaler.mean_
array([ 1. ...,
0. ...,
0.33...])
>>> scaler.scale_
array([ 0.81...,
0.81...,
1.24...])
>>> scaler.transform(X)
array([[ 0.
..., -1.22...,
1.33...],
[ 1.22...,
0.
..., -0.26...],
[-1.22...,
1.22..., -1.06...]])
The scaler instance can then be used on new data to transform it the same way it did on the training set:
498
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> scaler.transform([[-1.,
1., 0.]])
array([[-2.44...,
1.22..., -0.26...]])
It is possible to disable either centering or scaling by either passing with_mean=False or with_std=False to
the constructor of StandardScaler.
Scaling features to a range
An alternative standardization is scaling features to lie between a given minimum and maximum value, often between
zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using
MinMaxScaler or MaxAbsScaler, respectively.
The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero
entries in sparse data.
Here is an example to scale a toy data matrix to the [0,1] range:
>>> X_train = np.array([[ 1., -1.,
2.],
...
[ 2.,
0.,
0.],
...
[ 0.,
1., -1.]])
...
>>> min_max_scaler = preprocessing.MinMaxScaler()
>>> X_train_minmax = min_max_scaler.fit_transform(X_train)
>>> X_train_minmax
array([[ 0.5
,
0.
,
1.
],
[ 1.
,
0.5
,
0.33333333],
[ 0.
,
1.
,
0.
]])
The same instance of the transformer can then be applied to some new test data unseen during the ﬁt call: the same
scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:
>>> X_test = np.array([[ -3., -1.,
4.]])
>>> X_test_minmax = min_max_scaler.transform(X_test)
>>> X_test_minmax
array([[-1.5
,
0.
,
1.66666667]])
It is possible to introspect the scaler attributes to ﬁnd about the exact nature of the transformation learned on the
training data:
>>> min_max_scaler.scale_
array([ 0.5
,
0.5
,
0.33...])
>>> min_max_scaler.min_
array([ 0.
,
0.5
,
0.33...])
If MinMaxScaler is given an explicit feature_range=(min,max) the full formula is:
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_scaled = X_std / (max - min) + min
MaxAbsScaler works in a very similar fashion, but scales in a way that the training data lies within the range
[-1,1] by dividing through the largest maximum value in each feature. It is meant for data that is already centered
at zero or sparse data.
Here is how to use the toy data from the previous example with this scaler:
3.4. Dataset transformations
499
scikit-learn user guide, Release 0.18.2
>>> X_train = np.array([[ 1., -1.,
2.],
...
[ 2.,
0.,
0.],
...
[ 0.,
1., -1.]])
...
>>> max_abs_scaler = preprocessing.MaxAbsScaler()
>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)
>>> X_train_maxabs
# doctest +NORMALIZE_WHITESPACE^
array([[ 0.5, -1. ,
1. ],
[ 1. ,
0. ,
0. ],
[ 0. ,
1. , -0.5]])
>>> X_test = np.array([[ -3., -1.,
4.]])
>>> X_test_maxabs = max_abs_scaler.transform(X_test)
>>> X_test_maxabs
array([[-1.5, -1. ,
2. ]])
>>> max_abs_scaler.scale_
array([ 2.,
1.,
2.])
As with scale, the module further provides convenience functions minmax_scale and maxabs_scale if you
don’t want to create an object.
Scaling sparse data
Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do.
However, it can make sense to scale sparse inputs, especially if features are on different scales.
MaxAbsScaler and maxabs_scale were speciﬁcally designed for scaling sparse data, and are the recommended
way to go about this. However, scale and StandardScaler can accept scipy.sparse matrices as input, as
long as with_mean=False is explicitly passed to the constructor. Otherwise a ValueError will be raised as
silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of
memory unintentionally. RobustScaler cannot be ﬁted to sparse inputs, but you can use the transform method
on sparse inputs.
Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see
scipy.sparse.csr_matrix and scipy.sparse.csc_matrix). Any other sparse input will be converted
to the Compressed Sparse Rows representation. To avoid unnecessary memory copies, it is recommended to choose
the CSR or CSC representation upstream.
Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the
toarray method of sparse matrices is another option.
Scaling data with outliers
If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well.
In these cases, you can use robust_scale and RobustScaler as drop-in replacements instead. They use more
robust estimates for the center and range of your data.
References:
Further discussion on the importance of centering and scaling data is available on this FAQ: Should I normal-
ize/standardize/rescale the data?
500
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Scaling vs Whitening
It is sometimes not enough to center and scale the features independently, since a downstream model can further
make some assumption on the linear independence of the features.
To
address
this
issue
you
can
use
sklearn.decomposition.PCA
or
sklearn.decomposition.RandomizedPCA with whiten=True to further remove the linear corre-
lation across features.
Scaling target variables in regression
scale and StandardScaler work out-of-the-box with 1d arrays. This is very useful for scaling the target /
response variables used for regression.
Centering kernel matrices
If you have a kernel matrix of a kernel 𝐾that computes a dot product in a feature space deﬁned by function 𝑝ℎ𝑖, a
KernelCenterer can transform the kernel matrix so that it contains inner products in the feature space deﬁned by
𝑝ℎ𝑖followed by removal of the mean in that space.
Normalization
Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan
to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.
This assumption is the base of the Vector Space Model often used in text classiﬁcation and clustering contexts.
The function normalize provides a quick and easy way to perform this operation on a single array-like dataset,
either using the l1 or l2 norms:
>>> X = [[ 1., -1.,
2.],
...
[ 2.,
0.,
0.],
...
[ 0.,
1., -1.]]
>>> X_normalized = preprocessing.normalize(X, norm='l2')
>>> X_normalized
array([[ 0.40..., -0.40...,
0.81...],
[ 1.
...,
0.
...,
0.
...],
[ 0.
...,
0.70..., -0.70...]])
The preprocessing module further provides a utility class Normalizer that implements the same operation
using the Transformer API (even though the fit method is useless in this case: the class is stateless as this
operation treats samples independently).
This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline:
>>> normalizer = preprocessing.Normalizer().fit(X)
# fit does nothing
>>> normalizer
Normalizer(copy=True, norm='l2')
The normalizer instance can then be used on sample vectors as any transformer:
3.4. Dataset transformations
501
scikit-learn user guide, Release 0.18.2
>>> normalizer.transform(X)
array([[ 0.40..., -0.40...,
0.81...],
[ 1.
...,
0.
...,
0.
...],
[ 0.
...,
0.70..., -0.70...]])
>>> normalizer.transform([[-1.,
1., 0.]])
array([[-0.70...,
0.70...,
0.
...]])
Sparse input
normalize and Normalizer accept both dense array-like and sparse matrices from scipy.sparse as input.
For
sparse
input
the
data
is
converted
to
the
Compressed
Sparse
Rows
representation
(see
scipy.sparse.csr_matrix) before being fed to efﬁcient Cython routines.
To avoid unnecessary
memory copies, it is recommended to choose the CSR representation upstream.
Binarization
Feature binarization
Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for
downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate
Bernoulli distribution. For instance, this is the case for the sklearn.neural_network.BernoulliRBM.
It is also common among the text processing community to use binary feature values (probably to simplify the proba-
bilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly
better in practice.
As for the Normalizer,
the utility class Binarizer is meant to be used in the early stages of
sklearn.pipeline.Pipeline. The fit method does nothing as each sample is treated independently of
others:
>>> X = [[ 1., -1.,
2.],
...
[ 2.,
0.,
0.],
...
[ 0.,
1., -1.]]
>>> binarizer = preprocessing.Binarizer().fit(X)
# fit does nothing
>>> binarizer
Binarizer(copy=True, threshold=0.0)
>>> binarizer.transform(X)
array([[ 1.,
0.,
1.],
[ 1.,
0.,
0.],
[ 0.,
1.,
0.]])
It is possible to adjust the threshold of the binarizer:
>>> binarizer = preprocessing.Binarizer(threshold=1.1)
>>> binarizer.transform(X)
array([[ 0.,
0.,
1.],
[ 1.,
0.,
0.],
[ 0.,
0.,
0.]])
502
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
As for the StandardScaler and Normalizer classes, the preprocessing module provides a companion function
binarize to be used when the transformer API is not necessary.
Sparse input
binarize and Binarizer accept both dense array-like and sparse matrices from scipy.sparse as input.
For
sparse
input
the
data
is
converted
to
the
Compressed
Sparse
Rows
representation
(see
scipy.sparse.csr_matrix).
To avoid unnecessary memory copies, it is recommended to choose the
CSR representation upstream.
Encoding categorical features
Often features are not given as continuous values but categorical.
For example a person could have features
["male","female"], ["from Europe","from US","from Asia"], ["uses Firefox","uses
Chrome","uses Safari","uses Internet Explorer"]. Such features can be efﬁciently coded as in-
tegers, for instance ["male","from US","uses Internet Explorer"] could be expressed as [0,1,3]
while ["female","from Asia","uses Chrome"] would be [1,2,1].
Such integer representation can not be used directly with scikit-learn estimators, as these expect continuous input,
and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered
arbitrarily).
One possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-
of-K or one-hot encoding, which is implemented in OneHotEncoder. This estimator transforms each categorical
feature with m possible values into m binary features, with only one active.
Continuing the example above:
>>> enc = preprocessing.OneHotEncoder()
>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>,
handle_unknown='error', n_values='auto', sparse=True)
>>> enc.transform([[0, 1, 3]]).toarray()
array([[ 1.,
0.,
0.,
1.,
0.,
0.,
0.,
0.,
1.]])
By default, how many values each feature can take is inferred automatically from the dataset. It is possible to specify
this explicitly using the parameter n_values. There are two genders, three possible continents and four web browsers
in our dataset. Then we ﬁt the estimator, and transform a data point. In the result, the ﬁrst two numbers encode the
gender, the next set of three numbers the continent and the last four the web browser.
Note that, if there is a possibilty that the training data might have missing categorical features, one has to explicitly set
n_values. For example,
>>> enc = preprocessing.OneHotEncoder(n_values=[2, 3, 4])
>>> # Note that there are missing categorical values for the 2nd and 3rd
>>> # features
>>> enc.fit([[1, 2, 3], [0, 2, 0]])
OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>,
handle_unknown='error', n_values=[2, 3, 4], sparse=True)
>>> enc.transform([[1, 0, 0]]).toarray()
array([[ 0.,
1.,
1.,
0.,
0.,
1.,
0.,
0.,
0.]])
See Loading features from dicts for categorical features that are represented as a dict, not as integers.
3.4. Dataset transformations
503
scikit-learn user guide, Release 0.18.2
Imputation of missing values
For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other place-
holders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array
are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows
and/or columns containing missing values. However, this comes at the price of losing data which may be valuable
(even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of
the data.
The Imputer class provides basic strategies for imputing missing values, either using the mean, the median or the
most frequent value of the row or column in which the missing values are located. This class also allows for different
missing values encodings.
The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the
columns (axis 0) that contain the missing values:
>>> import numpy as np
>>> from sklearn.preprocessing import Imputer
>>> imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])
Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)
>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
>>> print(imp.transform(X))
[[ 4.
2.
]
[ 6.
3.666...]
[ 7.
6.
]]
The Imputer class also supports sparse matrices:
>>> import scipy.sparse as sp
>>> X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])
>>> imp = Imputer(missing_values=0, strategy='mean', axis=0)
>>> imp.fit(X)
Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)
>>> X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])
>>> print(imp.transform(X_test))
[[ 4.
2.
]
[ 6.
3.666...]
[ 7.
6.
]]
Note that, here, missing values are encoded by 0 and are thus implicitly stored in the matrix. This format is thus
suitable when there are many more missing values than observed values.
Imputer can be used in a Pipeline as a way to build a composite estimator that supports imputation. See Imputing
missing values before building an estimator
Generating polynomial features
Often it’s useful to add complexity to the model by considering nonlinear features of the input data. A simple and com-
mon method to use is polynomial features, which can get features’ high-order and interaction terms. It is implemented
in PolynomialFeatures:
>>> import numpy as np
>>> from sklearn.preprocessing import PolynomialFeatures
>>> X = np.arange(6).reshape(3, 2)
>>> X
array([[0, 1],
504
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
[2, 3],
[4, 5]])
>>> poly = PolynomialFeatures(2)
>>> poly.fit_transform(X)
array([[
1.,
0.,
1.,
0.,
0.,
1.],
[
1.,
2.,
3.,
4.,
6.,
9.],
[
1.,
4.,
5.,
16.,
20.,
25.]])
The features of X have been transformed from (𝑋1, 𝑋2) to (1, 𝑋1, 𝑋2, 𝑋2
1, 𝑋1𝑋2, 𝑋2
2).
In some cases, only interaction terms among features are required, and it can be gotten with the setting
interaction_only=True:
>>> X = np.arange(9).reshape(3, 3)
>>> X
array([[0, 1, 2],
[3, 4, 5],
[6, 7, 8]])
>>> poly = PolynomialFeatures(degree=3, interaction_only=True)
>>> poly.fit_transform(X)
array([[
1.,
0.,
1.,
2.,
0.,
0.,
2.,
0.],
[
1.,
3.,
4.,
5.,
12.,
15.,
20.,
60.],
[
1.,
6.,
7.,
8.,
42.,
48.,
56.,
336.]])
The features of X have been transformed from (𝑋1, 𝑋2, 𝑋3) to (1, 𝑋1, 𝑋2, 𝑋3, 𝑋1𝑋2, 𝑋1𝑋3, 𝑋2𝑋3, 𝑋1𝑋2𝑋3).
Note
that
polynomial
features
are
used
implicitily
in
kernel
methods
(e.g.,
sklearn.svm.SVC,
sklearn.decomposition.KernelPCA) when using polynomial Kernel functions.
See Polynomial interpolation for Ridge regression using created polynomial features.
Custom transformers
Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing.
You can implement a transformer from an arbitrary function with FunctionTransformer. For example, to build
a transformer that applies a log transformation in a pipeline, do:
>>> import numpy as np
>>> from sklearn.preprocessing import FunctionTransformer
>>> transformer = FunctionTransformer(np.log1p)
>>> X = np.array([[0, 1], [2, 3]])
>>> transformer.transform(X)
array([[ 0.
,
0.69314718],
[ 1.09861229,
1.38629436]])
For a full code example that demonstrates using a FunctionTransformer to do custom feature selection, see
Using FunctionTransformer to select columns
3.4.4 Unsupervised dimensionality reduction
If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps.
Many of the Unsupervised learning methods implement a transform method that can be used to reduce the dimen-
sionality. Below we discuss two speciﬁc example of this pattern that are heavily used.
3.4. Dataset transformations
505
scikit-learn user guide, Release 0.18.2
Pipelining
The unsupervised data reduction and the supervised estimator can be chained in one step. See Pipeline: chaining
estimators.
PCA: principal component analysis
decomposition.PCA looks for a combination of features that capture well the variance of the original features.
See Decomposing signals in components (matrix factorization problems).
Examples
• Faces recognition example using eigenfaces and SVMs
Random projections
The module: random_projection provides several tools for data reduction by random projections. See the
relevant section of the documentation: Random Projection.
Examples
• The Johnson-Lindenstrauss bound for embedding with random projections
Feature agglomeration
cluster.FeatureAgglomeration applies Hierarchical clustering to group together features that behave sim-
ilarly.
Examples
• Feature agglomeration vs. univariate selection
• Feature agglomeration
Feature scaling
Note that if features have very different scaling or statistical properties, cluster.FeatureAgglomeration
may not be able to capture the links between related features. Using a preprocessing.StandardScaler
can be useful in these settings.
3.4.5 Random Projection
The sklearn.random_projection module implements a simple and computationally efﬁcient way to reduce
the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing
506
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
times and smaller model sizes. This module implements two types of unstructured random matrix: Gaussian random
matrix and sparse random matrix.
The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances
between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance
based method.
References:
• Sanjoy Dasgupta. 2000. Experiments with random projection. In Proceedings of the Sixteenth conference
on Uncertainty in artiﬁcial intelligence (UAI‘00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.
• Ella Bingham and Heikki Mannila. 2001. Random projection in dimensionality reduction: applications to
image and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge
discovery and data mining (KDD ‘01). ACM, New York, NY, USA, 245-250.
The Johnson-Lindenstrauss lemma
The main theoretical result behind the efﬁciency of random projection is the Johnson-Lindenstrauss lemma (quoting
Wikipedia):
In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of
points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set
of points in a high-dimensional space can be embedded into a space of much lower dimension in such a
way that distances between the points are nearly preserved. The map used for the embedding is at least
Lipschitz, and can even be taken to be an orthogonal projection.
Knowing only the number of samples, the sklearn.random_projection.johnson_lindenstrauss_min_dim
estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the
random projection:
>>> from sklearn.random_projection import johnson_lindenstrauss_min_dim
>>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)
663
>>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])
array([
663,
11841, 1112658])
>>> johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)
array([ 7894,
9868, 11841])
Example:
• See The Johnson-Lindenstrauss bound for embedding with random projections for a theoretical explication
on the Johnson-Lindenstrauss lemma and an empirical validation using sparse random matrices.
References:
• Sanjoy Dasgupta and Anupam Gupta, 1999. An elementary proof of the Johnson-Lindenstrauss Lemma.
3.4. Dataset transformations
507
scikit-learn user guide, Release 0.18.2
508
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Gaussian random projection
The sklearn.random_projection.GaussianRandomProjection reduces the dimensionality by pro-
jecting the original input space on a randomly generated matrix where components are drawn from the following
distribution 𝑁(0,
1
𝑛𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡𝑠).
Here a small excerpt which illustrates how to use the Gaussian random projection transformer:
>>> import numpy as np
>>> from sklearn import random_projection
>>> X = np.random.rand(100, 10000)
>>> transformer = random_projection.GaussianRandomProjection()
>>> X_new = transformer.fit_transform(X)
>>> X_new.shape
(100, 3947)
Sparse random projection
The sklearn.random_projection.SparseRandomProjection reduces the dimensionality by projecting
the original input space using a sparse random matrix.
Sparse random matrices are an alternative to dense Gaussian random projection matrix that guarantees similar embed-
ding quality while being much more memory efﬁcient and allowing faster computation of the projected data.
If we deﬁne s = 1 / density, the elements of the random matrix are drawn from
⎧
⎪
⎪
⎨
⎪
⎪
⎩
−
√︁
𝑠
𝑛components
1/2𝑠
0
with probability
1 −1/𝑠
+
√︁
𝑠
𝑛components
1/2𝑠
where 𝑛components is the size of the projected subspace. By default the density of non zero elements is set to the
minimum density as recommended by Ping Li et al.: 1/√𝑛features.
Here a small excerpt which illustrates how to use the sparse random projection transformer:
>>> import numpy as np
>>> from sklearn import random_projection
>>> X = np.random.rand(100,10000)
>>> transformer = random_projection.SparseRandomProjection()
>>> X_new = transformer.fit_transform(X)
>>> X_new.shape
(100, 3947)
References:
• D. Achlioptas. 2003. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Jour-
nal of Computer and System Sciences 66 (2003) 671–687
• Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006. Very sparse random projections. In Proceedings
of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ‘06).
ACM, New York, NY, USA, 287-296.
3.4. Dataset transformations
509
scikit-learn user guide, Release 0.18.2
3.4.6 Kernel Approximation
This submodule contains functions that approximate the feature mappings that correspond to certain kernels, as they
are used for example in support vector machines (see Support Vector Machines). The following feature functions
perform non-linear transformations of the input, which can serve as a basis for linear classiﬁcation or other algorithms.
The advantage of using approximate explicit feature maps compared to the kernel trick, which makes use of feature
maps implicitly, is that explicit mappings can be better suited for online learning and can signiﬁcantly reduce the
cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an
approximate kernel map it is possible to use much more efﬁcient linear SVMs. In particular, the combination of kernel
map approximations with SGDClassifier can make non-linear learning on large datasets possible.
Since there has not been much empirical work using approximate embeddings, it is advisable to compare results
against exact kernel methods when possible.
See also:
Polynomial regression: extending linear models with basis functions for an exact polynomial transformation.
Nystroem Method for Kernel Approximation
The Nystroem method, as implemented in Nystroem is a general method for low-rank approximations of kernels.
It achieves this by essentially subsampling the data on which the kernel is evaluated. By default Nystroem uses the
rbf kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which
is also the dimensionality of the features computed - is given by the parameter n_components.
Radial Basis Function Kernel
The RBFSampler constructs an approximate mapping for the radial basis function kernel, also known as Random
Kitchen Sinks [RR2007]. This transformation can be used to explicitly model a kernel map, prior to applying a linear
algorithm, for example a linear SVM:
>>> from sklearn.kernel_approximation import RBFSampler
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
>>> y = [0, 0, 1, 1]
>>> rbf_feature = RBFSampler(gamma=1, random_state=1)
>>> X_features = rbf_feature.fit_transform(X)
>>> clf = SGDClassifier()
>>> clf.fit(X_features, y)
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
eta0=0.0, fit_intercept=True, l1_ratio=0.15,
learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
penalty='l2', power_t=0.5, random_state=None, shuffle=True,
verbose=0, warm_start=False)
>>> clf.score(X_features, y)
1.0
The mapping relies on a Monte Carlo approximation to the kernel values. The fit function performs the Monte Carlo
sampling, whereas the transform method performs the mapping of the data. Because of the inherent randomness
of the process, results may vary between different calls to the fit function.
The fit function takes two arguments: n_components, which is the target dimensionality of the feature transform,
and gamma, the parameter of the RBF-kernel. A higher n_components will result in a better approximation of the
kernel and will yield results more similar to those produced by a kernel SVM. Note that “ﬁtting” the feature function
does not actually depend on the data given to the fit function. Only the dimensionality of the data is used. Details
on the method can be found in [RR2007].
510
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
For a given value of n_components RBFSampler is often less accurate as Nystroem. RBFSampler is cheaper
to compute, though, making use of larger feature spaces more efﬁcient.
Fig. 3.9: Comparing an exact RBF kernel (left) with the approximation (right)
Examples:
• Explicit feature map approximation for RBF kernels
Additive Chi Squared Kernel
The additive chi squared kernel is a kernel on histograms, often used in computer vision.
The additive chi squared kernel as used here is given by
𝑘(𝑥, 𝑦) =
∑︁
𝑖
2𝑥𝑖𝑦𝑖
𝑥𝑖+ 𝑦𝑖
This is not exactly the same as sklearn.metrics.additive_chi2_kernel. The authors of [VZ2010] prefer
the version above as it is always positive deﬁnite. Since the kernel is additive, it is possible to treat all components
𝑥𝑖separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of
approximating using Monte Carlo sampling.
The class AdditiveChi2Sampler implements this component wise deterministic sampling. Each component
is sampled 𝑛times, yielding 2𝑛+ 1 dimensions per input dimension (the multiple of two stems from the real and
complex part of the Fourier transform). In the literature, 𝑛is usually chosen to be 1 or 2, transforming the dataset to
size n_samples * 5 * n_features (in the case of 𝑛= 2).
The approximate feature map provided by AdditiveChi2Sampler can be combined with the approximate feature
map provided by RBFSampler to yield an approximate feature map for the exponentiated chi squared kernel. See
the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler.
Skewed Chi Squared Kernel
The skewed chi squared kernel is given by:
𝑘(𝑥, 𝑦) =
∏︁
𝑖
2√𝑥𝑖+ 𝑐√𝑦𝑖+ 𝑐
𝑥𝑖+ 𝑦𝑖+ 2𝑐
It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for
a simple Monte Carlo approximation of the feature map.
The usage of the SkewedChi2Sampler is the same as the usage described above for the RBFSampler. The only
difference is in the free parameter, that is called 𝑐. For a motivation for this mapping and the mathematical details see
[LS2010].
3.4. Dataset transformations
511
scikit-learn user guide, Release 0.18.2
Mathematical Details
Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert
spaces. For any positive deﬁnite kernel function 𝑘(a so called Mercer kernel), it is guaranteed that there exists a
mapping 𝜑into a Hilbert space ℋ, such that
𝑘(𝑥, 𝑦) = ⟨𝜑(𝑥), 𝜑(𝑦)⟩
Where ⟨·, ·⟩denotes the inner product in the Hilbert space.
If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points 𝑥𝑖,
one may use the value of 𝑘(𝑥𝑖, 𝑥𝑗), which corresponds to applying the algorithm to the mapped data points 𝜑(𝑥𝑖). The
advantage of using 𝑘is that the mapping 𝜑never has to be calculated explicitly, allowing for arbitrary large features
(even inﬁnite).
One drawback of kernel methods is, that it might be necessary to store many kernel values 𝑘(𝑥𝑖, 𝑥𝑗) during optimiza-
tion. If a kernelized classiﬁer is applied to new data 𝑦𝑗, 𝑘(𝑥𝑖, 𝑦𝑗) needs to be computed to make predictions, possibly
for many different 𝑥𝑖in the training set.
The classes in this submodule allow to approximate the embedding 𝜑, thereby working explicitly with the representa-
tions 𝜑(𝑥𝑖), which obviates the need to apply the kernel or store training examples.
References:
3.4.7 Pairwise metrics, Afﬁnities and Kernels
The sklearn.metrics.pairwise submodule implements utilities to evaluate pairwise distances or afﬁnity of
sets of samples.
This module contains both distance metrics and kernels. A brief summary is given on the two here.
Distance metrics are functions d(a,b) such that d(a,b) < d(a,c) if objects a and b are considered “more
similar” than objects a and c. Two objects exactly alike would have a distance of zero. One of the most popular
examples is Euclidean distance. To be a ‘true’ metric, it must obey the following four conditions:
1. d(a, b) >= 0, for all a and b
2. d(a, b) == 0, if and only if a = b, positive definiteness
3. d(a, b) == d(b, a), symmetry
4. d(a, c) <= d(a, b) + d(b, c), the triangle inequality
Kernels are measures of similarity, i.e. s(a,b) > s(a,c) if objects a and b are considered “more similar” than
objects a and c. A kernel must also be positive semi-deﬁnite.
There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let D be
the distance, and S be the kernel:
1. S = np.exp(-D * gamma), where one heuristic for choosing gamma is 1 / num_features
2. S = 1.
/ (D / np.max(D))
Cosine similarity
cosine_similarity computes the L2-normalized dot product of vectors. That is, if 𝑥and 𝑦are row vectors, their
cosine similarity 𝑘is deﬁned as:
𝑘(𝑥, 𝑦) =
𝑥𝑦⊤
‖𝑥‖‖𝑦‖
512
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and
their dot product is then the cosine of the angle between the points denoted by the vectors.
This kernel is a popular choice for computing the similarity of documents represented as tf-idf vec-
tors.
cosine_similarity
accepts
scipy.sparse
matrices.
(Note
that
the
tf-idf
function-
ality
in
sklearn.feature_extraction.text
can
produce
normalized
vectors,
in
which
case
cosine_similarity is equivalent to linear_kernel, only slower.)
References:
• C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge Uni-
versity Press. http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html
Linear kernel
The function linear_kernel computes the linear kernel, that is, a special case of polynomial_kernel with
degree=1 and coef0=0 (homogeneous). If x and y are column vectors, their linear kernel is:
𝑘(𝑥, 𝑦) = 𝑥⊤𝑦
Polynomial kernel
The function polynomial_kernel computes the degree-d polynomial kernel between two vectors. The polyno-
mial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only
the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning
algorithms, this allows to account for feature interaction.
The polynomial kernel is deﬁned as:
𝑘(𝑥, 𝑦) = (𝛾𝑥⊤𝑦+ 𝑐0)𝑑
where:
• x, y are the input vectors
• d is the kernel degree
If 𝑐0 = 0 the kernel is said to be homogeneous.
Sigmoid kernel
The function sigmoid_kernel computes the sigmoid kernel between two vectors. The sigmoid kernel is also
known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network ﬁeld, it is often used as neuron
activation function). It is deﬁned as:
𝑘(𝑥, 𝑦) = tanh(𝛾𝑥⊤𝑦+ 𝑐0)
where:
• x, y are the input vectors
• 𝛾is known as slope
• 𝑐0 is known as intercept
3.4. Dataset transformations
513
scikit-learn user guide, Release 0.18.2
RBF kernel
The function rbf_kernel computes the radial basis function (RBF) kernel between two vectors. This kernel is
deﬁned as:
𝑘(𝑥, 𝑦) = exp(−𝛾‖𝑥−𝑦‖2)
where x and y are the input vectors. If 𝛾= 𝜎−2 the kernel is known as the Gaussian kernel of variance 𝜎2.
Laplacian kernel
The function laplacian_kernel is a variant on the radial basis function kernel deﬁned as:
𝑘(𝑥, 𝑦) = exp(−𝛾‖𝑥−𝑦‖1)
where x and y are the input vectors and ‖𝑥−𝑦‖1 is the Manhattan distance between the input vectors.
It has proven useful in ML applied to noiseless data. See e.g. Machine learning for quantum mechanics in a nutshell.
Chi-squared kernel
The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can
be computed using chi2_kernel and then passed to an sklearn.svm.SVC with kernel="precomputed":
>>> from sklearn.svm import SVC
>>> from sklearn.metrics.pairwise import chi2_kernel
>>> X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]
>>> y = [0, 1, 0, 1]
>>> K = chi2_kernel(X, gamma=.5)
>>> K
array([[ 1.
,
0.36...,
0.89...,
0.58...],
[ 0.36...,
1.
,
0.51...,
0.83...],
[ 0.89...,
0.51...,
1.
,
0.77... ],
[ 0.58...,
0.83...,
0.77... ,
1.
]])
>>> svm = SVC(kernel='precomputed').fit(K, y)
>>> svm.predict(K)
array([0, 1, 0, 1])
It can also be directly used as the kernel argument:
>>> svm = SVC(kernel=chi2_kernel).fit(X, y)
>>> svm.predict(X)
array([0, 1, 0, 1])
The chi squared kernel is given by
𝑘(𝑥, 𝑦) = exp
(︃
−𝛾
∑︁
𝑖
(𝑥[𝑖] −𝑦[𝑖])2
𝑥[𝑖] + 𝑦[𝑖]
)︃
The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is
rationalized with the connection to the chi squared distance, which is a distance between discrete probability distribu-
tions.
The chi squared kernel is most commonly used on histograms (bags) of visual words.
514
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
References:
• Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classiﬁcation
of texture and object categories: A comprehensive study International Journal of Computer Vision 2007
http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf
3.4.8 Transforming the prediction target (y)
Label binarization
LabelBinarizer is a utility class to help create a label indicator matrix from a list of multi-class labels:
>>> from sklearn import preprocessing
>>> lb = preprocessing.LabelBinarizer()
>>> lb.fit([1, 2, 6, 4, 2])
LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)
>>> lb.classes_
array([1, 2, 4, 6])
>>> lb.transform([1, 6])
array([[1, 0, 0, 0],
[0, 0, 0, 1]])
For multiple labels per instance, use MultiLabelBinarizer:
>>> lb = preprocessing.MultiLabelBinarizer()
>>> lb.fit_transform([(1, 2), (3,)])
array([[1, 1, 0],
[0, 0, 1]])
>>> lb.classes_
array([1, 2, 3])
Label encoding
LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-
1. This is sometimes useful for writing efﬁcient Cython routines. LabelEncoder can be used as follows:
>>> from sklearn import preprocessing
>>> le = preprocessing.LabelEncoder()
>>> le.fit([1, 2, 2, 6])
LabelEncoder()
>>> le.classes_
array([1, 2, 6])
>>> le.transform([1, 1, 2, 6])
array([0, 0, 1, 2])
>>> le.inverse_transform([0, 0, 1, 2])
array([1, 1, 2, 6])
It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical
labels:
>>> le = preprocessing.LabelEncoder()
>>> le.fit(["paris", "paris", "tokyo", "amsterdam"])
LabelEncoder()
3.4. Dataset transformations
515
scikit-learn user guide, Release 0.18.2
>>> list(le.classes_)
['amsterdam', 'paris', 'tokyo']
>>> le.transform(["tokyo", "tokyo", "paris"])
array([2, 2, 1])
>>> list(le.inverse_transform([2, 2, 1]))
['tokyo', 'tokyo', 'paris']
3.5 Dataset loading utilities
The sklearn.datasets package embeds some small toy datasets as introduced in the Getting Started section.
To evaluate the impact of the scale of the dataset (n_samples and n_features) while controlling the statistical
properties of the data (typically the correlation and informativeness of the features), it is also possible to generate
synthetic data.
This package also features helpers to fetch larger datasets commonly used by the machine learning community to
benchmark algorithm on data that comes from the ‘real world’.
3.5.1 General dataset API
There are three distinct kinds of dataset interfaces for different types of datasets. The simplest one is the interface for
sample images, which is described below in the Sample images section.
The dataset generation functions and the svmlight loader share a simplistic interface, returning a tuple (X,y) con-
sisting of a n_samples * n_features numpy array X and an array of length n_samples containing the targets
y.
The toy datasets as well as the ‘real world’ datasets and the datasets fetched from mldata.org have more sophisticated
structure. These functions return a dictionary-like object holding at least two items: an array of shape n_samples *
n_features with key data (except for 20newsgroups) and a numpy array of length n_samples, containing the
target values, with key target.
The datasets also contain a description in DESCR and some contain feature_names and target_names. See
the dataset descriptions below for details.
3.5.2 Toy datasets
scikit-learn comes with a few small standard datasets that do not require to download any ﬁle from some external
website.
load_boston([return_X_y])
Load and return the boston house-prices dataset (regres-
sion).
load_iris([return_X_y])
Load and return the iris dataset (classiﬁcation).
load_diabetes([return_X_y])
Load and return the diabetes dataset (regression).
load_digits([n_class, return_X_y])
Load and return the digits dataset (classiﬁcation).
load_linnerud([return_X_y])
Load and return the linnerud dataset (multivariate regres-
sion).
These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in the scikit. They
are however often too small to be representative of real world machine learning tasks.
516
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
3.5.3 Sample images
The scikit also embed a couple of sample JPEG images published under Creative Commons license by their authors.
Those image can be useful to test algorithms and pipeline on 2D data.
load_sample_images()
Load sample images for image manipulation.
load_sample_image(image_name)
Load the numpy array of a single sample image
Warning: The default coding of images is based on the uint8 dtype to spare memory. Often machine learning
algorithms work best if the input is converted to a ﬂoating point representation ﬁrst. Also, if you plan to use
matplotlib.pyplpt.imshow don’t forget to scale to the range 0 - 1 as done in the following example.
Examples:
• Color Quantization using K-Means
3.5.4 Sample generators
In addition, scikit-learn includes various random sample generators that can be used to build artiﬁcial datasets of
controlled size and complexity.
Generators for classiﬁcation and clustering
These generators produce a matrix of features and corresponding discrete targets.
Single label
Both make_blobs and make_classification create multiclass datasets by allocating each class one or more
normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard de-
viations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing
noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear
transformations of the feature space.
make_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated
by concentric hyperspheres.
make_hastie_10_2 generates a similar binary, 10-dimensional problem.
3.5. Dataset loading utilities
517
scikit-learn user guide, Release 0.18.2
make_circles and make_moons generate 2d binary
classiﬁcation datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classiﬁcation),
including optional Gaussian noise. They are useful for visualisation. produces Gaussian data with a spherical decision
boundary for binary classiﬁcation.
Multilabel
make_multilabel_classification generates random samples with multiple labels, reﬂecting a bag of words
drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the
topics themselves are drawn from a ﬁxed random distribution. Similarly, the number of words is drawn from Poisson,
with words drawn from a multinomial, where each topic deﬁnes a probability distribution over words. Simpliﬁcations
with respect to true bag-of-words mixtures include:
• Per-topic word distributions are independently drawn, where in reality all would be affected by a sparse base
distribution, and would be correlated.
• For a document generated from multiple topics, all topics are weighted equally in generating its bag of words.
• Documents without labels words at random, rather than from a base distribution.
Biclustering
518
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
make_biclusters(shape, n_clusters[, noise, ...])
Generate an array with constant block diagonal structure
for biclustering.
make_checkerboard(shape, n_clusters[, ...])
Generate an array with block checkerboard structure for bi-
clustering.
Generators for regression
make_regression produces regression targets as an optionally-sparse random linear combination of random fea-
tures, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the
variance).
Other
regression
generators
generate
functions
deterministically
from
randomized
features.
make_sparse_uncorrelated produces a target as a linear combination of four features with ﬁxed coef-
ﬁcients.
Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine
transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is
similar with an arctan transformation on the target.
Generators for manifold learning
make_s_curve([n_samples, noise, random_state])
Generate an S curve dataset.
make_swiss_roll([n_samples, noise, random_state])
Generate a swiss roll dataset.
Generators for decomposition
make_low_rank_matrix([n_samples, ...])
Generate a mostly low rank matrix with bell-shaped singu-
lar values
make_sparse_coded_signal(n_samples, ...[, ...])
Generate a signal as a sparse combination of dictionary el-
ements.
make_spd_matrix(n_dim[, random_state])
Generate a random symmetric, positive-deﬁnite matrix.
make_sparse_spd_matrix([dim, alpha, ...])
Generate a sparse symmetric deﬁnite positive matrix.
3.5.5 Datasets in svmlight / libsvm format
scikit-learn includes utility functions for loading datasets in the svmlight / libsvm format.
In this format, each
line takes the form <label> <feature-id>:<feature-value> <feature-id>:<feature-value>
.... This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for X
and numpy arrays are used for y.
You may load a dataset like as follows:
>>> from sklearn.datasets import load_svmlight_file
>>> X_train, y_train = load_svmlight_file("/path/to/train_dataset.txt")
...
You may also load two (or more) datasets at once:
>>> X_train, y_train, X_test, y_test = load_svmlight_files(
...
("/path/to/train_dataset.txt", "/path/to/test_dataset.txt"))
...
3.5. Dataset loading utilities
519
scikit-learn user guide, Release 0.18.2
In this case, X_train and X_test are guaranteed to have the same number of features. Another way to achieve the
same result is to ﬁx the number of features:
>>> X_test, y_test = load_svmlight_file(
...
"/path/to/test_dataset.txt", n_features=X_train.shape[1])
...
Related links:
Public datasets in svmlight / libsvm format: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
Faster API-compatible implementation: https://github.com/mblondel/svmlight-loader
3.5.6 Loading from external datasets
scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible
to numeric arrays such as pandas DataFrame are also acceptable.
Here are some recommended ways to load standard columnar data into a format usable by scikit-learn:
• pandas.io provides tools to read data from common formats including CSV, Excel, JSON and SQL. DataFrames
may also be constructed from lists of tuples or dicts. Pandas handles heterogeneous data smoothly and provides
tools for manipulation and conversion into a numeric array suitable for scikit-learn.
• scipy.io specializes in binary formats often used in scientiﬁc computing context such as .mat and .arff
• numpy/routines.io for standard loading of columnar data into numpy arrays
• scikit-learn’s datasets.load_svmlight_file for the svmlight or libSVM sparse format
• scikit-learn’s datasets.load_files for directories of text ﬁles where the name of each directory is the
name of each category and each ﬁle inside of each directory corresponds to one sample from that category
For some miscellaneous data such as images, videos, and audio, you may wish to refer to:
• skimage.io or Imageio for loading images and videos to numpy arrays
• scipy.misc.imread (requires the Pillow package) to load pixel intensities data from various image ﬁle formats
• scipy.io.wavﬁle.read for reading WAV ﬁles into a numpy array
Categorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting
to integers, and integer categorical variables may be best exploited when encoded as one-hot variables
(sklearn.preprocessing.OneHotEncoder) or similar. See Preprocessing data.
Note: if you manage your own numerical data it is recommended to use an optimized ﬁle format such as HDF5 to
reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading
and writing data in that format.
The Olivetti faces dataset
This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cam-
bridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function
that downloads the data archive from AT&T.
As described on the original website:
520
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken
at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and
facial details (glasses / no glasses). All the images were taken against a dark homogeneous background
with the subjects in an upright, frontal position (with tolerance for some side movement).
The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to ﬂoating
point values on the interval [0, 1], which are easier to work with for many algorithms.
The “target” for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with
only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised
perspective.
The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.
When using these images, please give credit to AT&T Laboratories Cambridge.
The 20 newsgroups text dataset
The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for
training (or development) and the other one for testing (or for performance evaluation). The split between the train
and test set is based upon a messages posted before and after a speciﬁc date.
This
module
contains
two
loaders.
The
ﬁrst
one,
sklearn.datasets.fetch_20newsgroups,
returns
a
list
of
the
raw
texts
that
can
be
fed
to
text
feature
extractors
such
as
sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract
feature vectors.
The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns
ready-to-use features, i.e., it is not necessary to use a feature extractor.
Usage
The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that
downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the
~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files on either
the training or testing set folder, or both of them:
>>> from sklearn.datasets import fetch_20newsgroups
>>> newsgroups_train = fetch_20newsgroups(subset='train')
>>> from pprint import pprint
>>> pprint(list(newsgroups_train.target_names))
['alt.atheism',
'comp.graphics',
'comp.os.ms-windows.misc',
'comp.sys.ibm.pc.hardware',
'comp.sys.mac.hardware',
'comp.windows.x',
'misc.forsale',
'rec.autos',
'rec.motorcycles',
'rec.sport.baseball',
'rec.sport.hockey',
'sci.crypt',
'sci.electronics',
'sci.med',
'sci.space',
'soc.religion.christian',
'talk.politics.guns',
3.5. Dataset loading utilities
521
scikit-learn user guide, Release 0.18.2
'talk.politics.mideast',
'talk.politics.misc',
'talk.religion.misc']
The real data lies in the filenames and target attributes. The target attribute is the integer index of the category:
>>> newsgroups_train.filenames.shape
(11314,)
>>> newsgroups_train.target.shape
(11314,)
>>> newsgroups_train.target[:10]
array([12,
6,
9,
8,
6,
7,
9,
2, 13, 19])
It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the
sklearn.datasets.fetch_20newsgroups function:
>>> cats = ['alt.atheism', 'sci.space']
>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)
>>> list(newsgroups_train.target_names)
['alt.atheism', 'sci.space']
>>> newsgroups_train.filenames.shape
(1073,)
>>> newsgroups_train.target.shape
(1073,)
>>> newsgroups_train.target[:10]
array([1, 1, 1, 0, 1, 0, 0, 1, 1, 1])
Converting text to vectors
In order to feed predictive or clustering models with the text data, one ﬁrst need to turn the text into vec-
tors of numerical values suitable for statistical analysis.
This can be achieved with the utilities of the
sklearn.feature_extraction.text as demonstrated in the following example that extract TF-IDF vectors
of unigram tokens from a subset of 20news:
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> categories = ['alt.atheism', 'talk.religion.misc',
...
'comp.graphics', 'sci.space']
>>> newsgroups_train = fetch_20newsgroups(subset='train',
...
categories=categories)
>>> vectorizer = TfidfVectorizer()
>>> vectors = vectorizer.fit_transform(newsgroups_train.data)
>>> vectors.shape
(2034, 34118)
The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than
30000-dimensional space (less than .5% non-zero features):
>>> vectors.nnz / float(vectors.shape[0])
159.01327433628319
sklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use tﬁdf
features instead of ﬁle names.
522
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Filtering text for more realistic training
It is easy for a classiﬁer to overﬁt on particular things that appear in the 20 Newsgroups data, such as newsgroup
headers. Many classiﬁers achieve very high F-scores, but their results would not generalize to other documents that
aren’t from this window of time.
For example, let’s look at the results of a multinomial Naive Bayes classiﬁer, which is fast to train and achieves a
decent F-score:
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn import metrics
>>> newsgroups_test = fetch_20newsgroups(subset='test',
...
categories=categories)
>>> vectors_test = vectorizer.transform(newsgroups_test.data)
>>> clf = MultinomialNB(alpha=.01)
>>> clf.fit(vectors, newsgroups_train.target)
>>> pred = clf.predict(vectors_test)
>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
0.88213592402729568
(The example Classiﬁcation of text documents using sparse features shufﬂes the training and test data, instead of
segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious
yet of what’s going on inside this classiﬁer?)
Let’s take a look at what the most informative features are:
>>> import numpy as np
>>> def show_top10(classifier, vectorizer, categories):
...
feature_names = np.asarray(vectorizer.get_feature_names())
...
for i, category in enumerate(categories):
...
top10 = np.argsort(classifier.coef_[i])[-10:]
...
print("%s: %s" % (category, " ".join(feature_names[top10])))
...
>>> show_top10(clf, vectorizer, newsgroups_train.target_names)
alt.atheism: sgi livesey atheists writes people caltech com god keith edu
comp.graphics: organization thanks files subject com image lines university edu
˓→graphics
sci.space: toronto moon gov com alaska access henry nasa edu space
talk.religion.misc: article writes kent people christian jesus sandvik edu com god
You can now see many things that these features have overﬁt to:
• Almost every group is distinguished by whether headers such as NNTP-Posting-Host:
and
Distribution: appear more or less often.
• Another signiﬁcant feature involves whether the sender is afﬁliated with a university, as indicated either by their
headers or their signature.
• The word “article” is a signiﬁcant feature, based on how often people quote previous posts like this: “In article
[article ID], [name] <[e-mail address]> wrote:”
• Other features match the names and e-mail addresses of particular people who were posting at the time.
With such an abundance of clues that distinguish newsgroups, the classiﬁers barely have to identify topics from text at
all, and they all perform at the same high level.
For this reason, the functions that load 20 Newsgroups data provide a parameter called remove, telling it
what kinds of information to strip out of each ﬁle.
remove should be a tuple containing any subset of
('headers','footers','quotes'), telling it to remove headers, signature blocks, and quotation blocks re-
spectively.
3.5. Dataset loading utilities
523
scikit-learn user guide, Release 0.18.2
>>> newsgroups_test = fetch_20newsgroups(subset='test',
...
remove=('headers', 'footers', 'quotes'),
...
categories=categories)
>>> vectors_test = vectorizer.transform(newsgroups_test.data)
>>> pred = clf.predict(vectors_test)
>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')
0.77310350681274775
This classiﬁer lost over a lot of its F-score, just because we removed metadata that has little to do with topic classiﬁ-
cation. It loses even more if we also strip this metadata from the training data:
>>> newsgroups_train = fetch_20newsgroups(subset='train',
...
remove=('headers', 'footers', 'quotes'),
...
categories=categories)
>>> vectors = vectorizer.fit_transform(newsgroups_train.data)
>>> clf = MultinomialNB(alpha=.01)
>>> clf.fit(vectors, newsgroups_train.target)
>>> vectors_test = vectorizer.transform(newsgroups_test.data)
>>> pred = clf.predict(vectors_test)
>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
0.76995175184521725
Some other classiﬁers cope better with this harder version of the task. Try running Sample pipeline for text feature
extraction and evaluation with and without the --filter option to compare the results.
Recommendation
When evaluating text classiﬁers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-
learn, you can do this by setting remove=('headers','footers','quotes'). The F-score will be lower
because it is more realistic.
Examples
• Sample pipeline for text feature extraction and evaluation
• Classiﬁcation of text documents using sparse features
Downloading datasets from the mldata.org repository
mldata.org is a public repository for machine learning data, supported by the PASCAL network .
The sklearn.datasets package is able to directly download data sets from the repository using the function
sklearn.datasets.fetch_mldata.
For example, to download the MNIST digit recognition database:
>>> from sklearn.datasets import fetch_mldata
>>> mnist = fetch_mldata('MNIST original', data_home=custom_data_home)
The MNIST database contains a total of 70000 examples of handwritten digits of size 28x28 pixels, labeled from 0 to
9:
524
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> mnist.data.shape
(70000, 784)
>>> mnist.target.shape
(70000,)
>>> np.unique(mnist.target)
array([ 0.,
1.,
2.,
3.,
4.,
5.,
6.,
7.,
8.,
9.])
After the ﬁrst download, the dataset is cached locally in the path speciﬁed by the data_home keyword argument,
which defaults to ~/scikit_learn_data/:
>>> os.listdir(os.path.join(custom_data_home, 'mldata'))
['mnist-original.mat']
Data
sets
in
mldata.org
do
not
adhere
to
a
strict
naming
or
formatting
convention.
sklearn.datasets.fetch_mldata is able to make sense of the most common cases, but allows to
tailor the defaults to individual datasets:
• The data arrays in mldata.org are most often shaped as (n_features,n_samples). This is the opposite
of the scikit-learn convention, so sklearn.datasets.fetch_mldata transposes the matrix by
default. The transpose_data keyword controls this behavior:
>>> iris = fetch_mldata('iris', data_home=custom_data_home)
>>> iris.data.shape
(150, 4)
>>> iris = fetch_mldata('iris', transpose_data=False,
...
data_home=custom_data_home)
>>> iris.data.shape
(4, 150)
• For datasets with multiple columns, sklearn.datasets.fetch_mldata tries to identify the target and
data columns and rename them to target and data. This is done by looking for arrays named label and
data in the dataset, and failing that by choosing the ﬁrst array to be target and the second to be data. This
behavior can be changed with the target_name and data_name keywords, setting them to a speciﬁc name
or index number (the name and order of the columns in the datasets can be found at its mldata.org under the tab
“Data”:
>>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1, data_name=0,
...
data_home=custom_data_home)
>>> iris3 = fetch_mldata('datasets-UCI iris', target_name='class',
...
data_name='double0', data_home=custom_data_home)
The Labeled Faces in the Wild face recognition dataset
This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on
the ofﬁcial website:
http://vis-www.cs.umass.edu/lfw/
Each picture is centered on a single face. The typical task is called Face Veriﬁcation: given a pair of two pictures, a
binary classiﬁer must predict whether the two images are from the same person.
An alternative task, Face Recognition or Face Identiﬁcation is: given the picture of the face of an unknown person,
identify the name of the person by referring to a gallery of previously seen pictures of identiﬁed persons.
Both Face Veriﬁcation and Face Recognition are tasks that are typically performed on the output of a model trained to
perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the
OpenCV library. The LFW faces were extracted by this face detector from various online websites.
3.5. Dataset loading utilities
525
scikit-learn user guide, Release 0.18.2
Usage
scikit-learn provides two loaders that will automatically download, cache, parse the metadata ﬁles, decode
the jpeg and convert the interesting slices into memmaped numpy arrays. This dataset size is more than 200 MB.
The ﬁrst load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG ﬁles into
numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a
memmaped version memoized on the disk in the ~/scikit_learn_data/lfw_home/ folder using joblib.
The ﬁrst loader is used for the Face Identiﬁcation task: a multi-class classiﬁcation task (hence supervised learning):
>>> from sklearn.datasets import fetch_lfw_people
>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
>>> for name in lfw_people.target_names:
...
print(name)
...
Ariel Sharon
Colin Powell
Donald Rumsfeld
George W Bush
Gerhard Schroeder
Hugo Chavez
Tony Blair
The default slice is a rectangular shape around the face, removing most of the background:
>>> lfw_people.data.dtype
dtype('float32')
>>> lfw_people.data.shape
(1288, 1850)
>>> lfw_people.images.shape
(1288, 50, 37)
Each of the 1140 faces is assigned to a single person id in the target array:
>>> lfw_people.target.shape
(1288,)
>>> list(lfw_people.target[:10])
[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]
The second loader is typically used for the face veriﬁcation task: each sample is a pair of two picture belonging or not
to the same person:
>>> from sklearn.datasets import fetch_lfw_pairs
>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')
>>> list(lfw_pairs_train.target_names)
['Different persons', 'Same person']
>>> lfw_pairs_train.pairs.shape
(2200, 2, 62, 47)
>>> lfw_pairs_train.data.shape
(2200, 5828)
526
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> lfw_pairs_train.target.shape
(2200,)
Both for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs
function it is possible to get an additional dimension with the RGB color channels by passing color=True, in
that case the shape will be (2200,2,62,47,3).
The sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development train
set, the development test set and an evaluation 10_folds set meant to compute performance metrics using a
10-folds cross validation scheme.
References:
• Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Gary
B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. University of Massachusetts, Amherst,
Technical Report 07-49, October, 2007.
Examples
Faces recognition example using eigenfaces and SVMs
Forest covertypes
The samples in this dataset correspond to 30×30m patches of forest in the US, collected for the task of predicting
each patch’s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass
classiﬁcation problem. Each sample has 54 features, described on the dataset’s homepage. Some of the features are
boolean indicators, while others are discrete or continuous measurements.
sklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with
the feature matrix in the data member and the target values in target. The dataset will be downloaded from the
web if necessary.
RCV1 dataset
Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available
by Reuters, Ltd. for research purposes. The dataset is extensively described in 1.
sklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multil-
abels:
>>> from sklearn.datasets import fetch_rcv1
>>> rcv1 = fetch_rcv1()
It returns a dictionary-like object, with the following attributes:
data: The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values
contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in 1: The ﬁrst 23149 samples
are the training set. The last 781265 samples are the testing set. This follows the ofﬁcial LYRL2004 chronological
split. The array has 0.16% of non zero values:
1 Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of
Machine Learning Research, 5, 361-397.
3.5. Dataset loading utilities
527
scikit-learn user guide, Release 0.18.2
>>> rcv1.data.shape
(804414, 47236)
target: The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each
sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values:
>>> rcv1.target.shape
(804414, 103)
sample_id: Each sample can be identiﬁed by its ID, ranging (with gaps) from 2286 to 810596:
>>> rcv1.sample_id[:3]
array([2286, 2287, 2288], dtype=uint32)
target_names: The target values are the topics of each sample. Each sample belongs to at least one topic, and
to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span ﬁve orders of
magnitude, from 5 occurrences for ‘GMIL’, to 381327 for ‘CCAT’:
>>> rcv1.target_names[:3].tolist()
['E11', 'ECAT', 'M11']
The dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB.
References
3.5.7 The Olivetti faces dataset
This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cam-
bridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function
that downloads the data archive from AT&T.
As described on the original website:
There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken
at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and
facial details (glasses / no glasses). All the images were taken against a dark homogeneous background
with the subjects in an upright, frontal position (with tolerance for some side movement).
The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to ﬂoating
point values on the interval [0, 1], which are easier to work with for many algorithms.
The “target” for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with
only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised
perspective.
The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.
When using these images, please give credit to AT&T Laboratories Cambridge.
3.5.8 The 20 newsgroups text dataset
The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for
training (or development) and the other one for testing (or for performance evaluation). The split between the train
and test set is based upon a messages posted before and after a speciﬁc date.
528
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
This
module
contains
two
loaders.
The
ﬁrst
one,
sklearn.datasets.fetch_20newsgroups,
returns
a
list
of
the
raw
texts
that
can
be
fed
to
text
feature
extractors
such
as
sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract
feature vectors.
The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns
ready-to-use features, i.e., it is not necessary to use a feature extractor.
Usage
The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that
downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the
~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files on either
the training or testing set folder, or both of them:
>>> from sklearn.datasets import fetch_20newsgroups
>>> newsgroups_train = fetch_20newsgroups(subset='train')
>>> from pprint import pprint
>>> pprint(list(newsgroups_train.target_names))
['alt.atheism',
'comp.graphics',
'comp.os.ms-windows.misc',
'comp.sys.ibm.pc.hardware',
'comp.sys.mac.hardware',
'comp.windows.x',
'misc.forsale',
'rec.autos',
'rec.motorcycles',
'rec.sport.baseball',
'rec.sport.hockey',
'sci.crypt',
'sci.electronics',
'sci.med',
'sci.space',
'soc.religion.christian',
'talk.politics.guns',
'talk.politics.mideast',
'talk.politics.misc',
'talk.religion.misc']
The real data lies in the filenames and target attributes. The target attribute is the integer index of the category:
>>> newsgroups_train.filenames.shape
(11314,)
>>> newsgroups_train.target.shape
(11314,)
>>> newsgroups_train.target[:10]
array([12,
6,
9,
8,
6,
7,
9,
2, 13, 19])
It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the
sklearn.datasets.fetch_20newsgroups function:
>>> cats = ['alt.atheism', 'sci.space']
>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)
>>> list(newsgroups_train.target_names)
['alt.atheism', 'sci.space']
>>> newsgroups_train.filenames.shape
3.5. Dataset loading utilities
529
scikit-learn user guide, Release 0.18.2
(1073,)
>>> newsgroups_train.target.shape
(1073,)
>>> newsgroups_train.target[:10]
array([1, 1, 1, 0, 1, 0, 0, 1, 1, 1])
Converting text to vectors
In order to feed predictive or clustering models with the text data, one ﬁrst need to turn the text into vec-
tors of numerical values suitable for statistical analysis.
This can be achieved with the utilities of the
sklearn.feature_extraction.text as demonstrated in the following example that extract TF-IDF vectors
of unigram tokens from a subset of 20news:
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> categories = ['alt.atheism', 'talk.religion.misc',
...
'comp.graphics', 'sci.space']
>>> newsgroups_train = fetch_20newsgroups(subset='train',
...
categories=categories)
>>> vectorizer = TfidfVectorizer()
>>> vectors = vectorizer.fit_transform(newsgroups_train.data)
>>> vectors.shape
(2034, 34118)
The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than
30000-dimensional space (less than .5% non-zero features):
>>> vectors.nnz / float(vectors.shape[0])
159.01327433628319
sklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use tﬁdf
features instead of ﬁle names.
Filtering text for more realistic training
It is easy for a classiﬁer to overﬁt on particular things that appear in the 20 Newsgroups data, such as newsgroup
headers. Many classiﬁers achieve very high F-scores, but their results would not generalize to other documents that
aren’t from this window of time.
For example, let’s look at the results of a multinomial Naive Bayes classiﬁer, which is fast to train and achieves a
decent F-score:
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn import metrics
>>> newsgroups_test = fetch_20newsgroups(subset='test',
...
categories=categories)
>>> vectors_test = vectorizer.transform(newsgroups_test.data)
>>> clf = MultinomialNB(alpha=.01)
>>> clf.fit(vectors, newsgroups_train.target)
>>> pred = clf.predict(vectors_test)
>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
0.88213592402729568
(The example Classiﬁcation of text documents using sparse features shufﬂes the training and test data, instead of
segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious
yet of what’s going on inside this classiﬁer?)
530
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Let’s take a look at what the most informative features are:
>>> import numpy as np
>>> def show_top10(classifier, vectorizer, categories):
...
feature_names = np.asarray(vectorizer.get_feature_names())
...
for i, category in enumerate(categories):
...
top10 = np.argsort(classifier.coef_[i])[-10:]
...
print("%s: %s" % (category, " ".join(feature_names[top10])))
...
>>> show_top10(clf, vectorizer, newsgroups_train.target_names)
alt.atheism: sgi livesey atheists writes people caltech com god keith edu
comp.graphics: organization thanks files subject com image lines university edu
˓→graphics
sci.space: toronto moon gov com alaska access henry nasa edu space
talk.religion.misc: article writes kent people christian jesus sandvik edu com god
You can now see many things that these features have overﬁt to:
• Almost every group is distinguished by whether headers such as NNTP-Posting-Host:
and
Distribution: appear more or less often.
• Another signiﬁcant feature involves whether the sender is afﬁliated with a university, as indicated either by their
headers or their signature.
• The word “article” is a signiﬁcant feature, based on how often people quote previous posts like this: “In article
[article ID], [name] <[e-mail address]> wrote:”
• Other features match the names and e-mail addresses of particular people who were posting at the time.
With such an abundance of clues that distinguish newsgroups, the classiﬁers barely have to identify topics from text at
all, and they all perform at the same high level.
For this reason, the functions that load 20 Newsgroups data provide a parameter called remove, telling it
what kinds of information to strip out of each ﬁle.
remove should be a tuple containing any subset of
('headers','footers','quotes'), telling it to remove headers, signature blocks, and quotation blocks re-
spectively.
>>> newsgroups_test = fetch_20newsgroups(subset='test',
...
remove=('headers', 'footers', 'quotes'),
...
categories=categories)
>>> vectors_test = vectorizer.transform(newsgroups_test.data)
>>> pred = clf.predict(vectors_test)
>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')
0.77310350681274775
This classiﬁer lost over a lot of its F-score, just because we removed metadata that has little to do with topic classiﬁ-
cation. It loses even more if we also strip this metadata from the training data:
>>> newsgroups_train = fetch_20newsgroups(subset='train',
...
remove=('headers', 'footers', 'quotes'),
...
categories=categories)
>>> vectors = vectorizer.fit_transform(newsgroups_train.data)
>>> clf = MultinomialNB(alpha=.01)
>>> clf.fit(vectors, newsgroups_train.target)
>>> vectors_test = vectorizer.transform(newsgroups_test.data)
>>> pred = clf.predict(vectors_test)
>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
0.76995175184521725
3.5. Dataset loading utilities
531
scikit-learn user guide, Release 0.18.2
Some other classiﬁers cope better with this harder version of the task. Try running Sample pipeline for text feature
extraction and evaluation with and without the --filter option to compare the results.
Recommendation
When evaluating text classiﬁers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-
learn, you can do this by setting remove=('headers','footers','quotes'). The F-score will be lower
because it is more realistic.
Examples
• Sample pipeline for text feature extraction and evaluation
• Classiﬁcation of text documents using sparse features
3.5.9 Downloading datasets from the mldata.org repository
mldata.org is a public repository for machine learning data, supported by the PASCAL network .
The sklearn.datasets package is able to directly download data sets from the repository using the function
sklearn.datasets.fetch_mldata.
For example, to download the MNIST digit recognition database:
>>> from sklearn.datasets import fetch_mldata
>>> mnist = fetch_mldata('MNIST original', data_home=custom_data_home)
The MNIST database contains a total of 70000 examples of handwritten digits of size 28x28 pixels, labeled from 0 to
9:
>>> mnist.data.shape
(70000, 784)
>>> mnist.target.shape
(70000,)
>>> np.unique(mnist.target)
array([ 0.,
1.,
2.,
3.,
4.,
5.,
6.,
7.,
8.,
9.])
After the ﬁrst download, the dataset is cached locally in the path speciﬁed by the data_home keyword argument,
which defaults to ~/scikit_learn_data/:
>>> os.listdir(os.path.join(custom_data_home, 'mldata'))
['mnist-original.mat']
Data
sets
in
mldata.org
do
not
adhere
to
a
strict
naming
or
formatting
convention.
sklearn.datasets.fetch_mldata is able to make sense of the most common cases, but allows to
tailor the defaults to individual datasets:
• The data arrays in mldata.org are most often shaped as (n_features,n_samples). This is the opposite
of the scikit-learn convention, so sklearn.datasets.fetch_mldata transposes the matrix by
default. The transpose_data keyword controls this behavior:
>>> iris = fetch_mldata('iris', data_home=custom_data_home)
>>> iris.data.shape
(150, 4)
532
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
>>> iris = fetch_mldata('iris', transpose_data=False,
...
data_home=custom_data_home)
>>> iris.data.shape
(4, 150)
• For datasets with multiple columns, sklearn.datasets.fetch_mldata tries to identify the target and
data columns and rename them to target and data. This is done by looking for arrays named label and
data in the dataset, and failing that by choosing the ﬁrst array to be target and the second to be data. This
behavior can be changed with the target_name and data_name keywords, setting them to a speciﬁc name
or index number (the name and order of the columns in the datasets can be found at its mldata.org under the tab
“Data”:
>>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1, data_name=0,
...
data_home=custom_data_home)
>>> iris3 = fetch_mldata('datasets-UCI iris', target_name='class',
...
data_name='double0', data_home=custom_data_home)
3.5.10 The Labeled Faces in the Wild face recognition dataset
This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on
the ofﬁcial website:
http://vis-www.cs.umass.edu/lfw/
Each picture is centered on a single face. The typical task is called Face Veriﬁcation: given a pair of two pictures, a
binary classiﬁer must predict whether the two images are from the same person.
An alternative task, Face Recognition or Face Identiﬁcation is: given the picture of the face of an unknown person,
identify the name of the person by referring to a gallery of previously seen pictures of identiﬁed persons.
Both Face Veriﬁcation and Face Recognition are tasks that are typically performed on the output of a model trained to
perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the
OpenCV library. The LFW faces were extracted by this face detector from various online websites.
Usage
scikit-learn provides two loaders that will automatically download, cache, parse the metadata ﬁles, decode
the jpeg and convert the interesting slices into memmaped numpy arrays. This dataset size is more than 200 MB.
The ﬁrst load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG ﬁles into
numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a
memmaped version memoized on the disk in the ~/scikit_learn_data/lfw_home/ folder using joblib.
The ﬁrst loader is used for the Face Identiﬁcation task: a multi-class classiﬁcation task (hence supervised learning):
>>> from sklearn.datasets import fetch_lfw_people
>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
>>> for name in lfw_people.target_names:
...
print(name)
...
Ariel Sharon
Colin Powell
Donald Rumsfeld
George W Bush
Gerhard Schroeder
3.5. Dataset loading utilities
533
scikit-learn user guide, Release 0.18.2
Hugo Chavez
Tony Blair
The default slice is a rectangular shape around the face, removing most of the background:
>>> lfw_people.data.dtype
dtype('float32')
>>> lfw_people.data.shape
(1288, 1850)
>>> lfw_people.images.shape
(1288, 50, 37)
Each of the 1140 faces is assigned to a single person id in the target array:
>>> lfw_people.target.shape
(1288,)
>>> list(lfw_people.target[:10])
[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]
The second loader is typically used for the face veriﬁcation task: each sample is a pair of two picture belonging or not
to the same person:
>>> from sklearn.datasets import fetch_lfw_pairs
>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')
>>> list(lfw_pairs_train.target_names)
['Different persons', 'Same person']
>>> lfw_pairs_train.pairs.shape
(2200, 2, 62, 47)
>>> lfw_pairs_train.data.shape
(2200, 5828)
>>> lfw_pairs_train.target.shape
(2200,)
Both for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs
function it is possible to get an additional dimension with the RGB color channels by passing color=True, in
that case the shape will be (2200,2,62,47,3).
The sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development train
set, the development test set and an evaluation 10_folds set meant to compute performance metrics using a
10-folds cross validation scheme.
References:
• Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Gary
B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. University of Massachusetts, Amherst,
Technical Report 07-49, October, 2007.
534
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Examples
Faces recognition example using eigenfaces and SVMs
3.5.11 Forest covertypes
The samples in this dataset correspond to 30×30m patches of forest in the US, collected for the task of predicting
each patch’s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass
classiﬁcation problem. Each sample has 54 features, described on the dataset’s homepage. Some of the features are
boolean indicators, while others are discrete or continuous measurements.
sklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with
the feature matrix in the data member and the target values in target. The dataset will be downloaded from the
web if necessary.
3.5.12 RCV1 dataset
Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available
by Reuters, Ltd. for research purposes. The dataset is extensively described in 1.
sklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multil-
abels:
>>> from sklearn.datasets import fetch_rcv1
>>> rcv1 = fetch_rcv1()
It returns a dictionary-like object, with the following attributes:
data: The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values
contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in 1: The ﬁrst 23149 samples
are the training set. The last 781265 samples are the testing set. This follows the ofﬁcial LYRL2004 chronological
split. The array has 0.16% of non zero values:
>>> rcv1.data.shape
(804414, 47236)
target: The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each
sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values:
>>> rcv1.target.shape
(804414, 103)
sample_id: Each sample can be identiﬁed by its ID, ranging (with gaps) from 2286 to 810596:
>>> rcv1.sample_id[:3]
array([2286, 2287, 2288], dtype=uint32)
target_names: The target values are the topics of each sample. Each sample belongs to at least one topic, and
to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span ﬁve orders of
magnitude, from 5 occurrences for ‘GMIL’, to 381327 for ‘CCAT’:
>>> rcv1.target_names[:3].tolist()
['E11', 'ECAT', 'M11']
1 Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of
Machine Learning Research, 5, 361-397.
3.5. Dataset loading utilities
535
scikit-learn user guide, Release 0.18.2
The dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB.
References
3.5.13 Boston House Prices dataset
Notes
Data Set Characteristics:
Number of Instances 506
Number of Attributes 13 numeric/categorical predictive
:Median Value (attribute 14) is usually the target
Attribute Information (in order)
• CRIM per capita crime rate by town
• ZN proportion of residential land zoned for lots over 25,000 sq.ft.
• INDUS proportion of non-retail business acres per town
• CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
• NOX nitric oxides concentration (parts per 10 million)
• RM average number of rooms per dwelling
• AGE proportion of owner-occupied units built prior to 1940
• DIS weighted distances to ﬁve Boston employment centres
• RAD index of accessibility to radial highways
• TAX full-value property-tax rate per $10,000
• PTRATIO pupil-teacher ratio by town
• B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
• LSTAT % lower status of the population
• MEDV Median value of owner-occupied homes in $1000’s
Missing Attribute Values None
Creator Harrison, D. and Rubinfeld, D.L.
This is a copy of UCI ML housing dataset. http://archive.ics.uci.edu/ml/datasets/Housing
This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.
The Boston house-price data of Harrison, D. and Rubinfeld, D.L. ‘Hedonic prices and the demand for clean air’, J.
Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, ‘Regression diagnostics
...’, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.
The Boston house-price data has been used in many machine learning papers that address regression problems.
References
• Belsley, Kuh & Welsch, ‘Regression diagnostics: Identifying Inﬂuential Data and Sources of Collinearity’,
Wiley, 1980. 244-261.
536
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth Inter-
national Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.
• many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)
3.5.14 Breast Cancer Wisconsin (Diagnostic) Database
Notes
Data Set Characteristics:
Number of Instances 569
Number of Attributes 30 numeric, predictive attributes and the class
Attribute Information
• radius (mean of distances from center to points on the perimeter)
• texture (standard deviation of gray-scale values)
• perimeter
• area
• smoothness (local variation in radius lengths)
• compactness (perimeter^2 / area - 1.0)
• concavity (severity of concave portions of the contour)
• concave points (number of concave portions of the contour)
• symmetry
• fractal dimension (“coastline approximation” - 1)
The mean, standard error, and “worst” or largest (mean of the three largest values) of these
features were computed for each image, resulting in 30 features. For instance, ﬁeld 3 is Mean
Radius, ﬁeld 13 is Radius SE, ﬁeld 23 is Worst Radius.
• class:
– WDBC-Malignant
– WDBC-Benign
Summary Statistics
radius (mean):
6.981
28.11
texture (mean):
9.71
39.28
perimeter (mean):
43.79
188.5
area (mean):
143.5
2501.0
smoothness (mean):
0.053
0.163
compactness (mean):
0.019
0.345
concavity (mean):
0.0
0.427
concave points (mean):
0.0
0.201
symmetry (mean):
0.106
0.304
fractal dimension (mean):
0.05
0.097
radius (standard error):
0.112
2.873
Continued on next page
3.5. Dataset loading utilities
537
scikit-learn user guide, Release 0.18.2
Table 3.32 – continued from previous page
texture (standard error):
0.36
4.885
perimeter (standard error):
0.757
21.98
area (standard error):
6.802
542.2
smoothness (standard error):
0.002
0.031
compactness (standard error):
0.002
0.135
concavity (standard error):
0.0
0.396
concave points (standard error):
0.0
0.053
symmetry (standard error):
0.008
0.079
fractal dimension (standard error):
0.001
0.03
radius (worst):
7.93
36.04
texture (worst):
12.02
49.54
perimeter (worst):
50.41
251.2
area (worst):
185.2
4254.0
smoothness (worst):
0.071
0.223
compactness (worst):
0.027
1.058
concavity (worst):
0.0
1.252
concave points (worst):
0.0
0.291
symmetry (worst):
0.156
0.664
fractal dimension (worst):
0.055
0.208
Missing Attribute Values None
Class Distribution 212 - Malignant, 357 - Benign
Creator Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian
Donor Nick Street
Date November, 1995
This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https://goo.gl/U2Uwz2
Features are computed from a digitized image of a ﬁne needle aspirate (FNA) of a breast mass. They describe charac-
teristics of the cell nuclei present in the image.
Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, “Decision
Tree Construction Via Linear Programming.” Proceedings of the 4th Midwest Artiﬁcial Intelligence and Cognitive
Science Society, pp. 97-101, 1992], a classiﬁcation method which uses linear programming to construct a decision
tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.
The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K.
P. Bennett and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”,
Optimization Methods and Software 1, 1992, 23-34].
This database is also available through the UW CS ftp server:
ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/
References
• W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis.
IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905,
pages 861-870, San Jose, CA, 1993.
• O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and prognosis via linear program-
ming. Operations Research, 43(4), pages 570-577, July-August 1995.
538
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
• W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from
ﬁne-needle aspirates. Cancer Letters 77 (1994) 163-171.
3.5.15 Diabetes dataset
Notes
Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were
obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease
progression one year after baseline.
Data Set Characteristics:
Number of Instances 442
Number of Attributes First 10 columns are numeric predictive values
Target Column 11 is a quantitative measure of disease progression one year after baseline
Attributes
Age
Sex
Body mass index
Average blood pressure
S1
S2
S3
S4
S5
S6
Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times n_samples
(i.e. the sum of squares of each column totals 1).
Source URL: http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html
For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) “Least An-
gle Regression,” Annals of Statistics (with discussion), 407-499.
(http://web.stanford.edu/~hastie/Papers/LARS/
LeastAngle_2002.pdf)
3.5.16 Optical Recognition of Handwritten Digits Data Set
Notes
Data Set Characteristics:
Number of Instances 5620
Number of Attributes 64
Attribute Information 8x8 image of integer pixels in the range 0..16.
Missing Attribute Values None
3.5. Dataset loading utilities
539
scikit-learn user guide, Release 0.18.2
Creator
5. Alpaydin (alpaydin ‘@’ boun.edu.tr)
Date July; 1998
This is a copy of the test set of the UCI ML hand-written digits datasets http://archive.ics.uci.edu/ml/datasets/Optical+
Recognition+of+Handwritten+Digits
The data set contains images of hand-written digits: 10 classes where each class refers to a digit.
Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from
a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32
bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This
generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and
gives invariance to small distortions.
For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J.
Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.
References
• C. Kaynak (1995) Methods of Combining Multiple Classiﬁers and Their Applications to Handwritten Digit
Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University.
•
5. Alpaydin, C. Kaynak (1998) Cascading Classiﬁers, Kybernetika.
• Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using
relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University.
2005.
• Claudio Gentile. A New Approximate Maximal Margin Classiﬁcation Algorithm. NIPS. 2000.
3.5.17 Iris Plants Database
Notes
Data Set Characteristics:
Number of Instances 150 (50 in each of three classes)
Number of Attributes 4 numeric, predictive attributes and the class
Attribute Information
• sepal length in cm
• sepal width in cm
• petal length in cm
• petal width in cm
• class:
– Iris-Setosa
– Iris-Versicolour
– Iris-Virginica
Summary Statistics
540
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
sepal length:
4.3
7.9
5.84
0.83
0.7826
sepal width:
2.0
4.4
3.05
0.43
-0.4194
petal length:
1.0
6.9
3.76
1.76
0.9490 (high!)
petal width:
0.1
2.5
1.20
0.76
0.9565 (high!)
Missing Attribute Values None
Class Distribution 33.3% for each of 3 classes.
Creator R.A. Fisher
Donor Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
Date July, 1988
This is a copy of UCI ML iris datasets. http://archive.ics.uci.edu/ml/datasets/Iris
The famous Iris database, ﬁrst used by Sir R.A Fisher
This is perhaps the best known database to be found in the pattern recognition literature. Fisher’s paper is a classic in
the ﬁeld and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of
50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.
References
• Fisher,R.A. “The use of multiple measurements in taxonomic problems” Annual Eugenics, 7, Part II, 179-188
(1936); also in “Contributions to Mathematical Statistics” (John Wiley, NY, 1950).
• Duda,R.O., & Hart,P.E. (1973) Pattern Classiﬁcation and Scene Analysis. (Q327.D83) John Wiley & Sons.
ISBN 0-471-22361-1. See page 218.
• Dasarathy, B.V. (1980) “Nosing Around the Neighborhood: A New System Structure and Classiﬁcation Rule
for Recognition in Partially Exposed Environments”. IEEE Transactions on Pattern Analysis and Machine
Intelligence, Vol. PAMI-2, No. 1, 67-71.
• Gates, G.W. (1972) “The Reduced Nearest Neighbor Rule”. IEEE Transactions on Information Theory, May
1972, 431-433.
• See also: 1988 MLC Proceedings, 54-64. Cheeseman et al”s AUTOCLASS II conceptual clustering system
ﬁnds 3 classes in the data.
• Many, many more ...
3.5.18 Linnerrud dataset
Notes
Data Set Characteristics:
Number of Instances 20
Number of Attributes 3
Missing Attribute Values None
The Linnerud dataset constains two small dataset:
• exercise: A list containing the following components: exercise data with 20 observations on 3 exercise variables:
Weight, Waist and Pulse.
3.5. Dataset loading utilities
541
scikit-learn user guide, Release 0.18.2
• physiological: Data frame with 20 observations on 3 physiological variables: Chins, Situps and Jumps.
References
• Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.
3.6 Strategies to scale computationally: bigger data
For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed
are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to
make your system scale.
3.6.1 Scaling with instances using out-of-core learning
Out-of-core (or “external memory”) learning is a technique used to learn from data that cannot ﬁt in a computer’s main
memory (RAM).
Here is sketch of a system designed to achieve this goal:
1. a way to stream instances
2. a way to extract features from instances
3. an incremental algorithm
Streaming instances
Basically, 1. may be a reader that yields instances from ﬁles on a hard drive, a database, from a network stream etc.
However, details on how to achieve this are beyond the scope of this documentation.
Extracting features
2. could be any relevant way to extract features among the different feature extraction methods supported by scikit-
learn. However, when working with data that needs vectorization and where the set of features or values is not
known in advance one should take explicit care. A good example is text classiﬁcation where unknown terms are
likely to be found during training. It is possible to use a statefull vectorizer if making multiple passes over the
data is reasonable from an application point of view. Otherwise, one can turn up the difﬁculty by using a stateless
feature extractor. Currently the preferred way to do this is to use the so-called hashing trick as implemented by
sklearn.feature_extraction.FeatureHasher for datasets with categorical variables represented as list
of Python dicts or sklearn.feature_extraction.text.HashingVectorizer for text documents.
Incremental learning
Finally, for 3. we have a number of options inside scikit-learn. Although all algorithms cannot learn incrementally
(i.e. without seeing all the instances at once), all estimators implementing the partial_fit API are candidates.
Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called “online learning”) is key
to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the
542
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve
some tuning 1.
Here is a list of incremental estimators for different tasks:
• Classiﬁcation
– sklearn.naive_bayes.MultinomialNB
– sklearn.naive_bayes.BernoulliNB
– sklearn.linear_model.Perceptron
– sklearn.linear_model.SGDClassifier
– sklearn.linear_model.PassiveAggressiveClassifier
• Regression
– sklearn.linear_model.SGDRegressor
– sklearn.linear_model.PassiveAggressiveRegressor
• Clustering
– sklearn.cluster.MiniBatchKMeans
• Decomposition / feature Extraction
– sklearn.decomposition.MiniBatchDictionaryLearning
– sklearn.decomposition.IncrementalPCA
– sklearn.decomposition.LatentDirichletAllocation
– sklearn.cluster.MiniBatchKMeans
For classiﬁcation, a somewhat important thing to note is that although a stateless feature extraction routine may be
able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets
classes. In this case you have to pass all the possible classes to the ﬁrst partial_fit call using the classes=
parameter.
Another aspect to consider when choosing a proper algorithm is that all of them don’t put the same importance on each
example over time. Namely, the Perceptron is still sensitive to badly labeled examples even after many examples
whereas the SGD* and PassiveAggressive* families are more robust to this kind of artifacts. Conversely, the
later also tend to give less importance to remarkably different, yet properly labeled examples when they come late in
the stream as their learning rate decreases over time.
Examples
Finally, we have a full-ﬂedged example of Out-of-core classiﬁcation of text documents. It is aimed at providing a
starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed
above.
Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed
examples.
1 Depending on the algorithm the mini-batch size can inﬂuence results or not. SGD*, PassiveAggressive*, and discrete NaiveBayes are truly
online and are not affected by batch size. Conversely, MiniBatchKMeans convergence rate is affected by the batch size. Also, its memory footprint
can vary dramatically with batch size.
3.6. Strategies to scale computationally: bigger data
543
scikit-learn user guide, Release 0.18.2
Now looking at the computation time of the different parts, we see that the vectorization is much more expensive
than learning itself. From the different algorithms, MultinomialNB is the most expensive, but its overhead can be
mitigated by increasing the size of the mini-batches (exercise: change minibatch_size to 100 and 10000 in the
program and compare).
544
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Notes
3.7 Computational Performance
For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It
may also be of interest to consider the training throughput but this is often less important in a production setup (where
it often takes place ofﬂine).
We will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different
contexts and provide some tips and tricks for overcoming performance bottlenecks.
Prediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds). Latency
is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this
distribution (e.g. the 90 percentile).
Prediction throughput is deﬁned as the number of predictions the software can deliver in a given amount of time (e.g.
in predictions per second).
An important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models
(e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account
the same exact properties of the data as more complex ones.
3.7.1 Prediction Latency
One of the most straight-forward concerns one may have when using/choosing a machine learning toolkit is the latency
at which predictions can be made in a production environment.
The main factors that inﬂuence the prediction latency are
1. Number of features
2. Input data representation and sparsity
3. Model complexity
4. Feature extraction
A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode.
Bulk versus Atomic mode
In general doing predictions in bulk (many instances at the same time) is more efﬁcient for a number of reasons
(branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few
features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders
of magnitude:
3.7. Computational Performance
545
scikit-learn user guide, Release 0.18.2
To benchmark different estimators for your case you can simply change the n_features parameter in this example:
Prediction Latency. This should give you an estimate of the order of magnitude of the prediction latency.
Inﬂuence of the Number of Features
Obviously when the number of features increases so does the memory consumption of each example. Indeed, for a
matrix of 𝑀instances with 𝑁features, the space complexity is in 𝑂(𝑁𝑀). From a computing perspective it also
means that the number of basic operations (e.g., multiplications for vector-matrix products in linear models) increases
too. Here is a graph of the evolution of the prediction latency with the number of features:
546
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
Overall you can expect the prediction time to increase at least linearly with the number of features (non-linear cases
can happen depending on the global memory footprint and estimator).
Inﬂuence of the Input Data Representation
Scipy provides sparse matrix datastructures which are optimized for storing sparse data. The main feature of sparse
formats is that you don’t store zeros so if your data is sparse then you use much less memory. A non-zero value in
a sparse (CSR or CSC) representation will only take on average one 32bit integer position + the 64 bit ﬂoating point
value + an additional 32bit per row or column in the matrix. Using sparse input on a dense (or sparse) linear model
can speedup prediction by quite a bit as only the non zero valued features impact the dot product and thus the model
predictions. Hence if you have 100 non zeros in 1e6 dimensional space, you only need 100 multiply and add operation
instead of 1e6.
Calculation over a dense representation, however, may leverage highly optimised vector operations and multithreading
in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros
max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input
representation on a machine with many CPUs and an optimized BLAS implementation.
Here is sample code to test the sparsity of your input:
def sparsity_ratio(X):
return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])
print("input sparsity ratio:", sparsity_ratio(X))
As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably beneﬁt from sparse
formats. Check Scipy’s sparse matrix formats documentation for more information on how to build (or convert your
data to) sparse matrix formats. Most of the time the CSR and CSC formats work best.
Inﬂuence of the Model Complexity
Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. In-
creasing predictive power is usually interesting, but for many applications we would better not increase prediction
latency too much. We will now review this idea for different families of supervised models.
3.7. Computational Performance
547
scikit-learn user guide, Release 0.18.2
For sklearn.linear_model (e.g. Lasso, ElasticNet, SGDClassiﬁer/Regressor, Ridge & RidgeClassiﬁer, Pas-
siveAgressiveClassiﬁer/Regressor, LinearSVC, LogisticRegression...) the decision function that is applied at predic-
tion time is the same (a dot product) , so latency should be equivalent.
Here is an example using sklearn.linear_model.stochastic_gradient.SGDClassifier with the
elasticnet penalty. The regularization strength is globally controlled by the alpha parameter. With a sufﬁciently
high alpha, one can then increase the l1_ratio parameter of elasticnet to enforce various levels of sparsity
in the model coefﬁcients. Higher sparsity here is interpreted as less model complexity as we need fewer coefﬁcients to
describe it fully. Of course sparsity inﬂuences in turn the prediction time as the sparse dot-product takes time roughly
proportional to the number of non-zero coefﬁcients.
For the sklearn.svm family of algorithms with a non-linear kernel, the latency is tied to the number of support vec-
tors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support
vectors in a SVC or SVR model. The kernel will also inﬂuence the latency as it is used to compute the projection of the
input vector once per support vector. In the following graph the nu parameter of sklearn.svm.classes.NuSVR
was used to inﬂuence the number of support vectors.
For
sklearn.ensemble
of
trees
(e.g.
RandomForest,
GBT,
ExtraTrees
etc)
the
number
of
trees
and
their
depth
play
the
most
important
role.
Latency
and
throughput
should
scale
lin-
early with the number of trees.
In this case we used directly the n_estimators parameter of
sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.
548
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-
linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in
the process.
Feature Extraction Latency
Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or
optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e.
turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For
example on the Reuters text classiﬁcation task the whole preparation (reading and parsing SGML ﬁles, tokenizing the
text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code,
depending on the chosen model.
3.7. Computational Performance
549
scikit-learn user guide, Release 0.18.2
In many cases it is thus recommended to carefully time and proﬁle your feature extraction code as it may be a good
place to start optimizing when your overall latency is too slow for your application.
3.7.2 Prediction Throughput
Another important metric to care about when sizing production systems is the throughput i.e. the number of predictions
you can make in a given amount of time. Here is a benchmark from the Prediction Latency example that measures this
quantity for a number of estimators on synthetic data:
550
Chapter 3. User Guide
scikit-learn user guide, Release 0.18.2
These throughputs are achieved on a single process. An obvious way to increase the throughput of your application
is to spawn additional instances (usually processes in Python because of the GIL) that share the same model. One
might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this
documentation though.
3.7.3 Tips and Tricks
Linear algebra libraries
As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the
versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized BLAS / LAPACK
library.
Not all models beneﬁt from optimized BLAS and Lapack implementations. For instance models based on (random-
ized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs (SVC, SVR, NuSVC,
NuSVR). On the other hand a linear model implemented with a BLAS DGEMM call (via numpy.dot) will typically
beneﬁt hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized
BLAS.
You can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the
following commands:
from numpy.distutils.system_info import get_info
print(get_info('blas_opt'))
print(get_info('lapack_opt'))
Optimized BLAS / LAPACK implementations include:
• Atlas (need hardware speciﬁc tuning by rebuilding on the target machine)
• OpenBLAS
• MKL
• Apple Accelerate and vecLib frameworks (OSX only)
More information can be found on the Scipy install page and in this blog post from Daniel Nouri which has some nice
step by step install instructions for Debian / Ubuntu.
Warning: Multithreaded BLAS libraries sometimes conﬂict with Python’s multiprocessing module, which
is used by e.g. GridSearchCV and most other estimators that take an n_jobs argument (with the exception of
SGDClassifier, SGDRegressor, Perceptron, PassiveAggressiveClassifier and tree-based
methods such as random forests). This is true of Apple’s Accelerate and OpenBLAS when built with OpenMP
support.
Besides scikit-learn, NumPy and SciPy also use BLAS internally, as explained earlier.
If you experience hanging subprocesses with n_jobs>1 or n_jobs=-1, make sure you have a single-threaded
BLAS library, or set n_jobs=1, or upgrade to Python 3.4 which has a new version of multiprocessing that
should be immune to this problem.
Model Compression
Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want
to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good
idea to combine model sparsity with sparse input data representation.
3.7. Computational Performance
551
scikit-learn user guide, Release 0.18.2
Here is sample code that illustrates the use of the sparsify() method:
clf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)
clf.fit(X_train, y_train).sparsify()
clf.predict(X_test)
In this example we prefer the elasticnet penalty as it is often a good compromise between model compactness
and prediction power. One can also further tune the l1_ratio parameter (in combination with the regularization
strength alpha) to control this tradeoff.
A typical benchmark on synthetic data yields a >30% decrease in latency when both the model and input are sparse
(with 0.000024 and 0.027400 non-zero coefﬁcients ratio respectively). Your mileage may vary depending on the
sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of
predictive models deployed on production servers.
Model Reshaping
Model reshaping consists in selecting only a portion of the available features to ﬁt a model. In other words, if a
model discards features during the learning phase we can then strip those from the input. This has several beneﬁts.
Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature
selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help
reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and
building features that are discarded by the model. For instance if the raw data come from a database, it can make it
possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the
moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in CSR
format), it is generally sufﬁcient to not generate the relevant features, leaving their columns empty.
Links
• scikit-learn developer performance documentation
• Scipy sparse matrix formats documentation
552
Chapter 3. User Guide
CHAPTER
FOUR
GENERAL EXAMPLES
General-purpose and introductory examples for the scikit.
4.1 Plotting Cross-Validated Predictions
This example shows how to use cross_val_predict to visualize prediction errors.
from sklearn import datasets
from sklearn.model_selection import cross_val_predict
from sklearn import linear_model
import matplotlib.pyplot as plt
553
scikit-learn user guide, Release 0.18.2
lr = linear_model.LinearRegression()
boston = datasets.load_boston()
y = boston.target
# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, boston.data, y, cv=10)
fig, ax = plt.subplots()
ax.scatter(y, predicted)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
plt.show()
Total running time of the script: (0 minutes 0.116 seconds)
Download Python source code: plot_cv_predict.py
Download IPython notebook: plot_cv_predict.ipynb
4.2 Isotonic Regression
An illustration of the isotonic regression on generated data. The isotonic regression ﬁnds a non-decreasing approx-
imation of a function while minimizing the mean squared error on the training data. The beneﬁt of such a model is
that it does not assume any form for the target function such as linearity. For comparison a linear regression is also
presented.
print(__doc__)
# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>
#
Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
from sklearn.linear_model import LinearRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.utils import check_random_state
n = 100
x = np.arange(n)
rs = check_random_state(0)
y = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))
Fit IsotonicRegression and LinearRegression models
ir = IsotonicRegression()
y_ = ir.fit_transform(x, y)
lr = LinearRegression()
lr.fit(x[:, np.newaxis], y)
# x needs to be 2d for LinearRegression
554
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
plot result
segments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]
lc = LineCollection(segments, zorder=0)
lc.set_array(np.ones(len(y)))
lc.set_linewidths(0.5 * np.ones(n))
fig = plt.figure()
plt.plot(x, y, 'r.', markersize=12)
plt.plot(x, y_, 'g.-', markersize=12)
plt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')
plt.gca().add_collection(lc)
plt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')
plt.title('Isotonic regression')
plt.show()
Total running time of the script: (0 minutes 0.093 seconds)
Download Python source code: plot_isotonic_regression.py
Download IPython notebook: plot_isotonic_regression.ipynb
4.2. Isotonic Regression
555
scikit-learn user guide, Release 0.18.2
4.3 Concatenating multiple feature extraction methods
In many real-world examples, there are many ways to extract features from a dataset. Often it is beneﬁcial to combine
several methods to obtain good performance. This example shows how to use FeatureUnion to combine features
obtained by PCA and univariate selection.
Combining features using this transformer has the beneﬁt that it allows cross validation and grid searches over the
whole process.
The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage
of FeatureUnion.
# Author: Andreas Mueller <amueller@ais.uni-bonn.de>
#
# License: BSD 3 clause
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest
iris = load_iris()
X, y = iris.data, iris.target
# This dataset is way too high-dimensional. Better do PCA:
pca = PCA(n_components=2)
# Maybe some original features where good, too?
selection = SelectKBest(k=1)
# Build estimator from PCA and Univariate selection:
combined_features = FeatureUnion([("pca", pca), ("univ_select", selection)])
# Use combined features to transform dataset:
X_features = combined_features.fit(X, y).transform(X)
svm = SVC(kernel="linear")
# Do grid search over k, n_components and C:
pipeline = Pipeline([("features", combined_features), ("svm", svm)])
param_grid = dict(features__pca__n_components=[1, 2, 3],
features__univ_select__k=[1, 2],
svm__C=[0.1, 1, 10])
grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)
grid_search.fit(X, y)
print(grid_search.best_estimator_)
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: feature_stacker.py
Download IPython notebook: feature_stacker.ipynb
556
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
4.4 Pipelining: chaining a PCA and a logistic regression
The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.
We use a GridSearchCV to set the dimensionality of the PCA
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model, decomposition, datasets
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
logistic = linear_model.LogisticRegression()
pca = decomposition.PCA()
pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])
digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target
Plot the PCA spectrum
pca.fit(X_digits)
plt.figure(1, figsize=(4, 3))
plt.clf()
plt.axes([.2, .2, .7, .7])
plt.plot(pca.explained_variance_, linewidth=2)
plt.axis('tight')
plt.xlabel('n_components')
plt.ylabel('explained_variance_')
4.4. Pipelining: chaining a PCA and a logistic regression
557
scikit-learn user guide, Release 0.18.2
Prediction
n_components = [20, 40, 64]
Cs = np.logspace(-4, 4, 3)
#Parameters of pipelines can be set using ‘__’ separated parameter names:
estimator = GridSearchCV(pipe,
dict(pca__n_components=n_components,
logistic__C=Cs))
estimator.fit(X_digits, y_digits)
plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,
linestyle=':', label='n_components chosen')
plt.legend(prop=dict(size=12))
plt.show()
558
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 9.009 seconds)
Download Python source code: plot_digits_pipe.py
Download IPython notebook: plot_digits_pipe.ipynb
4.5 Selecting dimensionality reduction with Pipeline and Grid-
SearchCV
This example constructs a pipeline that does dimensionality reduction followed by prediction with a support vector
classiﬁer. It demonstrates the use of GridSearchCV and Pipeline to optimize over different classes of estimators in a
single CV run – unsupervised PCA and NMF dimensionality reductions are compared to univariate feature selection
during the grid search.
4.5. Selecting dimensionality reduction with Pipeline and GridSearchCV
559
scikit-learn user guide, Release 0.18.2
# Authors: Robert McGibbon, Joel Nothman
from __future__ import print_function, division
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.decomposition import PCA, NMF
from sklearn.feature_selection import SelectKBest, chi2
print(__doc__)
pipe = Pipeline([
('reduce_dim', PCA()),
('classify', LinearSVC())
])
N_FEATURES_OPTIONS = [2, 4, 8]
C_OPTIONS = [1, 10, 100, 1000]
param_grid = [
{
'reduce_dim': [PCA(iterated_power=7), NMF()],
'reduce_dim__n_components': N_FEATURES_OPTIONS,
560
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
'classify__C': C_OPTIONS
},
{
'reduce_dim': [SelectKBest(chi2)],
'reduce_dim__k': N_FEATURES_OPTIONS,
'classify__C': C_OPTIONS
},
]
reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']
grid = GridSearchCV(pipe, cv=3, n_jobs=2, param_grid=param_grid)
digits = load_digits()
grid.fit(digits.data, digits.target)
mean_scores = np.array(grid.cv_results_['mean_test_score'])
# scores are in the order of param_grid iteration, which is alphabetical
mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))
# select score for best C
mean_scores = mean_scores.max(axis=0)
bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *
(len(reducer_labels) + 1) + .5)
plt.figure()
COLORS = 'bgrcmyk'
for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):
plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])
plt.title("Comparing feature reduction techniques")
plt.xlabel('Reduced number of features')
plt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)
plt.ylabel('Digit classification accuracy')
plt.ylim((0, 1))
plt.legend(loc='upper left')
plt.show()
Total running time of the script: (0 minutes 37.027 seconds)
Download Python source code: plot_compare_reduction.py
Download IPython notebook: plot_compare_reduction.ipynb
4.6 Imputing missing values before building an estimator
This example shows that imputing the missing values can give better results than discarding the samples containing
any missing value. Imputing does not always improve the predictions, so please check via cross-validation. Sometimes
dropping rows or using marker values is more effective.
Missing values can be replaced by the mean, the median or the most frequent value using the strategy hyper-
parameter. The median is a more robust estimator for data with high magnitude variables which could dominate
results (otherwise known as a ‘long tail’).
Script output:
Score with the entire dataset = 0.56
Score without the samples containing missing values = 0.48
Score after imputation of the missing values = 0.55
4.6. Imputing missing values before building an estimator
561
scikit-learn user guide, Release 0.18.2
In this case, imputing helps the classiﬁer get close to the original score.
import numpy as np
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import Imputer
from sklearn.model_selection import cross_val_score
rng = np.random.RandomState(0)
dataset = load_boston()
X_full, y_full = dataset.data, dataset.target
n_samples = X_full.shape[0]
n_features = X_full.shape[1]
# Estimate the score on the entire dataset, with no missing values
estimator = RandomForestRegressor(random_state=0, n_estimators=100)
score = cross_val_score(estimator, X_full, y_full).mean()
print("Score with the entire dataset = %.2f" % score)
# Add missing values in 75% of the lines
missing_rate = 0.75
n_missing_samples = np.floor(n_samples * missing_rate)
missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,
dtype=np.bool),
np.ones(n_missing_samples,
dtype=np.bool)))
rng.shuffle(missing_samples)
missing_features = rng.randint(0, n_features, n_missing_samples)
# Estimate the score without the lines containing missing values
X_filtered = X_full[~missing_samples, :]
y_filtered = y_full[~missing_samples]
estimator = RandomForestRegressor(random_state=0, n_estimators=100)
score = cross_val_score(estimator, X_filtered, y_filtered).mean()
print("Score without the samples containing missing values = %.2f" % score)
# Estimate the score after imputation of the missing values
X_missing = X_full.copy()
X_missing[np.where(missing_samples)[0], missing_features] = 0
y_missing = y_full.copy()
estimator = Pipeline([("imputer", Imputer(missing_values=0,
strategy="mean",
axis=0)),
("forest", RandomForestRegressor(random_state=0,
n_estimators=100))])
score = cross_val_score(estimator, X_missing, y_missing).mean()
print("Score after imputation of the missing values = %.2f" % score)
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: missing_values.py
Download IPython notebook: missing_values.ipynb
562
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
4.7 Face completion with a multi-output estimators
This example shows the use of multi-output estimator to complete images. The goal is to predict the lower half of a
face given its upper half.
The ﬁrst column of images shows true faces. The next columns illustrate how extremely randomized trees, k nearest
neighbors, linear regression and ridge regression complete the lower half of those faces.
4.7. Face completion with a multi-output estimators
563
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces
from sklearn.utils.validation import check_random_state
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import RidgeCV
# Load the faces datasets
data = fetch_olivetti_faces()
targets = data.target
data = data.images.reshape((len(data.images), -1))
train = data[targets < 30]
test = data[targets >= 30]
# Test on independent people
# Test on a subset of people
n_faces = 5
rng = check_random_state(4)
face_ids = rng.randint(test.shape[0], size=(n_faces, ))
test = test[face_ids, :]
n_pixels = data.shape[1]
# Upper half of the faces
X_train = train[:, :(n_pixels + 1) // 2]
# Lower half of the faces
y_train = train[:, n_pixels // 2:]
X_test = test[:, :(n_pixels + 1) // 2]
y_test = test[:, n_pixels // 2:]
# Fit estimators
ESTIMATORS = {
"Extra trees": ExtraTreesRegressor(n_estimators=10, max_features=32,
random_state=0),
"K-nn": KNeighborsRegressor(),
"Linear regression": LinearRegression(),
"Ridge": RidgeCV(),
}
y_test_predict = dict()
for name, estimator in ESTIMATORS.items():
estimator.fit(X_train, y_train)
y_test_predict[name] = estimator.predict(X_test)
# Plot the completed faces
image_shape = (64, 64)
n_cols = 1 + len(ESTIMATORS)
plt.figure(figsize=(2. * n_cols, 2.26 * n_faces))
plt.suptitle("Face completion with multi-output estimators", size=16)
for i in range(n_faces):
true_face = np.hstack((X_test[i], y_test[i]))
564
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
if i:
sub = plt.subplot(n_faces, n_cols, i * n_cols + 1)
else:
sub = plt.subplot(n_faces, n_cols, i * n_cols + 1,
title="true faces")
sub.axis("off")
sub.imshow(true_face.reshape(image_shape),
cmap=plt.cm.gray,
interpolation="nearest")
for j, est in enumerate(sorted(ESTIMATORS)):
completed_face = np.hstack((X_test[i], y_test_predict[est][i]))
if i:
sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j)
else:
sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j,
title=est)
sub.axis("off")
sub.imshow(completed_face.reshape(image_shape),
cmap=plt.cm.gray,
interpolation="nearest")
plt.show()
Total running time of the script: (0 minutes 4.676 seconds)
Download Python source code: plot_multioutput_face_completion.py
Download IPython notebook: plot_multioutput_face_completion.ipynb
4.8 Multilabel classiﬁcation
This example simulates a multi-label document classiﬁcation problem. The dataset is generated randomly based on
the following process:
• pick the number of labels: n ~ Poisson(n_labels)
• n times, choose a class c: c ~ Multinomial(theta)
• pick the document length: k ~ Poisson(length)
• k times, choose a word: w ~ Multinomial(theta_c)
In the above process, rejection sampling is used to make sure that n is more than 2, and that the document length is
never zero. Likewise, we reject classes which have already been chosen. The documents that are assigned to both
classes are plotted surrounded by two colored circles.
The classiﬁcation is performed by projecting to the ﬁrst two principal components found by PCA and CCA for visual-
isation purposes, followed by using the sklearn.multiclass.OneVsRestClassifier metaclassiﬁer using
two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an
unsupervised dimensionality reduction, while CCA is used to perform a supervised one.
Note: in the plot, “unlabeled samples” does not mean that we don’t know the labels (as in semi-supervised learning)
but that the samples simply do not have a label.
4.8. Multilabel classiﬁcation
565
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_multilabel_classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import LabelBinarizer
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import CCA
def plot_hyperplane(clf, min_x, max_x, linestyle, label):
# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(min_x - 5, max_x + 5)
# make sure the line is long enough
yy = a * xx - (clf.intercept_[0]) / w[1]
plt.plot(xx, yy, linestyle, label=label)
def plot_subfigure(X, Y, subplot, title, transform):
if transform == "pca":
X = PCA(n_components=2).fit_transform(X)
elif transform == "cca":
566
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
X = CCA(n_components=2).fit(X, Y).transform(X)
else:
raise ValueError
min_x = np.min(X[:, 0])
max_x = np.max(X[:, 0])
min_y = np.min(X[:, 1])
max_y = np.max(X[:, 1])
classif = OneVsRestClassifier(SVC(kernel='linear'))
classif.fit(X, Y)
plt.subplot(2, 2, subplot)
plt.title(title)
zero_class = np.where(Y[:, 0])
one_class = np.where(Y[:, 1])
plt.scatter(X[:, 0], X[:, 1], s=40, c='gray')
plt.scatter(X[zero_class, 0], X[zero_class, 1], s=160, edgecolors='b',
facecolors='none', linewidths=2, label='Class 1')
plt.scatter(X[one_class, 0], X[one_class, 1], s=80, edgecolors='orange',
facecolors='none', linewidths=2, label='Class 2')
plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',
'Boundary\nfor class 1')
plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',
'Boundary\nfor class 2')
plt.xticks(())
plt.yticks(())
plt.xlim(min_x - .5 * max_x, max_x + .5 * max_x)
plt.ylim(min_y - .5 * max_y, max_y + .5 * max_y)
if subplot == 2:
plt.xlabel('First principal component')
plt.ylabel('Second principal component')
plt.legend(loc="upper left")
plt.figure(figsize=(8, 6))
X, Y = make_multilabel_classification(n_classes=2, n_labels=1,
allow_unlabeled=True,
random_state=1)
plot_subfigure(X, Y, 1, "With unlabeled samples + CCA", "cca")
plot_subfigure(X, Y, 2, "With unlabeled samples + PCA", "pca")
X, Y = make_multilabel_classification(n_classes=2, n_labels=1,
allow_unlabeled=False,
random_state=1)
plot_subfigure(X, Y, 3, "Without unlabeled samples + CCA", "cca")
plot_subfigure(X, Y, 4, "Without unlabeled samples + PCA", "pca")
plt.subplots_adjust(.04, .02, .97, .94, .09, .2)
plt.show()
4.8. Multilabel classiﬁcation
567
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.445 seconds)
Download Python source code: plot_multilabel.py
Download IPython notebook: plot_multilabel.ipynb
4.9 The Johnson-Lindenstrauss bound for embedding with random
projections
The Johnson-Lindenstrauss lemma states that any high dimensional dataset can be randomly projected into a lower
dimensional Euclidean space while controlling the distortion in the pairwise distances.
4.9.1 Theoretical bounds
The distortion introduced by a random projection p is asserted by the fact that p is deﬁning an eps-embedding with
good probability as deﬁned by:
(1 −𝑒𝑝𝑠)‖𝑢−𝑣‖2 < ‖𝑝(𝑢) −𝑝(𝑣)‖2 < (1 + 𝑒𝑝𝑠)‖𝑢−𝑣‖2
Where u and v are any rows taken from a dataset of shape [n_samples, n_features] and p is a projection by a random
Gaussian N(0, 1) matrix with shape [n_components, n_features] (or a sparse Achlioptas matrix).
The minimum number of components to guarantees the eps-embedding is given by:
𝑛_𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡𝑠>= 4𝑙𝑜𝑔(𝑛_𝑠𝑎𝑚𝑝𝑙𝑒𝑠)/(𝑒𝑝𝑠2/2 −𝑒𝑝𝑠3/3)
The ﬁrst plot shows that with an increasing number of samples n_samples, the minimal number of dimensions
n_components increased logarithmically in order to guarantee an eps-embedding.
The second plot shows that an increase of the admissible distortion eps allows to reduce drastically the minimal
number of dimensions n_components for a given number of samples n_samples
4.9.2 Empirical validation
We validate the above bounds on the digits dataset or on the 20 newsgroups text document (TF-IDF word frequencies)
dataset:
• for the digits dataset, some 8x8 gray level pixels data for 500 handwritten digits pictures are randomly projected
to spaces for various larger number of dimensions n_components.
• for the 20 newsgroups dataset some 500 documents with 100k features in total are projected using a
sparse random matrix to smaller euclidean spaces with various values for the target number of dimensions
n_components.
The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the –twenty-
newsgroups command line argument to this script.
For each value of n_components, we plot:
• 2D distribution of sample pairs with pairwise distances in original and projected spaces as x and y axis respec-
tively.
• 1D histogram of the ratio of those distances (projected / original).
We can see that for low values of n_components the distribution is wide with many distorted pairs and a skewed
distribution (due to the hard limit of zero ratio on the left as distances are always positives) while for larger values of
n_components the distortion is controlled and the distances are well preserved by the random projection.
568
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
4.9.3 Remarks
According to the JL lemma, projecting 500 samples without too much distortion will require at least several thousands
dimensions, irrespective of the number of features of the original dataset.
Hence using random projections on the digits dataset which only has 64 features in the input space does not make
sense: it does not allow for dimensionality reduction in this case.
On the twenty newsgroups on the other hand the dimensionality can be decreased from 56436 down to 10000 while
reasonably preserving pairwise distances.
•
•
•
4.9. The Johnson-Lindenstrauss bound for embedding with random projections
569
scikit-learn user guide, Release 0.18.2
•
•
•
•
570
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
•
Out:
Embedding 500 samples with dim 64 using various random projections
Projected 500 samples from 64 to 300 in 0.018s
Random matrix with size: 0.029MB
Mean distances rate: 1.02 (0.09)
Projected 500 samples from 64 to 1000 in 0.037s
Random matrix with size: 0.097MB
Mean distances rate: 1.00 (0.04)
Projected 500 samples from 64 to 10000 in 0.441s
Random matrix with size: 0.964MB
Mean distances rate: 1.00 (0.02)
print(__doc__)
import sys
from time import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.random_projection import johnson_lindenstrauss_min_dim
from sklearn.random_projection import SparseRandomProjection
from sklearn.datasets import fetch_20newsgroups_vectorized
from sklearn.datasets import load_digits
from sklearn.metrics.pairwise import euclidean_distances
# Part 1: plot the theoretical dependency between n_components_min and
# n_samples
# range of admissible distortions
eps_range = np.linspace(0.1, 0.99, 5)
colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(eps_range)))
# range of number of samples (observation) to embed
n_samples_range = np.logspace(1, 9, 9)
plt.figure()
for eps, color in zip(eps_range, colors):
min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)
4.9. The Johnson-Lindenstrauss bound for embedding with random projections
571
scikit-learn user guide, Release 0.18.2
plt.loglog(n_samples_range, min_n_components, color=color)
plt.legend(["eps = %0.1f" % eps for eps in eps_range], loc="lower right")
plt.xlabel("Number of observations to eps-embed")
plt.ylabel("Minimum number of dimensions")
plt.title("Johnson-Lindenstrauss bounds:\nn_samples vs n_components")
# range of admissible distortions
eps_range = np.linspace(0.01, 0.99, 100)
# range of number of samples (observation) to embed
n_samples_range = np.logspace(2, 6, 5)
colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))
plt.figure()
for n_samples, color in zip(n_samples_range, colors):
min_n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps_range)
plt.semilogy(eps_range, min_n_components, color=color)
plt.legend(["n_samples = %d" % n for n in n_samples_range], loc="upper right")
plt.xlabel("Distortion eps")
plt.ylabel("Minimum number of dimensions")
plt.title("Johnson-Lindenstrauss bounds:\nn_components vs eps")
# Part 2: perform sparse random projection of some digits images which are
# quite low dimensional and dense or documents of the 20 newsgroups dataset
# which is both high dimensional and sparse
if '--twenty-newsgroups' in sys.argv:
# Need an internet connection hence not enabled by default
data = fetch_20newsgroups_vectorized().data[:500]
else:
data = load_digits().data[:500]
n_samples, n_features = data.shape
print("Embedding %d samples with dim %d using various random projections"
% (n_samples, n_features))
n_components_range = np.array([300, 1000, 10000])
dists = euclidean_distances(data, squared=True).ravel()
# select only non-identical samples pairs
nonzero = dists != 0
dists = dists[nonzero]
for n_components in n_components_range:
t0 = time()
rp = SparseRandomProjection(n_components=n_components)
projected_data = rp.fit_transform(data)
print("Projected %d samples from %d to %d in %0.3fs"
% (n_samples, n_features, n_components, time() - t0))
if hasattr(rp, 'components_'):
n_bytes = rp.components_.data.nbytes
n_bytes += rp.components_.indices.nbytes
print("Random matrix with size: %0.3fMB" % (n_bytes / 1e6))
projected_dists = euclidean_distances(
projected_data, squared=True).ravel()[nonzero]
572
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
plt.figure()
plt.hexbin(dists, projected_dists, gridsize=100, cmap=plt.cm.PuBu)
plt.xlabel("Pairwise squared distances in original space")
plt.ylabel("Pairwise squared distances in projected space")
plt.title("Pairwise distances distribution for n_components=%d" %
n_components)
cb = plt.colorbar()
cb.set_label('Sample pairs counts')
rates = projected_dists / dists
print("Mean distances rate: %0.2f (%0.2f)"
% (np.mean(rates), np.std(rates)))
plt.figure()
plt.hist(rates, bins=50, normed=True, range=(0., 2.))
plt.xlabel("Squared distances rate: projected / original")
plt.ylabel("Distribution of samples pairs")
plt.title("Histogram of pairwise distance rates for n_components=%d" %
n_components)
# TODO: compute the expected value of eps and add them to the previous plot
# as vertical lines / region
plt.show()
Total running time of the script: (0 minutes 3.245 seconds)
Download Python source code: plot_johnson_lindenstrauss_bound.py
Download IPython notebook: plot_johnson_lindenstrauss_bound.ipynb
4.10 Comparison of kernel ridge regression and SVR
Both kernel ridge regression (KRR) and SVR learn a non-linear function by employing the kernel trick, i.e., they
learn a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the
original space. They differ in the loss functions (ridge versus epsilon-insensitive loss). In contrast to SVR, ﬁtting a
KRR can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned
model is non-sparse and thus slower than SVR at prediction-time.
This example illustrates both methods on an artiﬁcial dataset, which consists of a sinusoidal target function and strong
noise added to every ﬁfth datapoint. The ﬁrst ﬁgure compares the learned model of KRR and SVR when both com-
plexity/regularization and bandwidth of the RBF kernel are optimized using grid-search. The learned functions are
very similar; however, ﬁtting KRR is approx. seven times faster than ﬁtting SVR (both with grid-search). However,
prediction of 100000 target values is more than tree times faster with SVR since it has learned a sparse model using
only approx. 1/3 of the 100 training datapoints as support vectors.
The next ﬁgure compares the time for ﬁtting and prediction of KRR and SVR for different sizes of the training set.
Fitting KRR is faster than SVR for medium- sized training sets (less than 1000 samples); however, for larger training
sets SVR scales better. With regard to prediction time, SVR is faster than KRR for all sizes of the training set because
of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters
epsilon and C of the SVR.
# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
# License: BSD 3 clause
4.10. Comparison of kernel ridge regression and SVR
573
scikit-learn user guide, Release 0.18.2
from __future__ import division
import time
import numpy as np
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import learning_curve
from sklearn.kernel_ridge import KernelRidge
import matplotlib.pyplot as plt
rng = np.random.RandomState(0)
Generate sample data
X = 5 * rng.rand(10000, 1)
y = np.sin(X).ravel()
# Add noise to targets
y[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))
X_plot = np.linspace(0, 5, 100000)[:, None]
Fit regression model
train_size = 100
svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,
param_grid={"C": [1e0, 1e1, 1e2, 1e3],
"gamma": np.logspace(-2, 2, 5)})
kr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), cv=5,
param_grid={"alpha": [1e0, 0.1, 1e-2, 1e-3],
"gamma": np.logspace(-2, 2, 5)})
t0 = time.time()
svr.fit(X[:train_size], y[:train_size])
svr_fit = time.time() - t0
print("SVR complexity and bandwidth selected and model fitted in %.3f s"
% svr_fit)
t0 = time.time()
kr.fit(X[:train_size], y[:train_size])
kr_fit = time.time() - t0
print("KRR complexity and bandwidth selected and model fitted in %.3f s"
% kr_fit)
sv_ratio = svr.best_estimator_.support_.shape[0] / train_size
print("Support vector ratio: %.3f" % sv_ratio)
t0 = time.time()
y_svr = svr.predict(X_plot)
svr_predict = time.time() - t0
print("SVR prediction for %d inputs in %.3f s"
% (X_plot.shape[0], svr_predict))
t0 = time.time()
y_kr = kr.predict(X_plot)
574
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
kr_predict = time.time() - t0
print("KRR prediction for %d inputs in %.3f s"
% (X_plot.shape[0], kr_predict))
Out:
SVR complexity and bandwidth selected and model fitted in 0.912 s
KRR complexity and bandwidth selected and model fitted in 0.547 s
Support vector ratio: 0.320
SVR prediction for 100000 inputs in 0.147 s
KRR prediction for 100000 inputs in 0.393 s
look at the results
sv_ind = svr.best_estimator_.support_
plt.scatter(X[sv_ind], y[sv_ind], c='r', s=50, label='SVR support vectors',
zorder=2)
plt.scatter(X[:100], y[:100], c='k', label='data', zorder=1)
plt.hold('on')
plt.plot(X_plot, y_svr, c='r',
label='SVR (fit: %.3fs, predict: %.3fs)' % (svr_fit, svr_predict))
plt.plot(X_plot, y_kr, c='g',
label='KRR (fit: %.3fs, predict: %.3fs)' % (kr_fit, kr_predict))
plt.xlabel('data')
plt.ylabel('target')
plt.title('SVR versus Kernel Ridge')
plt.legend()
# Visualize training and prediction time
plt.figure()
# Generate sample data
X = 5 * rng.rand(10000, 1)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))
sizes = np.logspace(1, 4, 7, dtype=np.int)
for name, estimator in {"KRR": KernelRidge(kernel='rbf', alpha=0.1,
gamma=10),
"SVR": SVR(kernel='rbf', C=1e1, gamma=10)}.items():
train_time = []
test_time = []
for train_test_size in sizes:
t0 = time.time()
estimator.fit(X[:train_test_size], y[:train_test_size])
train_time.append(time.time() - t0)
t0 = time.time()
estimator.predict(X_plot[:1000])
test_time.append(time.time() - t0)
plt.plot(sizes, train_time, 'o-', color="r" if name == "SVR" else "g",
label="%s (train)" % name)
plt.plot(sizes, test_time, 'o--', color="r" if name == "SVR" else "g",
label="%s (test)" % name)
plt.xscale("log")
plt.yscale("log")
plt.xlabel("Train size")
4.10. Comparison of kernel ridge regression and SVR
575
scikit-learn user guide, Release 0.18.2
plt.ylabel("Time (seconds)")
plt.title('Execution Time')
plt.legend(loc="best")
# Visualize learning curves
plt.figure()
svr = SVR(kernel='rbf', C=1e1, gamma=0.1)
kr = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.1)
train_sizes, train_scores_svr, test_scores_svr = \
learning_curve(svr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),
scoring="neg_mean_squared_error", cv=10)
train_sizes_abs, train_scores_kr, test_scores_kr = \
learning_curve(kr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),
scoring="neg_mean_squared_error", cv=10)
plt.plot(train_sizes, -test_scores_svr.mean(1), 'o-', color="r",
label="SVR")
plt.plot(train_sizes, -test_scores_kr.mean(1), 'o-', color="g",
label="KRR")
plt.xlabel("Train size")
plt.ylabel("Mean Squared Error")
plt.title('Learning curves')
plt.legend(loc="best")
plt.show()
•
•
576
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
•
Total running time of the script: (0 minutes 56.535 seconds)
Download Python source code: plot_kernel_ridge_regression.py
Download IPython notebook: plot_kernel_ridge_regression.ipynb
4.11 Feature Union with Heterogeneous Data Sources
Datasets can often contain components of that require different feature extraction and processing pipelines. This
scenario might occur when:
1. Your dataset consists of heterogeneous data types (e.g. raster images and text captions)
2. Your dataset is stored in a Pandas DataFrame and different columns require different processing pipelines.
This example demonstrates how to use sklearn.feature_extraction.FeatureUnion on a dataset con-
taining different types of features. We use the 20-newsgroups dataset and compute standard bag-of-words features for
the subject line and body in separate pipelines as well as ad hoc features on the body. We combine them (with weights)
using a FeatureUnion and ﬁnally train a classiﬁer on the combined set of features.
The choice of features is not particularly helpful, but serves to illustrate the technique.
# Author: Matt Terry <matt.terry@gmail.com>
#
# License: BSD 3 clause
from __future__ import print_function
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.datasets import fetch_20newsgroups
from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer
from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report
from sklearn.pipeline import FeatureUnion
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
class ItemSelector(BaseEstimator, TransformerMixin):
4.11. Feature Union with Heterogeneous Data Sources
577
scikit-learn user guide, Release 0.18.2
"""For data grouped by feature, select subset of data at a provided key.
The data is expected to be stored in a 2D data structure, where the first
index is over features and the second is over samples.
i.e.
>> len(data[key]) == n_samples
Please note that this is the opposite convention to scikit-learn feature
matrixes (where the first index corresponds to sample).
ItemSelector only requires that the collection implement getitem
(data[key]).
Examples include: a dict of lists, 2D numpy array, Pandas
DataFrame, numpy record array, etc.
>> data = {'a': [1, 5, 2, 5, 2, 8],
'b': [9, 4, 1, 4, 1, 3]}
>> ds = ItemSelector(key='a')
>> data['a'] == ds.transform(data)
ItemSelector is not designed to handle data grouped by sample.
(e.g. a
list of dicts).
If your data is structured this way, consider a
transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.
Parameters
----------
key : hashable, required
The key corresponding to the desired value in a mappable.
"""
def __init__(self, key):
self.key = key
def fit(self, x, y=None):
return self
def transform(self, data_dict):
return data_dict[self.key]
class TextStats(BaseEstimator, TransformerMixin):
"""Extract features from each document for DictVectorizer"""
def fit(self, x, y=None):
return self
def transform(self, posts):
return [{'length': len(text),
'num_sentences': text.count('.')}
for text in posts]
class SubjectBodyExtractor(BaseEstimator, TransformerMixin):
"""Extract the subject & body from a usenet post in a single pass.
Takes a sequence of strings and produces a dict of sequences.
Keys are
`subject` and `body`.
"""
def fit(self, x, y=None):
return self
578
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
def transform(self, posts):
features = np.recarray(shape=(len(posts),),
dtype=[('subject', object), ('body', object)])
for i, text in enumerate(posts):
headers, _, bod = text.partition('\n\n')
bod = strip_newsgroup_footer(bod)
bod = strip_newsgroup_quoting(bod)
features['body'][i] = bod
prefix = 'Subject:'
sub = ''
for line in headers.split('\n'):
if line.startswith(prefix):
sub = line[len(prefix):]
break
features['subject'][i] = sub
return features
pipeline = Pipeline([
# Extract the subject & body
('subjectbody', SubjectBodyExtractor()),
# Use FeatureUnion to combine the features from subject and body
('union', FeatureUnion(
transformer_list=[
# Pipeline for pulling features from the post's subject line
('subject', Pipeline([
('selector', ItemSelector(key='subject')),
('tfidf', TfidfVectorizer(min_df=50)),
])),
# Pipeline for standard bag-of-words model for body
('body_bow', Pipeline([
('selector', ItemSelector(key='body')),
('tfidf', TfidfVectorizer()),
('best', TruncatedSVD(n_components=50)),
])),
# Pipeline for pulling ad hoc features from post's body
('body_stats', Pipeline([
('selector', ItemSelector(key='body')),
('stats', TextStats()),
# returns a list of dicts
('vect', DictVectorizer()),
# list of dicts -> feature matrix
])),
],
# weight components in FeatureUnion
transformer_weights={
'subject': 0.8,
'body_bow': 0.5,
'body_stats': 1.0,
},
)),
4.11. Feature Union with Heterogeneous Data Sources
579
scikit-learn user guide, Release 0.18.2
# Use a SVC classifier on the combined features
('svc', SVC(kernel='linear')),
])
# limit the list of categories to make running this example faster.
categories = ['alt.atheism', 'talk.religion.misc']
train = fetch_20newsgroups(random_state=1,
subset='train',
categories=categories,
)
test = fetch_20newsgroups(random_state=1,
subset='test',
categories=categories,
)
pipeline.fit(train.data, train.target)
y = pipeline.predict(test.data)
print(classification_report(y, test.target))
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: hetero_feature_union.py
Download IPython notebook: hetero_feature_union.ipynb
4.12 Explicit feature map approximation for RBF kernels
An example illustrating the approximation of the feature map of an RBF kernel.
It shows how to use RBFSampler and Nystroem to approximate the feature map of an RBF kernel for classiﬁcation
with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approx-
imate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte
Carlo samplings (in the case of RBFSampler, which uses random Fourier features) and different sized subsets of the
training set (for Nystroem) for the approximate mapping are shown.
Please note that the dataset here is not large enough to show the beneﬁts of kernel approximation, as the exact SVM is
still reasonably fast.
Sampling more dimensions clearly leads to better classiﬁcation results, but comes at a greater cost. This means there
is a tradeoff between runtime and accuracy, given by the parameter n_components. Note that solving the Linear
SVM and also the approximate kernel SVM could be greatly accelerated by using stochastic gradient descent via
sklearn.linear_model.SGDClassifier. This is not easily possible for the case of the kernelized SVM.
The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel
maps. The plot shows decision surfaces of the classiﬁers projected onto the ﬁrst two principal components of the data.
This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface
in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classiﬁed into the
region it is lying in, since it will not lie on the plane that the ﬁrst two principal components span.
The usage of RBFSampler and Nystroem is described in detail in Kernel Approximation.
580
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
•
•
print(__doc__)
# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>
#
Andreas Mueller <amueller@ais.uni-bonn.de>
# License: BSD 3 clause
# Standard scientific Python imports
import matplotlib.pyplot as plt
import numpy as np
from time import time
# Import datasets, classifiers and performance metrics
from sklearn import datasets, svm, pipeline
from sklearn.kernel_approximation import (RBFSampler,
Nystroem)
from sklearn.decomposition import PCA
# The digits dataset
digits = datasets.load_digits(n_class=9)
# To apply an classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.data)
data = digits.data / 16.
data -= data.mean(axis=0)
# We learn the digits on the first half of the digits
data_train, targets_train = (data[:n_samples // 2],
digits.target[:n_samples // 2])
4.12. Explicit feature map approximation for RBF kernels
581
scikit-learn user guide, Release 0.18.2
# Now predict the value of the digit on the second half:
data_test, targets_test = (data[n_samples // 2:],
digits.target[n_samples // 2:])
# data_test = scaler.transform(data_test)
# Create a classifier: a support vector classifier
kernel_svm = svm.SVC(gamma=.2)
linear_svm = svm.LinearSVC()
# create pipeline from kernel approximation
# and linear svm
feature_map_fourier = RBFSampler(gamma=.2, random_state=1)
feature_map_nystroem = Nystroem(gamma=.2, random_state=1)
fourier_approx_svm = pipeline.Pipeline([("feature_map", feature_map_fourier),
("svm", svm.LinearSVC())])
nystroem_approx_svm = pipeline.Pipeline([("feature_map", feature_map_nystroem),
("svm", svm.LinearSVC())])
# fit and predict using linear and kernel svm:
kernel_svm_time = time()
kernel_svm.fit(data_train, targets_train)
kernel_svm_score = kernel_svm.score(data_test, targets_test)
kernel_svm_time = time() - kernel_svm_time
linear_svm_time = time()
linear_svm.fit(data_train, targets_train)
linear_svm_score = linear_svm.score(data_test, targets_test)
linear_svm_time = time() - linear_svm_time
sample_sizes = 30 * np.arange(1, 10)
fourier_scores = []
nystroem_scores = []
fourier_times = []
nystroem_times = []
for D in sample_sizes:
fourier_approx_svm.set_params(feature_map__n_components=D)
nystroem_approx_svm.set_params(feature_map__n_components=D)
start = time()
nystroem_approx_svm.fit(data_train, targets_train)
nystroem_times.append(time() - start)
start = time()
fourier_approx_svm.fit(data_train, targets_train)
fourier_times.append(time() - start)
fourier_score = fourier_approx_svm.score(data_test, targets_test)
nystroem_score = nystroem_approx_svm.score(data_test, targets_test)
nystroem_scores.append(nystroem_score)
fourier_scores.append(fourier_score)
# plot the results:
plt.figure(figsize=(8, 8))
accuracy = plt.subplot(211)
# second y axis for timeings
582
Chapter 4. General examples
scikit-learn user guide, Release 0.18.2
timescale = plt.subplot(212)
accuracy.plot(sample_sizes, nystroem_scores, label="Nystroem approx. kernel")
timescale.plot(sample_sizes, nystroem_times, '--',
label='Nystroem approx. kernel')
accuracy.plot(sample_sizes, fourier_scores, label="Fourier approx. kernel")
timescale.plot(sample_sizes, fourier_times, '--',
label='Fourier approx. kernel')
# horizontal lines for exact rbf and linear kernels:
accuracy.plot([sample_sizes[0], sample_sizes[-1]],
[linear_svm_score, linear_svm_score], label="linear svm")
timescale.plot([sample_sizes[0], sample_sizes[-1]],
[linear_svm_time, linear_svm_time], '--', label='linear svm')
accuracy.plot([sample_sizes[0], sample_sizes[-1]],
[kernel_svm_score, kernel_svm_score], label="rbf svm")
timescale.plot([sample_sizes[0], sample_sizes[-1]],
[kernel_svm_time, kernel_svm_time], '--', label='rbf svm')
# vertical line for dataset dimensionality = 64
accuracy.plot([64, 64], [0.7, 1], label="n_features")
# legends and labels
accuracy.set_title("Classification accuracy")
timescale.set_title("Training times")
accuracy.set_xlim(sample_sizes[0], sample_sizes[-1])
accuracy.set_xticks(())
accuracy.set_ylim(np.min(fourier_scores), 1)
timescale.set_xlabel("Sampling steps = transformed feature dimension")
accuracy.set_ylabel("Classification accuracy")
timescale.set_ylabel("Training time in seconds")
accuracy.legend(loc='best')
timescale.legend(loc='best')
# visualize the decision surface, projected down to the first
# two principal components of the dataset
pca = PCA(n_components=8).fit(data_train)
X = pca.transform(data_train)
# Generate grid along first two principal components
multiples = np.arange(-2, 2, 0.1)
# steps along first component
first = multiples[:, np.newaxis] * pca.components_[0, :]
# steps along second component
second = multiples[:, np.newaxis] * pca.components_[1, :]
# combine
grid = first[np.newaxis, :, :] + second[:, np.newaxis, :]
flat_grid = grid.reshape(-1, data.shape[1])
# title for the plots
titles = ['SVC with rbf kernel',
'SVC (linear kernel)\n with Fourier rbf feature map\n'
'n_components=100',
'SVC (linear kernel)\n with Nystroem rbf feature map\n'
'n_components=100']
4.12. Explicit feature map approximation for RBF kernels
583
scikit-learn user guide, Release 0.18.2
plt.tight_layout()
plt.figure(figsize=(12, 5))
# predict and plot
for i, clf in enumerate((kernel_svm, nystroem_approx_svm,
fourier_approx_svm)):
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
plt.subplot(1, 3, i + 1)
Z = clf.predict(flat_grid)
# Put the result into a color plot
Z = Z.reshape(grid.shape[:-1])
plt.contourf(multiples, multiples, Z, cmap=plt.cm.Paired)
plt.axis('off')
# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=targets_train, cmap=plt.cm.Paired)
plt.title(titles[i])
plt.tight_layout()
plt.show()
Total running time of the script: (0 minutes 2.322 seconds)
Download Python source code: plot_kernel_approximation.py
Download IPython notebook: plot_kernel_approximation.ipynb
584
Chapter 4. General examples
CHAPTER
FIVE
EXAMPLES BASED ON REAL WORLD DATASETS
Applications to real world problems with some medium sized datasets or interactive user interface.
5.1 Topic extraction with Non-negative Matrix Factorization and La-
tent Dirichlet Allocation
This is an example of applying Non-negative Matrix Factorization and Latent Dirichlet Allocation on a corpus of doc-
uments and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented
as a list of terms (weights are not shown).
The default parameters (n_samples / n_features / n_topics) should make the example runnable in a couple of tens of
seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial
in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).
# Author: Olivier Grisel <olivier.grisel@ensta.org>
#
Lars Buitinck
#
Chyi-Kwei Yau <chyikwei.yau@gmail.com>
# License: BSD 3 clause
from __future__ import print_function
from time import time
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation
from sklearn.datasets import fetch_20newsgroups
n_samples = 2000
n_features = 1000
n_topics = 10
n_top_words = 20
def print_top_words(model, feature_names, n_top_words):
for topic_idx, topic in enumerate(model.components_):
print("Topic #%d:" % topic_idx)
print(" ".join([feature_names[i]
for i in topic.argsort()[:-n_top_words - 1:-1]]))
print()
# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics
# to filter out useless terms early on: the posts are stripped of headers,
# footers and quoted replies, and common English words, words occurring in
585
scikit-learn user guide, Release 0.18.2
# only one document or in at least 95% of the documents are removed.
print("Loading dataset...")
t0 = time()
dataset = fetch_20newsgroups(shuffle=True, random_state=1,
remove=('headers', 'footers', 'quotes'))
data_samples = dataset.data[:n_samples]
print("done in %0.3fs." % (time() - t0))
# Use tf-idf features for NMF.
print("Extracting tf-idf features for NMF...")
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,
max_features=n_features,
stop_words='english')
t0 = time()
tfidf = tfidf_vectorizer.fit_transform(data_samples)
print("done in %0.3fs." % (time() - t0))
# Use tf (raw term count) features for LDA.
print("Extracting tf features for LDA...")
tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,
max_features=n_features,
stop_words='english')
t0 = time()
tf = tf_vectorizer.fit_transform(data_samples)
print("done in %0.3fs." % (time() - t0))
# Fit the NMF model
print("Fitting the NMF model with tf-idf features, "
"n_samples=%d and n_features=%d..."
% (n_samples, n_features))
t0 = time()
nmf = NMF(n_components=n_topics, random_state=1,
alpha=.1, l1_ratio=.5).fit(tfidf)
print("done in %0.3fs." % (time() - t0))
print("\nTopics in NMF model:")
tfidf_feature_names = tfidf_vectorizer.get_feature_names()
print_top_words(nmf, tfidf_feature_names, n_top_words)
print("Fitting LDA models with tf features, "
"n_samples=%d and n_features=%d..."
% (n_samples, n_features))
lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,
learning_method='online',
learning_offset=50.,
random_state=0)
t0 = time()
lda.fit(tf)
print("done in %0.3fs." % (time() - t0))
print("\nTopics in LDA model:")
tf_feature_names = tf_vectorizer.get_feature_names()
print_top_words(lda, tf_feature_names, n_top_words)
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: topics_extraction_with_nmf_lda.py
586
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
Download IPython notebook: topics_extraction_with_nmf_lda.ipynb
5.2 Outlier detection on a real data set
This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier
detection and for a better understanding of the data structure.
We selected two sets of two variables from the Boston housing data set as an illustration of what kind of analysis can
be done with several outlier detection tools. For the purpose of visualization, we are working with two-dimensional
examples, but one should be aware that things are not so trivial in high-dimension, as it will be pointed out.
In both examples below, the main result is that the empirical covariance estimate, as a non-robust one, is highly
inﬂuenced by the heterogeneous structure of the observations. Although the robust covariance estimate is able to
focus on the main mode of the data distribution, it sticks to the assumption that the data should be Gaussian distributed,
yielding some biased estimation of the data structure, but yet accurate to some extent. The One-Class SVM does not
assume any parametric form of the data distribution and can therefore model the complex shape of the data much
better.
5.2.1 First example
The ﬁrst example illustrates how robust covariance estimation can help concentrating on a relevant cluster when an-
other one exists. Here, many observations are confounded into one and break down the empirical covariance estima-
tion. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines,
Gaussian Mixture Models, univariate outlier detection, ...). But had it been a high-dimensional example, none of these
could be applied that easily.
5.2.2 Second example
The second example shows the ability of the Minimum Covariance Determinant robust estimator of covariance to
concentrate on the main mode of the data distribution: the location seems to be well estimated, although the covariance
is hard to estimate due to the banana-shaped distribution. Anyway, we can get rid of some outlying observations. The
One-Class SVM is able to capture the real data structure, but the difﬁculty is to adjust its kernel bandwidth parameter
so as to obtain a good compromise between the shape of the data scatter matrix and the risk of over-ﬁtting the data.
•
5.2. Outlier detection on a real data set
587
scikit-learn user guide, Release 0.18.2
•
print(__doc__)
# Author: Virgile Fritsch <virgile.fritsch@inria.fr>
# License: BSD 3 clause
import numpy as np
from sklearn.covariance import EllipticEnvelope
from sklearn.svm import OneClassSVM
import matplotlib.pyplot as plt
import matplotlib.font_manager
from sklearn.datasets import load_boston
# Get data
X1 = load_boston()['data'][:, [8, 10]]
# two clusters
X2 = load_boston()['data'][:, [5, 12]]
# "banana"-shaped
# Define "classifiers" to be used
classifiers = {
"Empirical Covariance": EllipticEnvelope(support_fraction=1.,
contamination=0.261),
"Robust Covariance (Minimum Covariance Determinant)":
EllipticEnvelope(contamination=0.261),
"OCSVM": OneClassSVM(nu=0.261, gamma=0.05)}
colors = ['m', 'g', 'b']
legend1 = {}
legend2 = {}
# Learn a frontier for outlier detection with several classifiers
xx1, yy1 = np.meshgrid(np.linspace(-8, 28, 500), np.linspace(3, 40, 500))
xx2, yy2 = np.meshgrid(np.linspace(3, 10, 500), np.linspace(-5, 45, 500))
for i, (clf_name, clf) in enumerate(classifiers.items()):
plt.figure(1)
clf.fit(X1)
Z1 = clf.decision_function(np.c_[xx1.ravel(), yy1.ravel()])
Z1 = Z1.reshape(xx1.shape)
legend1[clf_name] = plt.contour(
xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i])
plt.figure(2)
clf.fit(X2)
Z2 = clf.decision_function(np.c_[xx2.ravel(), yy2.ravel()])
Z2 = Z2.reshape(xx2.shape)
legend2[clf_name] = plt.contour(
xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i])
588
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
legend1_values_list = list(legend1.values())
legend1_keys_list = list(legend1.keys())
# Plot the results (= shape of the data points cloud)
plt.figure(1)
# two clusters
plt.title("Outlier detection on a real data set (boston housing)")
plt.scatter(X1[:, 0], X1[:, 1], color='black')
bbox_args = dict(boxstyle="round", fc="0.8")
arrow_args = dict(arrowstyle="->")
plt.annotate("several confounded points", xy=(24, 19),
xycoords="data", textcoords="data",
xytext=(13, 10), bbox=bbox_args, arrowprops=arrow_args)
plt.xlim((xx1.min(), xx1.max()))
plt.ylim((yy1.min(), yy1.max()))
plt.legend((legend1_values_list[0].collections[0],
legend1_values_list[1].collections[0],
legend1_values_list[2].collections[0]),
(legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2]),
loc="upper center",
prop=matplotlib.font_manager.FontProperties(size=12))
plt.ylabel("accessibility to radial highways")
plt.xlabel("pupil-teacher ratio by town")
legend2_values_list = list(legend2.values())
legend2_keys_list = list(legend2.keys())
plt.figure(2)
# "banana" shape
plt.title("Outlier detection on a real data set (boston housing)")
plt.scatter(X2[:, 0], X2[:, 1], color='black')
plt.xlim((xx2.min(), xx2.max()))
plt.ylim((yy2.min(), yy2.max()))
plt.legend((legend2_values_list[0].collections[0],
legend2_values_list[1].collections[0],
legend2_values_list[2].collections[0]),
(legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),
loc="upper center",
prop=matplotlib.font_manager.FontProperties(size=12))
plt.ylabel("% lower status of the population")
plt.xlabel("average number of rooms per dwelling")
plt.show()
Total running time of the script: (0 minutes 4.336 seconds)
Download Python source code: plot_outlier_detection_housing.py
Download IPython notebook: plot_outlier_detection_housing.ipynb
5.3 Compressive sensing: tomography reconstruction with L1 prior
(Lasso)
This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles.
Such a dataset is acquired in computed tomography (CT).
Without any prior information on the sample, the number of projections required to reconstruct the image is of the
5.3. Compressive sensing: tomography reconstruction with L1 prior (Lasso)
589
scikit-learn user guide, Release 0.18.2
order of the linear size l of the image (in pixels). For simplicity we consider here a sparse image, where only pixels
on the boundary of objects have a non-zero value. Such data could correspond for example to a cellular material.
Note however that most images are sparse in a different basis, such as the Haar wavelets. Only l/7 projections are
acquired, therefore it is necessary to use prior information available on the sample (its sparsity): this is an example of
compressive sensing.
The tomography projection operation is a linear transformation. In addition to the data-ﬁdelity term corresponding
to a linear regression, we penalize the L1 norm of the image to account for its sparsity. The resulting optimization
problem is called the Lasso. We use the class sklearn.linear_model.Lasso, that uses the coordinate descent
algorithm. Importantly, this implementation is more computationally efﬁcient on a sparse matrix, than the projection
operator used here.
The reconstruction with L1 penalization gives a result with zero error (all pixels are successfully labeled with 0 or 1),
even if noise was added to the projections. In comparison, an L2 penalization (sklearn.linear_model.Ridge)
produces a large number of labeling errors for the pixels. Important artifacts are observed on the reconstructed image,
contrary to the L1 penalization. Note in particular the circular artifact separating the pixels in the corners, that have
contributed to fewer projections than the central disk.
print(__doc__)
# Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>
# License: BSD 3 clause
import numpy as np
from scipy import sparse
from scipy import ndimage
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
def _weights(x, dx=1, orig=0):
x = np.ravel(x)
floor_x = np.floor((x - orig) / dx)
alpha = (x - orig - floor_x * dx) / dx
return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))
def _generate_center_coordinates(l_x):
X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)
590
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
center = l_x / 2.
X += 0.5 - center
Y += 0.5 - center
return X, Y
def build_projection_operator(l_x, n_dir):
""" Compute the tomography design matrix.
Parameters
----------
l_x : int
linear size of image array
n_dir : int
number of angles at which projections are acquired.
Returns
-------
p : sparse matrix of shape (n_dir l_x, l_x**2)
"""
X, Y = _generate_center_coordinates(l_x)
angles = np.linspace(0, np.pi, n_dir, endpoint=False)
data_inds, weights, camera_inds = [], [], []
data_unravel_indices = np.arange(l_x ** 2)
data_unravel_indices = np.hstack((data_unravel_indices,
data_unravel_indices))
for i, angle in enumerate(angles):
Xrot = np.cos(angle) * X - np.sin(angle) * Y
inds, w = _weights(Xrot, dx=1, orig=X.min())
mask = np.logical_and(inds >= 0, inds < l_x)
weights += list(w[mask])
camera_inds += list(inds[mask] + i * l_x)
data_inds += list(data_unravel_indices[mask])
proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))
return proj_operator
def generate_synthetic_data():
""" Synthetic binary data """
rs = np.random.RandomState(0)
n_pts = 36
x, y = np.ogrid[0:l, 0:l]
mask_outer = (x - l / 2) ** 2 + (y - l / 2) ** 2 < (l / 2) ** 2
mask = np.zeros((l, l))
points = l * rs.rand(2, n_pts)
mask[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1
mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)
res = np.logical_and(mask > mask.mean(), mask_outer)
return np.logical_xor(res, ndimage.binary_erosion(res))
# Generate synthetic images, and projections
l = 128
proj_operator = build_projection_operator(l, l / 7.)
data = generate_synthetic_data()
proj = proj_operator * data.ravel()[:, np.newaxis]
5.3. Compressive sensing: tomography reconstruction with L1 prior (Lasso)
591
scikit-learn user guide, Release 0.18.2
proj += 0.15 * np.random.randn(*proj.shape)
# Reconstruction with L2 (Ridge) penalization
rgr_ridge = Ridge(alpha=0.2)
rgr_ridge.fit(proj_operator, proj.ravel())
rec_l2 = rgr_ridge.coef_.reshape(l, l)
# Reconstruction with L1 (Lasso) penalization
# the best value of alpha was determined using cross validation
# with LassoCV
rgr_lasso = Lasso(alpha=0.001)
rgr_lasso.fit(proj_operator, proj.ravel())
rec_l1 = rgr_lasso.coef_.reshape(l, l)
plt.figure(figsize=(8, 3.3))
plt.subplot(131)
plt.imshow(data, cmap=plt.cm.gray, interpolation='nearest')
plt.axis('off')
plt.title('original image')
plt.subplot(132)
plt.imshow(rec_l2, cmap=plt.cm.gray, interpolation='nearest')
plt.title('L2 penalization')
plt.axis('off')
plt.subplot(133)
plt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest')
plt.title('L1 penalization')
plt.axis('off')
plt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0,
right=1)
plt.show()
Total running time of the script: (0 minutes 9.272 seconds)
Download Python source code: plot_tomography_l1_reconstruction.py
Download IPython notebook: plot_tomography_l1_reconstruction.ipynb
5.4 Faces recognition example using eigenfaces and SVMs
The dataset used in this example is a preprocessed excerpt of the “Labeled Faces in the Wild”, aka LFW:
http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)
Expected results for the top 5 most represented people in the dataset:
Ariel Sharon
0.67
0.92
0.77
13
Colin Powell
0.75
0.78
0.76
60
Donald Rumsfeld
0.78
0.67
0.72
27
George W Bush
0.86
0.86
0.86
146
Gerhard Schroeder
0.76
0.76
0.76
25
Hugo Chavez
0.67
0.67
0.67
15
Tony Blair
0.81
0.69
0.75
36
avg / total
0.80
0.80
0.80
322
592
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
from __future__ import print_function
from time import time
import logging
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA
from sklearn.svm import SVC
print(__doc__)
# Display progress logs on stdout
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')
Download the data, if not already on disk and load it as numpy arrays
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
# introspect the images arrays to find the shapes (for plotting)
n_samples, h, w = lfw_people.images.shape
# for machine learning we use the 2 data directly (as relative pixel
# positions info is ignored by this model)
X = lfw_people.data
n_features = X.shape[1]
# the label to predict is the id of the person
y = lfw_people.target
target_names = lfw_people.target_names
n_classes = target_names.shape[0]
print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)
print("n_classes: %d" % n_classes)
Split into a training set and a test set using a stratiﬁed k fold
# split into a training and testing set
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.25, random_state=42)
Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled dataset): unsupervised feature extraction /
dimensionality reduction
n_components = 150
print("Extracting the top %d eigenfaces from %d faces"
% (n_components, X_train.shape[0]))
t0 = time()
pca = PCA(n_components=n_components, svd_solver='randomized',
whiten=True).fit(X_train)
5.4. Faces recognition example using eigenfaces and SVMs
593
scikit-learn user guide, Release 0.18.2
print("done in %0.3fs" % (time() - t0))
eigenfaces = pca.components_.reshape((n_components, h, w))
print("Projecting the input data on the eigenfaces orthonormal basis")
t0 = time()
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print("done in %0.3fs" % (time() - t0))
Train a SVM classiﬁcation model
print("Fitting the classifier to the training set")
t0 = time()
param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }
clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)
clf = clf.fit(X_train_pca, y_train)
print("done in %0.3fs" % (time() - t0))
print("Best estimator found by grid search:")
print(clf.best_estimator_)
Quantitative evaluation of the model quality on the test set
print("Predicting people's names on the test set")
t0 = time()
y_pred = clf.predict(X_test_pca)
print("done in %0.3fs" % (time() - t0))
print(classification_report(y_test, y_pred, target_names=target_names))
print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))
Qualitative evaluation of the predictions using matplotlib
def plot_gallery(images, titles, h, w, n_row=3, n_col=4):
"""Helper function to plot a gallery of portraits"""
plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))
plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)
for i in range(n_row * n_col):
plt.subplot(n_row, n_col, i + 1)
plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)
plt.title(titles[i], size=12)
plt.xticks(())
plt.yticks(())
# plot the result of the prediction on a portion of the test set
def title(y_pred, y_test, target_names, i):
pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]
true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]
return 'predicted: %s\ntrue:
%s' % (pred_name, true_name)
prediction_titles = [title(y_pred, y_test, target_names, i)
for i in range(y_pred.shape[0])]
plot_gallery(X_test, prediction_titles, h, w)
594
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
# plot the gallery of the most significative eigenfaces
eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
plot_gallery(eigenfaces, eigenface_titles, h, w)
plt.show()
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: face_recognition.py
Download IPython notebook: face_recognition.ipynb
5.5 Model Complexity Inﬂuence
Demonstrate how model complexity inﬂuences both prediction accuracy and computational performance.
The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classiﬁcation).
For each class of models we make the model complexity vary through the choice of relevant model parameters and
measure the inﬂuence on both computational performance (latency) and predictive power (MSE or Hamming Loss).
print(__doc__)
# Author: Eustache Diemert <eustache@diemert.fr>
# License: BSD 3 clause
import time
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1.parasite_axes import host_subplot
from mpl_toolkits.axisartist.axislines import Axes
from scipy.sparse.csr import csr_matrix
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error
from sklearn.svm.classes import NuSVR
from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor
from sklearn.linear_model.stochastic_gradient import SGDClassifier
from sklearn.metrics import hamming_loss
Routines
# initialize random generator
np.random.seed(0)
def generate_data(case, sparse=False):
"""Generate regression/classification data."""
bunch = None
if case == 'regression':
bunch = datasets.load_boston()
elif case == 'classification':
bunch = datasets.fetch_20newsgroups_vectorized(subset='all')
X, y = shuffle(bunch.data, bunch.target)
offset = int(X.shape[0] * 0.8)
5.5. Model Complexity Inﬂuence
595
scikit-learn user guide, Release 0.18.2
X_train, y_train = X[:offset], y[:offset]
X_test, y_test = X[offset:], y[offset:]
if sparse:
X_train = csr_matrix(X_train)
X_test = csr_matrix(X_test)
else:
X_train = np.array(X_train)
X_test = np.array(X_test)
y_test = np.array(y_test)
y_train = np.array(y_train)
data = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train,
'y_test': y_test}
return data
def benchmark_influence(conf):
"""
Benchmark influence of :changing_param: on both MSE and latency.
"""
prediction_times = []
prediction_powers = []
complexities = []
for param_value in conf['changing_param_values']:
conf['tuned_params'][conf['changing_param']] = param_value
estimator = conf['estimator'](**conf['tuned_params'])
print("Benchmarking %s" % estimator)
estimator.fit(conf['data']['X_train'], conf['data']['y_train'])
conf['postfit_hook'](estimator)
complexity = conf['complexity_computer'](estimator)
complexities.append(complexity)
start_time = time.time()
for _ in range(conf['n_samples']):
y_pred = estimator.predict(conf['data']['X_test'])
elapsed_time = (time.time() - start_time) / float(conf['n_samples'])
prediction_times.append(elapsed_time)
pred_score = conf['prediction_performance_computer'](
conf['data']['y_test'], y_pred)
prediction_powers.append(pred_score)
print("Complexity: %d | %s: %.4f | Pred. Time: %fs\n" % (
complexity, conf['prediction_performance_label'], pred_score,
elapsed_time))
return prediction_powers, prediction_times, complexities
def plot_influence(conf, mse_values, prediction_times, complexities):
"""
Plot influence of model complexity on both accuracy and latency.
"""
plt.figure(figsize=(12, 6))
host = host_subplot(111, axes_class=Axes)
plt.subplots_adjust(right=0.75)
par1 = host.twinx()
host.set_xlabel('Model Complexity (%s)' % conf['complexity_label'])
y1_label = conf['prediction_performance_label']
y2_label = "Time (s)"
host.set_ylabel(y1_label)
par1.set_ylabel(y2_label)
p1, = host.plot(complexities, mse_values, 'b-', label="prediction error")
596
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
p2, = par1.plot(complexities, prediction_times, 'r-',
label="latency")
host.legend(loc='upper right')
host.axis["left"].label.set_color(p1.get_color())
par1.axis["right"].label.set_color(p2.get_color())
plt.title('Influence of Model Complexity - %s' % conf['estimator'].__name__)
plt.show()
def _count_nonzero_coefficients(estimator):
a = estimator.coef_.toarray()
return np.count_nonzero(a)
main code
regression_data = generate_data('regression')
classification_data = generate_data('classification', sparse=True)
configurations = [
{'estimator': SGDClassifier,
'tuned_params': {'penalty': 'elasticnet', 'alpha': 0.001, 'loss':
'modified_huber', 'fit_intercept': True},
'changing_param': 'l1_ratio',
'changing_param_values': [0.25, 0.5, 0.75, 0.9],
'complexity_label': 'non_zero coefficients',
'complexity_computer': _count_nonzero_coefficients,
'prediction_performance_computer': hamming_loss,
'prediction_performance_label': 'Hamming Loss (Misclassification Ratio)',
'postfit_hook': lambda x: x.sparsify(),
'data': classification_data,
'n_samples': 30},
{'estimator': NuSVR,
'tuned_params': {'C': 1e3, 'gamma': 2 ** -15},
'changing_param': 'nu',
'changing_param_values': [0.1, 0.25, 0.5, 0.75, 0.9],
'complexity_label': 'n_support_vectors',
'complexity_computer': lambda x: len(x.support_vectors_),
'data': regression_data,
'postfit_hook': lambda x: x,
'prediction_performance_computer': mean_squared_error,
'prediction_performance_label': 'MSE',
'n_samples': 30},
{'estimator': GradientBoostingRegressor,
'tuned_params': {'loss': 'ls'},
'changing_param': 'n_estimators',
'changing_param_values': [10, 50, 100, 200, 500],
'complexity_label': 'n_trees',
'complexity_computer': lambda x: x.n_estimators,
'data': regression_data,
'postfit_hook': lambda x: x,
'prediction_performance_computer': mean_squared_error,
'prediction_performance_label': 'MSE',
'n_samples': 30},
]
for conf in configurations:
prediction_performances, prediction_times, complexities = \
benchmark_influence(conf)
plot_influence(conf, prediction_performances, prediction_times,
complexities)
5.5. Model Complexity Inﬂuence
597
scikit-learn user guide, Release 0.18.2
•
•
•
Out:
Benchmarking SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.
˓→1,
eta0=0.0, fit_intercept=True, l1_ratio=0.25,
learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,
penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,
verbose=0, warm_start=False)
Complexity: 4454 | Hamming Loss (Misclassification Ratio): 0.2501 | Pred. Time: 0.
˓→026299s
Benchmarking SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,
eta0=0.0, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',
loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',
power_t=0.5, random_state=None, shuffle=True, verbose=0,
warm_start=False)
Complexity: 1624 | Hamming Loss (Misclassification Ratio): 0.2923 | Pred. Time: 0.
˓→019267s
Benchmarking SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,
eta0=0.0, fit_intercept=True, l1_ratio=0.75,
learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,
penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,
verbose=0, warm_start=False)
Complexity: 873 | Hamming Loss (Misclassification Ratio): 0.3191 | Pred. Time: 0.
˓→015463s
Benchmarking SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,
598
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
eta0=0.0, fit_intercept=True, l1_ratio=0.9, learning_rate='optimal',
loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',
power_t=0.5, random_state=None, shuffle=True, verbose=0,
warm_start=False)
Complexity: 655 | Hamming Loss (Misclassification Ratio): 0.3252 | Pred. Time: 0.
˓→013982s
Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-
˓→05,
kernel='rbf', max_iter=-1, nu=0.1, shrinking=True, tol=0.001,
verbose=False)
Complexity: 69 | MSE: 31.8133 | Pred. Time: 0.000363s
Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-
˓→05,
kernel='rbf', max_iter=-1, nu=0.25, shrinking=True, tol=0.001,
verbose=False)
Complexity: 136 | MSE: 25.6140 | Pred. Time: 0.000647s
Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-
˓→05,
kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,
verbose=False)
Complexity: 243 | MSE: 22.3315 | Pred. Time: 0.001103s
Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-
˓→05,
kernel='rbf', max_iter=-1, nu=0.75, shrinking=True, tol=0.001,
verbose=False)
Complexity: 350 | MSE: 21.3679 | Pred. Time: 0.001559s
Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-
˓→05,
kernel='rbf', max_iter=-1, nu=0.9, shrinking=True, tol=0.001,
verbose=False)
Complexity: 404 | MSE: 21.0915 | Pred. Time: 0.001788s
Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
learning_rate=0.1, loss='ls', max_depth=3, max_features=None,
max_leaf_nodes=None, min_impurity_split=1e-07,
min_samples_leaf=1, min_samples_split=2,
min_weight_fraction_leaf=0.0, n_estimators=10, presort='auto',
random_state=None, subsample=1.0, verbose=0, warm_start=False)
Complexity: 10 | MSE: 28.9793 | Pred. Time: 0.000111s
Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
learning_rate=0.1, loss='ls', max_depth=3, max_features=None,
max_leaf_nodes=None, min_impurity_split=1e-07,
min_samples_leaf=1, min_samples_split=2,
min_weight_fraction_leaf=0.0, n_estimators=50, presort='auto',
random_state=None, subsample=1.0, verbose=0, warm_start=False)
Complexity: 50 | MSE: 8.3398 | Pred. Time: 0.000196s
Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
learning_rate=0.1, loss='ls', max_depth=3, max_features=None,
max_leaf_nodes=None, min_impurity_split=1e-07,
min_samples_leaf=1, min_samples_split=2,
min_weight_fraction_leaf=0.0, n_estimators=100,
5.5. Model Complexity Inﬂuence
599
scikit-learn user guide, Release 0.18.2
presort='auto', random_state=None, subsample=1.0, verbose=0,
warm_start=False)
Complexity: 100 | MSE: 7.0096 | Pred. Time: 0.000274s
Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
learning_rate=0.1, loss='ls', max_depth=3, max_features=None,
max_leaf_nodes=None, min_impurity_split=1e-07,
min_samples_leaf=1, min_samples_split=2,
min_weight_fraction_leaf=0.0, n_estimators=200,
presort='auto', random_state=None, subsample=1.0, verbose=0,
warm_start=False)
Complexity: 200 | MSE: 6.1836 | Pred. Time: 0.000431s
Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
learning_rate=0.1, loss='ls', max_depth=3, max_features=None,
max_leaf_nodes=None, min_impurity_split=1e-07,
min_samples_leaf=1, min_samples_split=2,
min_weight_fraction_leaf=0.0, n_estimators=500,
presort='auto', random_state=None, subsample=1.0, verbose=0,
warm_start=False)
Complexity: 500 | MSE: 6.3426 | Pred. Time: 0.000931s
Total running time of the script: (0 minutes 24.125 seconds)
Download Python source code: plot_model_complexity_influence.py
Download IPython notebook: plot_model_complexity_influence.ipynb
5.6 Species distribution modeling
Modeling species’ geographic distributions is an important problem in conservation biology. In this example we model
the geographic distribution of two south american mammals given past observations and 14 environmental variables.
Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density
estimation problem and use the OneClassSVM provided by the package sklearn.svm as our modeling tool. The dataset
is provided by Phillips et. al. (2006). If available, the example uses basemap to plot the coast lines and national
boundaries of South America.
The two species are:
• “Bradypus variegatus” , the Brown-throated Sloth.
• “Microryzomys minutus” , also known as the Forest Small Rice Rat, a rodent that lives in Peru, Colombia,
Ecuador, Peru, and Venezuela.
5.6.1 References
• “Maximum entropy modeling of species geographic distributions” S. J. Phillips, R. P. Anderson, R. E. Schapire
- Ecological Modelling, 190:231-259, 2006.
600
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
Out:
________________________________________________________________________________
Modeling distribution of species 'bradypus variegatus'
- fit OneClassSVM ... done.
- plot coastlines from coverage
- predict species distribution
Area under the ROC curve : 0.868443
________________________________________________________________________________
Modeling distribution of species 'microryzomys minutus'
- fit OneClassSVM ... done.
- plot coastlines from coverage
- predict species distribution
Area under the ROC curve : 0.993919
time elapsed: 7.57s
# Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>
#
Jake Vanderplas <vanderplas@astro.washington.edu>
5.6. Species distribution modeling
601
scikit-learn user guide, Release 0.18.2
#
# License: BSD 3 clause
from __future__ import print_function
from time import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets.base import Bunch
from sklearn.datasets import fetch_species_distributions
from sklearn.datasets.species_distributions import construct_grids
from sklearn import svm, metrics
# if basemap is available, we'll use it.
# otherwise, we'll improvise later...
try:
from mpl_toolkits.basemap import Basemap
basemap = True
except ImportError:
basemap = False
print(__doc__)
def create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):
"""Create a bunch with information about a particular organism
This will use the test/train record arrays to extract the
data specific to the given species name.
"""
bunch = Bunch(name=' '.join(species_name.split("_")[:2]))
species_name = species_name.encode('ascii')
points = dict(test=test, train=train)
for label, pts in points.items():
# choose points associated with the desired species
pts = pts[pts['species'] == species_name]
bunch['pts_%s' % label] = pts
# determine coverage values for each of the training & testing points
ix = np.searchsorted(xgrid, pts['dd long'])
iy = np.searchsorted(ygrid, pts['dd lat'])
bunch['cov_%s' % label] = coverages[:, -iy, ix].T
return bunch
def plot_species_distribution(species=("bradypus_variegatus_0",
"microryzomys_minutus_0")):
"""
Plot the species distribution.
"""
if len(species) > 2:
print("Note: when more than two species are provided,"
" only the first two will be used")
602
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
t0 = time()
# Load the compressed data
data = fetch_species_distributions()
# Set up the data grid
xgrid, ygrid = construct_grids(data)
# The grid in x,y coordinates
X, Y = np.meshgrid(xgrid, ygrid[::-1])
# create a bunch for each species
BV_bunch = create_species_bunch(species[0],
data.train, data.test,
data.coverages, xgrid, ygrid)
MM_bunch = create_species_bunch(species[1],
data.train, data.test,
data.coverages, xgrid, ygrid)
# background points (grid coordinates) for evaluation
np.random.seed(13)
background_points = np.c_[np.random.randint(low=0, high=data.Ny,
size=10000),
np.random.randint(low=0, high=data.Nx,
size=10000)].T
# We'll make use of the fact that coverages[6] has measurements at all
# land points.
This will help us decide between land and water.
land_reference = data.coverages[6]
# Fit, predict, and plot for each species.
for i, species in enumerate([BV_bunch, MM_bunch]):
print("_" * 80)
print("Modeling distribution of species '%s'" % species.name)
# Standardize features
mean = species.cov_train.mean(axis=0)
std = species.cov_train.std(axis=0)
train_cover_std = (species.cov_train - mean) / std
# Fit OneClassSVM
print(" - fit OneClassSVM ... ", end='')
clf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.5)
clf.fit(train_cover_std)
print("done.")
# Plot map of South America
plt.subplot(1, 2, i + 1)
if basemap:
print(" - plot coastlines using basemap")
m = Basemap(projection='cyl', llcrnrlat=Y.min(),
urcrnrlat=Y.max(), llcrnrlon=X.min(),
urcrnrlon=X.max(), resolution='c')
m.drawcoastlines()
m.drawcountries()
else:
print(" - plot coastlines from coverage")
plt.contour(X, Y, land_reference,
5.6. Species distribution modeling
603
scikit-learn user guide, Release 0.18.2
levels=[-9999], colors="k",
linestyles="solid")
plt.xticks([])
plt.yticks([])
print(" - predict species distribution")
# Predict species distribution using the training data
Z = np.ones((data.Ny, data.Nx), dtype=np.float64)
# We'll predict only for the land points.
idx = np.where(land_reference > -9999)
coverages_land = data.coverages[:, idx[0], idx[1]].T
pred = clf.decision_function((coverages_land - mean) / std)[:, 0]
Z *= pred.min()
Z[idx[0], idx[1]] = pred
levels = np.linspace(Z.min(), Z.max(), 25)
Z[land_reference == -9999] = -9999
# plot contours of the prediction
plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)
plt.colorbar(format='%.2f')
# scatter training/testing points
plt.scatter(species.pts_train['dd long'], species.pts_train['dd lat'],
s=2 ** 2, c='black',
marker='^', label='train')
plt.scatter(species.pts_test['dd long'], species.pts_test['dd lat'],
s=2 ** 2, c='black',
marker='x', label='test')
plt.legend()
plt.title(species.name)
plt.axis('equal')
# Compute AUC with regards to background points
pred_background = Z[background_points[0], background_points[1]]
pred_test = clf.decision_function((species.cov_test - mean)
/ std)[:, 0]
scores = np.r_[pred_test, pred_background]
y = np.r_[np.ones(pred_test.shape), np.zeros(pred_background.shape)]
fpr, tpr, thresholds = metrics.roc_curve(y, scores)
roc_auc = metrics.auc(fpr, tpr)
plt.text(-35, -70, "AUC: %.3f" % roc_auc, ha="right")
print("\n Area under the ROC curve : %f" % roc_auc)
print("\ntime elapsed: %.2fs" % (time() - t0))
plot_species_distribution()
plt.show()
Total running time of the script: (0 minutes 7.567 seconds)
Download Python source code: plot_species_distribution_modeling.py
Download IPython notebook: plot_species_distribution_modeling.ipynb
604
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
5.7 Visualizing the stock market structure
This example employs several unsupervised learning techniques to extract the stock market structure from variations
in historical quotes.
The quantity that we use is the daily variation in quote price: quotes that are linked tend to coﬂuctuate during a day.
5.7.1 Learning a graph structure
We use sparse inverse covariance estimation to ﬁnd which quotes are correlated conditionally on the others. Speciﬁ-
cally, sparse inverse covariance gives us a graph, that is a list of connection. For each symbol, the symbols that it is
connected too are those useful to explain its ﬂuctuations.
5.7.2 Clustering
We use clustering to group together quotes that behave similarly. Here, amongst the various clustering techniques
available in the scikit-learn, we use Afﬁnity Propagation as it does not enforce equal-size clusters, and it can choose
automatically the number of clusters from the data.
Note that this gives us a different indication than the graph, as the graph reﬂects conditional relations between variables,
while the clustering reﬂects marginal properties: variables clustered together can be considered as having a similar
impact at the level of the full stock market.
5.7.3 Embedding in 2D space
For visualization purposes, we need to lay out the different symbols on a 2D canvas. For this we use Manifold learning
techniques to retrieve 2D embedding.
5.7.4 Visualization
The output of the 3 models are combined in a 2D graph where nodes represents the stocks and edges the:
• cluster labels are used to deﬁne the color of the nodes
• the sparse covariance model is used to display the strength of the edges
• the 2D embedding is used to position the nodes in the plan
This example has a fair amount of visualization-related code, as visualization is crucial here to display the graph. One
of the challenge is to position the labels minimizing overlap. For this we use an heuristic based on the direction of the
nearest neighbor along each axis.
print(__doc__)
# Author: Gael Varoquaux gael.varoquaux@normalesup.org
# License: BSD 3 clause
from datetime import datetime
import numpy as np
from matplotlib import pyplot as plt
from matplotlib.collections import LineCollection
from six.moves.urllib.request import urlopen
5.7. Visualizing the stock market structure
605
scikit-learn user guide, Release 0.18.2
from six.moves.urllib.parse import urlencode
from sklearn import cluster, covariance, manifold
Retrieve the data from Internet
def quotes_historical_google(symbol, date1, date2):
"""Get the historical data from Google finance.
Parameters
----------
symbol : str
Ticker symbol to query for, for example ``"DELL"``.
date1 : datetime.datetime
Start date.
date2 : datetime.datetime
End date.
Returns
-------
X : array
The columns are ``date`` -- datetime, ``open``, ``high``,
``low``, ``close`` and ``volume`` of type float.
"""
params = urlencode({
'q': symbol,
'startdate': date1.strftime('%b %d, %Y'),
'enddate': date2.strftime('%b %d, %Y'),
'output': 'csv'
})
url = 'http://www.google.com/finance/historical?' + params
response = urlopen(url)
dtype = {
'names': ['date', 'open', 'high', 'low', 'close', 'volume'],
'formats': ['object', 'f4', 'f4', 'f4', 'f4', 'f4']
}
converters = {0: lambda s: datetime.strptime(s.decode(), '%d-%b-%y')}
return np.genfromtxt(response, delimiter=',', skip_header=1,
dtype=dtype, converters=converters,
missing_values='-', filling_values=-1)
# Choose a time period reasonably calm (not too long ago so that we get
# high-tech firms, and before the 2008 crash)
d1 = datetime(2003, 1, 1)
d2 = datetime(2008, 1, 1)
symbol_dict = {
'TOT': 'Total',
'XOM': 'Exxon',
'CVX': 'Chevron',
'COP': 'ConocoPhillips',
'VLO': 'Valero Energy',
'MSFT': 'Microsoft',
'IBM': 'IBM',
'TWX': 'Time Warner',
'CMCSA': 'Comcast',
'CVC': 'Cablevision',
'YHOO': 'Yahoo',
606
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
'DELL': 'Dell',
'HPQ': 'HP',
'AMZN': 'Amazon',
'TM': 'Toyota',
'CAJ': 'Canon',
'SNE': 'Sony',
'F': 'Ford',
'HMC': 'Honda',
'NAV': 'Navistar',
'NOC': 'Northrop Grumman',
'BA': 'Boeing',
'KO': 'Coca Cola',
'MMM': '3M',
'MCD': 'McDonald\'s',
'PEP': 'Pepsi',
'K': 'Kellogg',
'UN': 'Unilever',
'MAR': 'Marriott',
'PG': 'Procter Gamble',
'CL': 'Colgate-Palmolive',
'GE': 'General Electrics',
'WFC': 'Wells Fargo',
'JPM': 'JPMorgan Chase',
'AIG': 'AIG',
'AXP': 'American express',
'BAC': 'Bank of America',
'GS': 'Goldman Sachs',
'AAPL': 'Apple',
'SAP': 'SAP',
'CSCO': 'Cisco',
'TXN': 'Texas Instruments',
'XRX': 'Xerox',
'WMT': 'Wal-Mart',
'HD': 'Home Depot',
'GSK': 'GlaxoSmithKline',
'PFE': 'Pfizer',
'SNY': 'Sanofi-Aventis',
'NVS': 'Novartis',
'KMB': 'Kimberly-Clark',
'R': 'Ryder',
'GD': 'General Dynamics',
'RTN': 'Raytheon',
'CVS': 'CVS',
'CAT': 'Caterpillar',
'DD': 'DuPont de Nemours'}
symbols, names = np.array(list(symbol_dict.items())).T
quotes = [
quotes_historical_google(symbol, d1, d2) for symbol in symbols
]
close_prices = np.vstack([q['close'] for q in quotes])
open_prices = np.vstack([q['open'] for q in quotes])
# The daily variations of the quotes are what carry most information
variation = close_prices - open_prices
5.7. Visualizing the stock market structure
607
scikit-learn user guide, Release 0.18.2
Learn a graphical structure from the correlations
edge_model = covariance.GraphLassoCV()
# standardize the time series: using correlations rather than covariance
# is more efficient for structure recovery
X = variation.copy().T
X /= X.std(axis=0)
edge_model.fit(X)
Cluster using afﬁnity propagation
_, labels = cluster.affinity_propagation(edge_model.covariance_)
n_labels = labels.max()
for i in range(n_labels + 1):
print('Cluster %i: %s' % ((i + 1), ', '.join(names[labels == i])))
Out:
Cluster 1: American express
Cluster 2: Boeing
Cluster 3: Pepsi, Coca Cola, Kellogg
Cluster 4: Navistar
Cluster 5: Apple, Amazon, Yahoo
Cluster 6: GlaxoSmithKline, Novartis, Sanofi-Aventis
Cluster 7: ConocoPhillips, Chevron, Total, Valero Energy, Exxon
Cluster 8: Time Warner
Cluster 9: Cablevision
Cluster 10: Sony, Caterpillar, Canon, Toyota, Honda, Xerox, Unilever
Cluster 11: Kimberly-Clark, Colgate-Palmolive, Procter Gamble
Cluster 12: Ryder, Goldman Sachs, Wal-Mart, General Electrics, Pfizer, Marriott, 3M,
˓→Comcast, Wells Fargo, DuPont de Nemours, CVS, Bank of America, AIG, Home Depot,
˓→Ford, JPMorgan Chase, McDonald's
Cluster 13: Microsoft, SAP, IBM, Texas Instruments, HP, Dell, Cisco
Cluster 14: Raytheon, General Dynamics, Northrop Grumman
Find a low-dimension embedding for visualization: ﬁnd the best position of the nodes (the stocks) on a 2D plane
# We use a dense eigen_solver to achieve reproducibility (arpack is
# initiated with random vectors that we don't control). In addition, we
# use a large number of neighbors to capture the large-scale structure.
node_position_model = manifold.LocallyLinearEmbedding(
n_components=2, eigen_solver='dense', n_neighbors=6)
embedding = node_position_model.fit_transform(X.T).T
Visualization
plt.figure(1, facecolor='w', figsize=(10, 8))
plt.clf()
ax = plt.axes([0., 0., 1., 1.])
plt.axis('off')
# Display a graph of the partial correlations
partial_correlations = edge_model.precision_.copy()
d = 1 / np.sqrt(np.diag(partial_correlations))
partial_correlations *= d
partial_correlations *= d[:, np.newaxis]
608
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
non_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)
# Plot the nodes using the coordinates of our embedding
plt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,
cmap=plt.cm.spectral)
# Plot the edges
start_idx, end_idx = np.where(non_zero)
# a sequence of (*line0*, *line1*, *line2*), where::
#
linen = (x0, y0), (x1, y1), ... (xm, ym)
segments = [[embedding[:, start], embedding[:, stop]]
for start, stop in zip(start_idx, end_idx)]
values = np.abs(partial_correlations[non_zero])
lc = LineCollection(segments,
zorder=0, cmap=plt.cm.hot_r,
norm=plt.Normalize(0, .7 * values.max()))
lc.set_array(values)
lc.set_linewidths(15 * values)
ax.add_collection(lc)
# Add a label to each node. The challenge here is that we want to
# position the labels to avoid overlap with other labels
for index, (name, label, (x, y)) in enumerate(
zip(names, labels, embedding.T)):
dx = x - embedding[0]
dx[index] = 1
dy = y - embedding[1]
dy[index] = 1
this_dx = dx[np.argmin(np.abs(dy))]
this_dy = dy[np.argmin(np.abs(dx))]
if this_dx > 0:
horizontalalignment = 'left'
x = x + .002
else:
horizontalalignment = 'right'
x = x - .002
if this_dy > 0:
verticalalignment = 'bottom'
y = y + .002
else:
verticalalignment = 'top'
y = y - .002
plt.text(x, y, name, size=10,
horizontalalignment=horizontalalignment,
verticalalignment=verticalalignment,
bbox=dict(facecolor='w',
edgecolor=plt.cm.spectral(label / float(n_labels)),
alpha=.6))
plt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),
embedding[0].max() + .10 * embedding[0].ptp(),)
plt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),
embedding[1].max() + .03 * embedding[1].ptp())
plt.show()
5.7. Visualizing the stock market structure
609
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 33.937 seconds)
Download Python source code: plot_stock_market.py
Download IPython notebook: plot_stock_market.ipynb
5.8 Wikipedia principal eigenvector
A classical way to assert the relative importance of vertices in a graph is to compute the principal eigenvector of the
adjacency matrix so as to assign to each vertex the values of the components of the ﬁrst eigenvector as a centrality
score:
https://en.wikipedia.org/wiki/Eigenvector_centrality
On the graph of webpages and links those values are called the PageRank scores by Google.
The goal of this example is to analyze the graph of links inside wikipedia articles to rank articles by relative importance
according to this eigenvector centrality.
The traditional way to compute the principal eigenvector is to use the power iteration method:
https://en.wikipedia.org/wiki/Power_iteration
Here the computation is achieved thanks to Martinsson’s Randomized SVD algorithm implemented in the scikit.
610
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
The graph data is fetched from the DBpedia dumps. DBpedia is an extraction of the latent structured data of the
Wikipedia content.
# Author: Olivier Grisel <olivier.grisel@ensta.org>
# License: BSD 3 clause
from __future__ import print_function
from bz2 import BZ2File
import os
from datetime import datetime
from pprint import pprint
from time import time
import numpy as np
from scipy import sparse
from sklearn.decomposition import randomized_svd
from sklearn.externals.joblib import Memory
from sklearn.externals.six.moves.urllib.request import urlopen
from sklearn.externals.six import iteritems
print(__doc__)
Where to download the data, if not already on disk
redirects_url = "http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2"
redirects_filename = redirects_url.rsplit("/", 1)[1]
page_links_url = "http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2"
page_links_filename = page_links_url.rsplit("/", 1)[1]
resources = [
(redirects_url, redirects_filename),
(page_links_url, page_links_filename),
]
for url, filename in resources:
if not os.path.exists(filename):
print("Downloading data from '%s', please wait..." % url)
opener = urlopen(url)
open(filename, 'wb').write(opener.read())
print()
Loading the redirect ﬁles
memory = Memory(cachedir=".")
def index(redirects, index_map, k):
"""Find the index of an article name after redirect resolution"""
k = redirects.get(k, k)
return index_map.setdefault(k, len(index_map))
DBPEDIA_RESOURCE_PREFIX_LEN = len("http://dbpedia.org/resource/")
SHORTNAME_SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)
5.8. Wikipedia principal eigenvector
611
scikit-learn user guide, Release 0.18.2
def short_name(nt_uri):
"""Remove the < and > URI markers and the common URI prefix"""
return nt_uri[SHORTNAME_SLICE]
def get_redirects(redirects_filename):
"""Parse the redirections and build a transitively closed map out of it"""
redirects = {}
print("Parsing the NT redirect file")
for l, line in enumerate(BZ2File(redirects_filename)):
split = line.split()
if len(split) != 4:
print("ignoring malformed line: " + line)
continue
redirects[short_name(split[0])] = short_name(split[2])
if l % 1000000 == 0:
print("[%s] line: %08d" % (datetime.now().isoformat(), l))
# compute the transitive closure
print("Computing the transitive closure of the redirect relation")
for l, source in enumerate(redirects.keys()):
transitive_target = None
target = redirects[source]
seen = set([source])
while True:
transitive_target = target
target = redirects.get(target)
if target is None or target in seen:
break
seen.add(target)
redirects[source] = transitive_target
if l % 1000000 == 0:
print("[%s] line: %08d" % (datetime.now().isoformat(), l))
return redirects
# disabling joblib as the pickling of large dicts seems much too slow
#@memory.cache
def get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):
"""Extract the adjacency graph as a scipy sparse matrix
Redirects are resolved first.
Returns X, the scipy sparse adjacency matrix, redirects as python
dict from article names to article names and index_map a python dict
from article names to python int (article indexes).
"""
print("Computing the redirect map")
redirects = get_redirects(redirects_filename)
print("Computing the integer index map")
index_map = dict()
links = list()
for l, line in enumerate(BZ2File(page_links_filename)):
612
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
split = line.split()
if len(split) != 4:
print("ignoring malformed line: " + line)
continue
i = index(redirects, index_map, short_name(split[0]))
j = index(redirects, index_map, short_name(split[2]))
links.append((i, j))
if l % 1000000 == 0:
print("[%s] line: %08d" % (datetime.now().isoformat(), l))
if limit is not None and l >= limit - 1:
break
print("Computing the adjacency matrix")
X = sparse.lil_matrix((len(index_map), len(index_map)), dtype=np.float32)
for i, j in links:
X[i, j] = 1.0
del links
print("Converting to CSR representation")
X = X.tocsr()
print("CSR conversion done")
return X, redirects, index_map
# stop after 5M links to make it possible to work in RAM
X, redirects, index_map = get_adjacency_matrix(
redirects_filename, page_links_filename, limit=5000000)
names = dict((i, name) for name, i in iteritems(index_map))
print("Computing the principal singular vectors using randomized_svd")
t0 = time()
U, s, V = randomized_svd(X, 5, n_iter=3)
print("done in %0.3fs" % (time() - t0))
# print the names of the wikipedia related strongest components of the
# principal singular vector which should be similar to the highest eigenvector
print("Top wikipedia pages according to principal singular vectors")
pprint([names[i] for i in np.abs(U.T[0]).argsort()[-10:]])
pprint([names[i] for i in np.abs(V[0]).argsort()[-10:]])
def centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):
"""Power iteration computation of the principal eigenvector
This method is also known as Google PageRank and the implementation
is based on the one from the NetworkX project (BSD licensed too)
with copyrights by:
Aric Hagberg <hagberg@lanl.gov>
Dan Schult <dschult@colgate.edu>
Pieter Swart <swart@lanl.gov>
"""
n = X.shape[0]
X = X.copy()
incoming_counts = np.asarray(X.sum(axis=1)).ravel()
print("Normalizing the graph")
for i in incoming_counts.nonzero()[0]:
5.8. Wikipedia principal eigenvector
613
scikit-learn user guide, Release 0.18.2
X.data[X.indptr[i]:X.indptr[i + 1]] *= 1.0 / incoming_counts[i]
dangle = np.asarray(np.where(X.sum(axis=1) == 0, 1.0 / n, 0)).ravel()
scores = np.ones(n, dtype=np.float32) / n
# initial guess
for i in range(max_iter):
print("power iteration #%d" % i)
prev_scores = scores
scores = (alpha * (scores * X + np.dot(dangle, prev_scores))
+ (1 - alpha) * prev_scores.sum() / n)
# check convergence: normalized l_inf norm
scores_max = np.abs(scores).max()
if scores_max == 0.0:
scores_max = 1.0
err = np.abs(scores - prev_scores).max() / scores_max
print("error: %0.6f" % err)
if err < n * tol:
return scores
return scores
print("Computing principal eigenvector score using a power iteration method")
t0 = time()
scores = centrality_scores(X, max_iter=100, tol=1e-10)
print("done in %0.3fs" % (time() - t0))
pprint([names[i] for i in np.abs(scores).argsort()[-10:]])
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: wikipedia_principal_eigenvector.py
Download IPython notebook: wikipedia_principal_eigenvector.ipynb
5.9 Libsvm GUI
A simple graphical frontend for Libsvm mainly intended for didactic purposes. You can create data points by point
and click and visualize the decision region induced by different kernels and parameter settings.
To create positive examples click the left mouse button; to create negative examples click the right button.
If all examples are from the same class, it uses a one-class SVM.
from __future__ import division, print_function
print(__doc__)
# Author: Peter Prettenhoer <peter.prettenhofer@gmail.com>
#
# License: BSD 3 clause
import matplotlib
matplotlib.use('TkAgg')
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg
from matplotlib.figure import Figure
from matplotlib.contour import ContourSet
614
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
try:
import tkinter as Tk
except ImportError:
# Backward compat for Python 2
import Tkinter as Tk
import sys
import numpy as np
from sklearn import svm
from sklearn.datasets import dump_svmlight_file
from sklearn.externals.six.moves import xrange
y_min, y_max = -50, 50
x_min, x_max = -50, 50
class Model(object):
"""The Model which hold the data. It implements the
observable in the observer pattern and notifies the
registered observers on change event.
"""
def __init__(self):
self.observers = []
self.surface = None
self.data = []
self.cls = None
self.surface_type = 0
def changed(self, event):
"""Notify the observers. """
for observer in self.observers:
observer.update(event, self)
def add_observer(self, observer):
"""Register an observer. """
self.observers.append(observer)
def set_surface(self, surface):
self.surface = surface
def dump_svmlight_file(self, file):
data = np.array(self.data)
X = data[:, 0:2]
y = data[:, 2]
dump_svmlight_file(X, y, file)
class Controller(object):
def __init__(self, model):
self.model = model
self.kernel = Tk.IntVar()
self.surface_type = Tk.IntVar()
# Whether or not a model has been fitted
self.fitted = False
def fit(self):
5.9. Libsvm GUI
615
scikit-learn user guide, Release 0.18.2
print("fit the model")
train = np.array(self.model.data)
X = train[:, 0:2]
y = train[:, 2]
C = float(self.complexity.get())
gamma = float(self.gamma.get())
coef0 = float(self.coef0.get())
degree = int(self.degree.get())
kernel_map = {0: "linear", 1: "rbf", 2: "poly"}
if len(np.unique(y)) == 1:
clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
gamma=gamma, coef0=coef0, degree=degree)
clf.fit(X)
else:
clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
gamma=gamma, coef0=coef0, degree=degree)
clf.fit(X, y)
if hasattr(clf, 'score'):
print("Accuracy:", clf.score(X, y) * 100)
X1, X2, Z = self.decision_surface(clf)
self.model.clf = clf
self.model.set_surface((X1, X2, Z))
self.model.surface_type = self.surface_type.get()
self.fitted = True
self.model.changed("surface")
def decision_surface(self, cls):
delta = 1
x = np.arange(x_min, x_max + delta, delta)
y = np.arange(y_min, y_max + delta, delta)
X1, X2 = np.meshgrid(x, y)
Z = cls.decision_function(np.c_[X1.ravel(), X2.ravel()])
Z = Z.reshape(X1.shape)
return X1, X2, Z
def clear_data(self):
self.model.data = []
self.fitted = False
self.model.changed("clear")
def add_example(self, x, y, label):
self.model.data.append((x, y, label))
self.model.changed("example_added")
# update decision surface if already fitted.
self.refit()
def refit(self):
"""Refit the model if already fitted. """
if self.fitted:
self.fit()
class View(object):
"""Test docstring. """
def __init__(self, root, controller):
f = Figure()
616
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
ax = f.add_subplot(111)
ax.set_xticks([])
ax.set_yticks([])
ax.set_xlim((x_min, x_max))
ax.set_ylim((y_min, y_max))
canvas = FigureCanvasTkAgg(f, master=root)
canvas.show()
canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)
canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)
canvas.mpl_connect('button_press_event', self.onclick)
toolbar = NavigationToolbar2TkAgg(canvas, root)
toolbar.update()
self.controllbar = ControllBar(root, controller)
self.f = f
self.ax = ax
self.canvas = canvas
self.controller = controller
self.contours = []
self.c_labels = None
self.plot_kernels()
def plot_kernels(self):
self.ax.text(-50, -60, "Linear: $u^T v$")
self.ax.text(-20, -60, "RBF: $\exp (-\gamma \| u-v \|^2)$")
self.ax.text(10, -60, "Poly: $(\gamma \, u^T v + r)^d$")
def onclick(self, event):
if event.xdata and event.ydata:
if event.button == 1:
self.controller.add_example(event.xdata, event.ydata, 1)
elif event.button == 3:
self.controller.add_example(event.xdata, event.ydata, -1)
def update_example(self, model, idx):
x, y, l = model.data[idx]
if l == 1:
color = 'w'
elif l == -1:
color = 'k'
self.ax.plot([x], [y], "%so" % color, scalex=0.0, scaley=0.0)
def update(self, event, model):
if event == "examples_loaded":
for i in xrange(len(model.data)):
self.update_example(model, i)
if event == "example_added":
self.update_example(model, -1)
if event == "clear":
self.ax.clear()
self.ax.set_xticks([])
self.ax.set_yticks([])
self.contours = []
self.c_labels = None
self.plot_kernels()
if event == "surface":
5.9. Libsvm GUI
617
scikit-learn user guide, Release 0.18.2
self.remove_surface()
self.plot_support_vectors(model.clf.support_vectors_)
self.plot_decision_surface(model.surface, model.surface_type)
self.canvas.draw()
def remove_surface(self):
"""Remove old decision surface."""
if len(self.contours) > 0:
for contour in self.contours:
if isinstance(contour, ContourSet):
for lineset in contour.collections:
lineset.remove()
else:
contour.remove()
self.contours = []
def plot_support_vectors(self, support_vectors):
"""Plot the support vectors by placing circles over the
corresponding data points and adds the circle collection
to the contours list."""
cs = self.ax.scatter(support_vectors[:, 0], support_vectors[:, 1],
s=80, edgecolors="k", facecolors="none")
self.contours.append(cs)
def plot_decision_surface(self, surface, type):
X1, X2, Z = surface
if type == 0:
levels = [-1.0, 0.0, 1.0]
linestyles = ['dashed', 'solid', 'dashed']
colors = 'k'
self.contours.append(self.ax.contour(X1, X2, Z, levels,
colors=colors,
linestyles=linestyles))
elif type == 1:
self.contours.append(self.ax.contourf(X1, X2, Z, 10,
cmap=matplotlib.cm.bone,
origin='lower', alpha=0.85))
self.contours.append(self.ax.contour(X1, X2, Z, [0.0], colors='k',
linestyles=['solid']))
else:
raise ValueError("surface type unknown")
class ControllBar(object):
def __init__(self, root, controller):
fm = Tk.Frame(root)
kernel_group = Tk.Frame(fm)
Tk.Radiobutton(kernel_group, text="Linear", variable=controller.kernel,
value=0, command=controller.refit).pack(anchor=Tk.W)
Tk.Radiobutton(kernel_group, text="RBF", variable=controller.kernel,
value=1, command=controller.refit).pack(anchor=Tk.W)
Tk.Radiobutton(kernel_group, text="Poly", variable=controller.kernel,
value=2, command=controller.refit).pack(anchor=Tk.W)
kernel_group.pack(side=Tk.LEFT)
valbox = Tk.Frame(fm)
controller.complexity = Tk.StringVar()
618
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
controller.complexity.set("1.0")
c = Tk.Frame(valbox)
Tk.Label(c, text="C:", anchor="e", width=7).pack(side=Tk.LEFT)
Tk.Entry(c, width=6, textvariable=controller.complexity).pack(
side=Tk.LEFT)
c.pack()
controller.gamma = Tk.StringVar()
controller.gamma.set("0.01")
g = Tk.Frame(valbox)
Tk.Label(g, text="gamma:", anchor="e", width=7).pack(side=Tk.LEFT)
Tk.Entry(g, width=6, textvariable=controller.gamma).pack(side=Tk.LEFT)
g.pack()
controller.degree = Tk.StringVar()
controller.degree.set("3")
d = Tk.Frame(valbox)
Tk.Label(d, text="degree:", anchor="e", width=7).pack(side=Tk.LEFT)
Tk.Entry(d, width=6, textvariable=controller.degree).pack(side=Tk.LEFT)
d.pack()
controller.coef0 = Tk.StringVar()
controller.coef0.set("0")
r = Tk.Frame(valbox)
Tk.Label(r, text="coef0:", anchor="e", width=7).pack(side=Tk.LEFT)
Tk.Entry(r, width=6, textvariable=controller.coef0).pack(side=Tk.LEFT)
r.pack()
valbox.pack(side=Tk.LEFT)
cmap_group = Tk.Frame(fm)
Tk.Radiobutton(cmap_group, text="Hyperplanes",
variable=controller.surface_type, value=0,
command=controller.refit).pack(anchor=Tk.W)
Tk.Radiobutton(cmap_group, text="Surface",
variable=controller.surface_type, value=1,
command=controller.refit).pack(anchor=Tk.W)
cmap_group.pack(side=Tk.LEFT)
train_button = Tk.Button(fm, text='Fit', width=5,
command=controller.fit)
train_button.pack()
fm.pack(side=Tk.LEFT)
Tk.Button(fm, text='Clear', width=5,
command=controller.clear_data).pack(side=Tk.LEFT)
def get_parser():
from optparse import OptionParser
op = OptionParser()
op.add_option("--output",
action="store", type="str", dest="output",
help="Path where to dump data.")
return op
def main(argv):
op = get_parser()
5.9. Libsvm GUI
619
scikit-learn user guide, Release 0.18.2
opts, args = op.parse_args(argv[1:])
root = Tk.Tk()
model = Model()
controller = Controller(model)
root.wm_title("Scikit-learn Libsvm GUI")
view = View(root, controller)
model.add_observer(view)
Tk.mainloop()
if opts.output:
model.dump_svmlight_file(opts.output)
if __name__ == "__main__":
main(sys.argv)
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: svm_gui.py
Download IPython notebook: svm_gui.ipynb
5.10 Prediction Latency
This is an example showing the prediction latency of various scikit-learn estimators.
The goal is to measure the latency one can expect when doing predictions either in bulk or atomic (i.e. one by one)
mode.
The plots represent the distribution of the prediction latency as a boxplot.
# Authors: Eustache Diemert <eustache@diemert.fr>
# License: BSD 3 clause
from __future__ import print_function
from collections import defaultdict
import time
import gc
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy.stats import scoreatpercentile
from sklearn.datasets.samples_generator import make_regression
from sklearn.ensemble.forest import RandomForestRegressor
from sklearn.linear_model.ridge import Ridge
from sklearn.linear_model.stochastic_gradient import SGDRegressor
from sklearn.svm.classes import SVR
from sklearn.utils import shuffle
def _not_in_sphinx():
# Hack to detect whether we are running by the sphinx builder
return '__file__' in globals()
620
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
def atomic_benchmark_estimator(estimator, X_test, verbose=False):
"""Measure runtime prediction of each instance."""
n_instances = X_test.shape[0]
runtimes = np.zeros(n_instances, dtype=np.float)
for i in range(n_instances):
instance = X_test[[i], :]
start = time.time()
estimator.predict(instance)
runtimes[i] = time.time() - start
if verbose:
print("atomic_benchmark runtimes:", min(runtimes), scoreatpercentile(
runtimes, 50), max(runtimes))
return runtimes
def bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):
"""Measure runtime prediction of the whole input."""
n_instances = X_test.shape[0]
runtimes = np.zeros(n_bulk_repeats, dtype=np.float)
for i in range(n_bulk_repeats):
start = time.time()
estimator.predict(X_test)
runtimes[i] = time.time() - start
runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))
if verbose:
print("bulk_benchmark runtimes:", min(runtimes), scoreatpercentile(
runtimes, 50), max(runtimes))
return runtimes
def benchmark_estimator(estimator, X_test, n_bulk_repeats=30, verbose=False):
"""
Measure runtimes of prediction in both atomic and bulk mode.
Parameters
----------
estimator : already trained estimator supporting `predict()`
X_test : test input
n_bulk_repeats : how many times to repeat when evaluating bulk mode
Returns
-------
atomic_runtimes, bulk_runtimes : a pair of `np.array` which contain the
runtimes in seconds.
"""
atomic_runtimes = atomic_benchmark_estimator(estimator, X_test, verbose)
bulk_runtimes = bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats,
verbose)
return atomic_runtimes, bulk_runtimes
def generate_dataset(n_train, n_test, n_features, noise=0.1, verbose=False):
"""Generate a regression dataset with the given parameters."""
if verbose:
print("generating dataset...")
X, y, coef = make_regression(n_samples=n_train + n_test,
5.10. Prediction Latency
621
scikit-learn user guide, Release 0.18.2
n_features=n_features, noise=noise, coef=True)
random_seed = 13
X_train, X_test, y_train, y_test = train_test_split(
X, y, train_size=n_train, random_state=random_seed)
X_train, y_train = shuffle(X_train, y_train, random_state=random_seed)
X_scaler = StandardScaler()
X_train = X_scaler.fit_transform(X_train)
X_test = X_scaler.transform(X_test)
y_scaler = StandardScaler()
y_train = y_scaler.fit_transform(y_train[:, None])[:, 0]
y_test = y_scaler.transform(y_test[:, None])[:, 0]
gc.collect()
if verbose:
print("ok")
return X_train, y_train, X_test, y_test
def boxplot_runtimes(runtimes, pred_type, configuration):
"""
Plot a new `Figure` with boxplots of prediction runtimes.
Parameters
----------
runtimes : list of `np.array` of latencies in micro-seconds
cls_names : list of estimator class names that generated the runtimes
pred_type : 'bulk' or 'atomic'
"""
fig, ax1 = plt.subplots(figsize=(10, 6))
bp = plt.boxplot(runtimes, )
cls_infos = ['%s\n(%d %s)' % (estimator_conf['name'],
estimator_conf['complexity_computer'](
estimator_conf['instance']),
estimator_conf['complexity_label']) for
estimator_conf in configuration['estimators']]
plt.setp(ax1, xticklabels=cls_infos)
plt.setp(bp['boxes'], color='black')
plt.setp(bp['whiskers'], color='black')
plt.setp(bp['fliers'], color='red', marker='+')
ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',
alpha=0.5)
ax1.set_axisbelow(True)
ax1.set_title('Prediction Time per Instance - %s, %d feats.' % (
pred_type.capitalize(),
configuration['n_features']))
ax1.set_ylabel('Prediction Time (us)')
plt.show()
622
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
def benchmark(configuration):
"""Run the whole benchmark."""
X_train, y_train, X_test, y_test = generate_dataset(
configuration['n_train'], configuration['n_test'],
configuration['n_features'])
stats = {}
for estimator_conf in configuration['estimators']:
print("Benchmarking", estimator_conf['instance'])
estimator_conf['instance'].fit(X_train, y_train)
gc.collect()
a, b = benchmark_estimator(estimator_conf['instance'], X_test)
stats[estimator_conf['name']] = {'atomic': a, 'bulk': b}
cls_names = [estimator_conf['name'] for estimator_conf in configuration[
'estimators']]
runtimes = [1e6 * stats[clf_name]['atomic'] for clf_name in cls_names]
boxplot_runtimes(runtimes, 'atomic', configuration)
runtimes = [1e6 * stats[clf_name]['bulk'] for clf_name in cls_names]
boxplot_runtimes(runtimes, 'bulk (%d)' % configuration['n_test'],
configuration)
def n_feature_influence(estimators, n_train, n_test, n_features, percentile):
"""
Estimate influence of the number of features on prediction time.
Parameters
----------
estimators : dict of (name (str), estimator) to benchmark
n_train : nber of training instances (int)
n_test : nber of testing instances (int)
n_features : list of feature-space dimensionality to test (int)
percentile : percentile at which to measure the speed (int [0-100])
Returns:
--------
percentiles : dict(estimator_name,
dict(n_features, percentile_perf_in_us))
"""
percentiles = defaultdict(defaultdict)
for n in n_features:
print("benchmarking with %d features" % n)
X_train, y_train, X_test, y_test = generate_dataset(n_train, n_test, n)
for cls_name, estimator in estimators.items():
estimator.fit(X_train, y_train)
gc.collect()
runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)
percentiles[cls_name][n] = 1e6 * scoreatpercentile(runtimes,
percentile)
return percentiles
def plot_n_features_influence(percentiles, percentile):
fig, ax1 = plt.subplots(figsize=(10, 6))
5.10. Prediction Latency
623
scikit-learn user guide, Release 0.18.2
colors = ['r', 'g', 'b']
for i, cls_name in enumerate(percentiles.keys()):
x = np.array(sorted([n for n in percentiles[cls_name].keys()]))
y = np.array([percentiles[cls_name][n] for n in x])
plt.plot(x, y, color=colors[i], )
ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',
alpha=0.5)
ax1.set_axisbelow(True)
ax1.set_title('Evolution of Prediction Time with #Features')
ax1.set_xlabel('#Features')
ax1.set_ylabel('Prediction Time at %d%%-ile (us)' % percentile)
plt.show()
def benchmark_throughputs(configuration, duration_secs=0.1):
"""benchmark throughput for different estimators."""
X_train, y_train, X_test, y_test = generate_dataset(
configuration['n_train'], configuration['n_test'],
configuration['n_features'])
throughputs = dict()
for estimator_config in configuration['estimators']:
estimator_config['instance'].fit(X_train, y_train)
start_time = time.time()
n_predictions = 0
while (time.time() - start_time) < duration_secs:
estimator_config['instance'].predict(X_test[[0]])
n_predictions += 1
throughputs[estimator_config['name']] = n_predictions / duration_secs
return throughputs
def plot_benchmark_throughput(throughputs, configuration):
fig, ax = plt.subplots(figsize=(10, 6))
colors = ['r', 'g', 'b']
cls_infos = ['%s\n(%d %s)' % (estimator_conf['name'],
estimator_conf['complexity_computer'](
estimator_conf['instance']),
estimator_conf['complexity_label']) for
estimator_conf in configuration['estimators']]
cls_values = [throughputs[estimator_conf['name']] for estimator_conf in
configuration['estimators']]
plt.bar(range(len(throughputs)), cls_values, width=0.5, color=colors)
ax.set_xticks(np.linspace(0.25, len(throughputs) - 0.75, len(throughputs)))
ax.set_xticklabels(cls_infos, fontsize=10)
ymax = max(cls_values) * 1.2
ax.set_ylim((0, ymax))
ax.set_ylabel('Throughput (predictions/sec)')
ax.set_title('Prediction Throughput for different estimators (%d '
'features)' % configuration['n_features'])
plt.show()
main code
start_time = time.time()
# benchmark bulk/atomic prediction speed for various regressors
configuration = {
'n_train': int(1e3),
624
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
'n_test': int(1e2),
'n_features': int(1e2),
'estimators': [
{'name': 'Linear Model',
'instance': SGDRegressor(penalty='elasticnet', alpha=0.01,
l1_ratio=0.25, fit_intercept=True),
'complexity_label': 'non-zero coefficients',
'complexity_computer': lambda clf: np.count_nonzero(clf.coef_)},
{'name': 'RandomForest',
'instance': RandomForestRegressor(),
'complexity_label': 'estimators',
'complexity_computer': lambda clf: clf.n_estimators},
{'name': 'SVR',
'instance': SVR(kernel='rbf'),
'complexity_label': 'support vectors',
'complexity_computer': lambda clf: len(clf.support_vectors_)},
]
}
benchmark(configuration)
# benchmark n_features influence on prediction speed
percentile = 90
percentiles = n_feature_influence({'ridge': Ridge()},
configuration['n_train'],
configuration['n_test'],
[100, 250, 500], percentile)
plot_n_features_influence(percentiles, percentile)
# benchmark throughput
throughputs = benchmark_throughputs(configuration)
plot_benchmark_throughput(throughputs, configuration)
stop_time = time.time()
print("example run in %.2fs" % (stop_time - start_time))
•
•
5.10. Prediction Latency
625
scikit-learn user guide, Release 0.18.2
•
•
Out:
Benchmarking SGDRegressor(alpha=0.01, average=False, epsilon=0.1, eta0=0.01,
fit_intercept=True, l1_ratio=0.25, learning_rate='invscaling',
loss='squared_loss', n_iter=5, penalty='elasticnet', power_t=0.25,
random_state=None, shuffle=True, verbose=0, warm_start=False)
Benchmarking RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
max_features='auto', max_leaf_nodes=None,
min_impurity_split=1e-07, min_samples_leaf=1,
min_samples_split=2, min_weight_fraction_leaf=0.0,
n_estimators=10, n_jobs=1, oob_score=False, random_state=None,
verbose=0, warm_start=False)
Benchmarking SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto
˓→',
kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
benchmarking with 100 features
benchmarking with 250 features
benchmarking with 500 features
example run in 3.64s
Total running time of the script: (0 minutes 3.642 seconds)
Download Python source code: plot_prediction_latency.py
Download IPython notebook: plot_prediction_latency.ipynb
5.11 Out-of-core classiﬁcation of text documents
This is an example showing how scikit-learn can be used for classiﬁcation using an out-of-core approach: learning
from data that doesn’t ﬁt into main memory. We make use of an online classiﬁer, i.e., one that supports the partial_ﬁt
method, that will be fed with batches of examples. To guarantee that the features space remains the same over time
we leverage a HashingVectorizer that will project each example into the same feature space. This is especially useful
in the case of text classiﬁcation where new features (words) may appear in each batch.
626
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
The dataset used in this example is Reuters-21578 as provided by the UCI ML repository. It will be automatically
downloaded and uncompressed on ﬁrst run.
The plot represents the learning curve of the classiﬁer: the evolution of classiﬁcation accuracy over the course of the
mini-batches. Accuracy is measured on the ﬁrst 1000 samples, held out as a validation set.
To limit the memory consumption, we queue examples up to a ﬁxed amount before feeding them to the learner.
# Authors: Eustache Diemert <eustache@diemert.fr>
#
@FedericoV <https://github.com/FedericoV/>
# License: BSD 3 clause
from __future__ import print_function
from glob import glob
import itertools
import os.path
import re
import tarfile
import time
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
from sklearn.externals.six.moves import html_parser
from sklearn.externals.six.moves import urllib
from sklearn.datasets import get_data_home
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import MultinomialNB
def _not_in_sphinx():
# Hack to detect whether we are running by the sphinx builder
return '__file__' in globals()
5.11.1 Reuters Dataset related routines
class ReutersParser(html_parser.HTMLParser):
"""Utility class to parse a SGML file and yield documents one at a time."""
def __init__(self, encoding='latin-1'):
html_parser.HTMLParser.__init__(self)
self._reset()
self.encoding = encoding
def handle_starttag(self, tag, attrs):
method = 'start_' + tag
getattr(self, method, lambda x: None)(attrs)
def handle_endtag(self, tag):
method = 'end_' + tag
getattr(self, method, lambda: None)()
5.11. Out-of-core classiﬁcation of text documents
627
scikit-learn user guide, Release 0.18.2
def _reset(self):
self.in_title = 0
self.in_body = 0
self.in_topics = 0
self.in_topic_d = 0
self.title = ""
self.body = ""
self.topics = []
self.topic_d = ""
def parse(self, fd):
self.docs = []
for chunk in fd:
self.feed(chunk.decode(self.encoding))
for doc in self.docs:
yield doc
self.docs = []
self.close()
def handle_data(self, data):
if self.in_body:
self.body += data
elif self.in_title:
self.title += data
elif self.in_topic_d:
self.topic_d += data
def start_reuters(self, attributes):
pass
def end_reuters(self):
self.body = re.sub(r'\s+', r' ', self.body)
self.docs.append({'title': self.title,
'body': self.body,
'topics': self.topics})
self._reset()
def start_title(self, attributes):
self.in_title = 1
def end_title(self):
self.in_title = 0
def start_body(self, attributes):
self.in_body = 1
def end_body(self):
self.in_body = 0
def start_topics(self, attributes):
self.in_topics = 1
def end_topics(self):
self.in_topics = 0
def start_d(self, attributes):
self.in_topic_d = 1
628
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
def end_d(self):
self.in_topic_d = 0
self.topics.append(self.topic_d)
self.topic_d = ""
def stream_reuters_documents(data_path=None):
"""Iterate over documents of the Reuters dataset.
The Reuters archive will automatically be downloaded and uncompressed if
the `data_path` directory does not exist.
Documents are represented as dictionaries with 'body' (str),
'title' (str), 'topics' (list(str)) keys.
"""
DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'
'reuters21578-mld/reuters21578.tar.gz')
ARCHIVE_FILENAME = 'reuters21578.tar.gz'
if data_path is None:
data_path = os.path.join(get_data_home(), "reuters")
if not os.path.exists(data_path):
"""Download the dataset."""
print("downloading dataset (once and for all) into %s" %
data_path)
os.mkdir(data_path)
def progress(blocknum, bs, size):
total_sz_mb = '%.2f MB' % (size / 1e6)
current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)
if _not_in_sphinx():
print('\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb),
end='')
archive_path = os.path.join(data_path, ARCHIVE_FILENAME)
urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,
reporthook=progress)
if _not_in_sphinx():
print('\r', end='')
print("untarring Reuters dataset...")
tarfile.open(archive_path, 'r:gz').extractall(data_path)
print("done.")
parser = ReutersParser()
for filename in glob(os.path.join(data_path, "*.sgm")):
for doc in parser.parse(open(filename, 'rb')):
yield doc
5.11.2 Main
Create the vectorizer and limit the number of features to a reasonable maximum
vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,
non_negative=True)
5.11. Out-of-core classiﬁcation of text documents
629
scikit-learn user guide, Release 0.18.2
# Iterator over parsed Reuters SGML files.
data_stream = stream_reuters_documents()
# We learn a binary classification between the "acq" class and all the others.
# "acq" was chosen as it is more or less evenly distributed in the Reuters
# files. For other datasets, one should take care of creating a test set with
# a realistic portion of positive instances.
all_classes = np.array([0, 1])
positive_class = 'acq'
# Here are some classifiers that support the `partial_fit` method
partial_fit_classifiers = {
'SGD': SGDClassifier(),
'Perceptron': Perceptron(),
'NB Multinomial': MultinomialNB(alpha=0.01),
'Passive-Aggressive': PassiveAggressiveClassifier(),
}
def get_minibatch(doc_iter, size, pos_class=positive_class):
"""Extract a minibatch of examples, return a tuple X_text, y.
Note: size is before excluding invalid docs with no topics assigned.
"""
data = [(u'{title}\n\n{body}'.format(**doc), pos_class in doc['topics'])
for doc in itertools.islice(doc_iter, size)
if doc['topics']]
if not len(data):
return np.asarray([], dtype=int), np.asarray([], dtype=int)
X_text, y = zip(*data)
return X_text, np.asarray(y, dtype=int)
def iter_minibatches(doc_iter, minibatch_size):
"""Generator of minibatches."""
X_text, y = get_minibatch(doc_iter, minibatch_size)
while len(X_text):
yield X_text, y
X_text, y = get_minibatch(doc_iter, minibatch_size)
# test data statistics
test_stats = {'n_test': 0, 'n_test_pos': 0}
# First we hold out a number of examples to estimate accuracy
n_test_documents = 1000
tick = time.time()
X_test_text, y_test = get_minibatch(data_stream, 1000)
parsing_time = time.time() - tick
tick = time.time()
X_test = vectorizer.transform(X_test_text)
vectorizing_time = time.time() - tick
test_stats['n_test'] += len(y_test)
test_stats['n_test_pos'] += sum(y_test)
print("Test set is %d documents (%d positive)" % (len(y_test), sum(y_test)))
630
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
def progress(cls_name, stats):
"""Report progress information, return a string."""
duration = time.time() - stats['t0']
s = "%20s classifier : \t" % cls_name
s += "%(n_train)6d train docs (%(n_train_pos)6d positive) " % stats
s += "%(n_test)6d test docs (%(n_test_pos)6d positive) " % test_stats
s += "accuracy: %(accuracy).3f " % stats
s += "in %.2fs (%5d docs/s)" % (duration, stats['n_train'] / duration)
return s
cls_stats = {}
for cls_name in partial_fit_classifiers:
stats = {'n_train': 0, 'n_train_pos': 0,
'accuracy': 0.0, 'accuracy_history': [(0, 0)], 't0': time.time(),
'runtime_history': [(0, 0)], 'total_fit_time': 0.0}
cls_stats[cls_name] = stats
get_minibatch(data_stream, n_test_documents)
# Discard test set
# We will feed the classifier with mini-batches of 1000 documents; this means
# we have at most 1000 docs in memory at any time.
The smaller the document
# batch, the bigger the relative overhead of the partial fit methods.
minibatch_size = 1000
# Create the data_stream that parses Reuters SGML files and iterates on
# documents as a stream.
minibatch_iterators = iter_minibatches(data_stream, minibatch_size)
total_vect_time = 0.0
# Main loop : iterate on mini-batches of examples
for i, (X_train_text, y_train) in enumerate(minibatch_iterators):
tick = time.time()
X_train = vectorizer.transform(X_train_text)
total_vect_time += time.time() - tick
for cls_name, cls in partial_fit_classifiers.items():
tick = time.time()
# update estimator with examples in the current mini-batch
cls.partial_fit(X_train, y_train, classes=all_classes)
# accumulate test accuracy stats
cls_stats[cls_name]['total_fit_time'] += time.time() - tick
cls_stats[cls_name]['n_train'] += X_train.shape[0]
cls_stats[cls_name]['n_train_pos'] += sum(y_train)
tick = time.time()
cls_stats[cls_name]['accuracy'] = cls.score(X_test, y_test)
cls_stats[cls_name]['prediction_time'] = time.time() - tick
acc_history = (cls_stats[cls_name]['accuracy'],
cls_stats[cls_name]['n_train'])
cls_stats[cls_name]['accuracy_history'].append(acc_history)
run_history = (cls_stats[cls_name]['accuracy'],
total_vect_time + cls_stats[cls_name]['total_fit_time'])
5.11. Out-of-core classiﬁcation of text documents
631
scikit-learn user guide, Release 0.18.2
cls_stats[cls_name]['runtime_history'].append(run_history)
if i % 3 == 0:
print(progress(cls_name, cls_stats[cls_name]))
if i % 3 == 0:
print('\n')
Out:
Test set is 878 documents (108 positive)
Passive-Aggressive classifier :
962 train docs (
132 positive)
878 test
˓→docs (
108 positive) accuracy: 0.913 in 1.78s (
539 docs/s)
Perceptron classifier :
962 train docs (
132 positive)
878 test
˓→docs (
108 positive) accuracy: 0.921 in 1.79s (
538 docs/s)
SGD classifier :
962 train docs (
132 positive)
878 test
˓→docs (
108 positive) accuracy: 0.925 in 1.79s (
536 docs/s)
NB Multinomial classifier :
962 train docs (
132 positive)
878 test
˓→docs (
108 positive) accuracy: 0.877 in 1.83s (
526 docs/s)
Passive-Aggressive classifier :
3911 train docs (
517 positive)
878 test
˓→docs (
108 positive) accuracy: 0.946 in 7.29s (
536 docs/s)
Perceptron classifier :
3911 train docs (
517 positive)
878 test
˓→docs (
108 positive) accuracy: 0.926 in 7.30s (
536 docs/s)
SGD classifier :
3911 train docs (
517 positive)
878 test
˓→docs (
108 positive) accuracy: 0.945 in 7.30s (
535 docs/s)
NB Multinomial classifier :
3911 train docs (
517 positive)
878 test
˓→docs (
108 positive) accuracy: 0.885 in 7.33s (
533 docs/s)
Passive-Aggressive classifier :
6821 train docs (
891 positive)
878 test
˓→docs (
108 positive) accuracy: 0.951 in 10.85s (
628 docs/s)
Perceptron classifier :
6821 train docs (
891 positive)
878 test
˓→docs (
108 positive) accuracy: 0.949 in 10.85s (
628 docs/s)
SGD classifier :
6821 train docs (
891 positive)
878 test
˓→docs (
108 positive) accuracy: 0.938 in 10.85s (
628 docs/s)
NB Multinomial classifier :
6821 train docs (
891 positive)
878 test
˓→docs (
108 positive) accuracy: 0.899 in 10.89s (
626 docs/s)
Passive-Aggressive classifier :
9759 train docs (
1276 positive)
878 test
˓→docs (
108 positive) accuracy: 0.964 in 14.26s (
684 docs/s)
Perceptron classifier :
9759 train docs (
1276 positive)
878 test
˓→docs (
108 positive) accuracy: 0.950 in 14.27s (
684 docs/s)
SGD classifier :
9759 train docs (
1276 positive)
878 test
˓→docs (
108 positive) accuracy: 0.958 in 14.27s (
683 docs/s)
NB Multinomial classifier :
9759 train docs (
1276 positive)
878 test
˓→docs (
108 positive) accuracy: 0.909 in 14.30s (
682 docs/s)
Passive-Aggressive classifier :
11680 train docs (
1499 positive)
878 test
˓→docs (
108 positive) accuracy: 0.951 in 17.31s (
674 docs/s)
Perceptron classifier :
11680 train docs (
1499 positive)
878 test
˓→docs (
108 positive) accuracy: 0.951 in 17.31s (
674 docs/s)
SGD classifier :
11680 train docs (
1499 positive)
878 test
˓→docs (
108 positive) accuracy: 0.951 in 17.32s (
674 docs/s)
NB Multinomial classifier :
11680 train docs (
1499 positive)
878 test
˓→docs (
108 positive) accuracy: 0.916 in 17.35s (
673 docs/s)
632
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
Passive-Aggressive classifier :
14625 train docs (
1865 positive)
878 test
˓→docs (
108 positive) accuracy: 0.966 in 20.80s (
703 docs/s)
Perceptron classifier :
14625 train docs (
1865 positive)
878 test
˓→docs (
108 positive) accuracy: 0.956 in 20.80s (
703 docs/s)
SGD classifier :
14625 train docs (
1865 positive)
878 test
˓→docs (
108 positive) accuracy: 0.954 in 20.81s (
702 docs/s)
NB Multinomial classifier :
14625 train docs (
1865 positive)
878 test
˓→docs (
108 positive) accuracy: 0.926 in 20.84s (
701 docs/s)
Passive-Aggressive classifier :
17360 train docs (
2179 positive)
878 test
˓→docs (
108 positive) accuracy: 0.954 in 23.94s (
725 docs/s)
Perceptron classifier :
17360 train docs (
2179 positive)
878 test
˓→docs (
108 positive) accuracy: 0.957 in 23.95s (
724 docs/s)
SGD classifier :
17360 train docs (
2179 positive)
878 test
˓→docs (
108 positive) accuracy: 0.949 in 23.95s (
724 docs/s)
NB Multinomial classifier :
17360 train docs (
2179 positive)
878 test
˓→docs (
108 positive) accuracy: 0.932 in 23.98s (
723 docs/s)
5.11.3 Plot results
def plot_accuracy(x, y, x_legend):
"""Plot accuracy as a function of x."""
x = np.array(x)
y = np.array(y)
plt.title('Classification accuracy as a function of %s' % x_legend)
plt.xlabel('%s' % x_legend)
plt.ylabel('Accuracy')
plt.grid(True)
plt.plot(x, y)
rcParams['legend.fontsize'] = 10
cls_names = list(sorted(cls_stats.keys()))
# Plot accuracy evolution
plt.figure()
for _, stats in sorted(cls_stats.items()):
# Plot accuracy evolution with #examples
accuracy, n_examples = zip(*stats['accuracy_history'])
plot_accuracy(n_examples, accuracy, "training examples (#)")
ax = plt.gca()
ax.set_ylim((0.8, 1))
plt.legend(cls_names, loc='best')
plt.figure()
for _, stats in sorted(cls_stats.items()):
# Plot accuracy evolution with runtime
accuracy, runtime = zip(*stats['runtime_history'])
plot_accuracy(runtime, accuracy, 'runtime (s)')
ax = plt.gca()
ax.set_ylim((0.8, 1))
plt.legend(cls_names, loc='best')
# Plot fitting times
5.11. Out-of-core classiﬁcation of text documents
633
scikit-learn user guide, Release 0.18.2
plt.figure()
fig = plt.gcf()
cls_runtime = []
for cls_name, stats in sorted(cls_stats.items()):
cls_runtime.append(stats['total_fit_time'])
cls_runtime.append(total_vect_time)
cls_names.append('Vectorization')
bar_colors = ['b', 'g', 'r', 'c', 'm', 'y']
ax = plt.subplot(111)
rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,
color=bar_colors)
ax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))
ax.set_xticklabels(cls_names, fontsize=10)
ymax = max(cls_runtime) * 1.2
ax.set_ylim((0, ymax))
ax.set_ylabel('runtime (s)')
ax.set_title('Training Times')
def autolabel(rectangles):
"""attach some text vi autolabel on rectangles."""
for rect in rectangles:
height = rect.get_height()
ax.text(rect.get_x() + rect.get_width() / 2.,
1.05 * height, '%.4f' % height,
ha='center', va='bottom')
autolabel(rectangles)
plt.show()
# Plot prediction times
plt.figure()
cls_runtime = []
cls_names = list(sorted(cls_stats.keys()))
for cls_name, stats in sorted(cls_stats.items()):
cls_runtime.append(stats['prediction_time'])
cls_runtime.append(parsing_time)
cls_names.append('Read/Parse\n+Feat.Extr.')
cls_runtime.append(vectorizing_time)
cls_names.append('Hashing\n+Vect.')
ax = plt.subplot(111)
rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,
color=bar_colors)
ax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))
ax.set_xticklabels(cls_names, fontsize=8)
plt.setp(plt.xticks()[1], rotation=30)
ymax = max(cls_runtime) * 1.2
ax.set_ylim((0, ymax))
ax.set_ylabel('runtime (s)')
ax.set_title('Prediction Times (%d instances)' % n_test_documents)
autolabel(rectangles)
plt.show()
634
Chapter 5. Examples based on real world datasets
scikit-learn user guide, Release 0.18.2
•
•
•
•
5.11. Out-of-core classiﬁcation of text documents
635
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 25.914 seconds)
Download Python source code: plot_out_of_core_classification.py
Download IPython notebook: plot_out_of_core_classification.ipynb
636
Chapter 5. Examples based on real world datasets
CHAPTER
SIX
BICLUSTERING
Examples concerning the sklearn.cluster.bicluster module.
6.1 A demo of the Spectral Co-Clustering algorithm
This example demonstrates how to generate a dataset and bicluster it using the Spectral Co-Clustering algorithm.
The dataset is generated using the make_biclusters function, which creates a matrix of small values and im-
plants bicluster with large values. The rows and columns are then shufﬂed and passed to the Spectral Co-Clustering
algorithm. Rearranging the shufﬂed matrix to make biclusters contiguous shows how accurately the algorithm found
the biclusters.
•
637
scikit-learn user guide, Release 0.18.2
•
•
Out:
consensus score: 1.000
print(__doc__)
# Author: Kemal Eren <kemal@kemaleren.com>
# License: BSD 3 clause
import numpy as np
from matplotlib import pyplot as plt
from sklearn.datasets import make_biclusters
from sklearn.datasets import samples_generator as sg
from sklearn.cluster.bicluster import SpectralCoclustering
638
Chapter 6. Biclustering
scikit-learn user guide, Release 0.18.2
from sklearn.metrics import consensus_score
data, rows, columns = make_biclusters(
shape=(300, 300), n_clusters=5, noise=5,
shuffle=False, random_state=0)
plt.matshow(data, cmap=plt.cm.Blues)
plt.title("Original dataset")
data, row_idx, col_idx = sg._shuffle(data, random_state=0)
plt.matshow(data, cmap=plt.cm.Blues)
plt.title("Shuffled dataset")
model = SpectralCoclustering(n_clusters=5, random_state=0)
model.fit(data)
score = consensus_score(model.biclusters_,
(rows[:, row_idx], columns[:, col_idx]))
print("consensus score: {:.3f}".format(score))
fit_data = data[np.argsort(model.row_labels_)]
fit_data = fit_data[:, np.argsort(model.column_labels_)]
plt.matshow(fit_data, cmap=plt.cm.Blues)
plt.title("After biclustering; rearranged to show biclusters")
plt.show()
Total running time of the script: (0 minutes 0.226 seconds)
Download Python source code: plot_spectral_coclustering.py
Download IPython notebook: plot_spectral_coclustering.ipynb
6.2 A demo of the Spectral Biclustering algorithm
This example demonstrates how to generate a checkerboard dataset and bicluster it using the Spectral Biclustering
algorithm.
The data is generated with the make_checkerboard function, then shufﬂed and passed to the Spectral Biclustering
algorithm. The rows and columns of the shufﬂed matrix are rearranged to show the biclusters found by the algorithm.
The outer product of the row and column label vectors shows a representation of the checkerboard structure.
6.2. A demo of the Spectral Biclustering algorithm
639
scikit-learn user guide, Release 0.18.2
•
•
•
640
Chapter 6. Biclustering
scikit-learn user guide, Release 0.18.2
•
Out:
consensus score: 1.0
print(__doc__)
# Author: Kemal Eren <kemal@kemaleren.com>
# License: BSD 3 clause
import numpy as np
from matplotlib import pyplot as plt
from sklearn.datasets import make_checkerboard
from sklearn.datasets import samples_generator as sg
from sklearn.cluster.bicluster import SpectralBiclustering
from sklearn.metrics import consensus_score
n_clusters = (4, 3)
data, rows, columns = make_checkerboard(
shape=(300, 300), n_clusters=n_clusters, noise=10,
shuffle=False, random_state=0)
plt.matshow(data, cmap=plt.cm.Blues)
plt.title("Original dataset")
data, row_idx, col_idx = sg._shuffle(data, random_state=0)
plt.matshow(data, cmap=plt.cm.Blues)
plt.title("Shuffled dataset")
model = SpectralBiclustering(n_clusters=n_clusters, method='log',
random_state=0)
model.fit(data)
score = consensus_score(model.biclusters_,
(rows[:, row_idx], columns[:, col_idx]))
6.2. A demo of the Spectral Biclustering algorithm
641
scikit-learn user guide, Release 0.18.2
print("consensus score: {:.1f}".format(score))
fit_data = data[np.argsort(model.row_labels_)]
fit_data = fit_data[:, np.argsort(model.column_labels_)]
plt.matshow(fit_data, cmap=plt.cm.Blues)
plt.title("After biclustering; rearranged to show biclusters")
plt.matshow(np.outer(np.sort(model.row_labels_) + 1,
np.sort(model.column_labels_) + 1),
cmap=plt.cm.Blues)
plt.title("Checkerboard structure of rearranged data")
plt.show()
Total running time of the script: (0 minutes 0.740 seconds)
Download Python source code: plot_spectral_biclustering.py
Download IPython notebook: plot_spectral_biclustering.ipynb
6.3 Biclustering documents with the Spectral Co-clustering algo-
rithm
This example demonstrates the Spectral Co-clustering algorithm on the twenty newsgroups dataset. The ‘comp.os.ms-
windows.misc’ category is excluded because it contains many posts containing nothing but data.
The TF-IDF vectorized posts form a word frequency matrix, which is then biclustered using Dhillon’s Spectral Co-
Clustering algorithm. The resulting document-word biclusters indicate subsets words used more often in those subsets
documents.
For a few of the best biclusters, its most common document categories and its ten most important words get printed.
The best biclusters are determined by their normalized cut. The best words are determined by comparing their sums
inside and outside the bicluster.
For comparison, the documents are also clustered using MiniBatchKMeans. The document clusters derived from the
biclusters achieve a better V-measure than clusters found by MiniBatchKMeans.
Output:
Vectorizing...
Coclustering...
Done in 9.53s. V-measure: 0.4455
MiniBatchKMeans...
Done in 12.00s. V-measure: 0.3309
Best biclusters:
----------------
bicluster 0 : 1951 documents, 4373 words
categories
: 23% talk.politics.guns, 19% talk.politics.misc, 14% sci.med
words
: gun, guns, geb, banks, firearms, drugs, gordon, clinton, cdt, amendment
bicluster 1 : 1165 documents, 3304 words
categories
: 29% talk.politics.mideast, 26% soc.religion.christian, 25% alt.atheism
words
: god, jesus, christians, atheists, kent, sin, morality, belief,
˓→resurrection, marriage
642
Chapter 6. Biclustering
scikit-learn user guide, Release 0.18.2
bicluster 2 : 2219 documents, 2830 words
categories
: 18% comp.sys.mac.hardware, 16% comp.sys.ibm.pc.hardware, 16% comp.
˓→graphics
words
: voltage, dsp, board, receiver, circuit, shipping, packages, stereo,
˓→compression, package
bicluster 3 : 1860 documents, 2745 words
categories
: 26% rec.motorcycles, 23% rec.autos, 13% misc.forsale
words
: bike, car, dod, engine, motorcycle, ride, honda, cars, bmw, bikes
bicluster 4 : 12 documents, 155 words
categories
: 100% rec.sport.hockey
words
: scorer, unassisted, reichel, semak, sweeney, kovalenko, ricci, audette,
˓→momesso, nedved
from __future__ import print_function
print(__doc__)
from collections import defaultdict
import operator
import re
from time import time
import numpy as np
from sklearn.cluster.bicluster import SpectralCoclustering
from sklearn.cluster import MiniBatchKMeans
from sklearn.externals.six import iteritems
from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.cluster import v_measure_score
def number_aware_tokenizer(doc):
""" Tokenizer that maps all numeric tokens to a placeholder.
For many applications, tokens that begin with a number are not directly
useful, but the fact that such a token exists can be relevant.
By applying
this form of dimensionality reduction, some methods may perform better.
"""
token_pattern = re.compile(u'(?u)\\b\\w\\w+\\b')
tokens = token_pattern.findall(doc)
tokens = ["#NUMBER" if token[0] in "0123456789_" else token
for token in tokens]
return tokens
# exclude 'comp.os.ms-windows.misc'
categories = ['alt.atheism', 'comp.graphics',
'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',
'comp.windows.x', 'misc.forsale', 'rec.autos',
'rec.motorcycles', 'rec.sport.baseball',
'rec.sport.hockey', 'sci.crypt', 'sci.electronics',
'sci.med', 'sci.space', 'soc.religion.christian',
'talk.politics.guns', 'talk.politics.mideast',
'talk.politics.misc', 'talk.religion.misc']
newsgroups = fetch_20newsgroups(categories=categories)
6.3. Biclustering documents with the Spectral Co-clustering algorithm
643
scikit-learn user guide, Release 0.18.2
y_true = newsgroups.target
vectorizer = TfidfVectorizer(stop_words='english', min_df=5,
tokenizer=number_aware_tokenizer)
cocluster = SpectralCoclustering(n_clusters=len(categories),
svd_method='arpack', random_state=0)
kmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000,
random_state=0)
print("Vectorizing...")
X = vectorizer.fit_transform(newsgroups.data)
print("Coclustering...")
start_time = time()
cocluster.fit(X)
y_cocluster = cocluster.row_labels_
print("Done in {:.2f}s. V-measure: {:.4f}".format(
time() - start_time,
v_measure_score(y_cocluster, y_true)))
print("MiniBatchKMeans...")
start_time = time()
y_kmeans = kmeans.fit_predict(X)
print("Done in {:.2f}s. V-measure: {:.4f}".format(
time() - start_time,
v_measure_score(y_kmeans, y_true)))
feature_names = vectorizer.get_feature_names()
document_names = list(newsgroups.target_names[i] for i in newsgroups.target)
def bicluster_ncut(i):
rows, cols = cocluster.get_indices(i)
if not (np.any(rows) and np.any(cols)):
import sys
return sys.float_info.max
row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]
col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]
# Note: the following is identical to X[rows[:, np.newaxis], cols].sum() but
# much faster in scipy <= 0.16
weight = X[rows][:, cols].sum()
cut = (X[row_complement][:, cols].sum() +
X[rows][:, col_complement].sum())
return cut / weight
def most_common(d):
"""Items of a defaultdict(int) with the highest values.
Like Counter.most_common in Python >=2.7.
"""
return sorted(iteritems(d), key=operator.itemgetter(1), reverse=True)
bicluster_ncuts = list(bicluster_ncut(i)
for i in range(len(newsgroups.target_names)))
best_idx = np.argsort(bicluster_ncuts)[:5]
644
Chapter 6. Biclustering
scikit-learn user guide, Release 0.18.2
print()
print("Best biclusters:")
print("----------------")
for idx, cluster in enumerate(best_idx):
n_rows, n_cols = cocluster.get_shape(cluster)
cluster_docs, cluster_words = cocluster.get_indices(cluster)
if not len(cluster_docs) or not len(cluster_words):
continue
# categories
counter = defaultdict(int)
for i in cluster_docs:
counter[document_names[i]] += 1
cat_string = ", ".join("{:.0f}% {}".format(float(c) / n_rows * 100, name)
for name, c in most_common(counter)[:3])
# words
out_of_cluster_docs = cocluster.row_labels_ != cluster
out_of_cluster_docs = np.where(out_of_cluster_docs)[0]
word_col = X[:, cluster_words]
word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -
word_col[out_of_cluster_docs, :].sum(axis=0))
word_scores = word_scores.ravel()
important_words = list(feature_names[cluster_words[i]]
for i in word_scores.argsort()[:-11:-1])
print("bicluster {} : {} documents, {} words".format(
idx, n_rows, n_cols))
print("categories
: {}".format(cat_string))
print("words
: {}\n".format(', '.join(important_words)))
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: bicluster_newsgroups.py
Download IPython notebook: bicluster_newsgroups.ipynb
6.3. Biclustering documents with the Spectral Co-clustering algorithm
645
scikit-learn user guide, Release 0.18.2
646
Chapter 6. Biclustering
CHAPTER
SEVEN
CALIBRATION
Examples illustrating the calibration of predicted probabilities of classiﬁers.
7.1 Comparison of Calibration of Classiﬁers
Well calibrated classiﬁers are probabilistic classiﬁers for which the output of the predict_proba method can be directly
interpreted as a conﬁdence level. For instance a well calibrated (binary) classiﬁer should classify the samples such that
among the samples to which it gave a predict_proba value close to 0.8, approx. 80% actually belong to the positive
class.
LogisticRegression returns well calibrated predictions as it directly optimizes log-loss. In contrast, the other methods
return biased probabilities, with different biases per method:
• GaussianNaiveBayes tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly
because it makes the assumption that features are conditionally independent given the class, which is not the
case in this dataset which contains 2 redundant features.
• RandomForestClassiﬁer shows the opposite behavior: the histograms show peaks at approx. 0.2 and 0.9 prob-
ability, while probabilities close to 0 or 1 are very rare. An explanation for this is given by Niculescu-Mizil
and Caruana [1]: “Methods such as bagging and random forests that average predictions from a base set of
models can have difﬁculty making predictions near 0 and 1 because variance in the underlying base models will
bias predictions that should be near zero or one away from these values. Because predictions are restricted to
the interval [0,1], errors caused by variance tend to be one- sided near zero and one. For example, if a model
should predict p = 0 for a case, the only way bagging can achieve this is if all bagged trees predict zero. If we
add noise to the trees that bagging is averaging over, this noise will cause some trees to predict values larger
than 0 for this case, thus moving the average prediction of the bagged ensemble away from 0. We observe this
effect most strongly with random forests because the base-level trees trained with random forests have relatively
high variance due to feature subseting.” As a result, the calibration curve shows a characteristic sigmoid shape,
indicating that the classiﬁer could trust its “intuition” more and return probabilities closer to 0 or 1 typically.
• Support Vector Classiﬁcation (SVC) shows an even more sigmoid curve as the RandomForestClassiﬁer, which is
typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [1]), which focus on hard samples
that are close to the decision boundary (the support vectors).
References:
print(__doc__)
# Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
# License: BSD Style.
647
scikit-learn user guide, Release 0.18.2
import numpy as np
np.random.seed(0)
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.calibration import calibration_curve
X, y = datasets.make_classification(n_samples=100000, n_features=20,
n_informative=2, n_redundant=2)
train_samples = 100
# Samples used for training the models
X_train = X[:train_samples]
X_test = X[train_samples:]
y_train = y[:train_samples]
y_test = y[train_samples:]
# Create classifiers
lr = LogisticRegression()
gnb = GaussianNB()
svc = LinearSVC(C=1.0)
rfc = RandomForestClassifier(n_estimators=100)
Plot calibration plots
plt.figure(figsize=(10, 10))
ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
ax2 = plt.subplot2grid((3, 1), (2, 0))
ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
for clf, name in [(lr, 'Logistic'),
(gnb, 'Naive Bayes'),
(svc, 'Support Vector Classification'),
(rfc, 'Random Forest')]:
clf.fit(X_train, y_train)
if hasattr(clf, "predict_proba"):
prob_pos = clf.predict_proba(X_test)[:, 1]
else:
# use decision function
prob_pos = clf.decision_function(X_test)
prob_pos = \
(prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())
fraction_of_positives, mean_predicted_value = \
calibration_curve(y_test, prob_pos, n_bins=10)
ax1.plot(mean_predicted_value, fraction_of_positives, "s-",
label="%s" % (name, ))
ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,
histtype="step", lw=2)
ax1.set_ylabel("Fraction of positives")
ax1.set_ylim([-0.05, 1.05])
ax1.legend(loc="lower right")
648
Chapter 7. Calibration
scikit-learn user guide, Release 0.18.2
ax1.set_title('Calibration plots
(reliability curve)')
ax2.set_xlabel("Mean predicted value")
ax2.set_ylabel("Count")
ax2.legend(loc="upper center", ncol=2)
plt.tight_layout()
plt.show()
Total running time of the script: (0 minutes 2.550 seconds)
Download Python source code: plot_compare_calibration.py
Download IPython notebook: plot_compare_calibration.ipynb
7.1. Comparison of Calibration of Classiﬁers
649
scikit-learn user guide, Release 0.18.2
7.2 Probability Calibration curves
When performing classiﬁcation one often wants to predict not only the class label, but also the associated probability.
This probability gives some kind of conﬁdence on the prediction. This example demonstrates how to display how well
calibrated the predicted probabilities are and how to calibrate an uncalibrated classiﬁer.
The experiment is performed on an artiﬁcial dataset for binary classiﬁcation with 100.000 samples (1.000 of them are
used for model ﬁtting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The ﬁrst
ﬁgure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive
Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier
score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated
while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the
assumption of feature-independence and result in an overly conﬁdent classiﬁer, which is indicated by the typical
transposed-sigmoid curve.
Calibration of the probabilities of Gaussian naive Bayes with isotonic regression can ﬁx this issue as can be seen from
the nearly diagonal calibration curve. Sigmoid calibration also improves the brier score slightly, albeit not as strongly
as the non-parametric isotonic regression. This can be attributed to the fact that we have plenty of calibration data such
that the greater ﬂexibility of the non-parametric model can be exploited.
The second ﬁgure shows the calibration curve of a linear support-vector classiﬁer (LinearSVC). LinearSVC shows
the opposite behavior as Gaussian naive Bayes: the calibration curve has a sigmoid curve, which is typical for an
under-conﬁdent classiﬁer. In the case of LinearSVC, this is caused by the margin property of the hinge loss, which
lets the model focus on hard samples that are close to the decision boundary (the support vectors).
Both kinds of calibration can ﬁx this issue and yield nearly identical results. This shows that sigmoid calibration can
deal with situations where the calibration curve of the base classiﬁer is sigmoid (e.g., for LinearSVC) but not where it
is transposed-sigmoid (e.g., Gaussian naive Bayes).
•
650
Chapter 7. Calibration
scikit-learn user guide, Release 0.18.2
•
Out:
Logistic:
Brier: 0.099
Precision: 0.872
Recall: 0.851
F1: 0.862
Naive Bayes:
Brier: 0.118
Precision: 0.857
Recall: 0.876
F1: 0.867
Naive Bayes + Isotonic:
Brier: 0.098
Precision: 0.883
Recall: 0.836
F1: 0.859
Naive Bayes + Sigmoid:
Brier: 0.109
Precision: 0.861
Recall: 0.871
F1: 0.866
Logistic:
Brier: 0.099
Precision: 0.872
Recall: 0.851
F1: 0.862
SVC:
Brier: 0.163
Precision: 0.872
Recall: 0.852
F1: 0.862
SVC + Isotonic:
7.2. Probability Calibration curves
651
scikit-learn user guide, Release 0.18.2
Brier: 0.100
Precision: 0.853
Recall: 0.878
F1: 0.865
SVC + Sigmoid:
Brier: 0.099
Precision: 0.874
Recall: 0.849
F1: 0.861
print(__doc__)
# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
#
Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
# License: BSD Style.
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (brier_score_loss, precision_score, recall_score,
f1_score)
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.model_selection import train_test_split
# Create dataset of classification task with many redundant and few
# informative features
X, y = datasets.make_classification(n_samples=100000, n_features=20,
n_informative=2, n_redundant=10,
random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,
random_state=42)
def plot_calibration_curve(est, name, fig_index):
"""Plot calibration curve for est w/o and with calibration. """
# Calibrated with isotonic calibration
isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')
# Calibrated with sigmoid calibration
sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')
# Logistic regression with no calibration as baseline
lr = LogisticRegression(C=1., solver='lbfgs')
fig = plt.figure(fig_index, figsize=(10, 10))
ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
ax2 = plt.subplot2grid((3, 1), (2, 0))
652
Chapter 7. Calibration
scikit-learn user guide, Release 0.18.2
ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
for clf, name in [(lr, 'Logistic'),
(est, name),
(isotonic, name + ' + Isotonic'),
(sigmoid, name + ' + Sigmoid')]:
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
if hasattr(clf, "predict_proba"):
prob_pos = clf.predict_proba(X_test)[:, 1]
else:
# use decision function
prob_pos = clf.decision_function(X_test)
prob_pos = \
(prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())
clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())
print("%s:" % name)
print("\tBrier: %1.3f" % (clf_score))
print("\tPrecision: %1.3f" % precision_score(y_test, y_pred))
print("\tRecall: %1.3f" % recall_score(y_test, y_pred))
print("\tF1: %1.3f\n" % f1_score(y_test, y_pred))
fraction_of_positives, mean_predicted_value = \
calibration_curve(y_test, prob_pos, n_bins=10)
ax1.plot(mean_predicted_value, fraction_of_positives, "s-",
label="%s (%1.3f)" % (name, clf_score))
ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,
histtype="step", lw=2)
ax1.set_ylabel("Fraction of positives")
ax1.set_ylim([-0.05, 1.05])
ax1.legend(loc="lower right")
ax1.set_title('Calibration plots
(reliability curve)')
ax2.set_xlabel("Mean predicted value")
ax2.set_ylabel("Count")
ax2.legend(loc="upper center", ncol=2)
plt.tight_layout()
# Plot calibration curve for Gaussian Naive Bayes
plot_calibration_curve(GaussianNB(), "Naive Bayes", 1)
# Plot calibration curve for Linear SVC
plot_calibration_curve(LinearSVC(), "SVC", 2)
plt.show()
Total running time of the script: (0 minutes 2.156 seconds)
Download Python source code: plot_calibration_curve.py
Download IPython notebook: plot_calibration_curve.ipynb
7.2. Probability Calibration curves
653
scikit-learn user guide, Release 0.18.2
7.3 Probability calibration of classiﬁers
When performing classiﬁcation you often want to predict not only the class label, but also the associated probability.
This probability gives you some kind of conﬁdence on the prediction. However, not all classiﬁers provide well-
calibrated probabilities, some being over-conﬁdent while others being under-conﬁdent. Thus, a separate calibration
of predicted probabilities is often desirable as a postprocessing. This example illustrates two different methods for
this calibration and evaluates the quality of the returned probabilities using Brier’s score (see https://en.wikipedia.org/
wiki/Brier_score).
Compared are the estimated probability using a Gaussian naive Bayes classiﬁer without calibration, with a sigmoid
calibration, and with a non-parametric isotonic calibration. One can observe that only the non-parametric model is
able to provide a probability calibration that returns probabilities close to the expected 0.5 for most of the samples
belonging to the middle cluster with heterogeneous labels. This results in a signiﬁcantly improved Brier score.
print(__doc__)
# Author: Mathieu Blondel <mathieu@mblondel.org>
#
Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
#
Balazs Kegl <balazs.kegl@gmail.com>
#
Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
# License: BSD Style.
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from sklearn.datasets import make_blobs
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import brier_score_loss
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import train_test_split
n_samples = 50000
n_bins = 3
# use 3 bins for calibration_curve as we have 3 clusters here
# Generate 3 blobs with 2 classes where the second blob contains
# half positive samples and half negative samples. Probability in this
# blob is therefore 0.5.
centers = [(-5, -5), (0, 0), (5, 5)]
X, y = make_blobs(n_samples=n_samples, n_features=2, cluster_std=1.0,
centers=centers, shuffle=False, random_state=42)
y[:n_samples // 2] = 0
y[n_samples // 2:] = 1
sample_weight = np.random.RandomState(42).rand(y.shape[0])
# split train, test for calibration
X_train, X_test, y_train, y_test, sw_train, sw_test = \
train_test_split(X, y, sample_weight, test_size=0.9, random_state=42)
# Gaussian Naive-Bayes with no calibration
clf = GaussianNB()
clf.fit(X_train, y_train)
# GaussianNB itself does not support sample-weights
prob_pos_clf = clf.predict_proba(X_test)[:, 1]
# Gaussian Naive-Bayes with isotonic calibration
654
Chapter 7. Calibration
scikit-learn user guide, Release 0.18.2
clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')
clf_isotonic.fit(X_train, y_train, sw_train)
prob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]
# Gaussian Naive-Bayes with sigmoid calibration
clf_sigmoid = CalibratedClassifierCV(clf, cv=2, method='sigmoid')
clf_sigmoid.fit(X_train, y_train, sw_train)
prob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]
print("Brier scores: (the smaller the better)")
clf_score = brier_score_loss(y_test, prob_pos_clf, sw_test)
print("No calibration: %1.3f" % clf_score)
clf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic, sw_test)
print("With isotonic calibration: %1.3f" % clf_isotonic_score)
clf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid, sw_test)
print("With sigmoid calibration: %1.3f" % clf_sigmoid_score)
Out:
Brier scores: (the smaller the better)
No calibration: 0.104
With isotonic calibration: 0.084
With sigmoid calibration: 0.109
Plot the data and the predicted probabilities
plt.figure()
y_unique = np.unique(y)
colors = cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))
for this_y, color in zip(y_unique, colors):
this_X = X_train[y_train == this_y]
this_sw = sw_train[y_train == this_y]
plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50, c=color, alpha=0.5,
label="Class %s" % this_y)
plt.legend(loc="best")
plt.title("Data")
plt.figure()
order = np.lexsort((prob_pos_clf, ))
plt.plot(prob_pos_clf[order], 'r', label='No calibration (%1.3f)' % clf_score)
plt.plot(prob_pos_isotonic[order], 'g', linewidth=3,
label='Isotonic calibration (%1.3f)' % clf_isotonic_score)
plt.plot(prob_pos_sigmoid[order], 'b', linewidth=3,
label='Sigmoid calibration (%1.3f)' % clf_sigmoid_score)
plt.plot(np.linspace(0, y_test.size, 51)[1::2],
y_test[order].reshape(25, -1).mean(1),
'k', linewidth=3, label=r'Empirical')
plt.ylim([-0.05, 1.05])
plt.xlabel("Instances sorted according to predicted probability "
"(uncalibrated GNB)")
plt.ylabel("P(y=1)")
plt.legend(loc="upper left")
plt.title("Gaussian naive Bayes probabilities")
plt.show()
7.3. Probability calibration of classiﬁers
655
scikit-learn user guide, Release 0.18.2
•
•
Total running time of the script: (0 minutes 0.272 seconds)
Download Python source code: plot_calibration.py
Download IPython notebook: plot_calibration.ipynb
7.4 Probability Calibration for 3-class classiﬁcation
This example illustrates how sigmoid calibration changes predicted probabilities for a 3-class classiﬁcation problem.
Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the
probability vectors predicted by an uncalibrated classiﬁer to the probability vectors predicted by the same classiﬁer
after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green:
class 2, blue: class 3).
The base classiﬁer is a random forest classiﬁer with 25 base estimators (trees). If this classiﬁer is trained on all 800
training datapoints, it is overly conﬁdent in its predictions and thus incurs a large log-loss. Calibrating an identical
classiﬁer, which was trained on 600 datapoints, with method=’sigmoid’ on the remaining 200 datapoints reduces the
conﬁdence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center.
This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base
estimators which would have resulted in a similar decrease in log-loss.
656
Chapter 7. Calibration
scikit-learn user guide, Release 0.18.2
•
•
Out:
Log-loss of
* uncalibrated classifier trained on 800 datapoints: 1.280
* classifier trained on 600 datapoints and calibrated on 200 datapoint: 0.534
print(__doc__)
# Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
# License: BSD Style.
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import log_loss
np.random.seed(0)
7.4. Probability Calibration for 3-class classiﬁcation
657
scikit-learn user guide, Release 0.18.2
# Generate data
X, y = make_blobs(n_samples=1000, n_features=2, random_state=42,
cluster_std=5.0)
X_train, y_train = X[:600], y[:600]
X_valid, y_valid = X[600:800], y[600:800]
X_train_valid, y_train_valid = X[:800], y[:800]
X_test, y_test = X[800:], y[800:]
# Train uncalibrated random forest classifier on whole train and validation
# data and evaluate on test data
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train_valid, y_train_valid)
clf_probs = clf.predict_proba(X_test)
score = log_loss(y_test, clf_probs)
# Train random forest classifier, calibrate on validation data and evaluate
# on test data
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train, y_train)
clf_probs = clf.predict_proba(X_test)
sig_clf = CalibratedClassifierCV(clf, method="sigmoid", cv="prefit")
sig_clf.fit(X_valid, y_valid)
sig_clf_probs = sig_clf.predict_proba(X_test)
sig_score = log_loss(y_test, sig_clf_probs)
# Plot changes in predicted probabilities via arrows
plt.figure(0)
colors = ["r", "g", "b"]
for i in range(clf_probs.shape[0]):
plt.arrow(clf_probs[i, 0], clf_probs[i, 1],
sig_clf_probs[i, 0] - clf_probs[i, 0],
sig_clf_probs[i, 1] - clf_probs[i, 1],
color=colors[y_test[i]], head_width=1e-2)
# Plot perfect predictions
plt.plot([1.0], [0.0], 'ro', ms=20, label="Class 1")
plt.plot([0.0], [1.0], 'go', ms=20, label="Class 2")
plt.plot([0.0], [0.0], 'bo', ms=20, label="Class 3")
# Plot boundaries of unit simplex
plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")
# Annotate points on the simplex
plt.annotate(r'($\frac{1}{3}$, $\frac{1}{3}$, $\frac{1}{3}$)',
xy=(1.0/3, 1.0/3), xytext=(1.0/3, .23), xycoords='data',
arrowprops=dict(facecolor='black', shrink=0.05),
horizontalalignment='center', verticalalignment='center')
plt.plot([1.0/3], [1.0/3], 'ko', ms=5)
plt.annotate(r'($\frac{1}{2}$, $0$, $\frac{1}{2}$)',
xy=(.5, .0), xytext=(.5, .1), xycoords='data',
arrowprops=dict(facecolor='black', shrink=0.05),
horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($0$, $\frac{1}{2}$, $\frac{1}{2}$)',
xy=(.0, .5), xytext=(.1, .5), xycoords='data',
arrowprops=dict(facecolor='black', shrink=0.05),
horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($\frac{1}{2}$, $\frac{1}{2}$, $0$)',
xy=(.5, .5), xytext=(.6, .6), xycoords='data',
658
Chapter 7. Calibration
scikit-learn user guide, Release 0.18.2
arrowprops=dict(facecolor='black', shrink=0.05),
horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($0$, $0$, $1$)',
xy=(0, 0), xytext=(.1, .1), xycoords='data',
arrowprops=dict(facecolor='black', shrink=0.05),
horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($1$, $0$, $0$)',
xy=(1, 0), xytext=(1, .1), xycoords='data',
arrowprops=dict(facecolor='black', shrink=0.05),
horizontalalignment='center', verticalalignment='center')
plt.annotate(r'($0$, $1$, $0$)',
xy=(0, 1), xytext=(.1, 1), xycoords='data',
arrowprops=dict(facecolor='black', shrink=0.05),
horizontalalignment='center', verticalalignment='center')
# Add grid
plt.grid("off")
for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
plt.plot([0, x], [x, 0], 'k', alpha=0.2)
plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)
plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)
plt.title("Change of predicted probabilities after sigmoid calibration")
plt.xlabel("Probability class 1")
plt.ylabel("Probability class 2")
plt.xlim(-0.05, 1.05)
plt.ylim(-0.05, 1.05)
plt.legend(loc="best")
print("Log-loss of")
print(" * uncalibrated classifier trained on 800 datapoints: %.3f "
% score)
print(" * classifier trained on 600 datapoints and calibrated on "
"200 datapoint: %.3f" % sig_score)
# Illustrate calibrator
plt.figure(1)
# generate grid over 2-simplex
p1d = np.linspace(0, 1, 20)
p0, p1 = np.meshgrid(p1d, p1d)
p2 = 1 - p0 - p1
p = np.c_[p0.ravel(), p1.ravel(), p2.ravel()]
p = p[p[:, 2] >= 0]
calibrated_classifier = sig_clf.calibrated_classifiers_[0]
prediction = np.vstack([calibrator.predict(this_p)
for calibrator, this_p in
zip(calibrated_classifier.calibrators_, p.T)]).T
prediction /= prediction.sum(axis=1)[:, None]
# Plot modifications of calibrator
for i in range(prediction.shape[0]):
plt.arrow(p[i, 0], p[i, 1],
prediction[i, 0] - p[i, 0], prediction[i, 1] - p[i, 1],
head_width=1e-2, color=colors[np.argmax(p[i])])
# Plot boundaries of unit simplex
plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")
plt.grid("off")
7.4. Probability Calibration for 3-class classiﬁcation
659
scikit-learn user guide, Release 0.18.2
for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
plt.plot([0, x], [x, 0], 'k', alpha=0.2)
plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)
plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)
plt.title("Illustration of sigmoid calibrator")
plt.xlabel("Probability class 1")
plt.ylabel("Probability class 2")
plt.xlim(-0.05, 1.05)
plt.ylim(-0.05, 1.05)
plt.show()
Total running time of the script: (0 minutes 0.761 seconds)
Download Python source code: plot_calibration_multiclass.py
Download IPython notebook: plot_calibration_multiclass.ipynb
660
Chapter 7. Calibration
CHAPTER
EIGHT
CLASSIFICATION
General examples about classiﬁcation algorithms.
8.1 Recognizing hand-written digits
An example showing how the scikit-learn can be used to recognize images of hand-written digits.
This example is commented in the tutorial section of the user manual.
Out:
661
scikit-learn user guide, Release 0.18.2
Classification report for classifier SVC(C=1.0, cache_size=200, class_weight=None,
˓→coef0=0.0,
decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False):
precision
recall
f1-score
support
0
1.00
0.99
0.99
88
1
0.99
0.97
0.98
91
2
0.99
0.99
0.99
86
3
0.98
0.87
0.92
91
4
0.99
0.96
0.97
92
5
0.95
0.97
0.96
91
6
0.99
0.99
0.99
91
7
0.96
0.99
0.97
89
8
0.94
1.00
0.97
88
9
0.93
0.98
0.95
92
avg / total
0.97
0.97
0.97
899
Confusion matrix:
[[87
0
0
0
1
0
0
0
0
0]
[ 0 88
1
0
0
0
0
0
1
1]
[ 0
0 85
1
0
0
0
0
0
0]
[ 0
0
0 79
0
3
0
4
5
0]
[ 0
0
0
0 88
0
0
0
0
4]
[ 0
0
0
0
0 88
1
0
0
2]
[ 0
1
0
0
0
0 90
0
0
0]
[ 0
0
0
0
0
1
0 88
0
0]
[ 0
0
0
0
0
0
0
0 88
0]
[ 0
0
0
1
0
1
0
0
0 90]]
print(__doc__)
# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>
# License: BSD 3 clause
# Standard scientific Python imports
import matplotlib.pyplot as plt
# Import datasets, classifiers and performance metrics
from sklearn import datasets, svm, metrics
# The digits dataset
digits = datasets.load_digits()
# The data that we are interested in is made of 8x8 images of digits, let's
# have a look at the first 4 images, stored in the `images` attribute of the
# dataset.
If we were working from image files, we could load them using
# matplotlib.pyplot.imread.
Note that each image must have the same size. For these
# images, we know which digit they represent: it is given in the 'target' of
# the dataset.
662
Chapter 8. Classiﬁcation
scikit-learn user guide, Release 0.18.2
images_and_labels = list(zip(digits.images, digits.target))
for index, (image, label) in enumerate(images_and_labels[:4]):
plt.subplot(2, 4, index + 1)
plt.axis('off')
plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
plt.title('Training: %i' % label)
# To apply a classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))
# Create a classifier: a support vector classifier
classifier = svm.SVC(gamma=0.001)
# We learn the digits on the first half of the digits
classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])
# Now predict the value of the digit on the second half:
expected = digits.target[n_samples // 2:]
predicted = classifier.predict(data[n_samples // 2:])
print("Classification report for classifier %s:\n%s\n"
% (classifier, metrics.classification_report(expected, predicted)))
print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))
images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
plt.subplot(2, 4, index + 5)
plt.axis('off')
plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
plt.title('Prediction: %i' % prediction)
plt.show()
Total running time of the script: (0 minutes 0.752 seconds)
Download Python source code: plot_digits_classification.py
Download IPython notebook: plot_digits_classification.ipynb
8.2 Normal and Shrinkage Linear Discriminant Analysis for classiﬁ-
cation
Shows how shrinkage improves classiﬁcation.
8.2. Normal and Shrinkage Linear Discriminant Analysis for classiﬁcation
663
scikit-learn user guide, Release 0.18.2
from __future__ import division
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
n_train = 20
# samples for training
n_test = 200
# samples for testing
n_averages = 50
# how often to repeat classification
n_features_max = 75
# maximum number of features
step = 4
# step size for the calculation
def generate_data(n_samples, n_features):
"""Generate random blob-ish data with noisy features.
This returns an array of input data with shape `(n_samples, n_features)`
and an array of `n_samples` target labels.
Only one feature contains discriminative information, the other features
contain only noise.
"""
X, y = make_blobs(n_samples=n_samples, n_features=1, centers=[[-2], [2]])
664
Chapter 8. Classiﬁcation
scikit-learn user guide, Release 0.18.2
# add non-discriminative features
if n_features > 1:
X = np.hstack([X, np.random.randn(n_samples, n_features - 1)])
return X, y
acc_clf1, acc_clf2 = [], []
n_features_range = range(1, n_features_max + 1, step)
for n_features in n_features_range:
score_clf1, score_clf2 = 0, 0
for _ in range(n_averages):
X, y = generate_data(n_train, n_features)
clf1 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto').fit(X, y)
clf2 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=None).fit(X, y)
X, y = generate_data(n_test, n_features)
score_clf1 += clf1.score(X, y)
score_clf2 += clf2.score(X, y)
acc_clf1.append(score_clf1 / n_averages)
acc_clf2.append(score_clf2 / n_averages)
features_samples_ratio = np.array(n_features_range) / n_train
plt.plot(features_samples_ratio, acc_clf1, linewidth=2,
label="Linear Discriminant Analysis with shrinkage", color='navy')
plt.plot(features_samples_ratio, acc_clf2, linewidth=2,
label="Linear Discriminant Analysis", color='gold')
plt.xlabel('n_features / n_samples')
plt.ylabel('Classification accuracy')
plt.legend(loc=1, prop={'size': 12})
plt.suptitle('Linear Discriminant Analysis vs. \
shrinkage Linear Discriminant Analysis (1 discriminative feature)')
plt.show()
Total running time of the script: (0 minutes 5.999 seconds)
Download Python source code: plot_lda.py
Download IPython notebook: plot_lda.ipynb
8.3 Plot classiﬁcation probability
Plot the classiﬁcation probability for different classiﬁers. We use a 3 class dataset, and we classify it with a Sup-
port Vector classiﬁer, L1 and L2 penalized logistic regression with either a One-Vs-Rest or multinomial setting, and
Gaussian process classiﬁcation.
The logistic regression is not a multiclass classiﬁer out of the box. As a result it can identify only the ﬁrst class.
8.3. Plot classiﬁcation probability
665
scikit-learn user guide, Release 0.18.2
666
Chapter 8. Classiﬁcation
scikit-learn user guide, Release 0.18.2
Out:
classif_rate for GPC : 82.666667
classif_rate for L2 logistic (OvR) : 76.666667
classif_rate for L1 logistic : 79.333333
classif_rate for Linear SVC : 82.000000
classif_rate for L2 logistic (Multinomial) : 82.000000
print(__doc__)
# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn import datasets
iris = datasets.load_iris()
X = iris.data[:, 0:2]
# we only take the first two features for visualization
y = iris.target
n_features = X.shape[1]
C = 1.0
kernel = 1.0 * RBF([1.0, 1.0])
# for GPC
# Create different classifiers. The logistic regression cannot do
# multiclass out of the box.
classifiers = {'L1 logistic': LogisticRegression(C=C, penalty='l1'),
'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2'),
'Linear SVC': SVC(kernel='linear', C=C, probability=True,
random_state=0),
'L2 logistic (Multinomial)': LogisticRegression(
C=C, solver='lbfgs', multi_class='multinomial'),
'GPC': GaussianProcessClassifier(kernel)
}
n_classifiers = len(classifiers)
plt.figure(figsize=(3 * 2, n_classifiers * 2))
plt.subplots_adjust(bottom=.2, top=.95)
xx = np.linspace(3, 9, 100)
yy = np.linspace(1, 5, 100).T
xx, yy = np.meshgrid(xx, yy)
Xfull = np.c_[xx.ravel(), yy.ravel()]
for index, (name, classifier) in enumerate(classifiers.items()):
8.3. Plot classiﬁcation probability
667
scikit-learn user guide, Release 0.18.2
classifier.fit(X, y)
y_pred = classifier.predict(X)
classif_rate = np.mean(y_pred.ravel() == y.ravel()) * 100
print("classif_rate for %s : %f " % (name, classif_rate))
# View probabilities=
probas = classifier.predict_proba(Xfull)
n_classes = np.unique(y_pred).size
for k in range(n_classes):
plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)
plt.title("Class %d" % k)
if k == 0:
plt.ylabel(name)
imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),
extent=(3, 9, 1, 5), origin='lower')
plt.xticks(())
plt.yticks(())
idx = (y_pred == k)
if idx.any():
plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='k')
ax = plt.axes([0.15, 0.04, 0.7, 0.05])
plt.title("Probability")
plt.colorbar(imshow_handle, cax=ax, orientation='horizontal')
plt.show()
Total running time of the script: (0 minutes 8.394 seconds)
Download Python source code: plot_classification_probability.py
Download IPython notebook: plot_classification_probability.ipynb
8.4 Classiﬁer comparison
A comparison of a several classiﬁers in scikit-learn on synthetic datasets. The point of this example is to illustrate
the nature of decision boundaries of different classiﬁers. This should be taken with a grain of salt, as the intuition
conveyed by these examples does not necessarily carry over to real datasets.
Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classiﬁers
such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classiﬁers.
The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classiﬁ-
cation accuracy on the test set.
668
Chapter 8. Classiﬁcation
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Code source: Gaël Varoquaux
#
Andreas Müller
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
h = .02
# step size in the mesh
names = ["Nearest Neighbors", "Linear SVM", "RBF SVM", "Gaussian Process",
"Decision Tree", "Random Forest", "Neural Net", "AdaBoost",
"Naive Bayes", "QDA"]
classifiers = [
KNeighborsClassifier(3),
SVC(kernel="linear", C=0.025),
SVC(gamma=2, C=1),
GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
DecisionTreeClassifier(max_depth=5),
RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
MLPClassifier(alpha=1),
AdaBoostClassifier(),
GaussianNB(),
QuadraticDiscriminantAnalysis()]
X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
8.4. Classiﬁer comparison
669
scikit-learn user guide, Release 0.18.2
random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)
datasets = [make_moons(noise=0.3, random_state=0),
make_circles(noise=0.2, factor=0.5, random_state=1),
linearly_separable
]
figure = plt.figure(figsize=(27, 9))
i = 1
# iterate over datasets
for ds_cnt, ds in enumerate(datasets):
# preprocess dataset, split into training and test part
X, y = ds
X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = \
train_test_split(X, y, test_size=.4, random_state=42)
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
# just plot the dataset first
cm = plt.cm.RdBu
cm_bright = ListedColormap(['#FF0000', '#0000FF'])
ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
if ds_cnt == 0:
ax.set_title("Input data")
# Plot the training points
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
# and testing points
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())
i += 1
# iterate over classifiers
for name, clf in zip(names, classifiers):
ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
if hasattr(clf, "decision_function"):
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
else:
Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
# Put the result into a color plot
Z = Z.reshape(xx.shape)
ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
670
Chapter 8. Classiﬁcation
scikit-learn user guide, Release 0.18.2
# Plot also the training points
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
# and testing points
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
alpha=0.6)
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())
if ds_cnt == 0:
ax.set_title(name)
ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
size=15, horizontalalignment='right')
i += 1
plt.tight_layout()
plt.show()
Total running time of the script: (0 minutes 11.439 seconds)
Download Python source code: plot_classifier_comparison.py
Download IPython notebook: plot_classifier_comparison.ipynb
8.5 Linear and Quadratic Discriminant Analysis with conﬁdence el-
lipsoid
Plot the conﬁdence ellipsoids of each class and decision boundary
print(__doc__)
from scipy import linalg
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import colors
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
colormap
cmap = colors.LinearSegmentedColormap(
'red_blue_classes',
{'red': [(0, 1, 1), (1, 0.7, 0.7)],
'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],
'blue': [(0, 0.7, 0.7), (1, 1, 1)]})
plt.cm.register_cmap(cmap=cmap)
generate datasets
def dataset_fixed_cov():
'''Generate 2 Gaussians samples with the same covariance matrix'''
n, dim = 300, 2
np.random.seed(0)
8.5. Linear and Quadratic Discriminant Analysis with conﬁdence ellipsoid
671
scikit-learn user guide, Release 0.18.2
C = np.array([[0., -0.23], [0.83, .23]])
X = np.r_[np.dot(np.random.randn(n, dim), C),
np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]
y = np.hstack((np.zeros(n), np.ones(n)))
return X, y
def dataset_cov():
'''Generate 2 Gaussians samples with different covariance matrices'''
n, dim = 300, 2
np.random.seed(0)
C = np.array([[0., -1.], [2.5, .7]]) * 2.
X = np.r_[np.dot(np.random.randn(n, dim), C),
np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]
y = np.hstack((np.zeros(n), np.ones(n)))
return X, y
plot functions
def plot_data(lda, X, y, y_pred, fig_index):
splot = plt.subplot(2, 2, fig_index)
if fig_index == 1:
plt.title('Linear Discriminant Analysis')
plt.ylabel('Data with fixed covariance')
elif fig_index == 2:
plt.title('Quadratic Discriminant Analysis')
elif fig_index == 3:
plt.ylabel('Data with varying covariances')
tp = (y == y_pred)
# True Positive
tp0, tp1 = tp[y == 0], tp[y == 1]
X0, X1 = X[y == 0], X[y == 1]
X0_tp, X0_fp = X0[tp0], X0[~tp0]
X1_tp, X1_fp = X1[tp1], X1[~tp1]
alpha = 0.5
# class 0: dots
plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', alpha=alpha,
color='red')
plt.plot(X0_fp[:, 0], X0_fp[:, 1], '*', alpha=alpha,
color='#990000')
# dark red
# class 1: dots
plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', alpha=alpha,
color='blue')
plt.plot(X1_fp[:, 0], X1_fp[:, 1], '*', alpha=alpha,
color='#000099')
# dark blue
# class 0 and 1 : areas
nx, ny = 200, 100
x_min, x_max = plt.xlim()
y_min, y_max = plt.ylim()
xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),
np.linspace(y_min, y_max, ny))
Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])
Z = Z[:, 1].reshape(xx.shape)
plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',
672
Chapter 8. Classiﬁcation
scikit-learn user guide, Release 0.18.2
norm=colors.Normalize(0., 1.))
plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='k')
# means
plt.plot(lda.means_[0][0], lda.means_[0][1],
'o', color='black', markersize=10)
plt.plot(lda.means_[1][0], lda.means_[1][1],
'o', color='black', markersize=10)
return splot
def plot_ellipse(splot, mean, cov, color):
v, w = linalg.eigh(cov)
u = w[0] / linalg.norm(w[0])
angle = np.arctan(u[1] / u[0])
angle = 180 * angle / np.pi
# convert to degrees
# filled Gaussian at 2 standard deviation
ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,
180 + angle, facecolor=color, edgecolor='yellow',
linewidth=2, zorder=2)
ell.set_clip_box(splot.bbox)
ell.set_alpha(0.5)
splot.add_artist(ell)
splot.set_xticks(())
splot.set_yticks(())
def plot_lda_cov(lda, splot):
plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')
plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')
def plot_qda_cov(qda, splot):
plot_ellipse(splot, qda.means_[0], qda.covariances_[0], 'red')
plot_ellipse(splot, qda.means_[1], qda.covariances_[1], 'blue')
for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):
# Linear Discriminant Analysis
lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
y_pred = lda.fit(X, y).predict(X)
splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)
plot_lda_cov(lda, splot)
plt.axis('tight')
# Quadratic Discriminant Analysis
qda = QuadraticDiscriminantAnalysis(store_covariances=True)
y_pred = qda.fit(X, y).predict(X)
splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)
plot_qda_cov(qda, splot)
plt.axis('tight')
plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis')
plt.show()
8.5. Linear and Quadratic Discriminant Analysis with conﬁdence ellipsoid
673
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.482 seconds)
Download Python source code: plot_lda_qda.py
Download IPython notebook: plot_lda_qda.ipynb
674
Chapter 8. Classiﬁcation
CHAPTER
NINE
CLUSTERING
Examples concerning the sklearn.cluster module.
9.1 A demo of the mean-shift clustering algorithm
Reference:
Dorin Comaniciu and Peter Meer, “Mean Shift: A robust approach toward feature space analysis”. IEEE Transactions
on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619.
print(__doc__)
import numpy as np
from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.datasets.samples_generator import make_blobs
Generate sample data
centers = [[1, 1], [-1, -1], [1, -1]]
X, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)
Compute clustering with MeanShift
# The following bandwidth can be automatically detected using
bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)
ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)
ms.fit(X)
labels = ms.labels_
cluster_centers = ms.cluster_centers_
labels_unique = np.unique(labels)
n_clusters_ = len(labels_unique)
print("number of estimated clusters : %d" % n_clusters_)
Out:
number of estimated clusters : 3
Plot result
675
scikit-learn user guide, Release 0.18.2
import matplotlib.pyplot as plt
from itertools import cycle
plt.figure(1)
plt.clf()
colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
for k, col in zip(range(n_clusters_), colors):
my_members = labels == k
cluster_center = cluster_centers[k]
plt.plot(X[my_members, 0], X[my_members, 1], col + '.')
plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
markeredgecolor='k', markersize=14)
plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()
Total running time of the script: (0 minutes 1.169 seconds)
Download Python source code: plot_mean_shift.py
Download IPython notebook: plot_mean_shift.ipynb
676
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
9.2 Feature agglomeration
These images how similar features are merged together using feature agglomeration.
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, cluster
from sklearn.feature_extraction.image import grid_to_graph
digits = datasets.load_digits()
images = digits.images
X = np.reshape(images, (len(images), -1))
connectivity = grid_to_graph(*images[0].shape)
agglo = cluster.FeatureAgglomeration(connectivity=connectivity,
n_clusters=32)
agglo.fit(X)
X_reduced = agglo.transform(X)
X_restored = agglo.inverse_transform(X_reduced)
images_restored = np.reshape(X_restored, images.shape)
plt.figure(1, figsize=(4, 3.5))
plt.clf()
plt.subplots_adjust(left=.01, right=.99, bottom=.01, top=.91)
for i in range(4):
plt.subplot(3, 4, i + 1)
9.2. Feature agglomeration
677
scikit-learn user guide, Release 0.18.2
plt.imshow(images[i], cmap=plt.cm.gray, vmax=16, interpolation='nearest')
plt.xticks(())
plt.yticks(())
if i == 1:
plt.title('Original data')
plt.subplot(3, 4, 4 + i + 1)
plt.imshow(images_restored[i], cmap=plt.cm.gray, vmax=16,
interpolation='nearest')
if i == 1:
plt.title('Agglomerated data')
plt.xticks(())
plt.yticks(())
plt.subplot(3, 4, 10)
plt.imshow(np.reshape(agglo.labels_, images[0].shape),
interpolation='nearest', cmap=plt.cm.spectral)
plt.xticks(())
plt.yticks(())
plt.title('Labels')
plt.show()
Total running time of the script: (0 minutes 0.799 seconds)
Download Python source code: plot_digits_agglomeration.py
Download IPython notebook: plot_digits_agglomeration.ipynb
9.3 Demonstration of k-means assumptions
This example is meant to illustrate situations where k-means will produce unintuitive and possibly unexpected clusters.
In the ﬁrst three plots, the input data does not conform to some implicit assumption that k-means makes and undesirable
clusters are produced as a result. In the last plot, k-means returns intuitive clusters despite unevenly sized blobs.
678
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Author: Phil Roth <mr.phil.roth@gmail.com>
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
plt.figure(figsize=(12, 12))
n_samples = 1500
random_state = 170
9.3. Demonstration of k-means assumptions
679
scikit-learn user guide, Release 0.18.2
X, y = make_blobs(n_samples=n_samples, random_state=random_state)
# Incorrect number of clusters
y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)
plt.subplot(221)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.title("Incorrect Number of Blobs")
# Anisotropicly distributed data
transformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]
X_aniso = np.dot(X, transformation)
y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)
plt.subplot(222)
plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)
plt.title("Anisotropicly Distributed Blobs")
# Different variance
X_varied, y_varied = make_blobs(n_samples=n_samples,
cluster_std=[1.0, 2.5, 0.5],
random_state=random_state)
y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)
plt.subplot(223)
plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)
plt.title("Unequal Variance")
# Unevenly sized blobs
X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))
y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)
plt.subplot(224)
plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)
plt.title("Unevenly Sized Blobs")
plt.show()
Total running time of the script: (0 minutes 0.364 seconds)
Download Python source code: plot_kmeans_assumptions.py
Download IPython notebook: plot_kmeans_assumptions.ipynb
9.4 A demo of structured Ward hierarchical clustering on a raccoon
face image
Compute the segmentation of a 2D image with Ward hierarchical clustering. The clustering is spatially constrained in
order for each segmented region to be in one piece.
# Author : Vincent Michel, 2010
#
Alexandre Gramfort, 2011
# License: BSD 3 clause
print(__doc__)
680
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
import time as time
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
from sklearn.feature_extraction.image import grid_to_graph
from sklearn.cluster import AgglomerativeClustering
from sklearn.utils.testing import SkipTest
from sklearn.utils.fixes import sp_version
if sp_version < (0, 12):
raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "
"thus does not include the scipy.misc.face() image.")
Generate data
try:
face = sp.face(gray=True)
except AttributeError:
# Newer versions of scipy have face in misc
from scipy import misc
face = misc.face(gray=True)
# Resize it to 10% of the original size to speed up the processing
face = sp.misc.imresize(face, 0.10) / 255.
X = np.reshape(face, (-1, 1))
Deﬁne the structure A of the data. Pixels connected to their neighbors.
connectivity = grid_to_graph(*face.shape)
Compute clustering
print("Compute structured hierarchical clustering...")
st = time.time()
n_clusters = 15
# number of regions
ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',
connectivity=connectivity)
ward.fit(X)
label = np.reshape(ward.labels_, face.shape)
print("Elapsed time: ", time.time() - st)
print("Number of pixels: ", label.size)
print("Number of clusters: ", np.unique(label).size)
Out:
Compute structured hierarchical clustering...
Elapsed time:
0.635977983475
Number of pixels:
7752
Number of clusters:
15
Plot the results on an image
plt.figure(figsize=(5, 5))
plt.imshow(face, cmap=plt.cm.gray)
9.4. A demo of structured Ward hierarchical clustering on a raccoon face image
681
scikit-learn user guide, Release 0.18.2
for l in range(n_clusters):
plt.contour(label == l, contours=1,
colors=[plt.cm.spectral(l / float(n_clusters)), ])
plt.xticks(())
plt.yticks(())
plt.show()
Total running time of the script: (0 minutes 1.023 seconds)
Download Python source code: plot_face_ward_segmentation.py
Download IPython notebook: plot_face_ward_segmentation.ipynb
9.5 Online learning of a dictionary of parts of faces
This example uses a large dataset of faces to learn a set of 20 x 20 images patches that constitute faces.
From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to
process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly
50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the partial_ﬁt
method of the online KMeans object, MiniBatchKMeans.
The verbose setting on the MiniBatchKMeans enables us to see that some clusters are reassigned during the successive
682
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
calls to partial-ﬁt. This is because the number of patches that they represent has become too low, and it is better to
choose a random new cluster.
print(__doc__)
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from sklearn.cluster import MiniBatchKMeans
from sklearn.feature_extraction.image import extract_patches_2d
faces = datasets.fetch_olivetti_faces()
Learn the dictionary of images
print('Learning the dictionary... ')
rng = np.random.RandomState(0)
kmeans = MiniBatchKMeans(n_clusters=81, random_state=rng, verbose=True)
patch_size = (20, 20)
buffer = []
index = 1
t0 = time.time()
# The online learning part: cycle over the whole dataset 6 times
index = 0
for _ in range(6):
for img in faces.images:
data = extract_patches_2d(img, patch_size, max_patches=50,
random_state=rng)
data = np.reshape(data, (len(data), -1))
buffer.append(data)
index += 1
if index % 10 == 0:
data = np.concatenate(buffer, axis=0)
data -= np.mean(data, axis=0)
data /= np.std(data, axis=0)
kmeans.partial_fit(data)
buffer = []
if index % 100 == 0:
print('Partial fit of %4i out of %i'
% (index, 6 * len(faces.images)))
dt = time.time() - t0
print('done in %.2fs.' % dt)
Out:
Learning the dictionary...
Partial fit of
100 out of 2400
Partial fit of
200 out of 2400
[MiniBatchKMeans] Reassigning 16 cluster centers.
Partial fit of
300 out of 2400
Partial fit of
400 out of 2400
Partial fit of
500 out of 2400
9.5. Online learning of a dictionary of parts of faces
683
scikit-learn user guide, Release 0.18.2
Partial fit of
600 out of 2400
Partial fit of
700 out of 2400
Partial fit of
800 out of 2400
Partial fit of
900 out of 2400
Partial fit of 1000 out of 2400
Partial fit of 1100 out of 2400
Partial fit of 1200 out of 2400
Partial fit of 1300 out of 2400
Partial fit of 1400 out of 2400
Partial fit of 1500 out of 2400
Partial fit of 1600 out of 2400
Partial fit of 1700 out of 2400
Partial fit of 1800 out of 2400
Partial fit of 1900 out of 2400
Partial fit of 2000 out of 2400
Partial fit of 2100 out of 2400
Partial fit of 2200 out of 2400
Partial fit of 2300 out of 2400
Partial fit of 2400 out of 2400
done in 3.15s.
Plot the results
plt.figure(figsize=(4.2, 4))
for i, patch in enumerate(kmeans.cluster_centers_):
plt.subplot(9, 9, i + 1)
plt.imshow(patch.reshape(patch_size), cmap=plt.cm.gray,
interpolation='nearest')
plt.xticks(())
plt.yticks(())
plt.suptitle('Patches of faces\nTrain time %.1fs on %d patches' %
(dt, 8 * len(faces.images)), fontsize=16)
plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)
plt.show()
684
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 7.898 seconds)
Download Python source code: plot_dict_face_patches.py
Download IPython notebook: plot_dict_face_patches.ipynb
9.6 Demo of afﬁnity propagation clustering algorithm
Reference: Brendan J. Frey and Delbert Dueck, “Clustering by Passing Messages Between Data Points”, Science Feb.
2007
print(__doc__)
from sklearn.cluster import AffinityPropagation
from sklearn import metrics
from sklearn.datasets.samples_generator import make_blobs
Generate sample data
centers = [[1, 1], [-1, -1], [1, -1]]
X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,
random_state=0)
Compute Afﬁnity Propagation
af = AffinityPropagation(preference=-50).fit(X)
cluster_centers_indices = af.cluster_centers_indices_
labels = af.labels_
n_clusters_ = len(cluster_centers_indices)
9.6. Demo of afﬁnity propagation clustering algorithm
685
scikit-learn user guide, Release 0.18.2
print('Estimated number of clusters: %d' % n_clusters_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
print("Adjusted Rand Index: %0.3f"
% metrics.adjusted_rand_score(labels_true, labels))
print("Adjusted Mutual Information: %0.3f"
% metrics.adjusted_mutual_info_score(labels_true, labels))
print("Silhouette Coefficient: %0.3f"
% metrics.silhouette_score(X, labels, metric='sqeuclidean'))
Out:
Estimated number of clusters: 3
Homogeneity: 0.872
Completeness: 0.872
V-measure: 0.872
Adjusted Rand Index: 0.912
Adjusted Mutual Information: 0.871
Silhouette Coefficient: 0.753
Plot result
import matplotlib.pyplot as plt
from itertools import cycle
plt.close('all')
plt.figure(1)
plt.clf()
colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
for k, col in zip(range(n_clusters_), colors):
class_members = labels == k
cluster_center = X[cluster_centers_indices[k]]
plt.plot(X[class_members, 0], X[class_members, 1], col + '.')
plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
markeredgecolor='k', markersize=14)
for x in X[class_members]:
plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)
plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()
686
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.658 seconds)
Download Python source code: plot_affinity_propagation.py
Download IPython notebook: plot_affinity_propagation.ipynb
9.7 Hierarchical clustering: structured vs unstructured ward
Example builds a swiss roll dataset and runs hierarchical clustering on their position.
For more information, see Hierarchical clustering.
In a ﬁrst step, the hierarchical clustering is performed without connectivity constraints on the structure and is solely
based on distance, whereas in a second step the clustering is restricted to the k-Nearest Neighbors graph: it’s a
hierarchical clustering with structure prior.
Some of the clusters learned without connectivity constraints do not respect the structure of the swiss roll and extend
across different folds of the manifolds. On the opposite, when opposing connectivity constraints, the clusters form a
nice parcellation of the swiss roll.
# Authors : Vincent Michel, 2010
#
Alexandre Gramfort, 2010
#
Gael Varoquaux, 2010
# License: BSD 3 clause
9.7. Hierarchical clustering: structured vs unstructured ward
687
scikit-learn user guide, Release 0.18.2
print(__doc__)
import time as time
import numpy as np
import matplotlib.pyplot as plt
import mpl_toolkits.mplot3d.axes3d as p3
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets.samples_generator import make_swiss_roll
Generate data (swiss roll dataset)
n_samples = 1500
noise = 0.05
X, _ = make_swiss_roll(n_samples, noise)
# Make it thinner
X[:, 1] *= .5
Compute clustering
print("Compute unstructured hierarchical clustering...")
st = time.time()
ward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X)
elapsed_time = time.time() - st
label = ward.labels_
print("Elapsed time: %.2fs" % elapsed_time)
print("Number of points: %i" % label.size)
Out:
Compute unstructured hierarchical clustering...
Elapsed time: 0.06s
Number of points: 1500
Plot result
fig = plt.figure()
ax = p3.Axes3D(fig)
ax.view_init(7, -80)
for l in np.unique(label):
ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],
'o', color=plt.cm.jet(np.float(l) / np.max(label + 1)))
plt.title('Without connectivity constraints (time %.2fs)' % elapsed_time)
688
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
Deﬁne the structure A of the data. Here a 10 nearest neighbors
from sklearn.neighbors import kneighbors_graph
connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)
Compute clustering
print("Compute structured hierarchical clustering...")
st = time.time()
ward = AgglomerativeClustering(n_clusters=6, connectivity=connectivity,
linkage='ward').fit(X)
elapsed_time = time.time() - st
label = ward.labels_
print("Elapsed time: %.2fs" % elapsed_time)
print("Number of points: %i" % label.size)
Out:
Compute structured hierarchical clustering...
Elapsed time: 0.16s
Number of points: 1500
Plot result
fig = plt.figure()
ax = p3.Axes3D(fig)
9.7. Hierarchical clustering: structured vs unstructured ward
689
scikit-learn user guide, Release 0.18.2
ax.view_init(7, -80)
for l in np.unique(label):
ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],
'o', color=plt.cm.jet(float(l) / np.max(label + 1)))
plt.title('With connectivity constraints (time %.2fs)' % elapsed_time)
plt.show()
Total running time of the script: (0 minutes 0.317 seconds)
Download Python source code: plot_ward_structured_vs_unstructured.py
Download IPython notebook: plot_ward_structured_vs_unstructured.ipynb
9.8 Agglomerative clustering with and without structure
This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is
simply the graph of 20 nearest neighbors.
Two consequences of imposing a connectivity can be seen. First clustering with a connectivity matrix is much faster.
Second, when using a connectivity matrix, average and complete linkage are unstable and tend to create a few clusters
that grow very quickly. Indeed, average and complete linkage ﬁght this percolation behavior by considering all the
distances between two clusters when merging them. The connectivity graph breaks this mechanism. This effect is more
690
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
pronounced for very sparse graphs (try decreasing the number of neighbors in kneighbors_graph) and with complete
linkage. In particular, having a very small number of neighbors in the graph, imposes a geometry that is close to that
of single linkage, which is well known to have this percolation instability.
•
•
•
•
# Authors: Gael Varoquaux, Nelle Varoquaux
# License: BSD 3 clause
import time
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph
# Generate sample data
n_samples = 1500
np.random.seed(0)
t = 1.5 * np.pi * (1 + 3 * np.random.rand(1, n_samples))
x = t * np.cos(t)
y = t * np.sin(t)
X = np.concatenate((x, y))
X += .7 * np.random.randn(2, n_samples)
X = X.T
9.8. Agglomerative clustering with and without structure
691
scikit-learn user guide, Release 0.18.2
# Create a graph capturing local connectivity. Larger number of neighbors
# will give more homogeneous clusters to the cost of computation
# time. A very large number of neighbors gives more evenly distributed
# cluster sizes, but may not impose the local manifold structure of
# the data
knn_graph = kneighbors_graph(X, 30, include_self=False)
for connectivity in (None, knn_graph):
for n_clusters in (30, 3):
plt.figure(figsize=(10, 4))
for index, linkage in enumerate(('average', 'complete', 'ward')):
plt.subplot(1, 3, index + 1)
model = AgglomerativeClustering(linkage=linkage,
connectivity=connectivity,
n_clusters=n_clusters)
t0 = time.time()
model.fit(X)
elapsed_time = time.time() - t0
plt.scatter(X[:, 0], X[:, 1], c=model.labels_,
cmap=plt.cm.spectral)
plt.title('linkage=%s (time %.2fs)' % (linkage, elapsed_time),
fontdict=dict(verticalalignment='top'))
plt.axis('equal')
plt.axis('off')
plt.subplots_adjust(bottom=0, top=.89, wspace=0,
left=0, right=1)
plt.suptitle('n_cluster=%i, connectivity=%r' %
(n_clusters, connectivity is not None), size=17)
plt.show()
Total running time of the script: (0 minutes 2.844 seconds)
Download Python source code: plot_agglomerative_clustering.py
Download IPython notebook: plot_agglomerative_clustering.ipynb
9.9 K-means Clustering
The plots display ﬁrstly what a K-means algorithm would yield using three clusters. It is then shown what the effect
of a bad initialization is on the classiﬁcation process: By setting n_init to only 1 (default is 10), the amount of times
that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters
would deliver and ﬁnally the ground truth.
•
692
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
•
•
•
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans
from sklearn import datasets
np.random.seed(5)
centers = [[1, 1], [-1, -1], [1, -1]]
iris = datasets.load_iris()
X = iris.data
y = iris.target
estimators = {'k_means_iris_3': KMeans(n_clusters=3),
'k_means_iris_8': KMeans(n_clusters=8),
'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,
init='random')}
fignum = 1
9.9. K-means Clustering
693
scikit-learn user guide, Release 0.18.2
for name, est in estimators.items():
fig = plt.figure(fignum, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
plt.cla()
est.fit(X)
labels = est.labels_
ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))
ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
ax.set_xlabel('Petal width')
ax.set_ylabel('Sepal length')
ax.set_zlabel('Petal length')
fignum = fignum + 1
# Plot the ground truth
fig = plt.figure(fignum, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
plt.cla()
for name, label in [('Setosa', 0),
('Versicolour', 1),
('Virginica', 2)]:
ax.text3D(X[y == label, 3].mean(),
X[y == label, 0].mean() + 1.5,
X[y == label, 2].mean(), name,
horizontalalignment='center',
bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)
ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
ax.set_xlabel('Petal width')
ax.set_ylabel('Sepal length')
ax.set_zlabel('Petal length')
plt.show()
Total running time of the script: (0 minutes 0.415 seconds)
Download Python source code: plot_cluster_iris.py
Download IPython notebook: plot_cluster_iris.ipynb
9.10 Segmenting the picture of a raccoon face in regions
This example uses Spectral clustering on a graph created from voxel-to-voxel difference on an image to break this
image into multiple partly-homogeneous regions.
694
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
This procedure (spectral clustering on an image) is an efﬁcient approximate solution for ﬁnding normalized graph cuts.
There are two options to assign labels:
• with ‘kmeans’ spectral clustering will cluster samples in the embedding space using a kmeans algorithm
• whereas ‘discrete’ will iteratively search for the closest partition space to the embedding space.
print(__doc__)
# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>, Brian Cheung
# License: BSD 3 clause
import time
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
from sklearn.feature_extraction import image
from sklearn.cluster import spectral_clustering
from sklearn.utils.testing import SkipTest
from sklearn.utils.fixes import sp_version
if sp_version < (0, 12):
raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "
"thus does not include the scipy.misc.face() image.")
# load the raccoon face as a numpy array
try:
face = sp.face(gray=True)
except AttributeError:
# Newer versions of scipy have face in misc
from scipy import misc
face = misc.face(gray=True)
# Resize it to 10% of the original size to speed up the processing
face = sp.misc.imresize(face, 0.10) / 255.
# Convert the image into a graph with the value of the gradient on the
# edges.
graph = image.img_to_graph(face)
# Take a decreasing function of the gradient: an exponential
# The smaller beta is, the more independent the segmentation is of the
# actual image. For beta=1, the segmentation is close to a voronoi
beta = 5
eps = 1e-6
graph.data = np.exp(-beta * graph.data / graph.data.std()) + eps
# Apply spectral clustering (this step goes much faster if you have pyamg
# installed)
N_REGIONS = 25
Visualize the resulting regions
for assign_labels in ('kmeans', 'discretize'):
t0 = time.time()
labels = spectral_clustering(graph, n_clusters=N_REGIONS,
9.10. Segmenting the picture of a raccoon face in regions
695
scikit-learn user guide, Release 0.18.2
assign_labels=assign_labels, random_state=1)
t1 = time.time()
labels = labels.reshape(face.shape)
plt.figure(figsize=(5, 5))
plt.imshow(face, cmap=plt.cm.gray)
for l in range(N_REGIONS):
plt.contour(labels == l, contours=1,
colors=[plt.cm.spectral(l / float(N_REGIONS))])
plt.xticks(())
plt.yticks(())
title = 'Spectral clustering: %s, %.2fs' % (assign_labels, (t1 - t0))
print(title)
plt.title(title)
plt.show()
•
•
Out:
Spectral clustering: kmeans, 6.28s
Spectral clustering: discretize, 5.39s
Total running time of the script: (0 minutes 12.710 seconds)
Download Python source code: plot_face_segmentation.py
Download IPython notebook: plot_face_segmentation.ipynb
696
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
9.11 Demo of DBSCAN clustering algorithm
Finds core samples of high density and expands clusters from them.
print(__doc__)
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets.samples_generator import make_blobs
from sklearn.preprocessing import StandardScaler
Generate sample data
centers = [[1, 1], [-1, -1], [1, -1]]
X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,
random_state=0)
X = StandardScaler().fit_transform(X)
Compute DBSCAN
db = DBSCAN(eps=0.3, min_samples=10).fit(X)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
print('Estimated number of clusters: %d' % n_clusters_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
print("Adjusted Rand Index: %0.3f"
% metrics.adjusted_rand_score(labels_true, labels))
print("Adjusted Mutual Information: %0.3f"
% metrics.adjusted_mutual_info_score(labels_true, labels))
print("Silhouette Coefficient: %0.3f"
% metrics.silhouette_score(X, labels))
Out:
Estimated number of clusters: 3
Homogeneity: 0.953
Completeness: 0.883
V-measure: 0.917
Adjusted Rand Index: 0.952
Adjusted Mutual Information: 0.883
Silhouette Coefficient: 0.626
Plot result
import matplotlib.pyplot as plt
# Black removed and is used for noise instead.
unique_labels = set(labels)
9.11. Demo of DBSCAN clustering algorithm
697
scikit-learn user guide, Release 0.18.2
colors = [plt.cm.Spectral(each)
for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
if k == -1:
# Black used for noise.
col = [0, 0, 0, 1]
class_member_mask = (labels == k)
xy = X[class_member_mask & core_samples_mask]
plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
markeredgecolor='k', markersize=14)
xy = X[class_member_mask & ~core_samples_mask]
plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
markeredgecolor='k', markersize=6)
plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()
Total running time of the script: (0 minutes 0.116 seconds)
Download Python source code: plot_dbscan.py
Download IPython notebook: plot_dbscan.ipynb
698
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
9.12 Spectral clustering for image segmentation
In this example, an image with connected circles is generated and spectral clustering is used to separate the circles.
In these settings, the Spectral clustering approach solves the problem know as ‘normalized graph cuts’: the image is
seen as a graph of connected voxels, and the spectral clustering algorithm amounts to choosing graph cuts deﬁning
regions while minimizing the ratio of the gradient along the cut, and the volume of the region.
As the algorithm tries to balance the volume (ie balance the region sizes), if we take circles with different sizes, the
segmentation fails.
In addition, as there is no useful information in the intensity of the image, or its gradient, we choose to perform the
spectral clustering on a graph that is only weakly informed by the gradient. This is close to performing a Voronoi
partition of the graph.
In addition, we use the mask of the objects to restrict the graph to the outline of the objects. In this example, we are
interested in separating the objects one from the other, and not from the background.
print(__doc__)
# Authors:
Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>
#
Gael Varoquaux <gael.varoquaux@normalesup.org>
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction import image
from sklearn.cluster import spectral_clustering
l = 100
x, y = np.indices((l, l))
center1 = (28, 24)
center2 = (40, 50)
center3 = (67, 58)
center4 = (24, 70)
radius1, radius2, radius3, radius4 = 16, 14, 15, 14
circle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2
circle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2
circle3 = (x - center3[0]) ** 2 + (y - center3[1]) ** 2 < radius3 ** 2
circle4 = (x - center4[0]) ** 2 + (y - center4[1]) ** 2 < radius4 ** 2
4 circles
img = circle1 + circle2 + circle3 + circle4
# We use a mask that limits to the foreground: the problem that we are
# interested in here is not separating the objects from the background,
# but separating them one from the other.
mask = img.astype(bool)
img = img.astype(float)
img += 1 + 0.2 * np.random.randn(*img.shape)
# Convert the image into a graph with the value of the gradient on the
9.12. Spectral clustering for image segmentation
699
scikit-learn user guide, Release 0.18.2
# edges.
graph = image.img_to_graph(img, mask=mask)
# Take a decreasing function of the gradient: we take it weakly
# dependent from the gradient the segmentation is close to a voronoi
graph.data = np.exp(-graph.data / graph.data.std())
# Force the solver to be arpack, since amg is numerically
# unstable on this example
labels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack')
label_im = -np.ones(mask.shape)
label_im[mask] = labels
plt.matshow(img)
plt.matshow(label_im)
•
•
2 circles
img = circle1 + circle2
mask = img.astype(bool)
700
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
img = img.astype(float)
img += 1 + 0.2 * np.random.randn(*img.shape)
graph = image.img_to_graph(img, mask=mask)
graph.data = np.exp(-graph.data / graph.data.std())
labels = spectral_clustering(graph, n_clusters=2, eigen_solver='arpack')
label_im = -np.ones(mask.shape)
label_im[mask] = labels
plt.matshow(img)
plt.matshow(label_im)
plt.show()
•
•
Total running time of the script: (0 minutes 1.151 seconds)
Download Python source code: plot_segmentation_toy.py
9.12. Spectral clustering for image segmentation
701
scikit-learn user guide, Release 0.18.2
Download IPython notebook: plot_segmentation_toy.ipynb
9.13 Vector Quantization Example
Face, a 1024 x 768 size image of a raccoon face, is used here to illustrate how k-means is used for vector quantization.
•
•
•
•
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
from sklearn import cluster
from sklearn.utils.testing import SkipTest
from sklearn.utils.fixes import sp_version
if sp_version < (0, 12):
raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "
"thus does not include the scipy.misc.face() image.")
try:
face = sp.face(gray=True)
702
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
except AttributeError:
# Newer versions of scipy have face in misc
from scipy import misc
face = misc.face(gray=True)
n_clusters = 5
np.random.seed(0)
X = face.reshape((-1, 1))
# We need an (n_sample, n_feature) array
k_means = cluster.KMeans(n_clusters=n_clusters, n_init=4)
k_means.fit(X)
values = k_means.cluster_centers_.squeeze()
labels = k_means.labels_
# create an array from labels and values
face_compressed = np.choose(labels, values)
face_compressed.shape = face.shape
vmin = face.min()
vmax = face.max()
# original face
plt.figure(1, figsize=(3, 2.2))
plt.imshow(face, cmap=plt.cm.gray, vmin=vmin, vmax=256)
# compressed face
plt.figure(2, figsize=(3, 2.2))
plt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)
# equal bins face
regular_values = np.linspace(0, 256, n_clusters + 1)
regular_labels = np.searchsorted(regular_values, face) - 1
regular_values = .5 * (regular_values[1:] + regular_values[:-1])
# mean
regular_face = np.choose(regular_labels.ravel(), regular_values, mode="clip")
regular_face.shape = face.shape
plt.figure(3, figsize=(3, 2.2))
plt.imshow(regular_face, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)
# histogram
plt.figure(4, figsize=(3, 2.2))
plt.clf()
plt.axes([.01, .01, .98, .98])
plt.hist(X, bins=256, color='.5', edgecolor='.5')
plt.yticks(())
plt.xticks(regular_values)
values = np.sort(values)
for center_1, center_2 in zip(values[:-1], values[1:]):
plt.axvline(.5 * (center_1 + center_2), color='b')
for center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):
plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')
plt.show()
Total running time of the script: (0 minutes 3.826 seconds)
Download Python source code: plot_face_compress.py
Download IPython notebook: plot_face_compress.ipynb
9.13. Vector Quantization Example
703
scikit-learn user guide, Release 0.18.2
9.14 Various Agglomerative Clustering on a 2D embedding of digits
An illustration of various linkage option for agglomerative clustering on a 2D embedding of the digits dataset.
The goal of this example is to show intuitively how the metrics behave, and not to ﬁnd good clusters for the digits.
This is why the example works on a 2D embedding.
What this example shows us is the behavior “rich getting richer” of agglomerative clustering that tends to create uneven
cluster sizes. This behavior is especially pronounced for the average linkage strategy, that ends up with a couple of
singleton clusters.
•
•
•
Out:
Computing embedding
Done.
ward : 0.39s
average : 0.28s
complete : 0.26s
704
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
# Authors: Gael Varoquaux
# License: BSD 3 clause (C) INRIA 2014
print(__doc__)
from time import time
import numpy as np
from scipy import ndimage
from matplotlib import pyplot as plt
from sklearn import manifold, datasets
digits = datasets.load_digits(n_class=10)
X = digits.data
y = digits.target
n_samples, n_features = X.shape
np.random.seed(0)
def nudge_images(X, y):
# Having a larger dataset shows more clearly the behavior of the
# methods, but we multiply the size of the dataset only by 2, as the
# cost of the hierarchical clustering methods are strongly
# super-linear in n_samples
shift = lambda x: ndimage.shift(x.reshape((8, 8)),
.3 * np.random.normal(size=2),
mode='constant',
).ravel()
X = np.concatenate([X, np.apply_along_axis(shift, 1, X)])
Y = np.concatenate([y, y], axis=0)
return X, Y
X, y = nudge_images(X, y)
#----------------------------------------------------------------------
# Visualize the clustering
def plot_clustering(X_red, X, labels, title=None):
x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)
X_red = (X_red - x_min) / (x_max - x_min)
plt.figure(figsize=(6, 4))
for i in range(X_red.shape[0]):
plt.text(X_red[i, 0], X_red[i, 1], str(y[i]),
color=plt.cm.spectral(labels[i] / 10.),
fontdict={'weight': 'bold', 'size': 9})
plt.xticks([])
plt.yticks([])
if title is not None:
plt.title(title, size=17)
plt.axis('off')
plt.tight_layout()
9.14. Various Agglomerative Clustering on a 2D embedding of digits
705
scikit-learn user guide, Release 0.18.2
#----------------------------------------------------------------------
# 2D embedding of the digits dataset
print("Computing embedding")
X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)
print("Done.")
from sklearn.cluster import AgglomerativeClustering
for linkage in ('ward', 'average', 'complete'):
clustering = AgglomerativeClustering(linkage=linkage, n_clusters=10)
t0 = time()
clustering.fit(X_red)
print("%s : %.2fs" % (linkage, time() - t0))
plot_clustering(X_red, X, clustering.labels_, "%s linkage" % linkage)
plt.show()
Total running time of the script: (0 minutes 21.258 seconds)
Download Python source code: plot_digits_linkage.py
Download IPython notebook: plot_digits_linkage.ipynb
9.15 Color Quantization using K-Means
Performs a pixel-wise Vector Quantization (VQ) of an image of the summer palace (China), reducing the number of
colors required to show the image from 96,615 unique colors to 64, while preserving the overall appearance quality.
In this example, pixels are represented in a 3D-space and K-means is used to ﬁnd 64 color clusters. In the image
processing literature, the codebook obtained from K-means (the cluster centers) is called the color palette. Using a
single byte, up to 256 colors can be addressed, whereas an RGB encoding requires 3 bytes per pixel. The GIF ﬁle
format, for example, uses such a palette.
For comparison, a quantized image using a random codebook (colors picked up randomly) is also shown.
•
706
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
•
•
Out:
Fitting model on a small sub-sample of the data
done in 0.358s.
Predicting color indices on the full image (k-means)
done in 0.222s.
Predicting color indices on the full image (random)
done in 0.221s.
# Authors: Robert Layton <robertlayton@gmail.com>
#
Olivier Grisel <olivier.grisel@ensta.org>
#
Mathieu Blondel <mathieu@mblondel.org>
#
# License: BSD 3 clause
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin
from sklearn.datasets import load_sample_image
from sklearn.utils import shuffle
from time import time
9.15. Color Quantization using K-Means
707
scikit-learn user guide, Release 0.18.2
n_colors = 64
# Load the Summer Palace photo
china = load_sample_image("china.jpg")
# Convert to floats instead of the default 8 bits integer coding. Dividing by
# 255 is important so that plt.imshow behaves works well on float data (need to
# be in the range [0-1])
china = np.array(china, dtype=np.float64) / 255
# Load Image and transform to a 2D numpy array.
w, h, d = original_shape = tuple(china.shape)
assert d == 3
image_array = np.reshape(china, (w * h, d))
print("Fitting model on a small sub-sample of the data")
t0 = time()
image_array_sample = shuffle(image_array, random_state=0)[:1000]
kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)
print("done in %0.3fs." % (time() - t0))
# Get labels for all points
print("Predicting color indices on the full image (k-means)")
t0 = time()
labels = kmeans.predict(image_array)
print("done in %0.3fs." % (time() - t0))
codebook_random = shuffle(image_array, random_state=0)[:n_colors + 1]
print("Predicting color indices on the full image (random)")
t0 = time()
labels_random = pairwise_distances_argmin(codebook_random,
image_array,
axis=0)
print("done in %0.3fs." % (time() - t0))
def recreate_image(codebook, labels, w, h):
"""Recreate the (compressed) image from the code book & labels"""
d = codebook.shape[1]
image = np.zeros((w, h, d))
label_idx = 0
for i in range(w):
for j in range(h):
image[i][j] = codebook[labels[label_idx]]
label_idx += 1
return image
# Display all results, alongside original image
plt.figure(1)
plt.clf()
ax = plt.axes([0, 0, 1, 1])
plt.axis('off')
plt.title('Original image (96,615 colors)')
plt.imshow(china)
plt.figure(2)
708
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
plt.clf()
ax = plt.axes([0, 0, 1, 1])
plt.axis('off')
plt.title('Quantized image (64 colors, K-Means)')
plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))
plt.figure(3)
plt.clf()
ax = plt.axes([0, 0, 1, 1])
plt.axis('off')
plt.title('Quantized image (64 colors, Random)')
plt.imshow(recreate_image(codebook_random, labels_random, w, h))
plt.show()
Total running time of the script: (0 minutes 1.687 seconds)
Download Python source code: plot_color_quantization.py
Download IPython notebook: plot_color_quantization.ipynb
9.16 Agglomerative clustering with different metrics
Demonstrates the effect of different metrics on the hierarchical clustering.
The example is engineered to show the effect of the choice of different metrics. It is applied to waveforms, which
can be seen as high-dimensional vector. Indeed, the difference between metrics is usually more pronounced in high
dimension (in particular for euclidean and cityblock).
We generate data from three groups of waveforms. Two of the waveforms (waveform 1 and waveform 2) are propor-
tional one to the other. The cosine distance is invariant to a scaling of the data, as a result, it cannot distinguish these
two waveforms. Thus even with no noise, clustering using this distance will not separate out waveform 1 and 2.
We add observation noise to these waveforms. We generate very sparse noise: only 6% of the time points contain
noise. As a result, the l1 norm of this noise (ie “cityblock” distance) is much smaller than it’s l2 norm (“euclidean”
distance). This can be seen on the inter-class distance matrices: the values on the diagonal, that characterize the spread
of the class, are much bigger for the Euclidean distance than for the cityblock distance.
When we apply clustering to the data, we ﬁnd that the clustering reﬂects what was in the distance matrices. Indeed,
for the Euclidean distance, the classes are ill-separated because of the noise, and thus the clustering does not separate
the waveforms. For the cityblock distance, the separation is good and the waveform classes are recovered. Finally, the
cosine distance does not separate at all waveform 1 and 2, thus the clustering puts them in the same cluster.
•
9.16. Agglomerative clustering with different metrics
709
scikit-learn user guide, Release 0.18.2
•
•
•
•
710
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
•
•
# Author: Gael Varoquaux
# License: BSD 3-Clause or CC-0
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import pairwise_distances
np.random.seed(0)
# Generate waveform data
n_features = 2000
t = np.pi * np.linspace(0, 1, n_features)
def sqr(x):
return np.sign(np.cos(x))
X = list()
y = list()
for i, (phi, a) in enumerate([(.5, .15), (.5, .6), (.3, .2)]):
for _ in range(30):
phase_noise = .01 * np.random.normal()
amplitude_noise = .04 * np.random.normal()
additional_noise = 1 - 2 * np.random.rand(n_features)
# Make the noise sparse
additional_noise[np.abs(additional_noise) < .997] = 0
9.16. Agglomerative clustering with different metrics
711
scikit-learn user guide, Release 0.18.2
X.append(12 * ((a + amplitude_noise)
* (sqr(6 * (t + phi + phase_noise)))
+ additional_noise))
y.append(i)
X = np.array(X)
y = np.array(y)
n_clusters = 3
labels = ('Waveform 1', 'Waveform 2', 'Waveform 3')
# Plot the ground-truth labelling
plt.figure()
plt.axes([0, 0, 1, 1])
for l, c, n in zip(range(n_clusters), 'rgb',
labels):
lines = plt.plot(X[y == l].T, c=c, alpha=.5)
lines[0].set_label(n)
plt.legend(loc='best')
plt.axis('tight')
plt.axis('off')
plt.suptitle("Ground truth", size=20)
# Plot the distances
for index, metric in enumerate(["cosine", "euclidean", "cityblock"]):
avg_dist = np.zeros((n_clusters, n_clusters))
plt.figure(figsize=(5, 4.5))
for i in range(n_clusters):
for j in range(n_clusters):
avg_dist[i, j] = pairwise_distances(X[y == i], X[y == j],
metric=metric).mean()
avg_dist /= avg_dist.max()
for i in range(n_clusters):
for j in range(n_clusters):
plt.text(i, j, '%5.3f' % avg_dist[i, j],
verticalalignment='center',
horizontalalignment='center')
plt.imshow(avg_dist, interpolation='nearest', cmap=plt.cm.gnuplot2,
vmin=0)
plt.xticks(range(n_clusters), labels, rotation=45)
plt.yticks(range(n_clusters), labels)
plt.colorbar()
plt.suptitle("Interclass %s distances" % metric, size=18)
plt.tight_layout()
# Plot clustering results
for index, metric in enumerate(["cosine", "euclidean", "cityblock"]):
model = AgglomerativeClustering(n_clusters=n_clusters,
linkage="average", affinity=metric)
model.fit(X)
plt.figure()
712
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
plt.axes([0, 0, 1, 1])
for l, c in zip(np.arange(model.n_clusters), 'rgbk'):
plt.plot(X[model.labels_ == l].T, c=c, alpha=.5)
plt.axis('tight')
plt.axis('off')
plt.suptitle("AgglomerativeClustering(affinity=%s)" % metric, size=20)
plt.show()
Total running time of the script: (0 minutes 1.210 seconds)
Download Python source code: plot_agglomerative_clustering_metrics.py
Download IPython notebook: plot_agglomerative_clustering_metrics.ipynb
9.17 Comparison of the K-Means and MiniBatchKMeans clustering
algorithms
We want to compare the performance of the MiniBatchKMeans and KMeans: the MiniBatchKMeans is faster, but
gives slightly different results (see Mini Batch K-Means).
We will cluster a set of data, ﬁrst with KMeans and then with MiniBatchKMeans, and plot the results. We will also
plot the points that are labelled differently between the two algorithms.
print(__doc__)
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MiniBatchKMeans, KMeans
from sklearn.metrics.pairwise import pairwise_distances_argmin
from sklearn.datasets.samples_generator import make_blobs
Generate sample data
np.random.seed(0)
batch_size = 45
centers = [[1, 1], [-1, -1], [1, -1]]
n_clusters = len(centers)
X, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)
Compute clustering with Means
k_means = KMeans(init='k-means++', n_clusters=3, n_init=10)
t0 = time.time()
k_means.fit(X)
t_batch = time.time() - t0
Compute clustering with MiniBatchKMeans
mbk = MiniBatchKMeans(init='k-means++', n_clusters=3, batch_size=batch_size,
n_init=10, max_no_improvement=10, verbose=0)
9.17. Comparison of the K-Means and MiniBatchKMeans clustering algorithms
713
scikit-learn user guide, Release 0.18.2
t0 = time.time()
mbk.fit(X)
t_mini_batch = time.time() - t0
Plot result
fig = plt.figure(figsize=(8, 3))
fig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)
colors = ['#4EACC5', '#FF9C34', '#4E9A06']
# We want to have the same colors for the same cluster from the
# MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per
# closest one.
k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=0)
mbk_means_cluster_centers = np.sort(mbk.cluster_centers_, axis=0)
k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)
mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)
order = pairwise_distances_argmin(k_means_cluster_centers,
mbk_means_cluster_centers)
# KMeans
ax = fig.add_subplot(1, 3, 1)
for k, col in zip(range(n_clusters), colors):
my_members = k_means_labels == k
cluster_center = k_means_cluster_centers[k]
ax.plot(X[my_members, 0], X[my_members, 1], 'w',
markerfacecolor=col, marker='.')
ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
markeredgecolor='k', markersize=6)
ax.set_title('KMeans')
ax.set_xticks(())
ax.set_yticks(())
plt.text(-3.5, 1.8,
'train time: %.2fs\ninertia: %f' % (
t_batch, k_means.inertia_))
# MiniBatchKMeans
ax = fig.add_subplot(1, 3, 2)
for k, col in zip(range(n_clusters), colors):
my_members = mbk_means_labels == order[k]
cluster_center = mbk_means_cluster_centers[order[k]]
ax.plot(X[my_members, 0], X[my_members, 1], 'w',
markerfacecolor=col, marker='.')
ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
markeredgecolor='k', markersize=6)
ax.set_title('MiniBatchKMeans')
ax.set_xticks(())
ax.set_yticks(())
plt.text(-3.5, 1.8, 'train time: %.2fs\ninertia: %f' %
(t_mini_batch, mbk.inertia_))
# Initialise the different array to all False
different = (mbk_means_labels == 4)
ax = fig.add_subplot(1, 3, 3)
for k in range(n_clusters):
different += ((k_means_labels == k) != (mbk_means_labels == order[k]))
identic = np.logical_not(different)
714
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
ax.plot(X[identic, 0], X[identic, 1], 'w',
markerfacecolor='#bbbbbb', marker='.')
ax.plot(X[different, 0], X[different, 1], 'w',
markerfacecolor='m', marker='.')
ax.set_title('Difference')
ax.set_xticks(())
ax.set_yticks(())
plt.show()
Total running time of the script: (0 minutes 0.344 seconds)
Download Python source code: plot_mini_batch_kmeans.py
Download IPython notebook: plot_mini_batch_kmeans.ipynb
9.18 Feature agglomeration vs. univariate selection
This example compares 2 dimensionality reduction strategies:
• univariate feature selection with Anova
• feature agglomeration with Ward hierarchical clustering
Both methods are compared in a regression problem using a BayesianRidge as supervised estimator.
# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause
print(__doc__)
import shutil
import tempfile
import numpy as np
import matplotlib.pyplot as plt
from scipy import linalg, ndimage
from sklearn.feature_extraction.image import grid_to_graph
from sklearn import feature_selection
from sklearn.cluster import FeatureAgglomeration
9.18. Feature agglomeration vs. univariate selection
715
scikit-learn user guide, Release 0.18.2
from sklearn.linear_model import BayesianRidge
from sklearn.pipeline import Pipeline
from sklearn.externals.joblib import Memory
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
Generate data
n_samples = 200
size = 40
# image size
roi_size = 15
snr = 5.
np.random.seed(0)
mask = np.ones([size, size], dtype=np.bool)
coef = np.zeros((size, size))
coef[0:roi_size, 0:roi_size] = -1.
coef[-roi_size:, -roi_size:] = 1.
X = np.random.randn(n_samples, size ** 2)
for x in X:
# smooth data
x[:] = ndimage.gaussian_filter(x.reshape(size, size), sigma=1.0).ravel()
X -= X.mean(axis=0)
X /= X.std(axis=0)
y = np.dot(X, coef.ravel())
noise = np.random.randn(y.shape[0])
noise_coef = (linalg.norm(y, 2) / np.exp(snr / 20.)) / linalg.norm(noise, 2)
y += noise_coef * noise
# add noise
Compute the coefs of a Bayesian Ridge with GridSearch
cv = KFold(2)
# cross-validation generator for model selection
ridge = BayesianRidge()
cachedir = tempfile.mkdtemp()
mem = Memory(cachedir=cachedir, verbose=1)
# Ward agglomeration followed by BayesianRidge
connectivity = grid_to_graph(n_x=size, n_y=size)
ward = FeatureAgglomeration(n_clusters=10, connectivity=connectivity,
memory=mem)
clf = Pipeline([('ward', ward), ('ridge', ridge)])
# Select the optimal number of parcels with grid search
clf = GridSearchCV(clf, {'ward__n_clusters': [10, 20, 30]}, n_jobs=1, cv=cv)
clf.fit(X, y)
# set the best parameters
coef_ = clf.best_estimator_.steps[-1][1].coef_
coef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_)
coef_agglomeration_ = coef_.reshape(size, size)
# Anova univariate feature selection followed by BayesianRidge
f_regression = mem.cache(feature_selection.f_regression)
# caching function
anova = feature_selection.SelectPercentile(f_regression)
clf = Pipeline([('anova', anova), ('ridge', ridge)])
# Select the optimal percentage of features with grid search
clf = GridSearchCV(clf, {'anova__percentile': [5, 10, 20]}, cv=cv)
clf.fit(X, y)
# set the best parameters
coef_ = clf.best_estimator_.steps[-1][1].coef_
coef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_.reshape(1, -1))
716
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
coef_selection_ = coef_.reshape(size, size)
Out:
________________________________________________________________________________
[Memory] Calling sklearn.cluster.hierarchical.ward_tree...
ward_tree(array([[-0.451933, ..., -0.675318],
...,
[ 0.275706, ..., -1.085711]]),
<1600x1600 sparse matrix of type '<type 'numpy.int64'>'
with 7840 stored elements in COOrdinate format>, n_clusters=None)
________________________________________________________ward_tree - 0.2s, 0.0min
________________________________________________________________________________
[Memory] Calling sklearn.cluster.hierarchical.ward_tree...
ward_tree(array([[ 0.905206, ...,
0.161245],
...,
[-0.849835, ..., -1.091621]]),
<1600x1600 sparse matrix of type '<type 'numpy.int64'>'
with 7840 stored elements in COOrdinate format>, n_clusters=None)
________________________________________________________ward_tree - 0.2s, 0.0min
________________________________________________________________________________
[Memory] Calling sklearn.cluster.hierarchical.ward_tree...
ward_tree(array([[ 0.905206, ..., -0.675318],
...,
[-0.849835, ..., -1.085711]]),
<1600x1600 sparse matrix of type '<type 'numpy.int64'>'
with 7840 stored elements in COOrdinate format>, n_clusters=None)
________________________________________________________ward_tree - 0.2s, 0.0min
________________________________________________________________________________
[Memory] Calling sklearn.feature_selection.univariate_selection.f_regression...
f_regression(array([[-0.451933, ...,
0.275706],
...,
[-0.675318, ..., -1.085711]]),
array([ 25.267703, ..., -25.026711]))
_____________________________________________________f_regression - 0.0s, 0.0min
________________________________________________________________________________
[Memory] Calling sklearn.feature_selection.univariate_selection.f_regression...
f_regression(array([[ 0.905206, ..., -0.849835],
...,
[ 0.161245, ..., -1.091621]]),
array([ -27.447268, ..., -112.638768]))
_____________________________________________________f_regression - 0.0s, 0.0min
________________________________________________________________________________
[Memory] Calling sklearn.feature_selection.univariate_selection.f_regression...
f_regression(array([[ 0.905206, ..., -0.849835],
...,
[-0.675318, ..., -1.085711]]),
array([-27.447268, ..., -25.026711]))
_____________________________________________________f_regression - 0.0s, 0.0min
Inverse the transformation to plot the results on an image
plt.close('all')
plt.figure(figsize=(7.3, 2.7))
plt.subplot(1, 3, 1)
plt.imshow(coef, interpolation="nearest", cmap=plt.cm.RdBu_r)
plt.title("True weights")
plt.subplot(1, 3, 2)
9.18. Feature agglomeration vs. univariate selection
717
scikit-learn user guide, Release 0.18.2
plt.imshow(coef_selection_, interpolation="nearest", cmap=plt.cm.RdBu_r)
plt.title("Feature Selection")
plt.subplot(1, 3, 3)
plt.imshow(coef_agglomeration_, interpolation="nearest", cmap=plt.cm.RdBu_r)
plt.title("Feature Agglomeration")
plt.subplots_adjust(0.04, 0.0, 0.98, 0.94, 0.16, 0.26)
plt.show()
# Attempt to remove the temporary cachedir, but don't worry if it fails
shutil.rmtree(cachedir, ignore_errors=True)
Total running time of the script: (0 minutes 1.359 seconds)
Download Python source code: plot_feature_agglomeration_vs_univariate_selection.py
Download IPython notebook: plot_feature_agglomeration_vs_univariate_selection.ipynb
9.19 Compare BIRCH and MiniBatchKMeans
This example compares the timing of Birch (with and without the global clustering step) and MiniBatchKMeans on a
synthetic dataset having 100,000 samples and 2 features generated using make_blobs.
If n_clusters is set to None, the data is reduced from 100,000 samples to a set of 158 clusters. This can be viewed
as a preprocessing step before the ﬁnal (global) clustering step that further reduces these 158 clusters to 100 clusters.
Out:
718
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
Birch without global clustering as the final step took 4.80 seconds
n_clusters : 158
Birch with global clustering as the final step took 4.86 seconds
n_clusters : 100
Time taken to run MiniBatchKMeans 4.99 seconds
# Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com
#
Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
# License: BSD 3 clause
print(__doc__)
from itertools import cycle
from time import time
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import Birch, MiniBatchKMeans
from sklearn.datasets.samples_generator import make_blobs
# Generate centers for the blobs so that it forms a 10 X 10 grid.
xx = np.linspace(-22, 22, 10)
yy = np.linspace(-22, 22, 10)
xx, yy = np.meshgrid(xx, yy)
n_centres = np.hstack((np.ravel(xx)[:, np.newaxis],
np.ravel(yy)[:, np.newaxis]))
# Generate blobs to do a comparison between MiniBatchKMeans and Birch.
X, y = make_blobs(n_samples=100000, centers=n_centres, random_state=0)
# Use all colors that matplotlib provides by default.
colors_ = cycle(colors.cnames.keys())
fig = plt.figure(figsize=(12, 4))
fig.subplots_adjust(left=0.04, right=0.98, bottom=0.1, top=0.9)
# Compute clustering with Birch with and without the final clustering step
# and plot.
birch_models = [Birch(threshold=1.7, n_clusters=None),
Birch(threshold=1.7, n_clusters=100)]
final_step = ['without global clustering', 'with global clustering']
for ind, (birch_model, info) in enumerate(zip(birch_models, final_step)):
t = time()
birch_model.fit(X)
time_ = time() - t
print("Birch %s as the final step took %0.2f seconds" % (
info, (time() - t)))
9.19. Compare BIRCH and MiniBatchKMeans
719
scikit-learn user guide, Release 0.18.2
# Plot result
labels = birch_model.labels_
centroids = birch_model.subcluster_centers_
n_clusters = np.unique(labels).size
print("n_clusters : %d" % n_clusters)
ax = fig.add_subplot(1, 3, ind + 1)
for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):
mask = labels == k
ax.plot(X[mask, 0], X[mask, 1], 'w',
markerfacecolor=col, marker='.')
if birch_model.n_clusters is None:
ax.plot(this_centroid[0], this_centroid[1], '+', markerfacecolor=col,
markeredgecolor='k', markersize=5)
ax.set_ylim([-25, 25])
ax.set_xlim([-25, 25])
ax.set_autoscaley_on(False)
ax.set_title('Birch %s' % info)
# Compute clustering with MiniBatchKMeans.
mbk = MiniBatchKMeans(init='k-means++', n_clusters=100, batch_size=100,
n_init=10, max_no_improvement=10, verbose=0,
random_state=0)
t0 = time()
mbk.fit(X)
t_mini_batch = time() - t0
print("Time taken to run MiniBatchKMeans %0.2f seconds" % t_mini_batch)
mbk_means_labels_unique = np.unique(mbk.labels_)
ax = fig.add_subplot(1, 3, 3)
for this_centroid, k, col in zip(mbk.cluster_centers_,
range(n_clusters), colors_):
mask = mbk.labels_ == k
ax.plot(X[mask, 0], X[mask, 1], 'w', markerfacecolor=col, marker='.')
ax.plot(this_centroid[0], this_centroid[1], '+', markeredgecolor='k',
markersize=5)
ax.set_xlim([-25, 25])
ax.set_ylim([-25, 25])
ax.set_title("MiniBatchKMeans")
ax.set_autoscaley_on(False)
plt.show()
Total running time of the script: (0 minutes 16.043 seconds)
Download Python source code: plot_birch_vs_minibatchkmeans.py
Download IPython notebook: plot_birch_vs_minibatchkmeans.ipynb
9.20 Empirical evaluation of the impact of k-means initialization
Evaluate the ability of k-means initializations strategies to make the algorithm convergence robust as measured by the
relative standard deviation of the inertia of the clustering (i.e. the sum of distances to the nearest cluster center).
The ﬁrst plot shows the best inertia reached for each combination of the model (KMeans or MiniBatchKMeans)
and the init method (init="random" or init="kmeans++") for increasing values of the n_init parameter
that controls the number of initializations.
720
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
The second plot demonstrate one single run of the MiniBatchKMeans estimator using a init="random" and
n_init=1. This run leads to a bad convergence (local optimum) with estimated centers stuck between ground truth
clusters.
The dataset used for evaluation is a 2D grid of isotropic Gaussian clusters widely spaced.
•
•
Out:
Evaluation of KMeans with k-means++ init
Evaluation of KMeans with random init
Evaluation of MiniBatchKMeans with k-means++ init
Evaluation of MiniBatchKMeans with random init
print(__doc__)
# Author: Olivier Grisel <olivier.grisel@ensta.org>
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from sklearn.utils import shuffle
from sklearn.utils import check_random_state
9.20. Empirical evaluation of the impact of k-means initialization
721
scikit-learn user guide, Release 0.18.2
from sklearn.cluster import MiniBatchKMeans
from sklearn.cluster import KMeans
random_state = np.random.RandomState(0)
# Number of run (with randomly generated dataset) for each strategy so as
# to be able to compute an estimate of the standard deviation
n_runs = 5
# k-means models can do several random inits so as to be able to trade
# CPU time for convergence robustness
n_init_range = np.array([1, 5, 10, 15, 20])
# Datasets generation parameters
n_samples_per_center = 100
grid_size = 3
scale = 0.1
n_clusters = grid_size ** 2
def make_data(random_state, n_samples_per_center, grid_size, scale):
random_state = check_random_state(random_state)
centers = np.array([[i, j]
for i in range(grid_size)
for j in range(grid_size)])
n_clusters_true, n_features = centers.shape
noise = random_state.normal(
scale=scale, size=(n_samples_per_center, centers.shape[1]))
X = np.concatenate([c + noise for c in centers])
y = np.concatenate([[i] * n_samples_per_center
for i in range(n_clusters_true)])
return shuffle(X, y, random_state=random_state)
# Part 1: Quantitative evaluation of various init methods
fig = plt.figure()
plots = []
legends = []
cases = [
(KMeans, 'k-means++', {}),
(KMeans, 'random', {}),
(MiniBatchKMeans, 'k-means++', {'max_no_improvement': 3}),
(MiniBatchKMeans, 'random', {'max_no_improvement': 3, 'init_size': 500}),
]
for factory, init, params in cases:
print("Evaluation of %s with %s init" % (factory.__name__, init))
inertia = np.empty((len(n_init_range), n_runs))
for run_id in range(n_runs):
X, y = make_data(run_id, n_samples_per_center, grid_size, scale)
for i, n_init in enumerate(n_init_range):
km = factory(n_clusters=n_clusters, init=init, random_state=run_id,
n_init=n_init, **params).fit(X)
inertia[i, run_id] = km.inertia_
722
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
p = plt.errorbar(n_init_range, inertia.mean(axis=1), inertia.std(axis=1))
plots.append(p[0])
legends.append("%s with %s init" % (factory.__name__, init))
plt.xlabel('n_init')
plt.ylabel('inertia')
plt.legend(plots, legends)
plt.title("Mean inertia for various k-means init across %d runs" % n_runs)
# Part 2: Qualitative visual inspection of the convergence
X, y = make_data(random_state, n_samples_per_center, grid_size, scale)
km = MiniBatchKMeans(n_clusters=n_clusters, init='random', n_init=1,
random_state=random_state).fit(X)
fig = plt.figure()
for k in range(n_clusters):
my_members = km.labels_ == k
color = cm.spectral(float(k) / n_clusters, 1)
plt.plot(X[my_members, 0], X[my_members, 1], 'o', marker='.', c=color)
cluster_center = km.cluster_centers_[k]
plt.plot(cluster_center[0], cluster_center[1], 'o',
markerfacecolor=color, markeredgecolor='k', markersize=6)
plt.title("Example cluster allocation with a single random init\n"
"with MiniBatchKMeans")
plt.show()
Total running time of the script: (0 minutes 3.249 seconds)
Download Python source code: plot_kmeans_stability_low_dim_dense.py
Download IPython notebook: plot_kmeans_stability_low_dim_dense.ipynb
9.21 Adjustment for chance in clustering performance evaluation
The following plots demonstrate the impact of the number of clusters and number of samples on various clustering
performance evaluation metrics.
Non-adjusted measures such as the V-Measure show a dependency between the number of clusters and the number of
samples: the mean V-Measure of random labeling increases signiﬁcantly as the number of clusters is closer to the total
number of samples used to compute the measure.
Adjusted for chance measure such as ARI display some random variations centered around a mean score of 0.0 for
any number of samples and clusters.
Only adjusted measures can hence safely be used as a consensus index to evaluate the average stability of clustering
algorithms for a given value of k on various overlapping sub-samples of the dataset.
9.21. Adjustment for chance in clustering performance evaluation
723
scikit-learn user guide, Release 0.18.2
•
•
Out:
Computing adjusted_rand_score for 10 values of n_clusters and n_samples=100
done in 0.041s
Computing v_measure_score for 10 values of n_clusters and n_samples=100
done in 0.061s
Computing adjusted_mutual_info_score for 10 values of n_clusters and n_samples=100
done in 0.442s
Computing mutual_info_score for 10 values of n_clusters and n_samples=100
done in 0.051s
Computing adjusted_rand_score for 10 values of n_clusters and n_samples=1000
done in 0.062s
Computing v_measure_score for 10 values of n_clusters and n_samples=1000
done in 0.079s
Computing adjusted_mutual_info_score for 10 values of n_clusters and n_samples=1000
done in 0.286s
Computing mutual_info_score for 10 values of n_clusters and n_samples=1000
done in 0.065s
print(__doc__)
# Author: Olivier Grisel <olivier.grisel@ensta.org>
# License: BSD 3 clause
724
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
import numpy as np
import matplotlib.pyplot as plt
from time import time
from sklearn import metrics
def uniform_labelings_scores(score_func, n_samples, n_clusters_range,
fixed_n_classes=None, n_runs=5, seed=42):
"""Compute score for 2 random uniform cluster labelings.
Both random labelings have the same number of clusters for each value
possible value in ``n_clusters_range``.
When fixed_n_classes is not None the first labeling is considered a ground
truth class assignment with fixed number of classes.
"""
random_labels = np.random.RandomState(seed).randint
scores = np.zeros((len(n_clusters_range), n_runs))
if fixed_n_classes is not None:
labels_a = random_labels(low=0, high=fixed_n_classes, size=n_samples)
for i, k in enumerate(n_clusters_range):
for j in range(n_runs):
if fixed_n_classes is None:
labels_a = random_labels(low=0, high=k, size=n_samples)
labels_b = random_labels(low=0, high=k, size=n_samples)
scores[i, j] = score_func(labels_a, labels_b)
return scores
score_funcs = [
metrics.adjusted_rand_score,
metrics.v_measure_score,
metrics.adjusted_mutual_info_score,
metrics.mutual_info_score,
]
# 2 independent random clusterings with equal cluster number
n_samples = 100
n_clusters_range = np.linspace(2, n_samples, 10).astype(np.int)
plt.figure(1)
plots = []
names = []
for score_func in score_funcs:
print("Computing %s for %d values of n_clusters and n_samples=%d"
% (score_func.__name__, len(n_clusters_range), n_samples))
t0 = time()
scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)
print("done in %0.3fs" % (time() - t0))
plots.append(plt.errorbar(
n_clusters_range, np.median(scores, axis=1), scores.std(axis=1))[0])
names.append(score_func.__name__)
9.21. Adjustment for chance in clustering performance evaluation
725
scikit-learn user guide, Release 0.18.2
plt.title("Clustering measures for 2 random uniform labelings\n"
"with equal number of clusters")
plt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)
plt.ylabel('Score value')
plt.legend(plots, names)
plt.ylim(ymin=-0.05, ymax=1.05)
# Random labeling with varying n_clusters against ground class labels
# with fixed number of clusters
n_samples = 1000
n_clusters_range = np.linspace(2, 100, 10).astype(np.int)
n_classes = 10
plt.figure(2)
plots = []
names = []
for score_func in score_funcs:
print("Computing %s for %d values of n_clusters and n_samples=%d"
% (score_func.__name__, len(n_clusters_range), n_samples))
t0 = time()
scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range,
fixed_n_classes=n_classes)
print("done in %0.3fs" % (time() - t0))
plots.append(plt.errorbar(
n_clusters_range, scores.mean(axis=1), scores.std(axis=1))[0])
names.append(score_func.__name__)
plt.title("Clustering measures for random uniform labeling\n"
"against reference assignment with %d classes" % n_classes)
plt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)
plt.ylabel('Score value')
plt.ylim(ymin=-0.05, ymax=1.05)
plt.legend(plots, names)
plt.show()
Total running time of the script: (0 minutes 1.259 seconds)
Download Python source code: plot_adjusted_for_chance_measures.py
Download IPython notebook: plot_adjusted_for_chance_measures.ipynb
9.22 A demo of K-Means clustering on the handwritten digits data
In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the
results.
As the ground truth is known here, we also apply different cluster quality metrics to judge the goodness of ﬁt of the
cluster labels to the ground truth.
Cluster quality metrics evaluated (see Clustering performance evaluation for deﬁnitions and discussions of the met-
rics):
726
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
Shorthand
full name
homo
homogeneity score
compl
completeness score
v-meas
V measure
ARI
adjusted Rand index
AMI
adjusted mutual information
silhouette
silhouette coefﬁcient
print(__doc__)
from time import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
np.random.seed(42)
digits = load_digits()
data = scale(digits.data)
n_samples, n_features = data.shape
n_digits = len(np.unique(digits.target))
labels = digits.target
sample_size = 300
print("n_digits: %d, \t n_samples %d, \t n_features %d"
% (n_digits, n_samples, n_features))
print(79 * '_')
print('% 9s' % 'init'
'
time
inertia
homo
compl
v-meas
ARI AMI
silhouette')
def bench_k_means(estimator, name, data):
t0 = time()
estimator.fit(data)
print('% 9s
%.2fs
%i
%.3f
%.3f
%.3f
%.3f
%.3f
%.3f'
% (name, (time() - t0), estimator.inertia_,
metrics.homogeneity_score(labels, estimator.labels_),
metrics.completeness_score(labels, estimator.labels_),
metrics.v_measure_score(labels, estimator.labels_),
metrics.adjusted_rand_score(labels, estimator.labels_),
metrics.adjusted_mutual_info_score(labels,
estimator.labels_),
metrics.silhouette_score(data, estimator.labels_,
metric='euclidean',
sample_size=sample_size)))
bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),
name="k-means++", data=data)
bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),
9.22. A demo of K-Means clustering on the handwritten digits data
727
scikit-learn user guide, Release 0.18.2
name="random", data=data)
# in this case the seeding of the centers is deterministic, hence we run the
# kmeans algorithm only once with n_init=1
pca = PCA(n_components=n_digits).fit(data)
bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),
name="PCA-based",
data=data)
print(79 * '_')
Out:
n_digits: 10,
n_samples 1797,
n_features 64
_______________________________________________________________________________
init
time
inertia
homo
compl
v-meas
ARI AMI
silhouette
k-means++
0.32s
69432
0.602
0.650
0.625
0.465
0.598
0.146
random
0.26s
69694
0.669
0.710
0.689
0.553
0.666
0.147
PCA-based
0.05s
70804
0.671
0.698
0.684
0.561
0.668
0.118
_______________________________________________________________________________
Visualize the results on PCA-reduced data
reduced_data = PCA(n_components=2).fit_transform(data)
kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)
kmeans.fit(reduced_data)
# Step size of the mesh. Decrease to increase the quality of the VQ.
h = .02
# point in the mesh [x_min, x_max]x[y_min, y_max].
# Plot the decision boundary. For that, we will assign a color to each
x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
# Obtain labels for each point in mesh. Use last trained model.
Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1)
plt.clf()
plt.imshow(Z, interpolation='nearest',
extent=(xx.min(), xx.max(), yy.min(), yy.max()),
cmap=plt.cm.Paired,
aspect='auto', origin='lower')
plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)
# Plot the centroids as a white X
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1],
marker='x', s=169, linewidths=3,
color='w', zorder=10)
plt.title('K-means clustering on the digits dataset (PCA-reduced data)\n'
'Centroids are marked with white cross')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())
728
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
plt.show()
Total running time of the script: (0 minutes 1.879 seconds)
Download Python source code: plot_kmeans_digits.py
Download IPython notebook: plot_kmeans_digits.ipynb
9.23 Comparing different clustering algorithms on toy datasets
This example aims at showing characteristics of different clustering algorithms on datasets that are “interesting” but
still in 2D. The last dataset is an example of a ‘null’ situation for clustering: the data is homogeneous, and there is no
good clustering.
While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional
data.
The results could be improved by tweaking the parameters for each clustering strategy, for instance setting the number
of clusters for the methods that needs this parameter speciﬁed. Note that afﬁnity propagation has a tendency to create
many clusters. Thus in this example its two parameters (damping and per-point preference) were set to mitigate this
behavior.
9.23. Comparing different clustering algorithms on toy datasets
729
scikit-learn user guide, Release 0.18.2
print(__doc__)
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn import cluster, datasets
from sklearn.neighbors import kneighbors_graph
from sklearn.preprocessing import StandardScaler
np.random.seed(0)
# Generate datasets. We choose the size big enough to see the scalability
# of the algorithms, but not too big to avoid too long running times
n_samples = 1500
noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,
noise=.05)
noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)
blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)
no_structure = np.random.rand(n_samples, 2), None
colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])
colors = np.hstack([colors] * 20)
clustering_names = [
'MiniBatchKMeans', 'AffinityPropagation', 'MeanShift',
'SpectralClustering', 'Ward', 'AgglomerativeClustering',
'DBSCAN', 'Birch']
plt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5))
plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,
hspace=.01)
plot_num = 1
730
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
datasets = [noisy_circles, noisy_moons, blobs, no_structure]
for i_dataset, dataset in enumerate(datasets):
X, y = dataset
# normalize dataset for easier parameter selection
X = StandardScaler().fit_transform(X)
# estimate bandwidth for mean shift
bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)
# connectivity matrix for structured Ward
connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)
# make connectivity symmetric
connectivity = 0.5 * (connectivity + connectivity.T)
# create clustering estimators
ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)
two_means = cluster.MiniBatchKMeans(n_clusters=2)
ward = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward',
connectivity=connectivity)
spectral = cluster.SpectralClustering(n_clusters=2,
eigen_solver='arpack',
affinity="nearest_neighbors")
dbscan = cluster.DBSCAN(eps=.2)
affinity_propagation = cluster.AffinityPropagation(damping=.9,
preference=-200)
average_linkage = cluster.AgglomerativeClustering(
linkage="average", affinity="cityblock", n_clusters=2,
connectivity=connectivity)
birch = cluster.Birch(n_clusters=2)
clustering_algorithms = [
two_means, affinity_propagation, ms, spectral, ward, average_linkage,
dbscan, birch]
for name, algorithm in zip(clustering_names, clustering_algorithms):
# predict cluster memberships
t0 = time.time()
algorithm.fit(X)
t1 = time.time()
if hasattr(algorithm, 'labels_'):
y_pred = algorithm.labels_.astype(np.int)
else:
y_pred = algorithm.predict(X)
# plot
plt.subplot(4, len(clustering_algorithms), plot_num)
if i_dataset == 0:
plt.title(name, size=18)
plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)
if hasattr(algorithm, 'cluster_centers_'):
centers = algorithm.cluster_centers_
center_colors = colors[:len(centers)]
plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)
plt.xlim(-2, 2)
plt.ylim(-2, 2)
plt.xticks(())
9.23. Comparing different clustering algorithms on toy datasets
731
scikit-learn user guide, Release 0.18.2
plt.yticks(())
plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),
transform=plt.gca().transAxes, size=15,
horizontalalignment='right')
plot_num += 1
plt.show()
Total running time of the script: (0 minutes 20.565 seconds)
Download Python source code: plot_cluster_comparison.py
Download IPython notebook: plot_cluster_comparison.ipynb
9.24 Selecting the number of clusters with silhouette analysis on
KMeans clustering
Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot
displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a
way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].
Silhouette coefﬁcients (as these values are referred to as) near +1 indicate that the sample is far away from the neigh-
boring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two
neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.
In this example the silhouette analysis is used to choose an optimal value for n_clusters. The silhouette plot shows
that the n_clusters value of 3, 5 and 6 are a bad pick for the given data due to the presence of clusters with below
average silhouette scores and also due to wide ﬂuctuations in the size of the silhouette plots. Silhouette analysis is
more ambivalent in deciding between 2 and 4.
Also from the thickness of the silhouette plot the cluster size can be visualized. The silhouette plot for cluster 0 when
n_clusters is equal to 2, is bigger in size owing to the grouping of the 3 sub clusters into one big cluster. However
when the n_clusters is equal to 4, all the plots are more or less of similar thickness and hence are of similar sizes
as can be also veriﬁed from the labelled scatter plot on the right.
•
•
732
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
•
•
•
Out:
For n_clusters = 2 The average silhouette_score is : 0.704978749608
For n_clusters = 3 The average silhouette_score is : 0.588200401213
For n_clusters = 4 The average silhouette_score is : 0.650518663273
For n_clusters = 5 The average silhouette_score is : 0.563764690262
For n_clusters = 6 The average silhouette_score is : 0.450466629437
from __future__ import print_function
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np
print(__doc__)
# Generating the sample data from make_blobs
# This particular setting has one distinct cluster and 3 clusters placed close
# together.
X, y = make_blobs(n_samples=500,
n_features=2,
centers=4,
cluster_std=1,
center_box=(-10.0, 10.0),
shuffle=True,
9.24. Selecting the number of clusters with silhouette analysis on KMeans clustering
733
scikit-learn user guide, Release 0.18.2
random_state=1)
# For reproducibility
range_n_clusters = [2, 3, 4, 5, 6]
for n_clusters in range_n_clusters:
# Create a subplot with 1 row and 2 columns
fig, (ax1, ax2) = plt.subplots(1, 2)
fig.set_size_inches(18, 7)
# The 1st subplot is the silhouette plot
# The silhouette coefficient can range from -1, 1 but in this example all
# lie within [-0.1, 1]
ax1.set_xlim([-0.1, 1])
# The (n_clusters+1)*10 is for inserting blank space between silhouette
# plots of individual clusters, to demarcate them clearly.
ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])
# Initialize the clusterer with n_clusters value and a random generator
# seed of 10 for reproducibility.
clusterer = KMeans(n_clusters=n_clusters, random_state=10)
cluster_labels = clusterer.fit_predict(X)
# The silhouette_score gives the average value for all the samples.
# This gives a perspective into the density and separation of the formed
# clusters
silhouette_avg = silhouette_score(X, cluster_labels)
print("For n_clusters =", n_clusters,
"The average silhouette_score is :", silhouette_avg)
# Compute the silhouette scores for each sample
sample_silhouette_values = silhouette_samples(X, cluster_labels)
y_lower = 10
for i in range(n_clusters):
# Aggregate the silhouette scores for samples belonging to
# cluster i, and sort them
ith_cluster_silhouette_values = \
sample_silhouette_values[cluster_labels == i]
ith_cluster_silhouette_values.sort()
size_cluster_i = ith_cluster_silhouette_values.shape[0]
y_upper = y_lower + size_cluster_i
color = cm.spectral(float(i) / n_clusters)
ax1.fill_betweenx(np.arange(y_lower, y_upper),
0, ith_cluster_silhouette_values,
facecolor=color, edgecolor=color, alpha=0.7)
# Label the silhouette plots with their cluster numbers at the middle
ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
# Compute the new y_lower for next plot
y_lower = y_upper + 10
# 10 for the 0 samples
ax1.set_title("The silhouette plot for the various clusters.")
ax1.set_xlabel("The silhouette coefficient values")
ax1.set_ylabel("Cluster label")
734
Chapter 9. Clustering
scikit-learn user guide, Release 0.18.2
# The vertical line for average silhouette score of all the values
ax1.axvline(x=silhouette_avg, color="red", linestyle="--")
ax1.set_yticks([])
# Clear the yaxis labels / ticks
ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
# 2nd Plot showing the actual clusters formed
colors = cm.spectral(cluster_labels.astype(float) / n_clusters)
ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
c=colors)
# Labeling the clusters
centers = clusterer.cluster_centers_
# Draw white circles at cluster centers
ax2.scatter(centers[:, 0], centers[:, 1],
marker='o', c="white", alpha=1, s=200)
for i, c in enumerate(centers):
ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)
ax2.set_title("The visualization of the clustered data.")
ax2.set_xlabel("Feature space for the 1st feature")
ax2.set_ylabel("Feature space for the 2nd feature")
plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
"with n_clusters = %d" % n_clusters),
fontsize=14, fontweight='bold')
plt.show()
Total running time of the script: (0 minutes 1.232 seconds)
Download Python source code: plot_kmeans_silhouette_analysis.py
Download IPython notebook: plot_kmeans_silhouette_analysis.ipynb
9.24. Selecting the number of clusters with silhouette analysis on KMeans clustering
735
scikit-learn user guide, Release 0.18.2
736
Chapter 9. Clustering
CHAPTER
TEN
COVARIANCE ESTIMATION
Examples concerning the sklearn.covariance module.
10.1 Ledoit-Wolf vs OAS estimation
The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a
close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the
Ledoit-Wolf covariance estimate.
Chen et al. proposed an improvement of the Ledoit-Wolf shrinkage parameter, the OAS coefﬁcient, whose convergence
is signiﬁcantly better under the assumption that the data are Gaussian.
This example, inspired from Chen’s publication [1], shows a comparison of the estimated MSE of the LW and OAS
methods, using Gaussian distributed data.
[1] “Shrinkage Algorithms for MMSE Covariance Estimation” Chen et al., IEEE Trans. on Sign. Proc., Volume 58,
Issue 10, October 2010.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import toeplitz, cholesky
from sklearn.covariance import LedoitWolf, OAS
np.random.seed(0)
n_features = 100
# simulation covariance matrix (AR(1) process)
r = 0.1
real_cov = toeplitz(r ** np.arange(n_features))
coloring_matrix = cholesky(real_cov)
n_samples_range = np.arange(6, 31, 1)
repeat = 100
lw_mse = np.zeros((n_samples_range.size, repeat))
oa_mse = np.zeros((n_samples_range.size, repeat))
lw_shrinkage = np.zeros((n_samples_range.size, repeat))
oa_shrinkage = np.zeros((n_samples_range.size, repeat))
for i, n_samples in enumerate(n_samples_range):
for j in range(repeat):
X = np.dot(
np.random.normal(size=(n_samples, n_features)), coloring_matrix.T)
737
scikit-learn user guide, Release 0.18.2
lw = LedoitWolf(store_precision=False, assume_centered=True)
lw.fit(X)
lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)
lw_shrinkage[i, j] = lw.shrinkage_
oa = OAS(store_precision=False, assume_centered=True)
oa.fit(X)
oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)
oa_shrinkage[i, j] = oa.shrinkage_
# plot MSE
plt.subplot(2, 1, 1)
plt.errorbar(n_samples_range, lw_mse.mean(1), yerr=lw_mse.std(1),
label='Ledoit-Wolf', color='navy', lw=2)
plt.errorbar(n_samples_range, oa_mse.mean(1), yerr=oa_mse.std(1),
label='OAS', color='darkorange', lw=2)
plt.ylabel("Squared error")
plt.legend(loc="upper right")
plt.title("Comparison of covariance estimators")
plt.xlim(5, 31)
# plot shrinkage coefficient
plt.subplot(2, 1, 2)
plt.errorbar(n_samples_range, lw_shrinkage.mean(1), yerr=lw_shrinkage.std(1),
label='Ledoit-Wolf', color='navy', lw=2)
plt.errorbar(n_samples_range, oa_shrinkage.mean(1), yerr=oa_shrinkage.std(1),
label='OAS', color='darkorange', lw=2)
plt.xlabel("n_samples")
plt.ylabel("Shrinkage")
plt.legend(loc="lower right")
plt.ylim(plt.ylim()[0], 1. + (plt.ylim()[1] - plt.ylim()[0]) / 10.)
plt.xlim(5, 31)
plt.show()
738
Chapter 10. Covariance estimation
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 3.408 seconds)
Download Python source code: plot_lw_vs_oas.py
Download IPython notebook: plot_lw_vs_oas.ipynb
10.2 Sparse inverse covariance estimation
Using the GraphLasso estimator to learn a covariance and sparse precision from a small number of samples.
To estimate a probabilistic model (e.g. a Gaussian model), estimating the precision matrix, that is the inverse covari-
ance matrix, is as important as estimating the covariance matrix. Indeed a Gaussian model is parametrized by the
precision matrix.
To be in favorable recovery conditions, we sample the data from a model with a sparse inverse covariance matrix. In
addition, we ensure that the data is not too much correlated (limiting the largest coefﬁcient of the precision matrix) and
that there a no small coefﬁcients in the precision matrix that cannot be recovered. In addition, with a small number of
observations, it is easier to recover a correlation matrix rather than a covariance, thus we scale the time series.
Here, the number of samples is slightly larger than the number of dimensions, thus the empirical covariance is still
invertible. However, as the observations are strongly correlated, the empirical covariance matrix is ill-conditioned and
as a result its inverse –the empirical precision matrix– is very far from the ground truth.
If we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number of samples is small, we need to shrink a lot.
As a result, the Ledoit-Wolf precision is fairly close to the ground truth precision, that is not far from being diagonal,
10.2. Sparse inverse covariance estimation
739
scikit-learn user guide, Release 0.18.2
but the off-diagonal structure is lost.
The l1-penalized estimator can recover part of this off-diagonal structure. It learns a sparse precision. It is not
able to recover the exact sparsity pattern: it detects too many non-zero coefﬁcients. However, the highest non-zero
coefﬁcients of the l1 estimated correspond to the non-zero coefﬁcients in the ground truth. Finally, the coefﬁcients of
the l1 precision estimate are biased toward zero: because of the penalty, they are all smaller than the corresponding
ground truth value, as can be seen on the ﬁgure.
Note that, the color range of the precision matrices is tweaked to improve readability of the ﬁgure. The full range of
values of the empirical precision is not displayed.
The alpha parameter of the GraphLasso setting the sparsity of the model is set by internal cross-validation in the
GraphLassoCV. As can be seen on ﬁgure 2, the grid to compute the cross-validation score is iteratively reﬁned in the
neighborhood of the maximum.
print(__doc__)
# author: Gael Varoquaux <gael.varoquaux@inria.fr>
# License: BSD 3 clause
# Copyright: INRIA
import numpy as np
from scipy import linalg
from sklearn.datasets import make_sparse_spd_matrix
from sklearn.covariance import GraphLassoCV, ledoit_wolf
import matplotlib.pyplot as plt
Generate the data
n_samples = 60
n_features = 20
prng = np.random.RandomState(1)
prec = make_sparse_spd_matrix(n_features, alpha=.98,
smallest_coef=.4,
largest_coef=.7,
random_state=prng)
cov = linalg.inv(prec)
d = np.sqrt(np.diag(cov))
cov /= d
cov /= d[:, np.newaxis]
prec *= d
prec *= d[:, np.newaxis]
X = prng.multivariate_normal(np.zeros(n_features), cov, size=n_samples)
X -= X.mean(axis=0)
X /= X.std(axis=0)
Estimate the covariance
emp_cov = np.dot(X.T, X) / n_samples
model = GraphLassoCV()
model.fit(X)
cov_ = model.covariance_
prec_ = model.precision_
lw_cov_, _ = ledoit_wolf(X)
lw_prec_ = linalg.inv(lw_cov_)
Plot the results
740
Chapter 10. Covariance estimation
scikit-learn user guide, Release 0.18.2
plt.figure(figsize=(10, 6))
plt.subplots_adjust(left=0.02, right=0.98)
# plot the covariances
covs = [('Empirical', emp_cov), ('Ledoit-Wolf', lw_cov_),
('GraphLasso', cov_), ('True', cov)]
vmax = cov_.max()
for i, (name, this_cov) in enumerate(covs):
plt.subplot(2, 4, i + 1)
plt.imshow(this_cov, interpolation='nearest', vmin=-vmax, vmax=vmax,
cmap=plt.cm.RdBu_r)
plt.xticks(())
plt.yticks(())
plt.title('%s covariance' % name)
# plot the precisions
precs = [('Empirical', linalg.inv(emp_cov)), ('Ledoit-Wolf', lw_prec_),
('GraphLasso', prec_), ('True', prec)]
vmax = .9 * prec_.max()
for i, (name, this_prec) in enumerate(precs):
ax = plt.subplot(2, 4, i + 5)
plt.imshow(np.ma.masked_equal(this_prec, 0),
interpolation='nearest', vmin=-vmax, vmax=vmax,
cmap=plt.cm.RdBu_r)
plt.xticks(())
plt.yticks(())
plt.title('%s precision' % name)
ax.set_axis_bgcolor('.7')
# plot the model selection metric
plt.figure(figsize=(4, 3))
plt.axes([.2, .15, .75, .7])
plt.plot(model.cv_alphas_, np.mean(model.grid_scores, axis=1), 'o-')
plt.axvline(model.alpha_, color='.5')
plt.title('Model selection')
plt.ylabel('Cross-validation score')
plt.xlabel('alpha')
plt.show()
•
10.2. Sparse inverse covariance estimation
741
scikit-learn user guide, Release 0.18.2
•
Total running time of the script: (0 minutes 1.192 seconds)
Download Python source code: plot_sparse_cov.py
Download IPython notebook: plot_sparse_cov.ipynb
10.3 Shrinkage covariance estimation: LedoitWolf vs OAS and max-
likelihood
When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as
the sklearn.covariance.EmpiricalCovariance. It is unbiased, i.e. it converges to the true (population)
covariance when given many observations. However, it can also be beneﬁcial to regularize it, in order to reduce
its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in Shrunk
Covariance estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the
bias-variance trade-off.
Here we compare 3 approaches:
• Setting the parameter by cross-validating the likelihood on three folds according to a grid of potential shrinkage
parameters.
• A close formula proposed by Ledoit and Wolf to compute the asymptotically optimal regularization parameter
(minimizing a MSE criterion), yielding the sklearn.covariance.LedoitWolf covariance estimate.
• An improvement of the Ledoit-Wolf shrinkage, the sklearn.covariance.OAS, proposed by Chen et al.
Its convergence is signiﬁcantly better under the assumption that the data are Gaussian, in particular for small
samples.
To quantify estimation error, we plot the likelihood of unseen data for different values of the shrinkage parameter. We
also show the choices by cross-validation, or with the LedoitWolf and OAS estimates.
Note that the maximum likelihood estimate corresponds to no shrinkage, and thus performs poorly. The Ledoit-Wolf
estimate performs really well, as it is close to the optimal and is computational not costly. In this example, the OAS
estimate is a bit further away. Interestingly, both approaches outperform cross-validation, which is signiﬁcantly most
computationally costly.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from scipy import linalg
from sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \
log_likelihood, empirical_covariance
from sklearn.model_selection import GridSearchCV
Generate sample data
742
Chapter 10. Covariance estimation
scikit-learn user guide, Release 0.18.2
n_features, n_samples = 40, 20
np.random.seed(42)
base_X_train = np.random.normal(size=(n_samples, n_features))
base_X_test = np.random.normal(size=(n_samples, n_features))
# Color samples
coloring_matrix = np.random.normal(size=(n_features, n_features))
X_train = np.dot(base_X_train, coloring_matrix)
X_test = np.dot(base_X_test, coloring_matrix)
Compute the likelihood on test data
# spanning a range of possible shrinkage coefficient values
shrinkages = np.logspace(-2, 0, 30)
negative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test)
for s in shrinkages]
# under the ground-truth model, which we would not have access to in real
# settings
real_cov = np.dot(coloring_matrix.T, coloring_matrix)
emp_cov = empirical_covariance(X_train)
loglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))
Compare different approaches to setting the parameter
# GridSearch for an optimal shrinkage coefficient
tuned_parameters = [{'shrinkage': shrinkages}]
cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)
cv.fit(X_train)
# Ledoit-Wolf optimal shrinkage coefficient estimate
lw = LedoitWolf()
loglik_lw = lw.fit(X_train).score(X_test)
# OAS coefficient estimate
oa = OAS()
loglik_oa = oa.fit(X_train).score(X_test)
Plot results
fig = plt.figure()
plt.title("Regularized covariance: likelihood and shrinkage coefficient")
plt.xlabel('Regularizaton parameter: shrinkage coefficient')
plt.ylabel('Error: negative log-likelihood on test data')
# range shrinkage curve
plt.loglog(shrinkages, negative_logliks, label="Negative log-likelihood")
plt.plot(plt.xlim(), 2 * [loglik_real], '--r',
label="Real covariance likelihood")
# adjust view
lik_max = np.amax(negative_logliks)
lik_min = np.amin(negative_logliks)
ymin = lik_min - 6. * np.log((plt.ylim()[1] - plt.ylim()[0]))
ymax = lik_max + 10. * np.log(lik_max - lik_min)
xmin = shrinkages[0]
xmax = shrinkages[-1]
# LW likelihood
10.3. Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood
743
scikit-learn user guide, Release 0.18.2
plt.vlines(lw.shrinkage_, ymin, -loglik_lw, color='magenta',
linewidth=3, label='Ledoit-Wolf estimate')
# OAS likelihood
plt.vlines(oa.shrinkage_, ymin, -loglik_oa, color='purple',
linewidth=3, label='OAS estimate')
# best CV estimator likelihood
plt.vlines(cv.best_estimator_.shrinkage, ymin,
-cv.best_estimator_.score(X_test), color='cyan',
linewidth=3, label='Cross-validation best estimate')
plt.ylim(ymin, ymax)
plt.xlim(xmin, xmax)
plt.legend()
plt.show()
Total running time of the script: (0 minutes 0.440 seconds)
Download Python source code: plot_covariance_estimation.py
Download IPython notebook: plot_covariance_estimation.ipynb
744
Chapter 10. Covariance estimation
scikit-learn user guide, Release 0.18.2
10.4 Outlier detection with several methods.
When the amount of contamination is known, this example illustrates three different ways of performing Novelty and
Outlier Detection:
• based on a robust estimator of covariance, which is assuming that the data are Gaussian distributed and performs
better than the One-Class SVM in that case.
• using the One-Class SVM and its ability to capture the shape of the data set, hence performing better when the
data is strongly non-Gaussian, i.e. with two well-separated clusters;
• using the Isolation Forest algorithm, which is based on random forests and hence more adapted to large-
dimensional settings, even if it performs quite well in the examples below.
The ground truth about inliers and outliers is given by the points colors while the orange-ﬁlled area indicates which
points are reported as inliers by each method.
Here, we assume that we know the fraction of outliers in the datasets. Thus rather than using the ‘predict’ method of
the objects, we set the threshold on the decision_function to separate out the corresponding fraction.
•
•
•
print(__doc__)
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import matplotlib.font_manager
from sklearn import svm
from sklearn.covariance import EllipticEnvelope
from sklearn.ensemble import IsolationForest
rng = np.random.RandomState(42)
# Example settings
n_samples = 200
outliers_fraction = 0.25
clusters_separation = [0, 1, 2]
10.4. Outlier detection with several methods.
745
scikit-learn user guide, Release 0.18.2
# define two outlier detection tools to be compared
classifiers = {
"One-Class SVM": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,
kernel="rbf", gamma=0.1),
"Robust covariance": EllipticEnvelope(contamination=outliers_fraction),
"Isolation Forest": IsolationForest(max_samples=n_samples,
contamination=outliers_fraction,
random_state=rng)}
# Compare given classifiers under given settings
xx, yy = np.meshgrid(np.linspace(-7, 7, 500), np.linspace(-7, 7, 500))
n_inliers = int((1. - outliers_fraction) * n_samples)
n_outliers = int(outliers_fraction * n_samples)
ground_truth = np.ones(n_samples, dtype=int)
ground_truth[-n_outliers:] = -1
# Fit the problem with varying cluster separation
for i, offset in enumerate(clusters_separation):
np.random.seed(42)
# Data generation
X1 = 0.3 * np.random.randn(n_inliers // 2, 2) - offset
X2 = 0.3 * np.random.randn(n_inliers // 2, 2) + offset
X = np.r_[X1, X2]
# Add outliers
X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))]
# Fit the model
plt.figure(figsize=(10.8, 3.6))
for i, (clf_name, clf) in enumerate(classifiers.items()):
# fit the data and tag outliers
clf.fit(X)
scores_pred = clf.decision_function(X)
threshold = stats.scoreatpercentile(scores_pred,
100 * outliers_fraction)
y_pred = clf.predict(X)
n_errors = (y_pred != ground_truth).sum()
# plot the levels lines and the points
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
subplot = plt.subplot(1, 3, i + 1)
subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),
cmap=plt.cm.Blues_r)
a = subplot.contour(xx, yy, Z, levels=[threshold],
linewidths=2, colors='red')
subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],
colors='orange')
b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white')
c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black')
subplot.axis('tight')
subplot.legend(
[a.collections[0], b, c],
['learned decision function', 'true inliers', 'true outliers'],
prop=matplotlib.font_manager.FontProperties(size=11),
loc='lower right')
subplot.set_title("%d. %s (errors: %d)" % (i + 1, clf_name, n_errors))
subplot.set_xlim((-7, 7))
subplot.set_ylim((-7, 7))
plt.subplots_adjust(0.04, 0.1, 0.96, 0.92, 0.1, 0.26)
746
Chapter 10. Covariance estimation
scikit-learn user guide, Release 0.18.2
plt.show()
Total running time of the script: (0 minutes 38.736 seconds)
Download Python source code: plot_outlier_detection.py
Download IPython notebook: plot_outlier_detection.ipynb
10.5 Robust covariance estimation and Mahalanobis distances rele-
vance
An example to show covariance estimation with the Mahalanobis distances on Gaussian distributed data.
For Gaussian distributed data, the distance of an observation 𝑥𝑖to the mode of the distribution can be computed using
its Mahalanobis distance: 𝑑(𝜇,Σ)(𝑥𝑖)2 = (𝑥𝑖−𝜇)′Σ−1(𝑥𝑖−𝜇) where 𝜇and Σ are the location and the covariance of
the underlying Gaussian distribution.
In practice, 𝜇and Σ are replaced by some estimates. The usual covariance maximum likelihood estimate is very
sensitive to the presence of outliers in the data set and therefor, the corresponding Mahalanobis distances are. One
would better have to use a robust estimator of covariance to guarantee that the estimation is resistant to “erroneous”
observations in the data set and that the associated Mahalanobis distances accurately reﬂect the true organisation of
the observations.
The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the
covariance matrix of highly contaminated datasets, up to 𝑛samples−𝑛features−1
2
outliers) estimator of covariance. The idea is
to ﬁnd 𝑛samples+𝑛features+1
2
observations whose empirical covariance has the smallest determinant, yielding a “pure” subset
of observations from which to compute standards estimates of location and covariance.
The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in [1].
This example illustrates how the Mahalanobis distances are affected by outlying data: observations drawn from a con-
taminating distribution are not distinguishable from the observations coming from the real, Gaussian distribution that
one may want to work with. Using MCD-based Mahalanobis distances, the two populations become distinguishable.
Associated applications are outliers detection, observations ranking, clustering, ... For visualization purpose, the cubic
root of the Mahalanobis distances are represented in the boxplot, as Wilson and Hilferty suggest [2]
[1] P. J. Rousseeuw. Least median of squares regression. J. Am Stat Ass, 79:871, 1984.
[2] Wilson, E. B., & Hilferty, M. M. (1931). The distribution of chi-square. Proceedings
of
the
National
Academy of Sciences of the United States of America, 17, 684-688.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.covariance import EmpiricalCovariance, MinCovDet
n_samples = 125
n_outliers = 25
n_features = 2
# generate data
gen_cov = np.eye(n_features)
gen_cov[0, 0] = 2.
X = np.dot(np.random.randn(n_samples, n_features), gen_cov)
10.5. Robust covariance estimation and Mahalanobis distances relevance
747
scikit-learn user guide, Release 0.18.2
# add some outliers
outliers_cov = np.eye(n_features)
outliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.
X[-n_outliers:] = np.dot(np.random.randn(n_outliers, n_features), outliers_cov)
# fit a Minimum Covariance Determinant (MCD) robust estimator to data
robust_cov = MinCovDet().fit(X)
# compare estimators learnt from the full data set with true parameters
emp_cov = EmpiricalCovariance().fit(X)
Display results
fig = plt.figure()
plt.subplots_adjust(hspace=-.1, wspace=.4, top=.95, bottom=.05)
# Show data set
subfig1 = plt.subplot(3, 1, 1)
inlier_plot = subfig1.scatter(X[:, 0], X[:, 1],
color='black', label='inliers')
outlier_plot = subfig1.scatter(X[:, 0][-n_outliers:], X[:, 1][-n_outliers:],
color='red', label='outliers')
subfig1.set_xlim(subfig1.get_xlim()[0], 11.)
subfig1.set_title("Mahalanobis distances of a contaminated data set:")
# Show contours of the distance functions
xx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),
np.linspace(plt.ylim()[0], plt.ylim()[1], 100))
zz = np.c_[xx.ravel(), yy.ravel()]
mahal_emp_cov = emp_cov.mahalanobis(zz)
mahal_emp_cov = mahal_emp_cov.reshape(xx.shape)
emp_cov_contour = subfig1.contour(xx, yy, np.sqrt(mahal_emp_cov),
cmap=plt.cm.PuBu_r,
linestyles='dashed')
mahal_robust_cov = robust_cov.mahalanobis(zz)
mahal_robust_cov = mahal_robust_cov.reshape(xx.shape)
robust_contour = subfig1.contour(xx, yy, np.sqrt(mahal_robust_cov),
cmap=plt.cm.YlOrBr_r, linestyles='dotted')
subfig1.legend([emp_cov_contour.collections[1], robust_contour.collections[1],
inlier_plot, outlier_plot],
['MLE dist', 'robust dist', 'inliers', 'outliers'],
loc="upper right", borderaxespad=0)
plt.xticks(())
plt.yticks(())
# Plot the scores for each point
emp_mahal = emp_cov.mahalanobis(X - np.mean(X, 0)) ** (0.33)
subfig2 = plt.subplot(2, 2, 3)
subfig2.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=.25)
subfig2.plot(1.26 * np.ones(n_samples - n_outliers),
emp_mahal[:-n_outliers], '+k', markeredgewidth=1)
subfig2.plot(2.26 * np.ones(n_outliers),
emp_mahal[-n_outliers:], '+k', markeredgewidth=1)
subfig2.axes.set_xticklabels(('inliers', 'outliers'), size=15)
subfig2.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)
748
Chapter 10. Covariance estimation
scikit-learn user guide, Release 0.18.2
subfig2.set_title("1. from non-robust estimates\n(Maximum Likelihood)")
plt.yticks(())
robust_mahal = robust_cov.mahalanobis(X - robust_cov.location_) ** (0.33)
subfig3 = plt.subplot(2, 2, 4)
subfig3.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]],
widths=.25)
subfig3.plot(1.26 * np.ones(n_samples - n_outliers),
robust_mahal[:-n_outliers], '+k', markeredgewidth=1)
subfig3.plot(2.26 * np.ones(n_outliers),
robust_mahal[-n_outliers:], '+k', markeredgewidth=1)
subfig3.axes.set_xticklabels(('inliers', 'outliers'), size=15)
subfig3.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)
subfig3.set_title("2. from robust estimates\n(Minimum Covariance Determinant)")
plt.yticks(())
plt.show()
Total running time of the script: (0 minutes 0.342 seconds)
Download Python source code: plot_mahalanobis_distances.py
Download IPython notebook: plot_mahalanobis_distances.ipynb
10.5. Robust covariance estimation and Mahalanobis distances relevance
749
scikit-learn user guide, Release 0.18.2
10.6 Robust vs Empirical covariance estimate
The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In
such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to
“erroneous” observations in the data set.
10.6.1 Minimum Covariance Determinant Estimator
The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the
covariance matrix of highly contaminated datasets, up to 𝑛samples−𝑛features−1
2
outliers) estimator of covariance. The idea is
to ﬁnd 𝑛samples+𝑛features+1
2
observations whose empirical covariance has the smallest determinant, yielding a “pure” subset
of observations from which to compute standards estimates of location and covariance. After a correction step aiming
at compensating the fact that the estimates were learned from only a portion of the initial data, we end up with robust
estimates of the data set location and covariance.
The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in 1.
10.6.2 Evaluation
In this example, we compare the estimation errors that are made when using various types of location and covariance
estimates on contaminated Gaussian distributed data sets:
• The mean and the empirical covariance of the full dataset, which break down as soon as there are outliers in the
data set
• The robust MCD, that has a low error provided 𝑛samples > 5𝑛features
• The mean and the empirical covariance of the observations that are known to be good ones. This can be consid-
ered as a “perfect” MCD estimation, so one can trust our implementation by comparing to this case.
1 P. J. Rousseeuw. Least median of squares regression. Journal of American Statistical Ass., 79:871, 1984.
750
Chapter 10. Covariance estimation
scikit-learn user guide, Release 0.18.2
10.6.3 References
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager
from sklearn.covariance import EmpiricalCovariance, MinCovDet
# example settings
n_samples = 80
n_features = 5
repeat = 10
range_n_outliers = np.concatenate(
(np.linspace(0, n_samples / 8, 5),
np.linspace(n_samples / 8, n_samples / 2, 5)[1:-1])).astype(np.int)
# definition of arrays to store results
err_loc_mcd = np.zeros((range_n_outliers.size, repeat))
err_cov_mcd = np.zeros((range_n_outliers.size, repeat))
err_loc_emp_full = np.zeros((range_n_outliers.size, repeat))
err_cov_emp_full = np.zeros((range_n_outliers.size, repeat))
err_loc_emp_pure = np.zeros((range_n_outliers.size, repeat))
10.6. Robust vs Empirical covariance estimate
751
scikit-learn user guide, Release 0.18.2
err_cov_emp_pure = np.zeros((range_n_outliers.size, repeat))
# computation
for i, n_outliers in enumerate(range_n_outliers):
for j in range(repeat):
rng = np.random.RandomState(i * j)
# generate data
X = rng.randn(n_samples, n_features)
# add some outliers
outliers_index = rng.permutation(n_samples)[:n_outliers]
outliers_offset = 10. * \
(np.random.randint(2, size=(n_outliers, n_features)) - 0.5)
X[outliers_index] += outliers_offset
inliers_mask = np.ones(n_samples).astype(bool)
inliers_mask[outliers_index] = False
# fit a Minimum Covariance Determinant (MCD) robust estimator to data
mcd = MinCovDet().fit(X)
# compare raw robust estimates with the true location and covariance
err_loc_mcd[i, j] = np.sum(mcd.location_ ** 2)
err_cov_mcd[i, j] = mcd.error_norm(np.eye(n_features))
# compare estimators learned from the full data set with true
# parameters
err_loc_emp_full[i, j] = np.sum(X.mean(0) ** 2)
err_cov_emp_full[i, j] = EmpiricalCovariance().fit(X).error_norm(
np.eye(n_features))
# compare with an empirical covariance learned from a pure data set
# (i.e. "perfect" mcd)
pure_X = X[inliers_mask]
pure_location = pure_X.mean(0)
pure_emp_cov = EmpiricalCovariance().fit(pure_X)
err_loc_emp_pure[i, j] = np.sum(pure_location ** 2)
err_cov_emp_pure[i, j] = pure_emp_cov.error_norm(np.eye(n_features))
# Display results
font_prop = matplotlib.font_manager.FontProperties(size=11)
plt.subplot(2, 1, 1)
lw = 2
plt.errorbar(range_n_outliers, err_loc_mcd.mean(1),
yerr=err_loc_mcd.std(1) / np.sqrt(repeat),
label="Robust location", lw=lw, color='m')
plt.errorbar(range_n_outliers, err_loc_emp_full.mean(1),
yerr=err_loc_emp_full.std(1) / np.sqrt(repeat),
label="Full data set mean", lw=lw, color='green')
plt.errorbar(range_n_outliers, err_loc_emp_pure.mean(1),
yerr=err_loc_emp_pure.std(1) / np.sqrt(repeat),
label="Pure data set mean", lw=lw, color='black')
plt.title("Influence of outliers on the location estimation")
plt.ylabel(r"Error ($||\mu - \hat{\mu}||_2^2$)")
plt.legend(loc="upper left", prop=font_prop)
plt.subplot(2, 1, 2)
x_size = range_n_outliers.size
plt.errorbar(range_n_outliers, err_cov_mcd.mean(1),
752
Chapter 10. Covariance estimation
scikit-learn user guide, Release 0.18.2
yerr=err_cov_mcd.std(1),
label="Robust covariance (mcd)", color='m')
plt.errorbar(range_n_outliers[:(x_size // 5 + 1)],
err_cov_emp_full.mean(1)[:(x_size // 5 + 1)],
yerr=err_cov_emp_full.std(1)[:(x_size // 5 + 1)],
label="Full data set empirical covariance", color='green')
plt.plot(range_n_outliers[(x_size // 5):(x_size // 2 - 1)],
err_cov_emp_full.mean(1)[(x_size // 5):(x_size // 2 - 1)],
color='green', ls='--')
plt.errorbar(range_n_outliers, err_cov_emp_pure.mean(1),
yerr=err_cov_emp_pure.std(1),
label="Pure data set empirical covariance", color='black')
plt.title("Influence of outliers on the covariance estimation")
plt.xlabel("Amount of contamination (%)")
plt.ylabel("RMSE")
plt.legend(loc="upper center", prop=font_prop)
plt.show()
Total running time of the script: (0 minutes 3.384 seconds)
Download Python source code: plot_robust_vs_empirical_covariance.py
Download IPython notebook: plot_robust_vs_empirical_covariance.ipynb
10.6. Robust vs Empirical covariance estimate
753
scikit-learn user guide, Release 0.18.2
754
Chapter 10. Covariance estimation
CHAPTER
ELEVEN
CROSS DECOMPOSITION
Examples concerning the sklearn.cross_decomposition module.
11.1 Compare cross decomposition methods
Simple usage of various cross decomposition algorithms: - PLSCanonical - PLSRegression, with multivariate re-
sponse, a.k.a. PLS2 - PLSRegression, with univariate response, a.k.a. PLS1 - CCA
Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the ‘directions of covariance’, i.e.
the components of each datasets that explain the most shared variance between both datasets. This is apparent on the
scatterplot matrix display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the
ﬁrst diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different
components is weak: the point cloud is very spherical.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSCanonical, PLSRegression, CCA
Dataset based latent variables model
n = 500
# 2 latents vars:
l1 = np.random.normal(size=n)
l2 = np.random.normal(size=n)
latents = np.array([l1, l1, l2, l2]).T
X = latents + np.random.normal(size=4 * n).reshape((n, 4))
Y = latents + np.random.normal(size=4 * n).reshape((n, 4))
X_train = X[:n // 2]
Y_train = Y[:n // 2]
X_test = X[n // 2:]
Y_test = Y[n // 2:]
print("Corr(X)")
print(np.round(np.corrcoef(X.T), 2))
print("Corr(Y)")
print(np.round(np.corrcoef(Y.T), 2))
Out:
755
scikit-learn user guide, Release 0.18.2
Corr(X)
[[ 1.
0.52 -0.03
0.
]
[ 0.52
1.
0.02 -0.01]
[-0.03
0.02
1.
0.45]
[ 0.
-0.01
0.45
1.
]]
Corr(Y)
[[ 1.
0.52
0.01 -0.01]
[ 0.52
1.
0.
0.06]
[ 0.01
0.
1.
0.52]
[-0.01
0.06
0.52
1.
]]
Canonical (symmetric) PLS
# Transform data
# ~~~~~~~~~~~~~~
plsca = PLSCanonical(n_components=2)
plsca.fit(X_train, Y_train)
X_train_r, Y_train_r = plsca.transform(X_train, Y_train)
X_test_r, Y_test_r = plsca.transform(X_test, Y_test)
# Scatter plot of scores
# ~~~~~~~~~~~~~~~~~~~~~~
# 1) On diagonal plot X vs Y scores on each components
plt.figure(figsize=(12, 8))
plt.subplot(221)
plt.plot(X_train_r[:, 0], Y_train_r[:, 0], "ob", label="train")
plt.plot(X_test_r[:, 0], Y_test_r[:, 0], "or", label="test")
plt.xlabel("x scores")
plt.ylabel("y scores")
plt.title('Comp. 1: X vs Y (test corr = %.2f)' %
np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1])
plt.xticks(())
plt.yticks(())
plt.legend(loc="best")
plt.subplot(224)
plt.plot(X_train_r[:, 1], Y_train_r[:, 1], "ob", label="train")
plt.plot(X_test_r[:, 1], Y_test_r[:, 1], "or", label="test")
plt.xlabel("x scores")
plt.ylabel("y scores")
plt.title('Comp. 2: X vs Y (test corr = %.2f)' %
np.corrcoef(X_test_r[:, 1], Y_test_r[:, 1])[0, 1])
plt.xticks(())
plt.yticks(())
plt.legend(loc="best")
# 2) Off diagonal plot components 1 vs 2 for X and Y
plt.subplot(222)
plt.plot(X_train_r[:, 0], X_train_r[:, 1], "*b", label="train")
plt.plot(X_test_r[:, 0], X_test_r[:, 1], "*r", label="test")
plt.xlabel("X comp. 1")
plt.ylabel("X comp. 2")
plt.title('X comp. 1 vs X comp. 2 (test corr = %.2f)'
% np.corrcoef(X_test_r[:, 0], X_test_r[:, 1])[0, 1])
plt.legend(loc="best")
plt.xticks(())
plt.yticks(())
756
Chapter 11. Cross decomposition
scikit-learn user guide, Release 0.18.2
plt.subplot(223)
plt.plot(Y_train_r[:, 0], Y_train_r[:, 1], "*b", label="train")
plt.plot(Y_test_r[:, 0], Y_test_r[:, 1], "*r", label="test")
plt.xlabel("Y comp. 1")
plt.ylabel("Y comp. 2")
plt.title('Y comp. 1 vs Y comp. 2 , (test corr = %.2f)'
% np.corrcoef(Y_test_r[:, 0], Y_test_r[:, 1])[0, 1])
plt.legend(loc="best")
plt.xticks(())
plt.yticks(())
plt.show()
PLS regression, with multivariate response, a.k.a. PLS2
n = 1000
q = 3
p = 10
X = np.random.normal(size=n * p).reshape((n, p))
B = np.array([[1, 2] + [0] * (p - 2)] * q).T
# each Yj = 1*X1 + 2*X2 + noize
Y = np.dot(X, B) + np.random.normal(size=n * q).reshape((n, q)) + 5
pls2 = PLSRegression(n_components=3)
pls2.fit(X, Y)
print("True B (such that: Y = XB + Err)")
print(B)
# compare pls2.coef_ with B
print("Estimated B")
print(np.round(pls2.coef_, 1))
11.1. Compare cross decomposition methods
757
scikit-learn user guide, Release 0.18.2
pls2.predict(X)
Out:
True B (such that: Y = XB + Err)
[[1 1 1]
[2 2 2]
[0 0 0]
[0 0 0]
[0 0 0]
[0 0 0]
[0 0 0]
[0 0 0]
[0 0 0]
[0 0 0]]
Estimated B
[[ 1.
1.
1. ]
[ 2.
2.
2. ]
[-0.
0.
-0. ]
[-0.
-0.
-0. ]
[-0.
-0.
-0. ]
[ 0.
-0.1 -0. ]
[-0.
-0.1
0. ]
[-0.
0.
-0. ]
[ 0.
0.
0. ]
[ 0.
0.
0. ]]
PLS regression, with univariate response, a.k.a. PLS1
n = 1000
p = 10
X = np.random.normal(size=n * p).reshape((n, p))
y = X[:, 0] + 2 * X[:, 1] + np.random.normal(size=n * 1) + 5
pls1 = PLSRegression(n_components=3)
pls1.fit(X, y)
# note that the number of components exceeds 1 (the dimension of y)
print("Estimated betas")
print(np.round(pls1.coef_, 1))
Out:
Estimated betas
[[ 1. ]
[ 2. ]
[-0. ]
[ 0. ]
[-0. ]
[ 0. ]
[ 0.1]
[-0.1]
[-0. ]
[ 0. ]]
CCA (PLS mode B with symmetric deﬂation)
cca = CCA(n_components=2)
cca.fit(X_train, Y_train)
758
Chapter 11. Cross decomposition
scikit-learn user guide, Release 0.18.2
X_train_r, Y_train_r = plsca.transform(X_train, Y_train)
X_test_r, Y_test_r = plsca.transform(X_test, Y_test)
Total running time of the script: (0 minutes 0.322 seconds)
Download Python source code: plot_compare_cross_decomposition.py
Download IPython notebook: plot_compare_cross_decomposition.ipynb
11.1. Compare cross decomposition methods
759
scikit-learn user guide, Release 0.18.2
760
Chapter 11. Cross decomposition
CHAPTER
TWELVE
DATASET EXAMPLES
Examples concerning the sklearn.datasets module.
12.1 The Digit Dataset
This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order
to utilize an 8x8 ﬁgure like this, we’d have to ﬁrst transform it into a feature vector with length 64.
See here for more information about this dataset.
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
from sklearn import datasets
import matplotlib.pyplot as plt
#Load the digits dataset
digits = datasets.load_digits()
761
scikit-learn user guide, Release 0.18.2
#Display the first digit
plt.figure(1, figsize=(3, 3))
plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')
plt.show()
Total running time of the script: (0 minutes 0.158 seconds)
Download Python source code: plot_digits_last_image.py
Download IPython notebook: plot_digits_last_image.ipynb
12.2 The Iris Dataset
This data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored
in a 150x4 numpy.ndarray
The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.
The below plot uses the ﬁrst two features. See here for more information on this dataset.
•
•
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
762
Chapter 12. Dataset examples
scikit-learn user guide, Release 0.18.2
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets
from sklearn.decomposition import PCA
# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]
# we only take the first two features.
Y = iris.target
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
plt.figure(2, figsize=(8, 6))
plt.clf()
# Plot the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())
# To getter a better understanding of interaction of the dimensions
# plot the first three PCA dimensions
fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
X_reduced = PCA(n_components=3).fit_transform(iris.data)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,
cmap=plt.cm.Paired)
ax.set_title("First three PCA directions")
ax.set_xlabel("1st eigenvector")
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel("2nd eigenvector")
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel("3rd eigenvector")
ax.w_zaxis.set_ticklabels([])
plt.show()
Total running time of the script: (0 minutes 0.158 seconds)
Download Python source code: plot_iris_dataset.py
Download IPython notebook: plot_iris_dataset.ipynb
12.3 Plot randomly generated classiﬁcation dataset
Plot
several
randomly
generated
2D
classiﬁcation
datasets.
This
example
il-
lustrates
the
datasets.make_classification
datasets.make_blobs
and
datasets.make_gaussian_quantiles functions.
12.3. Plot randomly generated classiﬁcation dataset
763
scikit-learn user guide, Release 0.18.2
For make_classification, three binary and two multi-class classiﬁcation datasets are generated, with different
numbers of informative features and clusters per class.
print(__doc__)
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.datasets import make_blobs
from sklearn.datasets import make_gaussian_quantiles
plt.figure(figsize=(8, 8))
plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)
plt.subplot(321)
764
Chapter 12. Dataset examples
scikit-learn user guide, Release 0.18.2
plt.title("One informative feature, one cluster per class", fontsize='small')
X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,
n_clusters_per_class=1)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
plt.subplot(322)
plt.title("Two informative features, one cluster per class", fontsize='small')
X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,
n_clusters_per_class=1)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
plt.subplot(323)
plt.title("Two informative features, two clusters per class", fontsize='small')
X2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)
plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2)
plt.subplot(324)
plt.title("Multi-class, two informative features, one cluster",
fontsize='small')
X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,
n_clusters_per_class=1, n_classes=3)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
plt.subplot(325)
plt.title("Three blobs", fontsize='small')
X1, Y1 = make_blobs(n_features=2, centers=3)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
plt.subplot(326)
plt.title("Gaussian divided into three quantiles", fontsize='small')
X1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)
plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)
plt.show()
Total running time of the script: (0 minutes 0.324 seconds)
Download Python source code: plot_random_dataset.py
Download IPython notebook: plot_random_dataset.ipynb
12.4 Plot randomly generated multilabel dataset
This illustrates the datasets.make_multilabel_classiﬁcation dataset generator. Each sample consists of counts of two
features (up to 50 in total), which are differently distributed in each of two classes.
Points are labeled as follows, where Y means the class is present:
12.4. Plot randomly generated multilabel dataset
765
scikit-learn user guide, Release 0.18.2
1
2
3
Color
Y
N
N
Red
N
Y
N
Blue
N
N
Y
Yellow
Y
Y
N
Purple
Y
N
Y
Orange
Y
Y
N
Green
Y
Y
Y
Brown
A star marks the expected sample for each class; its size reﬂects the probability of selecting that class label.
The left and right examples highlight the n_labels parameter: more of the samples in the right plot have 2 or 3
labels.
Note that this two-dimensional example is very degenerate: generally the number of features would be much greater
than the “document length”, while here we have much larger documents than vocabulary. Similarly, with n_classes
> n_features, it is much less likely that a feature distinguishes a particular class.
Out:
The data was generated from (random_state=54):
Class
P(C)
P(w0|C) P(w1|C)
red
0.43
0.39
0.61
blue
0.38
0.01
0.99
yellow
0.19
0.59
0.41
from __future__ import print_function
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_multilabel_classification as make_ml_clf
766
Chapter 12. Dataset examples
scikit-learn user guide, Release 0.18.2
print(__doc__)
COLORS = np.array(['!',
'#FF3333',
# red
'#0198E1',
# blue
'#BF5FFF',
# purple
'#FCD116',
# yellow
'#FF7216',
# orange
'#4DBD33',
# green
'#87421F'
# brown
])
# Use same random seed for multiple calls to make_multilabel_classification to
# ensure same distributions
RANDOM_SEED = np.random.randint(2 ** 10)
def plot_2d(ax, n_labels=1, n_classes=3, length=50):
X, Y, p_c, p_w_c = make_ml_clf(n_samples=150, n_features=2,
n_classes=n_classes, n_labels=n_labels,
length=length, allow_unlabeled=False,
return_distributions=True,
random_state=RANDOM_SEED)
ax.scatter(X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]
).sum(axis=1)),
marker='.')
ax.scatter(p_w_c[0] * length, p_w_c[1] * length,
marker='*', linewidth=.5, edgecolor='black',
s=20 + 1500 * p_c ** 2,
color=COLORS.take([1, 2, 4]))
ax.set_xlabel('Feature 0 count')
return p_c, p_w_c
_, (ax1, ax2) = plt.subplots(1, 2, sharex='row', sharey='row', figsize=(8, 4))
plt.subplots_adjust(bottom=.15)
p_c, p_w_c = plot_2d(ax1, n_labels=1)
ax1.set_title('n_labels=1, length=50')
ax1.set_ylabel('Feature 1 count')
plot_2d(ax2, n_labels=3)
ax2.set_title('n_labels=3, length=50')
ax2.set_xlim(left=0, auto=True)
ax2.set_ylim(bottom=0, auto=True)
plt.show()
print('The data was generated from (random_state=%d):' % RANDOM_SEED)
print('Class', 'P(C)', 'P(w0|C)', 'P(w1|C)', sep='\t')
for k, p, p_w in zip(['red', 'blue', 'yellow'], p_c, p_w_c.T):
print('%s\t%0.2f\t%0.2f\t%0.2f' % (k, p, p_w[0], p_w[1]))
Total running time of the script: (0 minutes 0.157 seconds)
Download Python source code: plot_random_multilabel_dataset.py
Download IPython notebook: plot_random_multilabel_dataset.ipynb
12.4. Plot randomly generated multilabel dataset
767
scikit-learn user guide, Release 0.18.2
768
Chapter 12. Dataset examples
CHAPTER
THIRTEEN
DECOMPOSITION
Examples concerning the sklearn.decomposition module.
13.1 PCA example with Iris Data-set
Principal Component Analysis applied to the Iris dataset.
See here for more information on this dataset.
print(__doc__)
# Code source: Gaël Varoquaux
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn import datasets
769
scikit-learn user guide, Release 0.18.2
np.random.seed(5)
centers = [[1, 1], [-1, -1], [1, -1]]
iris = datasets.load_iris()
X = iris.data
y = iris.target
fig = plt.figure(1, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
plt.cla()
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)
for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
ax.text3D(X[y == label, 0].mean(),
X[y == label, 1].mean() + 1.5,
X[y == label, 2].mean(), name,
horizontalalignment='center',
bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.spectral)
ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
plt.show()
Total running time of the script: (0 minutes 0.119 seconds)
Download Python source code: plot_pca_iris.py
Download IPython notebook: plot_pca_iris.ipynb
13.2 Incremental PCA
Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis
(PCA) when the dataset to be decomposed is too large to ﬁt in memory. IPCA builds a low-rank approximation for the
input data using an amount of memory which is independent of the number of input data samples. It is still dependent
on the input data features, but changing the batch size allows for control of memory usage.
This example serves as a visual check that IPCA is able to ﬁnd a similar projection of the data to PCA (to a sign ﬂip),
while only processing a few samples at a time. This can be considered a “toy example”, as IPCA is intended for large
datasets which do not ﬁt in main memory, requiring incremental approaches.
770
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
•
•
print(__doc__)
# Authors: Kyle Kastner
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA, IncrementalPCA
iris = load_iris()
X = iris.data
y = iris.target
n_components = 2
ipca = IncrementalPCA(n_components=n_components, batch_size=10)
X_ipca = ipca.fit_transform(X)
13.2. Incremental PCA
771
scikit-learn user guide, Release 0.18.2
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X)
colors = ['navy', 'turquoise', 'darkorange']
for X_transformed, title in [(X_ipca, "Incremental PCA"), (X_pca, "PCA")]:
plt.figure(figsize=(8, 8))
for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):
plt.scatter(X_transformed[y == i, 0], X_transformed[y == i, 1],
color=color, lw=2, label=target_name)
if "Incremental" in title:
err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()
plt.title(title + " of iris dataset\nMean absolute unsigned error "
"%.6f" % err)
else:
plt.title(title + " of iris dataset")
plt.legend(loc="best", shadow=False, scatterpoints=1)
plt.axis([-4, 4, -1.5, 1.5])
plt.show()
Total running time of the script: (0 minutes 0.184 seconds)
Download Python source code: plot_incremental_pca.py
Download IPython notebook: plot_incremental_pca.ipynb
13.3 Comparison of LDA and PCA 2D projection of Iris dataset
The Iris dataset represents 3 kind of Iris ﬂowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length,
sepal width, petal length and petal width.
Principal Component Analysis (PCA) applied to this data identiﬁes the combination of attributes (principal compo-
nents, or directions in the feature space) that account for the most variance in the data. Here we plot the different
samples on the 2 ﬁrst principal components.
Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In
particular, LDA, in contrast to PCA, is a supervised method, using known class labels.
•
772
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
•
Out:
explained variance ratio (first two components): [ 0.92461621
0.05301557]
print(__doc__)
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
iris = datasets.load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names
pca = PCA(n_components=2)
X_r = pca.fit(X).transform(X)
lda = LinearDiscriminantAnalysis(n_components=2)
X_r2 = lda.fit(X, y).transform(X)
# Percentage of variance explained for each components
print('explained variance ratio (first two components): %s'
% str(pca.explained_variance_ratio_))
plt.figure()
colors = ['navy', 'turquoise', 'darkorange']
lw = 2
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,
label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('PCA of IRIS dataset')
13.3. Comparison of LDA and PCA 2D projection of Iris dataset
773
scikit-learn user guide, Release 0.18.2
plt.figure()
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,
label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('LDA of IRIS dataset')
plt.show()
Total running time of the script: (0 minutes 0.180 seconds)
Download Python source code: plot_pca_vs_lda.py
Download IPython notebook: plot_pca_vs_lda.ipynb
13.4 Blind source separation using FastICA
An example of estimating sources from noisy data.
Independent component analysis (ICA) is used to estimate sources given noisy measurements. Imagine 3 instruments
playing simultaneously and 3 microphones recording the mixed signals. ICA is used to recover the sources ie. what
is played by each instrument. Importantly, PCA fails at recovering our instruments since the related signals reﬂect
non-Gaussian processes.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
from sklearn.decomposition import FastICA, PCA
Generate sample data
np.random.seed(0)
n_samples = 2000
time = np.linspace(0, 8, n_samples)
s1 = np.sin(2 * time)
# Signal 1 : sinusoidal signal
s2 = np.sign(np.sin(3 * time))
# Signal 2 : square signal
s3 = signal.sawtooth(2 * np.pi * time)
# Signal 3: saw tooth signal
S = np.c_[s1, s2, s3]
S += 0.2 * np.random.normal(size=S.shape)
# Add noise
S /= S.std(axis=0)
# Standardize data
# Mix data
A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])
# Mixing matrix
X = np.dot(S, A.T)
# Generate observations
# Compute ICA
ica = FastICA(n_components=3)
S_ = ica.fit_transform(X)
# Reconstruct signals
A_ = ica.mixing_
# Get estimated mixing matrix
# We can `prove` that the ICA model applies by reverting the unmixing.
assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)
774
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
# For comparison, compute PCA
pca = PCA(n_components=3)
H = pca.fit_transform(X)
# Reconstruct signals based on orthogonal components
Plot results
plt.figure()
models = [X, S, S_, H]
names = ['Observations (mixed signal)',
'True Sources',
'ICA recovered signals',
'PCA recovered signals']
colors = ['red', 'steelblue', 'orange']
for ii, (model, name) in enumerate(zip(models, names), 1):
plt.subplot(4, 1, ii)
plt.title(name)
for sig, color in zip(model.T, colors):
plt.plot(sig, color=color)
plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)
plt.show()
13.4. Blind source separation using FastICA
775
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.299 seconds)
Download Python source code: plot_ica_blind_source_separation.py
Download IPython notebook: plot_ica_blind_source_separation.ipynb
13.5 FastICA on 2D point clouds
This example illustrates visually in the feature space a comparison by results using two different component analysis
techniques.
Independent component analysis (ICA) vs Principal component analysis (PCA).
Representing ICA in the feature space gives the view of ‘geometric ICA’: ICA is an algorithm that ﬁnds directions in
the feature space corresponding to projections with high non-Gaussianity. These directions need not be orthogonal in
the original feature space, but they are orthogonal in the whitened feature space, in which all directions correspond to
the same variance.
PCA, on the other hand, ﬁnds orthogonal directions in the raw feature space that correspond to directions accounting
for maximum variance.
Here we simulate independent sources using a highly non-Gaussian process, 2 student T with a low number of degrees
of freedom (top left ﬁgure). We mix them to create observations (top right ﬁgure). In this raw observation space,
directions identiﬁed by PCA are represented by orange vectors. We represent the signal in the PCA space, after
whitening by the variance corresponding to the PCA vectors (lower left). Running ICA corresponds to ﬁnding a
rotation in this space to identify the directions of largest non-Gaussianity (lower right).
print(__doc__)
# Authors: Alexandre Gramfort, Gael Varoquaux
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA, FastICA
Generate sample data
rng = np.random.RandomState(42)
S = rng.standard_t(1.5, size=(20000, 2))
S[:, 0] *= 2.
# Mix data
A = np.array([[1, 1], [0, 2]])
# Mixing matrix
X = np.dot(S, A.T)
# Generate observations
pca = PCA()
S_pca_ = pca.fit(X).transform(X)
ica = FastICA(random_state=rng)
S_ica_ = ica.fit(X).transform(X)
# Estimate the sources
S_ica_ /= S_ica_.std(axis=0)
Plot results
776
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
def plot_samples(S, axis_list=None):
plt.scatter(S[:, 0], S[:, 1], s=2, marker='o', zorder=10,
color='steelblue', alpha=0.5)
if axis_list is not None:
colors = ['orange', 'red']
for color, axis in zip(colors, axis_list):
axis /= axis.std()
x_axis, y_axis = axis
# Trick to get legend to work
plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)
plt.quiver(0, 0, x_axis, y_axis, zorder=11, width=0.01, scale=6,
color=color)
plt.hlines(0, -3, 3)
plt.vlines(0, -3, 3)
plt.xlim(-3, 3)
plt.ylim(-3, 3)
plt.xlabel('x')
plt.ylabel('y')
plt.figure()
plt.subplot(2, 2, 1)
plot_samples(S / S.std())
plt.title('True Independent Sources')
axis_list = [pca.components_.T, ica.mixing_]
plt.subplot(2, 2, 2)
plot_samples(X / np.std(X), axis_list=axis_list)
legend = plt.legend(['PCA', 'ICA'], loc='upper right')
legend.set_zorder(100)
plt.title('Observations')
plt.subplot(2, 2, 3)
plot_samples(S_pca_ / np.std(S_pca_, axis=0))
plt.title('PCA recovered signals')
plt.subplot(2, 2, 4)
plot_samples(S_ica_ / np.std(S_ica_))
plt.title('ICA recovered signals')
plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)
plt.show()
13.5. FastICA on 2D point clouds
777
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.551 seconds)
Download Python source code: plot_ica_vs_pca.py
Download IPython notebook: plot_ica_vs_pca.ipynb
13.6 Kernel PCA
This example shows that Kernel PCA is able to ﬁnd a projection of the data that makes data linearly separable.
778
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Authors: Mathieu Blondel
#
Andreas Mueller
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA, KernelPCA
from sklearn.datasets import make_circles
np.random.seed(0)
X, y = make_circles(n_samples=400, factor=.3, noise=.05)
kpca = KernelPCA(kernel="rbf", fit_inverse_transform=True, gamma=10)
X_kpca = kpca.fit_transform(X)
X_back = kpca.inverse_transform(X_kpca)
pca = PCA()
X_pca = pca.fit_transform(X)
# Plot results
plt.figure()
plt.subplot(2, 2, 1, aspect='equal')
13.6. Kernel PCA
779
scikit-learn user guide, Release 0.18.2
plt.title("Original space")
reds = y == 0
blues = y == 1
plt.plot(X[reds, 0], X[reds, 1], "ro")
plt.plot(X[blues, 0], X[blues, 1], "bo")
plt.xlabel("$x_1$")
plt.ylabel("$x_2$")
X1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))
X_grid = np.array([np.ravel(X1), np.ravel(X2)]).T
# projection on the first principal component (in the phi space)
Z_grid = kpca.transform(X_grid)[:, 0].reshape(X1.shape)
plt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')
plt.subplot(2, 2, 2, aspect='equal')
plt.plot(X_pca[reds, 0], X_pca[reds, 1], "ro")
plt.plot(X_pca[blues, 0], X_pca[blues, 1], "bo")
plt.title("Projection by PCA")
plt.xlabel("1st principal component")
plt.ylabel("2nd component")
plt.subplot(2, 2, 3, aspect='equal')
plt.plot(X_kpca[reds, 0], X_kpca[reds, 1], "ro")
plt.plot(X_kpca[blues, 0], X_kpca[blues, 1], "bo")
plt.title("Projection by KPCA")
plt.xlabel("1st principal component in space induced by $\phi$")
plt.ylabel("2nd component")
plt.subplot(2, 2, 4, aspect='equal')
plt.plot(X_back[reds, 0], X_back[reds, 1], "ro")
plt.plot(X_back[blues, 0], X_back[blues, 1], "bo")
plt.title("Original space after inverse transform")
plt.xlabel("$x_1$")
plt.ylabel("$x_2$")
plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)
plt.show()
Total running time of the script: (0 minutes 0.499 seconds)
Download Python source code: plot_kernel_pca.py
Download IPython notebook: plot_kernel_pca.ipynb
13.7 Principal components analysis (PCA)
These ﬁgures aid in illustrating how a point cloud can be very ﬂat in one direction–which is where PCA comes in to
choose a direction that is not ﬂat.
print(__doc__)
# Authors: Gael Varoquaux
#
Jaques Grobler
#
Kevin Hughes
780
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
# License: BSD 3 clause
from sklearn.decomposition import PCA
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
Create the data
e = np.exp(1)
np.random.seed(4)
def pdf(x):
return 0.5 * (stats.norm(scale=0.25 / e).pdf(x)
+ stats.norm(scale=4 / e).pdf(x))
y = np.random.normal(scale=0.5, size=(30000))
x = np.random.normal(scale=0.5, size=(30000))
z = np.random.normal(scale=0.1, size=len(x))
density = pdf(x) * pdf(y)
pdf_z = pdf(5 * z)
density *= pdf_z
a = x + y
b = 2 * y
c = a - b + z
norm = np.sqrt(a.var() + b.var())
a /= norm
b /= norm
Plot the ﬁgures
def plot_figs(fig_num, elev, azim):
fig = plt.figure(fig_num, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=elev, azim=azim)
ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker='+', alpha=.4)
Y = np.c_[a, b, c]
# Using SciPy's SVD, this would be:
# _, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)
pca = PCA(n_components=3)
pca.fit(Y)
pca_score = pca.explained_variance_ratio_
V = pca.components_
x_pca_axis, y_pca_axis, z_pca_axis = V.T * pca_score / pca_score.min()
x_pca_axis, y_pca_axis, z_pca_axis = 3 * V.T
x_pca_plane = np.r_[x_pca_axis[:2], - x_pca_axis[1::-1]]
13.7. Principal components analysis (PCA)
781
scikit-learn user guide, Release 0.18.2
y_pca_plane = np.r_[y_pca_axis[:2], - y_pca_axis[1::-1]]
z_pca_plane = np.r_[z_pca_axis[:2], - z_pca_axis[1::-1]]
x_pca_plane.shape = (2, 2)
y_pca_plane.shape = (2, 2)
z_pca_plane.shape = (2, 2)
ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)
ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
elev = -40
azim = -80
plot_figs(1, elev, azim)
elev = 30
azim = 20
plot_figs(2, elev, azim)
plt.show()
•
•
Total running time of the script: (0 minutes 0.277 seconds)
Download Python source code: plot_pca_3d.py
Download IPython notebook: plot_pca_3d.ipynb
13.8 Model selection with Probabilistic PCA and Factor Analysis (FA)
Probabilistic PCA and Factor Analysis are probabilistic models. The consequence is that the likelihood of new data
can be used for model selection and covariance estimation. Here we compare PCA and FA with cross-validation on
low rank data corrupted with homoscedastic noise (noise variance is the same for each feature) or heteroscedastic noise
(noise variance is the different for each feature). In a second step we compare the model likelihood to the likelihoods
obtained from shrinkage covariance estimators.
One can observe that with homoscedastic noise both FA and PCA succeed in recovering the size of the low rank
subspace. The likelihood with PCA is higher than FA in this case. However PCA fails and overestimates the rank
782
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
when heteroscedastic noise is present. Under appropriate circumstances the low rank models are more likely than
shrinkage models.
The automatic estimation from Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604 by Thomas P.
Minka is also compared.
# Authors: Alexandre Gramfort
#
Denis A. Engemann
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from scipy import linalg
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.covariance import ShrunkCovariance, LedoitWolf
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
print(__doc__)
Create the data
n_samples, n_features, rank = 1000, 50, 10
sigma = 1.
rng = np.random.RandomState(42)
U, _, _ = linalg.svd(rng.randn(n_features, n_features))
X = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)
# Adding homoscedastic noise
X_homo = X + sigma * rng.randn(n_samples, n_features)
# Adding heteroscedastic noise
sigmas = sigma * rng.rand(n_features) + sigma / 2.
X_hetero = X + rng.randn(n_samples, n_features) * sigmas
Fit the models
n_components = np.arange(0, n_features, 5)
# options for n_components
def compute_scores(X):
pca = PCA(svd_solver='full')
fa = FactorAnalysis()
pca_scores, fa_scores = [], []
for n in n_components:
pca.n_components = n
fa.n_components = n
pca_scores.append(np.mean(cross_val_score(pca, X)))
fa_scores.append(np.mean(cross_val_score(fa, X)))
return pca_scores, fa_scores
def shrunk_cov_score(X):
shrinkages = np.logspace(-2, 0, 30)
cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})
return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))
13.8. Model selection with Probabilistic PCA and Factor Analysis (FA)
783
scikit-learn user guide, Release 0.18.2
def lw_score(X):
return np.mean(cross_val_score(LedoitWolf(), X))
for X, title in [(X_homo, 'Homoscedastic Noise'),
(X_hetero, 'Heteroscedastic Noise')]:
pca_scores, fa_scores = compute_scores(X)
n_components_pca = n_components[np.argmax(pca_scores)]
n_components_fa = n_components[np.argmax(fa_scores)]
pca = PCA(svd_solver='full', n_components='mle')
pca.fit(X)
n_components_pca_mle = pca.n_components_
print("best n_components by PCA CV = %d" % n_components_pca)
print("best n_components by FactorAnalysis CV = %d" % n_components_fa)
print("best n_components by PCA MLE = %d" % n_components_pca_mle)
plt.figure()
plt.plot(n_components, pca_scores, 'b', label='PCA scores')
plt.plot(n_components, fa_scores, 'r', label='FA scores')
plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')
plt.axvline(n_components_pca, color='b',
label='PCA CV: %d' % n_components_pca, linestyle='--')
plt.axvline(n_components_fa, color='r',
label='FactorAnalysis CV: %d' % n_components_fa,
linestyle='--')
plt.axvline(n_components_pca_mle, color='k',
label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')
# compare with other covariance estimators
plt.axhline(shrunk_cov_score(X), color='violet',
label='Shrunk Covariance MLE', linestyle='-.')
plt.axhline(lw_score(X), color='orange',
label='LedoitWolf MLE' % n_components_pca_mle, linestyle='-.')
plt.xlabel('nb of components')
plt.ylabel('CV scores')
plt.legend(loc='lower right')
plt.title(title)
plt.show()
784
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
•
•
Out:
best n_components by PCA CV = 10
best n_components by FactorAnalysis CV = 10
best n_components by PCA MLE = 10
best n_components by PCA CV = 40
best n_components by FactorAnalysis CV = 10
best n_components by PCA MLE = 38
Total running time of the script: (0 minutes 12.618 seconds)
Download Python source code: plot_pca_vs_fa_model_selection.py
Download IPython notebook: plot_pca_vs_fa_model_selection.ipynb
13.9 Sparse coding with a precomputed dictionary
Transform a signal as a sparse combination of Ricker wavelets. This example visually compares different sparse coding
methods using the sklearn.decomposition.SparseCoder estimator. The Ricker (also known as Mexican
hat or the second derivative of a Gaussian) is not a particularly good kernel to represent piecewise constant signals
like this one. It can therefore be seen how much adding different widths of atoms matters and it therefore motivates
learning the dictionary to best ﬁt your type of signals.
The richer dictionary on the right is not larger in size, heavier subsampling is performed in order to stay on the same
order of magnitude.
13.9. Sparse coding with a precomputed dictionary
785
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import SparseCoder
def ricker_function(resolution, center, width):
"""Discrete sub-sampled Ricker (Mexican hat) wavelet"""
x = np.linspace(0, resolution - 1, resolution)
x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4)))
* (1 - ((x - center) ** 2 / width ** 2))
* np.exp((-(x - center) ** 2) / (2 * width ** 2)))
return x
def ricker_matrix(width, resolution, n_components):
"""Dictionary of Ricker (Mexican hat) wavelets"""
centers = np.linspace(0, resolution - 1, n_components)
D = np.empty((n_components, resolution))
for i, center in enumerate(centers):
D[i] = ricker_function(resolution, center, width)
D /= np.sqrt(np.sum(D ** 2, axis=1))[:, np.newaxis]
return D
resolution = 1024
subsampling = 3
# subsampling factor
width = 100
n_components = resolution // subsampling
# Compute a wavelet dictionary
D_fixed = ricker_matrix(width=width, resolution=resolution,
n_components=n_components)
D_multi = np.r_[tuple(ricker_matrix(width=w, resolution=resolution,
n_components=n_components // 5)
for w in (10, 50, 100, 500, 1000))]
786
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
# Generate a signal
y = np.linspace(0, resolution - 1, resolution)
first_quarter = y < resolution / 4
y[first_quarter] = 3.
y[np.logical_not(first_quarter)] = -1.
# List the different sparse coding methods in the following format:
# (title, transform_algorithm, transform_alpha, transform_n_nozero_coefs)
estimators = [('OMP', 'omp', None, 15, 'navy'),
('Lasso', 'lasso_cd', 2, None, 'turquoise'), ]
lw = 2
plt.figure(figsize=(13, 6))
for subplot, (D, title) in enumerate(zip((D_fixed, D_multi),
('fixed width', 'multiple widths'))):
plt.subplot(1, 2, subplot + 1)
plt.title('Sparse coding against %s dictionary' % title)
plt.plot(y, lw=lw, linestyle='--', label='Original signal')
# Do a wavelet approximation
for title, algo, alpha, n_nonzero, color in estimators:
coder = SparseCoder(dictionary=D, transform_n_nonzero_coefs=n_nonzero,
transform_alpha=alpha, transform_algorithm=algo)
x = coder.transform(y.reshape(1, -1))
density = len(np.flatnonzero(x))
x = np.ravel(np.dot(x, D))
squared_error = np.sum((y - x) ** 2)
plt.plot(x, color=color, lw=lw,
label='%s: %s nonzero coefs,\n%.2f error'
% (title, density, squared_error))
# Soft thresholding debiasing
coder = SparseCoder(dictionary=D, transform_algorithm='threshold',
transform_alpha=20)
x = coder.transform(y.reshape(1, -1))
_, idx = np.where(x != 0)
x[0, idx], _, _, _ = np.linalg.lstsq(D[idx, :].T, y)
x = np.ravel(np.dot(x, D))
squared_error = np.sum((y - x) ** 2)
plt.plot(x, color='darkorange', lw=lw,
label='Thresholding w/ debiasing:\n%d nonzero coefs, %.2f error'
% (len(idx), squared_error))
plt.axis('tight')
plt.legend(shadow=False, loc='best')
plt.subplots_adjust(.04, .07, .97, .90, .09, .2)
plt.show()
Total running time of the script: (0 minutes 0.366 seconds)
Download Python source code: plot_sparse_coding.py
Download IPython notebook: plot_sparse_coding.ipynb
13.10 Faces dataset decompositions
This example applies to The Olivetti faces dataset different unsupervised matrix decomposition (dimension reduction)
methods from the module sklearn.decomposition (see the documentation chapter Decomposing signals in
13.10. Faces dataset decompositions
787
scikit-learn user guide, Release 0.18.2
components (matrix factorization problems)) .
print(__doc__)
# Authors: Vlad Niculae, Alexandre Gramfort
# License: BSD 3 clause
import logging
from time import time
from numpy.random import RandomState
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces
from sklearn.cluster import MiniBatchKMeans
from sklearn import decomposition
# Display progress logs on stdout
logging.basicConfig(level=logging.INFO,
format='%(asctime)s %(levelname)s %(message)s')
n_row, n_col = 2, 3
n_components = n_row * n_col
image_shape = (64, 64)
rng = RandomState(0)
Load faces data
dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)
faces = dataset.data
n_samples, n_features = faces.shape
# global centering
faces_centered = faces - faces.mean(axis=0)
# local centering
faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)
print("Dataset consists of %d faces" % n_samples)
Out:
Dataset consists of 400 faces
def plot_gallery(title, images, n_col=n_col, n_row=n_row):
plt.figure(figsize=(2. * n_col, 2.26 * n_row))
plt.suptitle(title, size=16)
for i, comp in enumerate(images):
plt.subplot(n_row, n_col, i + 1)
vmax = max(comp.max(), -comp.min())
plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,
interpolation='nearest',
vmin=-vmax, vmax=vmax)
plt.xticks(())
plt.yticks(())
plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)
788
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
List of the different estimators, whether to center and transpose the problem, and whether the transformer uses the
clustering API.
estimators = [
('Eigenfaces - PCA using randomized SVD',
decomposition.PCA(n_components=n_components, svd_solver='randomized',
whiten=True),
True),
('Non-negative components - NMF',
decomposition.NMF(n_components=n_components, init='nndsvda', tol=5e-3),
False),
('Independent components - FastICA',
decomposition.FastICA(n_components=n_components, whiten=True),
True),
('Sparse comp. - MiniBatchSparsePCA',
decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,
n_iter=100, batch_size=3,
random_state=rng),
True),
('MiniBatchDictionaryLearning',
decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
n_iter=50, batch_size=3,
random_state=rng),
True),
('Cluster centers - MiniBatchKMeans',
MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20,
max_iter=50, random_state=rng),
True),
('Factor Analysis components - FA',
decomposition.FactorAnalysis(n_components=n_components, max_iter=2),
True),
]
Plot a sample of the input data
plot_gallery("First centered Olivetti faces", faces_centered[:n_components])
13.10. Faces dataset decompositions
789
scikit-learn user guide, Release 0.18.2
Do the estimation and plot it
for name, estimator, center in estimators:
print("Extracting the top %d %s..." % (n_components, name))
t0 = time()
data = faces
if center:
data = faces_centered
estimator.fit(data)
train_time = (time() - t0)
print("done in %0.3fs" % train_time)
if hasattr(estimator, 'cluster_centers_'):
components_ = estimator.cluster_centers_
else:
components_ = estimator.components_
if (hasattr(estimator, 'noise_variance_') and
estimator.noise_variance_.shape != ()):
plot_gallery("Pixelwise variance",
estimator.noise_variance_.reshape(1, -1), n_col=1,
n_row=1)
plot_gallery('%s - Train time %.1fs' % (name, train_time),
components_[:n_components])
plt.show()
790
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
•
•
•
•
13.10. Faces dataset decompositions
791
scikit-learn user guide, Release 0.18.2
•
•
•
•
Out:
Extracting the top 6 Eigenfaces - PCA using randomized SVD...
done in 0.094s
Extracting the top 6 Non-negative components - NMF...
done in 0.660s
Extracting the top 6 Independent components - FastICA...
792
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
done in 0.319s
Extracting the top 6 Sparse comp. - MiniBatchSparsePCA...
done in 1.134s
Extracting the top 6 MiniBatchDictionaryLearning...
done in 1.007s
Extracting the top 6 Cluster centers - MiniBatchKMeans...
done in 0.096s
Extracting the top 6 Factor Analysis components - FA...
done in 0.137s
Total running time of the script: (0 minutes 6.636 seconds)
Download Python source code: plot_faces_decomposition.py
Download IPython notebook: plot_faces_decomposition.ipynb
13.11 Image denoising using dictionary learning
An example comparing the effect of reconstructing noisy fragments of a raccoon face image using ﬁrstly online
Dictionary Learning and various transform methods.
The dictionary is ﬁtted on the distorted left half of the image, and subsequently used to reconstruct the right half. Note
that even better performance could be achieved by ﬁtting to an undistorted (i.e. noiseless) image, but here we start
from the assumption that it is not available.
A common practice for evaluating the results of image denoising is by looking at the difference between the recon-
struction and the original image. If the reconstruction is perfect this will look like Gaussian noise.
It can be seen from the plots that the results of Orthogonal Matching Pursuit (OMP) with two non-zero coefﬁcients is
a bit less biased than when keeping only one (the edges look less prominent). It is in addition closer from the ground
truth in Frobenius norm.
The result of Least Angle Regression is much more strongly biased: the difference is reminiscent of the local intensity
value of the original image.
Thresholding is clearly not useful for denoising, but it is here to show that it can produce a suggestive output with
very high speed, and thus be useful for other tasks such as object classiﬁcation, where performance is not necessarily
related to visualisation.
print(__doc__)
from time import time
import matplotlib.pyplot as plt
import numpy as np
import scipy as sp
from sklearn.decomposition import MiniBatchDictionaryLearning
from sklearn.feature_extraction.image import extract_patches_2d
from sklearn.feature_extraction.image import reconstruct_from_patches_2d
from sklearn.utils.testing import SkipTest
from sklearn.utils.fixes import sp_version
if sp_version < (0, 12):
raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "
"thus does not include the scipy.misc.face() image.")
13.11. Image denoising using dictionary learning
793
scikit-learn user guide, Release 0.18.2
try:
from scipy import misc
face = misc.face(gray=True)
except AttributeError:
# Old versions of scipy have face in the top level package
face = sp.face(gray=True)
# Convert from uint8 representation with values between 0 and 255 to
# a floating point representation with values between 0 and 1.
face = face / 255
# downsample for higher speed
face = face[::2, ::2] + face[1::2, ::2] + face[::2, 1::2] + face[1::2, 1::2]
face /= 4.0
height, width = face.shape
# Distort the right half of the image
print('Distorting image...')
distorted = face.copy()
distorted[:, width // 2:] += 0.075 * np.random.randn(height, width // 2)
# Extract all reference patches from the left half of the image
print('Extracting reference patches...')
t0 = time()
patch_size = (7, 7)
data = extract_patches_2d(distorted[:, :width // 2], patch_size)
data = data.reshape(data.shape[0], -1)
data -= np.mean(data, axis=0)
data /= np.std(data, axis=0)
print('done in %.2fs.' % (time() - t0))
Out:
Distorting image...
Extracting reference patches...
done in 0.07s.
Learn the dictionary from reference patches
print('Learning the dictionary...')
t0 = time()
dico = MiniBatchDictionaryLearning(n_components=100, alpha=1, n_iter=500)
V = dico.fit(data).components_
dt = time() - t0
print('done in %.2fs.' % dt)
plt.figure(figsize=(4.2, 4))
for i, comp in enumerate(V[:100]):
plt.subplot(10, 10, i + 1)
plt.imshow(comp.reshape(patch_size), cmap=plt.cm.gray_r,
interpolation='nearest')
plt.xticks(())
plt.yticks(())
plt.suptitle('Dictionary learned from face patches\n' +
'Train time %.1fs on %d patches' % (dt, len(data)),
fontsize=16)
plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)
794
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
Out:
Learning the dictionary...
done in 7.73s.
Display the distorted image
def show_with_diff(image, reference, title):
"""Helper function to display denoising"""
plt.figure(figsize=(5, 3.3))
plt.subplot(1, 2, 1)
plt.title('Image')
plt.imshow(image, vmin=0, vmax=1, cmap=plt.cm.gray,
interpolation='nearest')
plt.xticks(())
plt.yticks(())
plt.subplot(1, 2, 2)
difference = image - reference
plt.title('Difference (norm: %.2f)' % np.sqrt(np.sum(difference ** 2)))
plt.imshow(difference, vmin=-0.5, vmax=0.5, cmap=plt.cm.PuOr,
interpolation='nearest')
plt.xticks(())
plt.yticks(())
plt.suptitle(title, size=16)
plt.subplots_adjust(0.02, 0.02, 0.98, 0.79, 0.02, 0.2)
show_with_diff(distorted, face, 'Distorted image')
13.11. Image denoising using dictionary learning
795
scikit-learn user guide, Release 0.18.2
Extract noisy patches and reconstruct them using the dictionary
print('Extracting noisy patches... ')
t0 = time()
data = extract_patches_2d(distorted[:, width // 2:], patch_size)
data = data.reshape(data.shape[0], -1)
intercept = np.mean(data, axis=0)
data -= intercept
print('done in %.2fs.' % (time() - t0))
transform_algorithms = [
('Orthogonal Matching Pursuit\n1 atom', 'omp',
{'transform_n_nonzero_coefs': 1}),
('Orthogonal Matching Pursuit\n2 atoms', 'omp',
{'transform_n_nonzero_coefs': 2}),
('Least-angle regression\n5 atoms', 'lars',
{'transform_n_nonzero_coefs': 5}),
('Thresholding\n alpha=0.1', 'threshold', {'transform_alpha': .1})]
reconstructions = {}
for title, transform_algorithm, kwargs in transform_algorithms:
print(title + '...')
reconstructions[title] = face.copy()
t0 = time()
dico.set_params(transform_algorithm=transform_algorithm, **kwargs)
code = dico.transform(data)
patches = np.dot(code, V)
patches += intercept
patches = patches.reshape(len(data), *patch_size)
if transform_algorithm == 'threshold':
patches -= patches.min()
patches /= patches.max()
reconstructions[title][:, width // 2:] = reconstruct_from_patches_2d(
patches, (height, width // 2))
dt = time() - t0
796
Chapter 13. Decomposition
scikit-learn user guide, Release 0.18.2
print('done in %.2fs.' % dt)
show_with_diff(reconstructions[title], face,
title + ' (time: %.1fs)' % dt)
plt.show()
•
•
•
•
Out:
Extracting noisy patches...
done in 0.02s.
Orthogonal Matching Pursuit
1 atom...
done in 9.76s.
Orthogonal Matching Pursuit
2 atoms...
13.11. Image denoising using dictionary learning
797
scikit-learn user guide, Release 0.18.2
done in 20.42s.
Least-angle regression
5 atoms...
done in 96.18s.
Thresholding
alpha=0.1...
done in 1.17s.
Total running time of the script: (2 minutes 22.129 seconds)
Download Python source code: plot_image_denoising.py
Download IPython notebook: plot_image_denoising.ipynb
798
Chapter 13. Decomposition
CHAPTER
FOURTEEN
ENSEMBLE METHODS
Examples concerning the sklearn.ensemble module.
14.1 Decision Tree Regression with AdaBoost
A decision tree is boosted using the AdaBoost.R2 [1] algorithm on a 1D sinusoidal dataset with a small amount of
Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of
boosts is increased the regressor can ﬁt more detail.
799
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Author: Noel Dawe <noel.dawe@gmail.com>
#
# License: BSD 3 clause
# importing necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
# Create the dataset
rng = np.random.RandomState(1)
X = np.linspace(0, 6, 100)[:, np.newaxis]
y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])
# Fit regression model
regr_1 = DecisionTreeRegressor(max_depth=4)
regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),
n_estimators=300, random_state=rng)
regr_1.fit(X, y)
regr_2.fit(X, y)
# Predict
y_1 = regr_1.predict(X)
y_2 = regr_2.predict(X)
# Plot the results
plt.figure()
plt.scatter(X, y, c="k", label="training samples")
plt.plot(X, y_1, c="g", label="n_estimators=1", linewidth=2)
plt.plot(X, y_2, c="r", label="n_estimators=300", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("Boosted Decision Tree Regression")
plt.legend()
plt.show()
Total running time of the script: (0 minutes 0.749 seconds)
Download Python source code: plot_adaboost_regression.py
Download IPython notebook: plot_adaboost_regression.ipynb
14.2 Pixel importances with a parallel forest of trees
This example shows the use of forests of trees to evaluate the importance of the pixels in an image classiﬁcation task
(faces). The hotter the pixel, the more important.
The code below also illustrates how the construction and the computation of the predictions can be parallelized within
multiple jobs.
800
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
Out:
Fitting ExtraTreesClassifier on faces data with 1 cores...
done in 3.359s
print(__doc__)
from time import time
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces
from sklearn.ensemble import ExtraTreesClassifier
14.2. Pixel importances with a parallel forest of trees
801
scikit-learn user guide, Release 0.18.2
# Number of cores to use to perform parallel fitting of the forest model
n_jobs = 1
# Load the faces dataset
data = fetch_olivetti_faces()
X = data.images.reshape((len(data.images), -1))
y = data.target
mask = y < 5
# Limit to 5 classes
X = X[mask]
y = y[mask]
# Build a forest and compute the pixel importances
print("Fitting ExtraTreesClassifier on faces data with %d cores..." % n_jobs)
t0 = time()
forest = ExtraTreesClassifier(n_estimators=1000,
max_features=128,
n_jobs=n_jobs,
random_state=0)
forest.fit(X, y)
print("done in %0.3fs" % (time() - t0))
importances = forest.feature_importances_
importances = importances.reshape(data.images[0].shape)
# Plot pixel importances
plt.matshow(importances, cmap=plt.cm.hot)
plt.title("Pixel importances with forests of trees")
plt.show()
Total running time of the script: (0 minutes 4.096 seconds)
Download Python source code: plot_forest_importances_faces.py
Download IPython notebook: plot_forest_importances_faces.ipynb
14.3 IsolationForest example
An example using IsolationForest for anomaly detection.
The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value
between the maximum and minimum values of the selected feature.
Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample
is equivalent to the path length from the root node to the terminating node.
This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.
Random partitioning produces noticeable shorter paths for anomalies. Hence, when a forest of random trees collec-
tively produce shorter path lengths for particular samples, they are highly likely to be anomalies.
802
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
rng = np.random.RandomState(42)
# Generate train data
X = 0.3 * rng.randn(100, 2)
X_train = np.r_[X + 2, X - 2]
# Generate some regular novel observations
X = 0.3 * rng.randn(20, 2)
X_test = np.r_[X + 2, X - 2]
# Generate some abnormal novel observations
X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))
# fit the model
clf = IsolationForest(max_samples=100, random_state=rng)
clf.fit(X_train)
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)
y_pred_outliers = clf.predict(X_outliers)
# plot the line, the samples, and the nearest vectors to the plane
xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))
14.3. IsolationForest example
803
scikit-learn user guide, Release 0.18.2
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.title("IsolationForest")
plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)
b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')
b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')
c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')
plt.axis('tight')
plt.xlim((-5, 5))
plt.ylim((-5, 5))
plt.legend([b1, b2, c],
["training observations",
"new regular observations", "new abnormal observations"],
loc="upper left")
plt.show()
Total running time of the script: (0 minutes 0.568 seconds)
Download Python source code: plot_isolation_forest.py
Download IPython notebook: plot_isolation_forest.ipynb
14.4 Feature importances with forests of trees
This examples shows the use of forests of trees to evaluate the importance of features on an artiﬁcial classiﬁcation
task. The red bars are the feature importances of the forest, along with their inter-trees variability.
As expected, the plot suggests that 3 features are informative, while the remaining are not.
804
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
Out:
Feature ranking:
1. feature 1 (0.295902)
2. feature 2 (0.208351)
3. feature 0 (0.177632)
4. feature 3 (0.047121)
5. feature 6 (0.046303)
6. feature 8 (0.046013)
7. feature 7 (0.045575)
8. feature 4 (0.044614)
9. feature 9 (0.044577)
10. feature 5 (0.043912)
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier
14.4. Feature importances with forests of trees
805
scikit-learn user guide, Release 0.18.2
# Build a classification task using 3 informative features
X, y = make_classification(n_samples=1000,
n_features=10,
n_informative=3,
n_redundant=0,
n_repeated=0,
n_classes=2,
random_state=0,
shuffle=False)
# Build a forest and compute the feature importances
forest = ExtraTreesClassifier(n_estimators=250,
random_state=0)
forest.fit(X, y)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
axis=0)
indices = np.argsort(importances)[::-1]
# Print the feature ranking
print("Feature ranking:")
for f in range(X.shape[1]):
print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))
# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()
Total running time of the script: (0 minutes 1.204 seconds)
Download Python source code: plot_forest_importances.py
Download IPython notebook: plot_forest_importances.ipynb
14.5 Plot the decision boundaries of a VotingClassiﬁer
Plot the decision boundaries of a VotingClassiﬁer for two features of the Iris dataset.
Plot the class probabilities of the ﬁrst sample in a toy dataset predicted by three different classiﬁers and averaged by
the VotingClassiﬁer.
First, three exemplary classiﬁers are initialized (DecisionTreeClassiﬁer, KNeighborsClassiﬁer, and SVC) and used to
initialize a soft-voting VotingClassiﬁer with weights [2, 1, 2], which means that the predicted probabilities of the
DecisionTreeClassiﬁer and SVC count 5 times as much as the weights of the KNeighborsClassiﬁer classiﬁer when the
averaged probability is calculated.
806
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
print(__doc__)
from itertools import product
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
# Loading some example data
iris = datasets.load_iris()
X = iris.data[:, [0, 2]]
y = iris.target
# Training classifiers
clf1 = DecisionTreeClassifier(max_depth=4)
clf2 = KNeighborsClassifier(n_neighbors=7)
clf3 = SVC(kernel='rbf', probability=True)
eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2),
14.5. Plot the decision boundaries of a VotingClassiﬁer
807
scikit-learn user guide, Release 0.18.2
('svc', clf3)],
voting='soft', weights=[2, 1, 2])
clf1.fit(X, y)
clf2.fit(X, y)
clf3.fit(X, y)
eclf.fit(X, y)
# Plotting decision regions
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
np.arange(y_min, y_max, 0.1))
f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))
for idx, clf, tt in zip(product([0, 1], [0, 1]),
[clf1, clf2, clf3, eclf],
['Decision Tree (depth=4)', 'KNN (k=7)',
'Kernel SVM', 'Soft Voting']):
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)
axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
axarr[idx[0], idx[1]].set_title(tt)
plt.show()
Total running time of the script: (0 minutes 0.326 seconds)
Download Python source code: plot_voting_decision_regions.py
Download IPython notebook: plot_voting_decision_regions.ipynb
14.6 Comparing random forests and the multi-output meta estimator
An example to compare multi-output regression with random forest and the multioutput.MultiOutputRegressor meta-
estimator.
This example illustrates the use of the multioutput.MultiOutputRegressor meta-estimator to perform multi-output re-
gression. A random forest regressor is used, which supports multi-output regression natively, so the results can be
compared.
The random forest regressor will only ever predict values within the range of observations or closer to zero for each of
the targets. As a result the predictions are biased towards the centre of the circle.
Using a single underlying feature the model learns both the x and y coordinate as output.
808
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Author: Tim Head <betatim@gmail.com>
#
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
# Create a random dataset
rng = np.random.RandomState(1)
X = np.sort(200 * rng.rand(600, 1) - 100, axis=0)
y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T
y += (0.5 - rng.rand(*y.shape))
X_train, X_test, y_train, y_test = train_test_split(X, y,
train_size=400,
random_state=4)
max_depth = 30
regr_multirf = MultiOutputRegressor(RandomForestRegressor(max_depth=max_depth,
random_state=0))
14.6. Comparing random forests and the multi-output meta estimator
809
scikit-learn user guide, Release 0.18.2
regr_multirf.fit(X_train, y_train)
regr_rf = RandomForestRegressor(max_depth=max_depth, random_state=2)
regr_rf.fit(X_train, y_train)
# Predict on new data
y_multirf = regr_multirf.predict(X_test)
y_rf = regr_rf.predict(X_test)
# Plot the results
plt.figure()
s = 50
a = 0.4
plt.scatter(y_test[:, 0], y_test[:, 1],
c="navy", s=s, marker="s", alpha=a, label="Data")
plt.scatter(y_multirf[:, 0], y_multirf[:, 1],
c="cornflowerblue", s=s, alpha=a,
label="Multi RF score=%.2f" % regr_multirf.score(X_test, y_test))
plt.scatter(y_rf[:, 0], y_rf[:, 1],
c="c", s=s, marker="^", alpha=a,
label="RF score=%.2f" % regr_rf.score(X_test, y_test))
plt.xlim([-6, 6])
plt.ylim([-6, 6])
plt.xlabel("target 1")
plt.ylabel("target 2")
plt.title("Comparing random forests and the multi-output meta estimator")
plt.legend()
plt.show()
Total running time of the script: (0 minutes 0.213 seconds)
Download Python source code: plot_random_forest_regression_multioutput.py
Download IPython notebook: plot_random_forest_regression_multioutput.ipynb
14.7 Gradient Boosting regression
Demonstrate Gradient Boosting on the Boston housing dataset.
This example ﬁts a Gradient Boosting model with least squares loss and 500 regression trees of depth 4.
print(__doc__)
# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
#
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import ensemble
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error
Load data
810
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
boston = datasets.load_boston()
X, y = shuffle(boston.data, boston.target, random_state=13)
X = X.astype(np.float32)
offset = int(X.shape[0] * 0.9)
X_train, y_train = X[:offset], y[:offset]
X_test, y_test = X[offset:], y[offset:]
Fit regression model
params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,
'learning_rate': 0.01, 'loss': 'ls'}
clf = ensemble.GradientBoostingRegressor(**params)
clf.fit(X_train, y_train)
mse = mean_squared_error(y_test, clf.predict(X_test))
print("MSE: %.4f" % mse)
Out:
MSE: 6.6213
Plot training deviance
# compute test set deviance
test_score = np.zeros((params['n_estimators'],), dtype=np.float64)
for i, y_pred in enumerate(clf.staged_predict(X_test)):
test_score[i] = clf.loss_(y_test, y_pred)
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title('Deviance')
plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',
label='Training Set Deviance')
plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',
label='Test Set Deviance')
plt.legend(loc='upper right')
plt.xlabel('Boosting Iterations')
plt.ylabel('Deviance')
14.7. Gradient Boosting regression
811
scikit-learn user guide, Release 0.18.2
Plot feature importance
feature_importance = clf.feature_importances_
# make importances relative to max importance
feature_importance = 100.0 * (feature_importance / feature_importance.max())
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
plt.subplot(1, 2, 2)
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, boston.feature_names[sorted_idx])
plt.xlabel('Relative Importance')
plt.title('Variable Importance')
plt.show()
812
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.548 seconds)
Download Python source code: plot_gradient_boosting_regression.py
Download IPython notebook: plot_gradient_boosting_regression.ipynb
14.8 Prediction Intervals for Gradient Boosting Regression
This example shows how quantile regression can be used to create prediction intervals.
14.8. Prediction Intervals for Gradient Boosting Regression
813
scikit-learn user guide, Release 0.18.2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
np.random.seed(1)
def f(x):
"""The function to predict."""
return x * np.sin(x)
#----------------------------------------------------------------------
#
First the noiseless case
X = np.atleast_2d(np.random.uniform(0, 10.0, size=100)).T
X = X.astype(np.float32)
# Observations
y = f(X).ravel()
dy = 1.5 + 1.0 * np.random.random(y.shape)
noise = np.random.normal(0, dy)
y += noise
y = y.astype(np.float32)
# Mesh the input space for evaluations of the real function, the prediction and
814
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
# its MSE
xx = np.atleast_2d(np.linspace(0, 10, 1000)).T
xx = xx.astype(np.float32)
alpha = 0.95
clf = GradientBoostingRegressor(loss='quantile', alpha=alpha,
n_estimators=250, max_depth=3,
learning_rate=.1, min_samples_leaf=9,
min_samples_split=9)
clf.fit(X, y)
# Make the prediction on the meshed x-axis
y_upper = clf.predict(xx)
clf.set_params(alpha=1.0 - alpha)
clf.fit(X, y)
# Make the prediction on the meshed x-axis
y_lower = clf.predict(xx)
clf.set_params(loss='ls')
clf.fit(X, y)
# Make the prediction on the meshed x-axis
y_pred = clf.predict(xx)
# Plot the function, the prediction and the 90% confidence interval based on
# the MSE
fig = plt.figure()
plt.plot(xx, f(xx), 'g:', label=u'$f(x) = x\,\sin(x)$')
plt.plot(X, y, 'b.', markersize=10, label=u'Observations')
plt.plot(xx, y_pred, 'r-', label=u'Prediction')
plt.plot(xx, y_upper, 'k-')
plt.plot(xx, y_lower, 'k-')
plt.fill(np.concatenate([xx, xx[::-1]]),
np.concatenate([y_upper, y_lower[::-1]]),
alpha=.5, fc='b', ec='None', label='90% prediction interval')
plt.xlabel('$x$')
plt.ylabel('$f(x)$')
plt.ylim(-10, 20)
plt.legend(loc='upper left')
plt.show()
Total running time of the script: (0 minutes 0.312 seconds)
Download Python source code: plot_gradient_boosting_quantile.py
Download IPython notebook: plot_gradient_boosting_quantile.ipynb
14.9 Plot class probabilities calculated by the VotingClassiﬁer
Plot the class probabilities of the ﬁrst sample in a toy dataset predicted by three different classiﬁers and averaged by
the VotingClassiﬁer.
First, three examplary classiﬁers are initialized (LogisticRegression, GaussianNB, and RandomForestClassiﬁer) and
14.9. Plot class probabilities calculated by the VotingClassiﬁer
815
scikit-learn user guide, Release 0.18.2
used to initialize a soft-voting VotingClassiﬁer with weights [1, 1, 5], which means that the predicted probabilities of
the RandomForestClassiﬁer count 5 times as much as the weights of the other classiﬁers when the averaged probability
is calculated.
To visualize the probability weighting, we ﬁt each classiﬁer on the training set and plot the predicted class probabilities
for the ﬁrst sample in this example dataset.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
clf1 = LogisticRegression(random_state=123)
clf2 = RandomForestClassifier(random_state=123)
clf3 = GaussianNB()
X = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])
y = np.array([1, 1, 2, 2])
eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
voting='soft',
weights=[1, 1, 5])
816
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
# predict class probabilities for all classifiers
probas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]
# get class probabilities for the first sample in the dataset
class1_1 = [pr[0, 0] for pr in probas]
class2_1 = [pr[0, 1] for pr in probas]
# plotting
N = 4
# number of groups
ind = np.arange(N)
# group positions
width = 0.35
# bar width
fig, ax = plt.subplots()
# bars for classifier 1-3
p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color='green')
p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color='lightgreen')
# bars for VotingClassifier
p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color='blue')
p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color='steelblue')
# plot annotations
plt.axvline(2.8, color='k', linestyle='dashed')
ax.set_xticks(ind + width)
ax.set_xticklabels(['LogisticRegression\nweight 1',
'GaussianNB\nweight 1',
'RandomForestClassifier\nweight 5',
'VotingClassifier\n(average probabilities)'],
rotation=40,
ha='right')
plt.ylim([0, 1])
plt.title('Class probabilities for sample 1 by different classifiers')
plt.legend([p1[0], p2[0]], ['class 1', 'class 2'], loc='upper left')
plt.show()
Total running time of the script: (0 minutes 0.235 seconds)
Download Python source code: plot_voting_probas.py
Download IPython notebook: plot_voting_probas.ipynb
14.10 Gradient Boosting regularization
Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie
et al 2009.
The loss function used is binomial deviance. Regularization via shrinkage (learning_rate < 1.0) improves
performance considerably. In combination with shrinkage, stochastic gradient boosting (subsample < 1.0) can
produce more accurate models by reducing the variance via bagging. Subsampling without shrinkage usually does
poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in
Random Forests (via the max_features parameter).
14.10. Gradient Boosting regularization
817
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
#
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import ensemble
from sklearn import datasets
X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)
X = X.astype(np.float32)
# map labels from {-1, 1} to {0, 1}
labels, y = np.unique(y, return_inverse=True)
X_train, X_test = X[:2000], X[2000:]
y_train, y_test = y[:2000], y[2000:]
original_params = {'n_estimators': 1000, 'max_leaf_nodes': 4, 'max_depth': None,
˓→'random_state': 2,
'min_samples_split': 5}
818
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
plt.figure()
for label, color, setting in [('No shrinkage', 'orange',
{'learning_rate': 1.0, 'subsample': 1.0}),
('learning_rate=0.1', 'turquoise',
{'learning_rate': 0.1, 'subsample': 1.0}),
('subsample=0.5', 'blue',
{'learning_rate': 1.0, 'subsample': 0.5}),
('learning_rate=0.1, subsample=0.5', 'gray',
{'learning_rate': 0.1, 'subsample': 0.5}),
('learning_rate=0.1, max_features=2', 'magenta',
{'learning_rate': 0.1, 'max_features': 2})]:
params = dict(original_params)
params.update(setting)
clf = ensemble.GradientBoostingClassifier(**params)
clf.fit(X_train, y_train)
# compute test set deviance
test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)
for i, y_pred in enumerate(clf.staged_decision_function(X_test)):
# clf.loss_ assumes that y_test[i] in {0, 1}
test_deviance[i] = clf.loss_(y_test, y_pred)
plt.plot((np.arange(test_deviance.shape[0]) + 1)[::5], test_deviance[::5],
'-', color=color, label=label)
plt.legend(loc='upper left')
plt.xlabel('Boosting Iterations')
plt.ylabel('Test Set Deviance')
plt.show()
Total running time of the script: (0 minutes 12.004 seconds)
Download Python source code: plot_gradient_boosting_regularization.py
Download IPython notebook: plot_gradient_boosting_regularization.ipynb
14.11 OOB Errors for Random Forests
The RandomForestClassifier is trained using bootstrap aggregation, where each new tree is ﬁt from a boot-
strap sample of the training observations 𝑧𝑖= (𝑥𝑖, 𝑦𝑖). The out-of-bag (OOB) error is the average error for each 𝑧𝑖
calculated using predictions from the trees that do not contain 𝑧𝑖in their respective bootstrap sample. This allows the
RandomForestClassifier to be ﬁt and validated whilst being trained [1].
The example below demonstrates how the OOB error can be measured at the addition of each new tree during train-
ing. The resulting plot allows a practitioner to approximate a suitable value of n_estimators at which the error
stabilizes.
14.11. OOB Errors for Random Forests
819
scikit-learn user guide, Release 0.18.2
import matplotlib.pyplot as plt
from collections import OrderedDict
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
# Author: Kian Ho <hui.kian.ho@gmail.com>
#
Gilles Louppe <g.louppe@gmail.com>
#
Andreas Mueller <amueller@ais.uni-bonn.de>
#
# License: BSD 3 Clause
print(__doc__)
RANDOM_STATE = 123
# Generate a binary classification dataset.
X, y = make_classification(n_samples=500, n_features=25,
n_clusters_per_class=1, n_informative=15,
random_state=RANDOM_STATE)
# NOTE: Setting the `warm_start` construction parameter to `True` disables
# support for parallelized ensembles but is necessary for tracking the OOB
# error trajectory during training.
ensemble_clfs = [
("RandomForestClassifier, max_features='sqrt'",
820
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
RandomForestClassifier(warm_start=True, oob_score=True,
max_features="sqrt",
random_state=RANDOM_STATE)),
("RandomForestClassifier, max_features='log2'",
RandomForestClassifier(warm_start=True, max_features='log2',
oob_score=True,
random_state=RANDOM_STATE)),
("RandomForestClassifier, max_features=None",
RandomForestClassifier(warm_start=True, max_features=None,
oob_score=True,
random_state=RANDOM_STATE))
]
# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.
error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)
# Range of `n_estimators` values to explore.
min_estimators = 15
max_estimators = 175
for label, clf in ensemble_clfs:
for i in range(min_estimators, max_estimators + 1):
clf.set_params(n_estimators=i)
clf.fit(X, y)
# Record the OOB error for each `n_estimators=i` setting.
oob_error = 1 - clf.oob_score_
error_rate[label].append((i, oob_error))
# Generate the "OOB error rate" vs. "n_estimators" plot.
for label, clf_err in error_rate.items():
xs, ys = zip(*clf_err)
plt.plot(xs, ys, label=label)
plt.xlim(min_estimators, max_estimators)
plt.xlabel("n_estimators")
plt.ylabel("OOB error rate")
plt.legend(loc="upper right")
plt.show()
Total running time of the script: (0 minutes 10.994 seconds)
Download Python source code: plot_ensemble_oob.py
Download IPython notebook: plot_ensemble_oob.ipynb
14.12 Two-class AdaBoost
This example ﬁts an AdaBoosted decision stump on a non-linearly separable classiﬁcation dataset composed of two
“Gaussian quantiles” clusters (see sklearn.datasets.make_gaussian_quantiles) and plots the decision
boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B.
The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores
greater than zero are classiﬁed as B, and are otherwise classiﬁed as A. The magnitude of a decision score determines
the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a
desired purity of class B, for example, by only selecting samples with a decision score above some value.
14.12. Two-class AdaBoost
821
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Author: Noel Dawe <noel.dawe@gmail.com>
#
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_gaussian_quantiles
# Construct dataset
X1, y1 = make_gaussian_quantiles(cov=2.,
n_samples=200, n_features=2,
n_classes=2, random_state=1)
X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,
n_samples=300, n_features=2,
n_classes=2, random_state=1)
X = np.concatenate((X1, X2))
y = np.concatenate((y1, - y2 + 1))
# Create and fit an AdaBoosted decision tree
bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
algorithm="SAMME",
n_estimators=200)
bdt.fit(X, y)
plot_colors = "br"
plot_step = 0.02
class_names = "AB"
plt.figure(figsize=(10, 5))
822
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
# Plot the decision boundaries
plt.subplot(121)
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
np.arange(y_min, y_max, plot_step))
Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
plt.axis("tight")
# Plot the training points
for i, n, c in zip(range(2), class_names, plot_colors):
idx = np.where(y == i)
plt.scatter(X[idx, 0], X[idx, 1],
c=c, cmap=plt.cm.Paired,
label="Class %s" % n)
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.legend(loc='upper right')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Decision Boundary')
# Plot the two-class decision scores
twoclass_output = bdt.decision_function(X)
plot_range = (twoclass_output.min(), twoclass_output.max())
plt.subplot(122)
for i, n, c in zip(range(2), class_names, plot_colors):
plt.hist(twoclass_output[y == i],
bins=10,
range=plot_range,
facecolor=c,
label='Class %s' % n,
alpha=.5)
x1, x2, y1, y2 = plt.axis()
plt.axis((x1, x2, y1, y2 * 1.2))
plt.legend(loc='upper right')
plt.ylabel('Samples')
plt.xlabel('Score')
plt.title('Decision Scores')
plt.tight_layout()
plt.subplots_adjust(wspace=0.35)
plt.show()
Total running time of the script: (0 minutes 4.682 seconds)
Download Python source code: plot_adaboost_twoclass.py
Download IPython notebook: plot_adaboost_twoclass.ipynb
14.12. Two-class AdaBoost
823
scikit-learn user guide, Release 0.18.2
14.13 Hashing feature transformation using Totally Random Trees
RandomTreesEmbedding provides a way to map data to a very high-dimensional, sparse representation, which might
be beneﬁcial for classiﬁcation. The mapping is completely unsupervised and very efﬁcient.
This example visualizes the partitions given by several trees and shows how the transformation can also be used for
non-linear dimensionality reduction or non-linear classiﬁcation.
Points that are neighboring often share the same leaf of a tree and therefore share large parts of their hashed repre-
sentation. This allows to separate two concentric circles simply based on the principal components of the transformed
data with truncated SVD.
In high-dimensional spaces, linear classiﬁers often achieve excellent accuracy. For sparse binary data, BernoulliNB is
particularly well-suited. The bottom row compares the decision boundary obtained by BernoulliNB in the transformed
space with an ExtraTreesClassiﬁer forests learned on the original data.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
824
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
from sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier
from sklearn.decomposition import TruncatedSVD
from sklearn.naive_bayes import BernoulliNB
# make a synthetic dataset
X, y = make_circles(factor=0.5, random_state=0, noise=0.05)
# use RandomTreesEmbedding to transform data
hasher = RandomTreesEmbedding(n_estimators=10, random_state=0, max_depth=3)
X_transformed = hasher.fit_transform(X)
# Visualize result after dimensionality reduction using truncated SVD
svd = TruncatedSVD(n_components=2)
X_reduced = svd.fit_transform(X_transformed)
# Learn a Naive Bayes classifier on the transformed data
nb = BernoulliNB()
nb.fit(X_transformed, y)
# Learn an ExtraTreesClassifier for comparison
trees = ExtraTreesClassifier(max_depth=3, n_estimators=10, random_state=0)
trees.fit(X, y)
# scatter plot of original and reduced data
fig = plt.figure(figsize=(9, 8))
ax = plt.subplot(221)
ax.scatter(X[:, 0], X[:, 1], c=y, s=50)
ax.set_title("Original Data (2d)")
ax.set_xticks(())
ax.set_yticks(())
ax = plt.subplot(222)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50)
ax.set_title("Truncated SVD reduction (2d) of transformed data (%dd)" %
X_transformed.shape[1])
ax.set_xticks(())
ax.set_yticks(())
# Plot the decision in original space. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
h = .01
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
# transform grid using RandomTreesEmbedding
transformed_grid = hasher.transform(np.c_[xx.ravel(), yy.ravel()])
y_grid_pred = nb.predict_proba(transformed_grid)[:, 1]
ax = plt.subplot(223)
ax.set_title("Naive Bayes on Transformed data")
ax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))
ax.scatter(X[:, 0], X[:, 1], c=y, s=50)
ax.set_ylim(-1.4, 1.4)
ax.set_xlim(-1.4, 1.4)
14.13. Hashing feature transformation using Totally Random Trees
825
scikit-learn user guide, Release 0.18.2
ax.set_xticks(())
ax.set_yticks(())
# transform grid using ExtraTreesClassifier
y_grid_pred = trees.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
ax = plt.subplot(224)
ax.set_title("ExtraTrees predictions")
ax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))
ax.scatter(X[:, 0], X[:, 1], c=y, s=50)
ax.set_ylim(-1.4, 1.4)
ax.set_xlim(-1.4, 1.4)
ax.set_xticks(())
ax.set_yticks(())
plt.tight_layout()
plt.show()
Total running time of the script: (0 minutes 0.779 seconds)
Download Python source code: plot_random_forest_embedding.py
Download IPython notebook: plot_random_forest_embedding.ipynb
14.14 Partial Dependence Plots
Partial dependence plots show the dependence between the target function 2 and a set of ‘target’ features, marginalizing
over the values of all other features (the complement features). Due to the limits of human perception the size of the
target feature set must be small (usually, one or two) thus the target features are usually chosen among the most
important features (see feature_importances_).
This example shows how to obtain partial dependence plots from a GradientBoostingRegressor trained on
the California housing dataset. The example is taken from 1.
The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP
are: median income (MedInc), avg. occupants per household (AvgOccup), median house age (HouseAge), and avg.
rooms per household (AveRooms).
We can clearly see that the median house price shows a linear relationship with the median income (top left) and that
the house price drops when the avg. occupants per household increases (top middle). The top right plot shows that the
house age in a district does not have a strong inﬂuence on the (median) house price; so does the average rooms per
household. The tick marks on the x-axis represent the deciles of the feature values in the training data.
Partial dependence plots with two target features enable us to visualize interactions among them. The two-way partial
dependence plot shows the dependence of median house price on joint values of house age and avg. occupants per
household. We can clearly see an interaction between the two features: For an avg. occupancy greater than two, the
house price is nearly independent of the house age, whereas for values less than two there is a strong dependence on
age.
2 For classiﬁcation you can think of it as the regression score before the link function.
1 T. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, Springer, 2009.
826
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
•
•
Out:
Training GBRT...
done.
Convenience plot with ``partial_dependence_plots``
Custom 3d plot via ``partial_dependence``
from __future__ import print_function
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble.partial_dependence import plot_partial_dependence
from sklearn.ensemble.partial_dependence import partial_dependence
from sklearn.datasets.california_housing import fetch_california_housing
def main():
14.14. Partial Dependence Plots
827
scikit-learn user guide, Release 0.18.2
cal_housing = fetch_california_housing()
# split 80/20 train-test
X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,
cal_housing.target,
test_size=0.2,
random_state=1)
names = cal_housing.feature_names
print("Training GBRT...")
clf = GradientBoostingRegressor(n_estimators=100, max_depth=4,
learning_rate=0.1, loss='huber',
random_state=1)
clf.fit(X_train, y_train)
print(" done.")
print('Convenience plot with ``partial_dependence_plots``')
features = [0, 5, 1, 2, (5, 1)]
fig, axs = plot_partial_dependence(clf, X_train, features,
feature_names=names,
n_jobs=3, grid_resolution=50)
fig.suptitle('Partial dependence of house value on nonlocation features\n'
'for the California housing dataset')
plt.subplots_adjust(top=0.9)
# tight_layout causes overlap with suptitle
print('Custom 3d plot via ``partial_dependence``')
fig = plt.figure()
target_feature = (1, 5)
pdp, axes = partial_dependence(clf, target_feature,
X=X_train, grid_resolution=50)
XX, YY = np.meshgrid(axes[0], axes[1])
Z = pdp[0].reshape(list(map(np.size, axes))).T
ax = Axes3D(fig)
surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu)
ax.set_xlabel(names[target_feature[0]])
ax.set_ylabel(names[target_feature[1]])
ax.set_zlabel('Partial dependence')
#
pretty init view
ax.view_init(elev=22, azim=122)
plt.colorbar(surf)
plt.suptitle('Partial dependence of house value on median age and '
'average occupancy')
plt.subplots_adjust(top=0.9)
plt.show()
# Needed on Windows because plot_partial_dependence uses multiprocessing
if __name__ == '__main__':
main()
Total running time of the script: (0 minutes 3.234 seconds)
Download Python source code: plot_partial_dependence.py
Download IPython notebook: plot_partial_dependence.ipynb
828
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
14.15 Discrete versus Real AdaBoost
This example is based on Figure 10.2 from Hastie et al 2009 [1] and illustrates the difference in performance between
the discrete SAMME [2] boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated
on a binary classiﬁcation task where the target Y is a non-linear function of 10 input features.
Discrete SAMME AdaBoost adapts based on errors in predicted class labels whereas real SAMME.R uses the pre-
dicted class probabilities.
print(__doc__)
# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
#
Noel Dawe <noel.dawe@gmail.com>
#
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import zero_one_loss
from sklearn.ensemble import AdaBoostClassifier
14.15. Discrete versus Real AdaBoost
829
scikit-learn user guide, Release 0.18.2
n_estimators = 400
# A learning rate of 1. may not be optimal for both SAMME and SAMME.R
learning_rate = 1.
X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)
X_test, y_test = X[2000:], y[2000:]
X_train, y_train = X[:2000], y[:2000]
dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)
dt_stump.fit(X_train, y_train)
dt_stump_err = 1.0 - dt_stump.score(X_test, y_test)
dt = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)
dt.fit(X_train, y_train)
dt_err = 1.0 - dt.score(X_test, y_test)
ada_discrete = AdaBoostClassifier(
base_estimator=dt_stump,
learning_rate=learning_rate,
n_estimators=n_estimators,
algorithm="SAMME")
ada_discrete.fit(X_train, y_train)
ada_real = AdaBoostClassifier(
base_estimator=dt_stump,
learning_rate=learning_rate,
n_estimators=n_estimators,
algorithm="SAMME.R")
ada_real.fit(X_train, y_train)
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot([1, n_estimators], [dt_stump_err] * 2, 'k-',
label='Decision Stump Error')
ax.plot([1, n_estimators], [dt_err] * 2, 'k--',
label='Decision Tree Error')
ada_discrete_err = np.zeros((n_estimators,))
for i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):
ada_discrete_err[i] = zero_one_loss(y_pred, y_test)
ada_discrete_err_train = np.zeros((n_estimators,))
for i, y_pred in enumerate(ada_discrete.staged_predict(X_train)):
ada_discrete_err_train[i] = zero_one_loss(y_pred, y_train)
ada_real_err = np.zeros((n_estimators,))
for i, y_pred in enumerate(ada_real.staged_predict(X_test)):
ada_real_err[i] = zero_one_loss(y_pred, y_test)
ada_real_err_train = np.zeros((n_estimators,))
for i, y_pred in enumerate(ada_real.staged_predict(X_train)):
ada_real_err_train[i] = zero_one_loss(y_pred, y_train)
ax.plot(np.arange(n_estimators) + 1, ada_discrete_err,
label='Discrete AdaBoost Test Error',
color='red')
830
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
ax.plot(np.arange(n_estimators) + 1, ada_discrete_err_train,
label='Discrete AdaBoost Train Error',
color='blue')
ax.plot(np.arange(n_estimators) + 1, ada_real_err,
label='Real AdaBoost Test Error',
color='orange')
ax.plot(np.arange(n_estimators) + 1, ada_real_err_train,
label='Real AdaBoost Train Error',
color='green')
ax.set_ylim((0.0, 0.5))
ax.set_xlabel('n_estimators')
ax.set_ylabel('error rate')
leg = ax.legend(loc='upper right', fancybox=True)
leg.get_frame().set_alpha(0.7)
plt.show()
Total running time of the script: (0 minutes 6.416 seconds)
Download Python source code: plot_adaboost_hastie_10_2.py
Download IPython notebook: plot_adaboost_hastie_10_2.ipynb
14.16 Multi-class AdaBoosted Decision Trees
This example reproduces Figure 1 of Zhu et al [1] and shows how boosting can improve prediction accuracy on a
multi-class problem. The classiﬁcation dataset is constructed by taking a ten-dimensional standard normal distribution
and deﬁning three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of
samples are in each class (quantiles of the 𝜒2 distribution).
The performance of the SAMME and SAMME.R [1] algorithms are compared. SAMME.R uses the probability
estimates to update the additive model, while SAMME uses the classiﬁcations only. As the example illustrates,
the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting
iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classiﬁcation
error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All
trees have a weight of one in the SAMME.R algorithm and therefore are not shown.
print(__doc__)
14.16. Multi-class AdaBoosted Decision Trees
831
scikit-learn user guide, Release 0.18.2
# Author: Noel Dawe <noel.dawe@gmail.com>
#
# License: BSD 3 clause
from sklearn.externals.six.moves import zip
import matplotlib.pyplot as plt
from sklearn.datasets import make_gaussian_quantiles
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
X, y = make_gaussian_quantiles(n_samples=13000, n_features=10,
n_classes=3, random_state=1)
n_split = 3000
X_train, X_test = X[:n_split], X[n_split:]
y_train, y_test = y[:n_split], y[n_split:]
bdt_real = AdaBoostClassifier(
DecisionTreeClassifier(max_depth=2),
n_estimators=600,
learning_rate=1)
bdt_discrete = AdaBoostClassifier(
DecisionTreeClassifier(max_depth=2),
n_estimators=600,
learning_rate=1.5,
algorithm="SAMME")
bdt_real.fit(X_train, y_train)
bdt_discrete.fit(X_train, y_train)
real_test_errors = []
discrete_test_errors = []
for real_test_predict, discrete_train_predict in zip(
bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):
real_test_errors.append(
1. - accuracy_score(real_test_predict, y_test))
discrete_test_errors.append(
1. - accuracy_score(discrete_train_predict, y_test))
n_trees_discrete = len(bdt_discrete)
n_trees_real = len(bdt_real)
# Boosting might terminate early, but the following arrays are always
# n_estimators long. We crop them to the actual number of trees here:
discrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]
real_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]
discrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]
plt.figure(figsize=(15, 5))
plt.subplot(131)
832
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
plt.plot(range(1, n_trees_discrete + 1),
discrete_test_errors, c='black', label='SAMME')
plt.plot(range(1, n_trees_real + 1),
real_test_errors, c='black',
linestyle='dashed', label='SAMME.R')
plt.legend()
plt.ylim(0.18, 0.62)
plt.ylabel('Test Error')
plt.xlabel('Number of Trees')
plt.subplot(132)
plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,
"b", label='SAMME', alpha=.5)
plt.plot(range(1, n_trees_real + 1), real_estimator_errors,
"r", label='SAMME.R', alpha=.5)
plt.legend()
plt.ylabel('Error')
plt.xlabel('Number of Trees')
plt.ylim((.2,
max(real_estimator_errors.max(),
discrete_estimator_errors.max()) * 1.2))
plt.xlim((-20, len(bdt_discrete) + 20))
plt.subplot(133)
plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,
"b", label='SAMME')
plt.legend()
plt.ylabel('Weight')
plt.xlabel('Number of Trees')
plt.ylim((0, discrete_estimator_weights.max() * 1.2))
plt.xlim((-20, n_trees_discrete + 20))
# prevent overlapping y-axis labels
plt.subplots_adjust(wspace=0.25)
plt.show()
Total running time of the script: (0 minutes 15.034 seconds)
Download Python source code: plot_adaboost_multiclass.py
Download IPython notebook: plot_adaboost_multiclass.ipynb
14.17 Feature transformations with ensembles of trees
Transform your features into a higher dimensional, sparse space. Then train a linear model on these features.
First ﬁt an ensemble of trees (totally random trees, a random forest, or gradient boosted trees) on the training set. Then
each leaf of each tree in the ensemble is assigned a ﬁxed arbitrary feature index in a new feature space. These leaf
indices are then encoded in a one-hot fashion.
Each sample goes through the decisions of each tree of the ensemble and ends up in one leaf per tree. The sample is
encoded by setting feature values for these leaves to 1 and the other feature values to 0.
The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data.
14.17. Feature transformations with ensembles of trees
833
scikit-learn user guide, Release 0.18.2
•
•
# Author: Tim Head <betatim@gmail.com>
#
# License: BSD 3 clause
import numpy as np
np.random.seed(10)
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,
GradientBoostingClassifier)
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.pipeline import make_pipeline
n_estimator = 10
X, y = make_classification(n_samples=80000)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)
# It is important to train the ensemble of trees on a different subset
# of the training data than the linear regression model to avoid
# overfitting, in particular if the total number of leaves is
# similar to the number of training samples
X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train,
y_train,
test_size=0.5)
834
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
# Unsupervised transformation based on totally random trees
rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,
random_state=0)
rt_lm = LogisticRegression()
pipeline = make_pipeline(rt, rt_lm)
pipeline.fit(X_train, y_train)
y_pred_rt = pipeline.predict_proba(X_test)[:, 1]
fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)
# Supervised transformation based on random forests
rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)
rf_enc = OneHotEncoder()
rf_lm = LogisticRegression()
rf.fit(X_train, y_train)
rf_enc.fit(rf.apply(X_train))
rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)
y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]
fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)
grd = GradientBoostingClassifier(n_estimators=n_estimator)
grd_enc = OneHotEncoder()
grd_lm = LogisticRegression()
grd.fit(X_train, y_train)
grd_enc.fit(grd.apply(X_train)[:, :, 0])
grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)
y_pred_grd_lm = grd_lm.predict_proba(
grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]
fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)
# The gradient boosted model by itself
y_pred_grd = grd.predict_proba(X_test)[:, 1]
fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)
# The random forest model by itself
y_pred_rf = rf.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)
plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')
plt.plot(fpr_rf, tpr_rf, label='RF')
plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')
plt.plot(fpr_grd, tpr_grd, label='GBT')
plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
plt.show()
plt.figure(2)
plt.xlim(0, 0.2)
14.17. Feature transformations with ensembles of trees
835
scikit-learn user guide, Release 0.18.2
plt.ylim(0.8, 1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')
plt.plot(fpr_rf, tpr_rf, label='RF')
plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')
plt.plot(fpr_grd, tpr_grd, label='GBT')
plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve (zoomed in at top left)')
plt.legend(loc='best')
plt.show()
Total running time of the script: (0 minutes 2.023 seconds)
Download Python source code: plot_feature_transformation.py
Download IPython notebook: plot_feature_transformation.ipynb
14.18 Gradient Boosting Out-of-Bag estimates
Out-of-bag (OOB) estimates can be a useful heuristic to estimate the “optimal” number of boosting iterations. OOB
estimates are almost identical to cross-validation estimates but they can be computed on-the-ﬂy without the need for
repeated model ﬁtting. OOB estimates are only available for Stochastic Gradient Boosting (i.e. subsample <
1.0), the estimates are derived from the improvement in loss based on the examples not included in the bootstrap
sample (the so-called out-of-bag examples). The OOB estimator is a pessimistic estimator of the true test loss, but
remains a fairly good approximation for a small number of trees.
The ﬁgure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As
you can see, it tracks the test loss for the ﬁrst hundred iterations but then diverges in a pessimistic way. The ﬁgure
also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is
computationally more demanding.
836
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
Out:
Accuracy: 0.6840
print(__doc__)
# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
#
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import ensemble
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
# Generate data (adapted from G. Ridgeway's gbm example)
n_samples = 1000
random_state = np.random.RandomState(13)
14.18. Gradient Boosting Out-of-Bag estimates
837
scikit-learn user guide, Release 0.18.2
x1 = random_state.uniform(size=n_samples)
x2 = random_state.uniform(size=n_samples)
x3 = random_state.randint(0, 4, size=n_samples)
p = 1 / (1.0 + np.exp(-(np.sin(3 * x1) - 4 * x2 + x3)))
y = random_state.binomial(1, p, size=n_samples)
X = np.c_[x1, x2, x3]
X = X.astype(np.float32)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,
random_state=9)
# Fit classifier with out-of-bag estimates
params = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,
'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}
clf = ensemble.GradientBoostingClassifier(**params)
clf.fit(X_train, y_train)
acc = clf.score(X_test, y_test)
print("Accuracy: {:.4f}".format(acc))
n_estimators = params['n_estimators']
x = np.arange(n_estimators) + 1
def heldout_score(clf, X_test, y_test):
"""compute deviance scores on ``X_test`` and ``y_test``. """
score = np.zeros((n_estimators,), dtype=np.float64)
for i, y_pred in enumerate(clf.staged_decision_function(X_test)):
score[i] = clf.loss_(y_test, y_pred)
return score
def cv_estimate(n_splits=3):
cv = KFold(n_splits=n_splits)
cv_clf = ensemble.GradientBoostingClassifier(**params)
val_scores = np.zeros((n_estimators,), dtype=np.float64)
for train, test in cv.split(X_train, y_train):
cv_clf.fit(X_train[train], y_train[train])
val_scores += heldout_score(cv_clf, X_train[test], y_train[test])
val_scores /= n_splits
return val_scores
# Estimate best n_estimator using cross-validation
cv_score = cv_estimate(3)
# Compute best n_estimator for test data
test_score = heldout_score(clf, X_test, y_test)
# negative cumulative sum of oob improvements
cumsum = -np.cumsum(clf.oob_improvement_)
# min loss according to OOB
oob_best_iter = x[np.argmin(cumsum)]
# min loss according to test (normalize such that first loss is 0)
838
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
test_score -= test_score[0]
test_best_iter = x[np.argmin(test_score)]
# min loss according to cv (normalize such that first loss is 0)
cv_score -= cv_score[0]
cv_best_iter = x[np.argmin(cv_score)]
# color brew for the three curves
oob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))
test_color = list(map(lambda x: x / 256.0, (127, 201, 127)))
cv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))
# plot curves and vertical lines for best iterations
plt.plot(x, cumsum, label='OOB loss', color=oob_color)
plt.plot(x, test_score, label='Test loss', color=test_color)
plt.plot(x, cv_score, label='CV loss', color=cv_color)
plt.axvline(x=oob_best_iter, color=oob_color)
plt.axvline(x=test_best_iter, color=test_color)
plt.axvline(x=cv_best_iter, color=cv_color)
# add three vertical lines to xticks
xticks = plt.xticks()
xticks_pos = np.array(xticks[0].tolist() +
[oob_best_iter, cv_best_iter, test_best_iter])
xticks_label = np.array(list(map(lambda t: int(t), xticks[0])) +
['OOB', 'CV', 'Test'])
ind = np.argsort(xticks_pos)
xticks_pos = xticks_pos[ind]
xticks_label = xticks_label[ind]
plt.xticks(xticks_pos, xticks_label)
plt.legend(loc='upper right')
plt.ylabel('normalized loss')
plt.xlabel('number of iterations')
plt.show()
Total running time of the script: (0 minutes 4.261 seconds)
Download Python source code: plot_gradient_boosting_oob.py
Download IPython notebook: plot_gradient_boosting_oob.ipynb
14.19 Single estimator versus bagging: bias-variance decomposition
This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single
estimator against a bagging ensemble.
In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and
noise. On average over datasets of the regression problem, the bias term measures the average amount by which the
predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes
model). The variance term measures the variability of the predictions of the estimator when ﬁt over different instances
LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data.
The upper left ﬁgure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS
(the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision
trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term
14.19. Single estimator versus bagging: bias-variance decomposition
839
scikit-learn user guide, Release 0.18.2
here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the
variance, the more sensitive are the predictions for x to small changes in the training set. The bias term corresponds to
the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On
this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other)
while the variance is large (the red beam is rather wide).
The lower left ﬁgure plots the pointwise decomposition of the expected mean squared error of a single decision tree.
It conﬁrms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of
the error which, as expected, appears to be constant and around 0.01.
The right ﬁgures correspond to the same plots but using instead a bagging ensemble of decision trees. In both ﬁgures,
we can observe that the bias term is larger than in the previous case. In the upper right ﬁgure, the difference between
the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around x=2). In the lower
right ﬁgure, the bias curve is also slightly higher than in the lower left ﬁgure. In terms of variance however, the beam
of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right ﬁgure conﬁrms, the
variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore
no longer the same. The tradeoff is better for bagging: averaging several decision trees ﬁt on bootstrap copies of the
dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall
mean squared error (compare the red curves int the lower ﬁgures). The script output also conﬁrms this intuition. The
total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed
mainly stems from a reduced variance.
For further details on bias-variance decomposition, see section 7.3 of 1.
1 T. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning”, Springer, 2009.
840
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
14.19.1 References
Out:
Tree: 0.0255 (error) = 0.0003 (bias^2)
+ 0.0152 (var) + 0.0098 (noise)
Bagging(Tree): 0.0196 (error) = 0.0004 (bias^2)
+ 0.0092 (var) + 0.0098 (noise)
print(__doc__)
# Author: Gilles Louppe <g.louppe@gmail.com>
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
# Settings
n_repeat = 50
# Number of iterations for computing expectations
14.19. Single estimator versus bagging: bias-variance decomposition
841
scikit-learn user guide, Release 0.18.2
n_train = 50
# Size of the training set
n_test = 1000
# Size of the test set
noise = 0.1
# Standard deviation of the noise
np.random.seed(0)
# Change this for exploring the bias-variance decomposition of other
# estimators. This should work well for estimators with high variance (e.g.,
# decision trees or KNN), but poorly for estimators with low variance (e.g.,
# linear models).
estimators = [("Tree", DecisionTreeRegressor()),
("Bagging(Tree)", BaggingRegressor(DecisionTreeRegressor()))]
n_estimators = len(estimators)
# Generate data
def f(x):
x = x.ravel()
return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)
def generate(n_samples, noise, n_repeat=1):
X = np.random.rand(n_samples) * 10 - 5
X = np.sort(X)
if n_repeat == 1:
y = f(X) + np.random.normal(0.0, noise, n_samples)
else:
y = np.zeros((n_samples, n_repeat))
for i in range(n_repeat):
y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)
X = X.reshape((n_samples, 1))
return X, y
X_train = []
y_train = []
for i in range(n_repeat):
X, y = generate(n_samples=n_train, noise=noise)
X_train.append(X)
y_train.append(y)
X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)
# Loop over estimators to compare
for n, (name, estimator) in enumerate(estimators):
# Compute predictions
y_predict = np.zeros((n_test, n_repeat))
for i in range(n_repeat):
estimator.fit(X_train[i], y_train[i])
y_predict[:, i] = estimator.predict(X_test)
# Bias^2 + Variance + Noise decomposition of the mean squared error
y_error = np.zeros(n_test)
842
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
for i in range(n_repeat):
for j in range(n_repeat):
y_error += (y_test[:, j] - y_predict[:, i]) ** 2
y_error /= (n_repeat * n_repeat)
y_noise = np.var(y_test, axis=1)
y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2
y_var = np.var(y_predict, axis=1)
print("{0}: {1:.4f} (error) = {2:.4f} (bias^2) "
" + {3:.4f} (var) + {4:.4f} (noise)".format(name,
np.mean(y_error),
np.mean(y_bias),
np.mean(y_var),
np.mean(y_noise)))
# Plot figures
plt.subplot(2, n_estimators, n + 1)
plt.plot(X_test, f(X_test), "b", label="$f(x)$")
plt.plot(X_train[0], y_train[0], ".b", label="LS ~ $y = f(x)+noise$")
for i in range(n_repeat):
if i == 0:
plt.plot(X_test, y_predict[:, i], "r", label="$\^y(x)$")
else:
plt.plot(X_test, y_predict[:, i], "r", alpha=0.05)
plt.plot(X_test, np.mean(y_predict, axis=1), "c",
label="$\mathbb{E}_{LS} \^y(x)$")
plt.xlim([-5, 5])
plt.title(name)
if n == 0:
plt.legend(loc="upper left", prop={"size": 11})
plt.subplot(2, n_estimators, n_estimators + n + 1)
plt.plot(X_test, y_error, "r", label="$error(x)$")
plt.plot(X_test, y_bias, "b", label="$bias^2(x)$"),
plt.plot(X_test, y_var, "g", label="$variance(x)$"),
plt.plot(X_test, y_noise, "c", label="$noise(x)$")
plt.xlim([-5, 5])
plt.ylim([0, 0.1])
if n == 0:
plt.legend(loc="upper left", prop={"size": 11})
plt.show()
Total running time of the script: (0 minutes 1.765 seconds)
Download Python source code: plot_bias_variance.py
Download IPython notebook: plot_bias_variance.ipynb
14.19. Single estimator versus bagging: bias-variance decomposition
843
scikit-learn user guide, Release 0.18.2
14.20 Plot the decision surfaces of ensembles of trees on the iris
dataset
Plot the decision surfaces of forests of randomized trees trained on pairs of features of the iris dataset.
This plot compares the decision surfaces learned by a decision tree classiﬁer (ﬁrst column), by a random forest classi-
ﬁer (second column), by an extra- trees classiﬁer (third column) and by an AdaBoost classiﬁer (fourth column).
In the ﬁrst row, the classiﬁers are built using the sepal width and the sepal length features only, on the second row
using the petal length and sepal length only, and on the third row using the petal width and the petal length only.
In descending order of quality, when trained (outside of this example) on all 4 features using 30 estimators and scored
using 10 fold cross validation, we see:
ExtraTreesClassifier()
# 0.95 score
RandomForestClassifier()
# 0.94 score
AdaBoost(DecisionTree(max_depth=3))
# 0.94 score
DecisionTree(max_depth=None)
# 0.94 score
Increasing max_depth for AdaBoost lowers the standard deviation of the scores (but the average score does not im-
prove).
See the console’s output for further details about each model.
In this example you might try to:
1. vary
the
max_depth
for
the
DecisionTreeClassifier
and
AdaBoostClassifier,
perhaps
try
max_depth=3
for
the
DecisionTreeClassifier
or
max_depth=None
for
AdaBoostClassifier
2. vary n_estimators
It is worth noting that RandomForests and ExtraTrees can be ﬁtted in parallel on many cores as each tree is built
independently of the others. AdaBoost’s samples are built sequentially and so do not use multiple cores.
844
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
Out:
DecisionTree with features [0, 1] has a score of 0.926666666667
RandomForest with 30 estimators with features [0, 1] has a score of 0.926666666667
ExtraTrees with 30 estimators with features [0, 1] has a score of 0.926666666667
AdaBoost with 30 estimators with features [0, 1] has a score of 0.84
DecisionTree with features [0, 2] has a score of 0.993333333333
RandomForest with 30 estimators with features [0, 2] has a score of 0.993333333333
ExtraTrees with 30 estimators with features [0, 2] has a score of 0.993333333333
AdaBoost with 30 estimators with features [0, 2] has a score of 0.993333333333
DecisionTree with features [2, 3] has a score of 0.993333333333
RandomForest with 30 estimators with features [2, 3] has a score of 0.993333333333
ExtraTrees with 30 estimators with features [2, 3] has a score of 0.993333333333
AdaBoost with 30 estimators with features [2, 3] has a score of 0.993333333333
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import clone
14.20. Plot the decision surfaces of ensembles of trees on the iris dataset
845
scikit-learn user guide, Release 0.18.2
from sklearn.datasets import load_iris
from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,
AdaBoostClassifier)
from sklearn.externals.six.moves import xrange
from sklearn.tree import DecisionTreeClassifier
# Parameters
n_classes = 3
n_estimators = 30
plot_colors = "ryb"
cmap = plt.cm.RdYlBu
plot_step = 0.02
# fine step width for decision surface contours
plot_step_coarser = 0.5
# step widths for coarse classifier guesses
RANDOM_SEED = 13
# fix the seed on each iteration
# Load data
iris = load_iris()
plot_idx = 1
models = [DecisionTreeClassifier(max_depth=None),
RandomForestClassifier(n_estimators=n_estimators),
ExtraTreesClassifier(n_estimators=n_estimators),
AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),
n_estimators=n_estimators)]
for pair in ([0, 1], [0, 2], [2, 3]):
for model in models:
# We only take the two corresponding features
X = iris.data[:, pair]
y = iris.target
# Shuffle
idx = np.arange(X.shape[0])
np.random.seed(RANDOM_SEED)
np.random.shuffle(idx)
X = X[idx]
y = y[idx]
# Standardize
mean = X.mean(axis=0)
std = X.std(axis=0)
X = (X - mean) / std
# Train
clf = clone(model)
clf = model.fit(X, y)
scores = clf.score(X, y)
# Create a title for each column and the console by using str() and
# slicing away useless parts of the string
model_title = str(type(model)).split(".")[-1][:-2][:-len("Classifier")]
model_details = model_title
if hasattr(model, "estimators_"):
model_details += " with {} estimators".format(len(model.estimators_))
print( model_details + " with features", pair, "has a score of", scores )
plt.subplot(3, 4, plot_idx)
846
Chapter 14. Ensemble methods
scikit-learn user guide, Release 0.18.2
if plot_idx <= len(models):
# Add a title at the top of each column
plt.title(model_title)
# Now plot the decision boundary using a fine mesh as input to a
# filled contour plot
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
np.arange(y_min, y_max, plot_step))
# Plot either a single DecisionTreeClassifier or alpha blend the
# decision surfaces of the ensemble of classifiers
if isinstance(model, DecisionTreeClassifier):
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=cmap)
else:
# Choose alpha blend level with respect to the number of estimators
# that are in use (noting that AdaBoost can use fewer estimators
# than its maximum if it achieves a good enough fit early on)
estimator_alpha = 1.0 / len(model.estimators_)
for tree in model.estimators_:
Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)
# Build a coarser grid to plot a set of ensemble classifications
# to show how these are different to what we see in the decision
# surfaces. These points are regularly space and do not have a black outline
xx_coarser, yy_coarser = np.meshgrid(np.arange(x_min, x_max, plot_step_
˓→coarser),
np.arange(y_min, y_max, plot_step_
˓→coarser))
Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(), yy_coarser.
˓→ravel()]).reshape(xx_coarser.shape)
cs_points = plt.scatter(xx_coarser, yy_coarser, s=15, c=Z_points_coarser,
˓→cmap=cmap, edgecolors="none")
# Plot the training points, these are clustered together and have a
# black outline
for i, c in zip(xrange(n_classes), plot_colors):
idx = np.where(y == i)
plt.scatter(X[idx, 0], X[idx, 1], c=c, label=iris.target_names[i],
cmap=cmap)
plot_idx += 1
# move on to the next plot in sequence
plt.suptitle("Classifiers on feature subsets of the Iris dataset")
plt.axis("tight")
plt.show()
Total running time of the script: (0 minutes 10.421 seconds)
Download Python source code: plot_forest_iris.py
Download IPython notebook: plot_forest_iris.ipynb
14.20. Plot the decision surfaces of ensembles of trees on the iris dataset
847
scikit-learn user guide, Release 0.18.2
848
Chapter 14. Ensemble methods
CHAPTER
FIFTEEN
TUTORIAL EXERCISES
Exercises for the tutorials
15.1 Digits Classiﬁcation Exercise
A tutorial exercise regarding the use of classiﬁcation techniques on the Digits dataset.
This exercise is used in the Classiﬁcation part of the Supervised learning: predicting an output variable from high-
dimensional observations section of the A tutorial on statistical-learning for scientiﬁc data processing.
print(__doc__)
from sklearn import datasets, neighbors, linear_model
digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target
n_samples = len(X_digits)
X_train = X_digits[:.9 * n_samples]
y_train = y_digits[:.9 * n_samples]
X_test = X_digits[.9 * n_samples:]
y_test = y_digits[.9 * n_samples:]
knn = neighbors.KNeighborsClassifier()
logistic = linear_model.LogisticRegression()
print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))
print('LogisticRegression score: %f'
% logistic.fit(X_train, y_train).score(X_test, y_test))
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: digits_classification_exercise.py
Download IPython notebook: digits_classification_exercise.ipynb
15.2 Cross-validation on Digits Dataset Exercise
A tutorial exercise using Cross-validation with an SVM on the Digits dataset.
849
scikit-learn user guide, Release 0.18.2
This exercise is used in the Cross-validation generators part of the Model selection: choosing estimators and their
parameters section of the A tutorial on statistical-learning for scientiﬁc data processing.
print(__doc__)
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn import datasets, svm
digits = datasets.load_digits()
X = digits.data
y = digits.target
svc = svm.SVC(kernel='linear')
C_s = np.logspace(-10, 0, 10)
scores = list()
scores_std = list()
for C in C_s:
svc.C = C
this_scores = cross_val_score(svc, X, y, n_jobs=1)
scores.append(np.mean(this_scores))
scores_std.append(np.std(this_scores))
# Do the plotting
import matplotlib.pyplot as plt
plt.figure(1, figsize=(4, 3))
plt.clf()
plt.semilogx(C_s, scores)
plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')
plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')
locs, labels = plt.yticks()
plt.yticks(locs, list(map(lambda x: "%g" % x, locs)))
plt.ylabel('CV score')
plt.xlabel('Parameter C')
plt.ylim(0, 1.1)
plt.show()
850
Chapter 15. Tutorial exercises
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 5.325 seconds)
Download Python source code: plot_cv_digits.py
Download IPython notebook: plot_cv_digits.ipynb
15.3 SVM Exercise
A tutorial exercise for using different SVM kernels.
This exercise is used in the Using kernels part of the Supervised learning: predicting an output variable from high-
dimensional observations section of the A tutorial on statistical-learning for scientiﬁc data processing.
•
•
•
15.3. SVM Exercise
851
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, svm
iris = datasets.load_iris()
X = iris.data
y = iris.target
X = X[y != 0, :2]
y = y[y != 0]
n_sample = len(X)
np.random.seed(0)
order = np.random.permutation(n_sample)
X = X[order]
y = y[order].astype(np.float)
X_train = X[:int(.9 * n_sample)]
y_train = y[:int(.9 * n_sample)]
X_test = X[int(.9 * n_sample):]
y_test = y[int(.9 * n_sample):]
# fit the model
for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
clf = svm.SVC(kernel=kernel, gamma=10)
clf.fit(X_train, y_train)
plt.figure(fig_num)
plt.clf()
plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired)
# Circle out the test data
plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)
plt.axis('tight')
x_min = X[:, 0].min()
x_max = X[:, 0].max()
y_min = X[:, 1].min()
y_max = X[:, 1].max()
XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])
# Put the result into a color plot
Z = Z.reshape(XX.shape)
plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)
plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],
linestyles=['--', '-', '--'], levels=[-.5, 0, .5])
plt.title(kernel)
plt.show()
Total running time of the script: (0 minutes 6.988 seconds)
852
Chapter 15. Tutorial exercises
scikit-learn user guide, Release 0.18.2
Download Python source code: plot_iris_exercise.py
Download IPython notebook: plot_iris_exercise.ipynb
15.4 Cross-validation on diabetes Dataset Exercise
A tutorial exercise which uses cross-validation with linear models.
This exercise is used in the Cross-validated estimators part of the Model selection: choosing estimators and their
parameters section of the A tutorial on statistical-learning for scientiﬁc data processing.
from __future__ import print_function
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.linear_model import LassoCV
from sklearn.linear_model import Lasso
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
diabetes = datasets.load_diabetes()
X = diabetes.data[:150]
y = diabetes.target[:150]
lasso = Lasso(random_state=0)
alphas = np.logspace(-4, -0.5, 30)
scores = list()
scores_std = list()
n_folds = 3
for alpha in alphas:
lasso.alpha = alpha
this_scores = cross_val_score(lasso, X, y, cv=n_folds, n_jobs=1)
scores.append(np.mean(this_scores))
scores_std.append(np.std(this_scores))
scores, scores_std = np.array(scores), np.array(scores_std)
plt.figure().set_size_inches(8, 6)
plt.semilogx(alphas, scores)
# plot error lines showing +/- std. errors of the scores
std_error = scores_std / np.sqrt(n_folds)
plt.semilogx(alphas, scores + std_error, 'b--')
plt.semilogx(alphas, scores - std_error, 'b--')
# alpha=0.2 controls the translucency of the fill color
plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)
plt.ylabel('CV score +/- std error')
plt.xlabel('alpha')
15.4. Cross-validation on diabetes Dataset Exercise
853
scikit-learn user guide, Release 0.18.2
plt.axhline(np.max(scores), linestyle='--', color='.5')
plt.xlim([alphas[0], alphas[-1]])
Bonus: how much can you trust the selection of alpha?
# To answer this question we use the LassoCV object that sets its alpha
# parameter automatically from the data by internal cross-validation (i.e. it
# performs cross-validation on the training data it receives).
# We use external cross-validation to see how much the automatically obtained
# alphas differ across different cross-validation folds.
lasso_cv = LassoCV(alphas=alphas, random_state=0)
k_fold = KFold(3)
print("Answer to the bonus question:",
"how much can you trust the selection of alpha?")
print()
print("Alpha parameters maximising the generalization score on different")
print("subsets of the data:")
for k, (train, test) in enumerate(k_fold.split(X, y)):
lasso_cv.fit(X[train], y[train])
print("[fold {0}] alpha: {1:.5f}, score: {2:.5f}".
format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])))
print()
print("Answer: Not very much since we obtained different alphas for different")
print("subsets of the data and moreover, the scores for these alphas differ")
854
Chapter 15. Tutorial exercises
scikit-learn user guide, Release 0.18.2
print("quite substantially.")
plt.show()
Out:
Answer to the bonus question: how much can you trust the selection of alpha?
Alpha parameters maximising the generalization score on different
subsets of the data:
[fold 0] alpha: 0.10405, score: 0.53573
[fold 1] alpha: 0.05968, score: 0.16278
[fold 2] alpha: 0.10405, score: 0.44437
Answer: Not very much since we obtained different alphas for different
subsets of the data and moreover, the scores for these alphas differ
quite substantially.
Total running time of the script: (0 minutes 0.457 seconds)
Download Python source code: plot_cv_diabetes.py
Download IPython notebook: plot_cv_diabetes.ipynb
15.4. Cross-validation on diabetes Dataset Exercise
855
scikit-learn user guide, Release 0.18.2
856
Chapter 15. Tutorial exercises
CHAPTER
SIXTEEN
FEATURE SELECTION
Examples concerning the sklearn.feature_selection module.
16.1 Pipeline Anova SVM
Simple usage of Pipeline that runs successively a univariate feature selection with anova and then a C-SVM of the
selected features.
print(__doc__)
from sklearn import svm
from sklearn.datasets import samples_generator
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.pipeline import make_pipeline
# import some data to play with
X, y = samples_generator.make_classification(
n_features=20, n_informative=3, n_redundant=0, n_classes=4,
n_clusters_per_class=2)
# ANOVA SVM-C
# 1) anova filter, take 3 best ranked features
anova_filter = SelectKBest(f_regression, k=3)
# 2) svm
clf = svm.SVC(kernel='linear')
anova_svm = make_pipeline(anova_filter, clf)
anova_svm.fit(X, y)
anova_svm.predict(X)
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: feature_selection_pipeline.py
Download IPython notebook: feature_selection_pipeline.ipynb
16.2 Recursive feature elimination
A recursive feature elimination example showing the relevance of pixels in a digit classiﬁcation task.
857
scikit-learn user guide, Release 0.18.2
Note: See also Recursive feature elimination with cross-validation
print(__doc__)
from sklearn.svm import SVC
from sklearn.datasets import load_digits
from sklearn.feature_selection import RFE
import matplotlib.pyplot as plt
# Load the digits dataset
digits = load_digits()
X = digits.images.reshape((len(digits.images), -1))
y = digits.target
# Create the RFE object and rank each pixel
svc = SVC(kernel="linear", C=1)
858
Chapter 16. Feature Selection
scikit-learn user guide, Release 0.18.2
rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
rfe.fit(X, y)
ranking = rfe.ranking_.reshape(digits.images[0].shape)
# Plot pixel ranking
plt.matshow(ranking, cmap=plt.cm.Blues)
plt.colorbar()
plt.title("Ranking of pixels with RFE")
plt.show()
Total running time of the script: (0 minutes 4.869 seconds)
Download Python source code: plot_rfe_digits.py
Download IPython notebook: plot_rfe_digits.ipynb
16.3 Comparison of F-test and mutual information
This example illustrates the differences between univariate F-test statistics and mutual information.
We consider 3 features x_1, x_2, x_3 distributed uniformly over [0, 1], the target depends on them as follows:
y = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1), that is the third features is completely irrelevant.
The code below plots the dependency of y against individual x_i and normalized values of univariate F-tests statistics
and mutual information.
As F-test captures only linear dependency, it rates x_1 as the most discriminative feature. On the other hand, mutual
information can capture any kind of dependency between variables and it rates x_2 as the most discriminative feature,
which probably agrees better with our intuitive perception for this example. Both methods correctly marks x_3 as
irrelevant.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_selection import f_regression, mutual_info_regression
np.random.seed(0)
X = np.random.rand(1000, 3)
y = X[:, 0] + np.sin(6 * np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)
f_test, _ = f_regression(X, y)
16.3. Comparison of F-test and mutual information
859
scikit-learn user guide, Release 0.18.2
f_test /= np.max(f_test)
mi = mutual_info_regression(X, y)
mi /= np.max(mi)
plt.figure(figsize=(15, 5))
for i in range(3):
plt.subplot(1, 3, i + 1)
plt.scatter(X[:, i], y)
plt.xlabel("$x_{}$".format(i + 1), fontsize=14)
if i == 0:
plt.ylabel("$y$", fontsize=14)
plt.title("F-test={:.2f}, MI={:.2f}".format(f_test[i], mi[i]),
fontsize=16)
plt.show()
Total running time of the script: (0 minutes 0.226 seconds)
Download Python source code: plot_f_test_vs_mi.py
Download IPython notebook: plot_f_test_vs_mi.ipynb
16.4 Recursive feature elimination with cross-validation
A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.
860
Chapter 16. Feature Selection
scikit-learn user guide, Release 0.18.2
Out:
Optimal number of features : 3
print(__doc__)
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.datasets import make_classification
# Build a classification task using 3 informative features
X, y = make_classification(n_samples=1000, n_features=25, n_informative=3,
n_redundant=2, n_repeated=0, n_classes=8,
n_clusters_per_class=1, random_state=0)
# Create the RFE object and compute a cross-validated score.
svc = SVC(kernel="linear")
# The "accuracy" scoring is proportional to the number of correct
# classifications
16.4. Recursive feature elimination with cross-validation
861
scikit-learn user guide, Release 0.18.2
rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),
scoring='accuracy')
rfecv.fit(X, y)
print("Optimal number of features : %d" % rfecv.n_features_)
# Plot number of features VS. cross-validation scores
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross validation score (nb of correct classifications)")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()
Total running time of the script: (0 minutes 2.340 seconds)
Download Python source code: plot_rfe_with_cross_validation.py
Download IPython notebook: plot_rfe_with_cross_validation.ipynb
16.5 Feature selection using SelectFromModel and LassoCV
Use SelectFromModel meta-transformer along with Lasso to select the best couple of features from the Boston dataset.
862
Chapter 16. Feature Selection
scikit-learn user guide, Release 0.18.2
# Author: Manoj Kumar <mks542@nyu.edu>
# License: BSD 3 clause
print(__doc__)
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_boston
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LassoCV
# Load the boston dataset.
boston = load_boston()
X, y = boston['data'], boston['target']
# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.
clf = LassoCV()
# Set a minimum threshold of 0.25
sfm = SelectFromModel(clf, threshold=0.25)
sfm.fit(X, y)
n_features = sfm.transform(X).shape[1]
# Reset the threshold till the number of features equals two.
# Note that the attribute can be set directly instead of repeatedly
# fitting the metatransformer.
while n_features > 2:
sfm.threshold += 0.1
X_transform = sfm.transform(X)
n_features = X_transform.shape[1]
# Plot the selected two features from X.
plt.title(
"Features selected from Boston using SelectFromModel with "
"threshold %0.3f." % sfm.threshold)
feature1 = X_transform[:, 0]
feature2 = X_transform[:, 1]
plt.plot(feature1, feature2, 'r.')
plt.xlabel("Feature number 1")
plt.ylabel("Feature number 2")
plt.ylim([np.min(feature2), np.max(feature2)])
plt.show()
Total running time of the script: (0 minutes 0.105 seconds)
Download Python source code: plot_select_from_model_boston.py
Download IPython notebook: plot_select_from_model_boston.ipynb
16.6 Univariate Feature Selection
An example showing univariate feature selection.
Noisy (non informative) features are added to the iris data and univariate feature selection is applied. For each feature,
we plot the p-values for the univariate feature selection and the corresponding weights of an SVM. We can see that
univariate feature selection selects the informative features and that these have larger SVM weights.
16.6. Univariate Feature Selection
863
scikit-learn user guide, Release 0.18.2
In the total set of features, only the 4 ﬁrst ones are signiﬁcant. We can see that they have the highest score with
univariate feature selection. The SVM assigns a large weight to one of these features, but also Selects many of the
non-informative features. Applying univariate feature selection before the SVM increases the SVM weight attributed
to the signiﬁcant features, and will thus improve classiﬁcation.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, svm
from sklearn.feature_selection import SelectPercentile, f_classif
import some data to play with
# The iris dataset
iris = datasets.load_iris()
# Some noisy data not correlated
E = np.random.uniform(0, 0.1, size=(len(iris.data), 20))
# Add the noisy data to the informative features
X = np.hstack((iris.data, E))
y = iris.target
plt.figure(1)
plt.clf()
X_indices = np.arange(X.shape[-1])
864
Chapter 16. Feature Selection
scikit-learn user guide, Release 0.18.2
Univariate feature selection with F-test for feature scoring We use the default selection function: the 10% most signif-
icant features
selector = SelectPercentile(f_classif, percentile=10)
selector.fit(X, y)
scores = -np.log10(selector.pvalues_)
scores /= scores.max()
plt.bar(X_indices - .45, scores, width=.2,
label=r'Univariate score ($-Log(p_{value})$)', color='darkorange')
16.6. Univariate Feature Selection
865
scikit-learn user guide, Release 0.18.2
Compare to the weights of an SVM
clf = svm.SVC(kernel='linear')
clf.fit(X, y)
svm_weights = (clf.coef_ ** 2).sum(axis=0)
svm_weights /= svm_weights.max()
plt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight',
color='navy')
clf_selected = svm.SVC(kernel='linear')
clf_selected.fit(selector.transform(X), y)
svm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)
svm_weights_selected /= svm_weights_selected.max()
plt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,
width=.2, label='SVM weights after selection', color='c')
plt.title("Comparing feature selection")
plt.xlabel('Feature number')
plt.yticks(())
plt.axis('tight')
plt.legend(loc='upper right')
866
Chapter 16. Feature Selection
scikit-learn user guide, Release 0.18.2
plt.show()
Total running time of the script: (0 minutes 0.233 seconds)
Download Python source code: plot_feature_selection.py
Download IPython notebook: plot_feature_selection.ipynb
16.7 Test with permutations the signiﬁcance of a classiﬁcation score
In order to test if a classiﬁcation score is signiﬁcative a technique in repeating the classiﬁcation procedure after ran-
domizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is
greater than the classiﬁcation score obtained in the ﬁrst place.
# Author:
Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
16.7. Test with permutations the signiﬁcance of a classiﬁcation score
867
scikit-learn user guide, Release 0.18.2
from sklearn.model_selection import permutation_test_score
from sklearn import datasets
Loading a dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target
n_classes = np.unique(y).size
# Some noisy data not correlated
random = np.random.RandomState(seed=0)
E = random.normal(size=(len(X), 2200))
# Add noisy data to the informative features for make the task harder
X = np.c_[X, E]
svm = SVC(kernel='linear')
cv = StratifiedKFold(2)
score, permutation_scores, pvalue = permutation_test_score(
svm, X, y, scoring="accuracy", cv=cv, n_permutations=100, n_jobs=1)
print("Classification score %s (pvalue : %s)" % (score, pvalue))
Out:
Classification score 0.513333333333 (pvalue : 0.00990099009901)
View histogram of permutation scores
plt.hist(permutation_scores, 20, label='Permutation scores')
ylim = plt.ylim()
# BUG: vlines(..., linestyle='--') fails on older versions of matplotlib
#plt.vlines(score, ylim[0], ylim[1], linestyle='--',
#
color='g', linewidth=3, label='Classification Score'
#
' (pvalue %s)' % pvalue)
#plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',
#
color='k', linewidth=3, label='Luck')
plt.plot(2 * [score], ylim, '--g', linewidth=3,
label='Classification Score'
' (pvalue %s)' % pvalue)
plt.plot(2 * [1. / n_classes], ylim, '--k', linewidth=3, label='Luck')
plt.ylim(ylim)
plt.legend()
plt.xlabel('Score')
plt.show()
868
Chapter 16. Feature Selection
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 6.622 seconds)
Download Python source code: plot_permutation_test_for_classification.py
Download IPython notebook: plot_permutation_test_for_classification.ipynb
16.7. Test with permutations the signiﬁcance of a classiﬁcation score
869
scikit-learn user guide, Release 0.18.2
870
Chapter 16. Feature Selection
CHAPTER
SEVENTEEN
GAUSSIAN PROCESS FOR MACHINE LEARNING
Examples concerning the sklearn.gaussian_process module.
17.1 Illustration of Gaussian process classiﬁcation (GPC) on the XOR
dataset
This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary
kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because the
class-boundaries are linear and coincide with the coordinate axes. In general, stationary kernels often obtain better
results.
print(__doc__)
# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
#
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
871
scikit-learn user guide, Release 0.18.2
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF, DotProduct
xx, yy = np.meshgrid(np.linspace(-3, 3, 50),
np.linspace(-3, 3, 50))
rng = np.random.RandomState(0)
X = rng.randn(200, 2)
Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)
# fit the model
plt.figure(figsize=(10, 5))
kernels = [1.0 * RBF(length_scale=1.0), 1.0 * DotProduct(sigma_0=1.0)**2]
for i, kernel in enumerate(kernels):
clf = GaussianProcessClassifier(kernel=kernel, warm_start=True).fit(X, Y)
# plot the decision function for each datapoint on the grid
Z = clf.predict_proba(np.vstack((xx.ravel(), yy.ravel())).T)[:, 1]
Z = Z.reshape(xx.shape)
plt.subplot(1, 2, i + 1)
image = plt.imshow(Z, interpolation='nearest',
extent=(xx.min(), xx.max(), yy.min(), yy.max()),
aspect='auto', origin='lower', cmap=plt.cm.PuOr_r)
contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,
linetypes='--')
plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired)
plt.xticks(())
plt.yticks(())
plt.axis([-3, 3, -3, 3])
plt.colorbar(image)
plt.title("%s\n Log-Marginal-Likelihood:%.3f"
% (clf.kernel_, clf.log_marginal_likelihood(clf.kernel_.theta)),
fontsize=12)
plt.tight_layout()
plt.show()
Total running time of the script: (0 minutes 4.654 seconds)
Download Python source code: plot_gpc_xor.py
Download IPython notebook: plot_gpc_xor.ipynb
17.2 Gaussian process classiﬁcation (GPC) on iris dataset
This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-
dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood
by assigning different length-scales to the two feature dimensions.
872
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]
# we only take the first two features.
y = np.array(iris.target, dtype=int)
h = .02
# step size in the mesh
kernel = 1.0 * RBF([1.0])
gpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)
kernel = 1.0 * RBF([1.0, 1.0])
gpc_rbf_anisotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)
# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
titles = ["Isotropic RBF", "Anisotropic RBF"]
plt.figure(figsize=(10, 5))
for i, clf in enumerate((gpc_rbf_isotropic, gpc_rbf_anisotropic)):
# Plot the predicted probabilities. For that, we will assign a color to
# each point in the mesh [x_min, m_max]x[y_min, y_max].
plt.subplot(1, 2, i + 1)
Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
17.2. Gaussian process classiﬁcation (GPC) on iris dataset
873
scikit-learn user guide, Release 0.18.2
Z = Z.reshape((xx.shape[0], xx.shape[1], 3))
plt.imshow(Z, extent=(x_min, x_max, y_min, y_max), origin="lower")
# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=np.array(["r", "g", "b"])[y])
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())
plt.title("%s, LML: %.3f" %
(titles[i], clf.log_marginal_likelihood(clf.kernel_.theta)))
plt.tight_layout()
plt.show()
Total running time of the script: (1 minutes 5.438 seconds)
Download Python source code: plot_gpc_iris.py
Download IPython notebook: plot_gpc_iris.ipynb
17.3 Comparison of kernel ridge and Gaussian process regression
Both kernel ridge regression (KRR) and Gaussian process regression (GPR) learn a target function by employing
internally the “kernel trick”. KRR learns a linear function in the space induced by the respective kernel which corre-
sponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the
mean-squared error loss with ridge regularization. GPR uses the kernel to deﬁne the covariance of a prior distribution
over the target functions and uses the observed training data to deﬁne a likelihood function. Based on Bayes theorem,
a (Gaussian) posterior distribution over target functions is deﬁned, whose mean is used for prediction.
A major difference is that GPR can choose the kernel’s hyperparameters based on gradient-ascent on the marginal
likelihood function while KRR needs to perform a grid search on a cross-validated loss function (mean-squared error
loss). A further difference is that GPR learns a generative, probabilistic model of the target function and can thus
provide meaningful conﬁdence intervals and posterior samples along with the predictions while KRR only provides
predictions.
This example illustrates both methods on an artiﬁcial dataset, which consists of a sinusoidal target function and strong
noise. The ﬁgure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited
for learning periodic functions. The kernel’s hyperparameters control the smoothness (l) and periodicity of the kernel
(p). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the
kernel and by the regularization parameter alpha of KRR.
The ﬁgure shows that both methods learn reasonable models of the target function. GPR correctly identiﬁes the peri-
odicity of the function to be roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides that, GPR
provides reasonable conﬁdence bounds on the prediction which are not available for KRR. A major difference between
the two methods is the time required for ﬁtting and predicting: while ﬁtting KRR is fast in principle, the grid-search
for hyperparameter optimization scales exponentially with the number of hyperparameters (“curse of dimensional-
ity”). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is
thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar;
however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting
the mean.
874
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
Out:
Time for KRR fitting: 10.733
Time for GPR fitting: 0.227
Time for KRR prediction: 0.078
Time for GPR prediction: 0.091
Time for GPR prediction with standard-deviation: 0.381
print(__doc__)
# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
# License: BSD 3 clause
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import GridSearchCV
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared
rng = np.random.RandomState(0)
# Generate sample data
X = 15 * rng.rand(100, 1)
y = np.sin(X).ravel()
y += 3 * (0.5 - rng.rand(X.shape[0]))
# add noise
17.3. Comparison of kernel ridge and Gaussian process regression
875
scikit-learn user guide, Release 0.18.2
# Fit KernelRidge with parameter selection based on 5-fold cross validation
param_grid = {"alpha": [1e0, 1e-1, 1e-2, 1e-3],
"kernel": [ExpSineSquared(l, p)
for l in np.logspace(-2, 2, 10)
for p in np.logspace(0, 2, 10)]}
kr = GridSearchCV(KernelRidge(), cv=5, param_grid=param_grid)
stime = time.time()
kr.fit(X, y)
print("Time for KRR fitting: %.3f" % (time.time() - stime))
gp_kernel = ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) \
+ WhiteKernel(1e-1)
gpr = GaussianProcessRegressor(kernel=gp_kernel)
stime = time.time()
gpr.fit(X, y)
print("Time for GPR fitting: %.3f" % (time.time() - stime))
# Predict using kernel ridge
X_plot = np.linspace(0, 20, 10000)[:, None]
stime = time.time()
y_kr = kr.predict(X_plot)
print("Time for KRR prediction: %.3f" % (time.time() - stime))
# Predict using kernel ridge
stime = time.time()
y_gpr = gpr.predict(X_plot, return_std=False)
print("Time for GPR prediction: %.3f" % (time.time() - stime))
stime = time.time()
y_gpr, y_std = gpr.predict(X_plot, return_std=True)
print("Time for GPR prediction with standard-deviation: %.3f"
% (time.time() - stime))
# Plot results
plt.figure(figsize=(10, 5))
lw = 2
plt.scatter(X, y, c='k', label='data')
plt.plot(X_plot, np.sin(X_plot), color='navy', lw=lw, label='True')
plt.plot(X_plot, y_kr, color='turquoise', lw=lw,
label='KRR (%s)' % kr.best_params_)
plt.plot(X_plot, y_gpr, color='darkorange', lw=lw,
label='GPR (%s)' % gpr.kernel_)
plt.fill_between(X_plot[:, 0], y_gpr - y_std, y_gpr + y_std, color='darkorange',
alpha=0.2)
plt.xlabel('data')
plt.ylabel('target')
plt.xlim(0, 20)
plt.ylim(-4, 4)
plt.title('GPR versus Kernel Ridge')
plt.legend(loc="best",
scatterpoints=1, prop={'size': 8})
plt.show()
Total running time of the script: (0 minutes 11.581 seconds)
Download Python source code: plot_compare_gpr_krr.py
Download IPython notebook: plot_compare_gpr_krr.ipynb
876
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
17.4 Gaussian process regression (GPR) on Mauna Loa CO2 data.
This example is based on Section 5.4.3 of “Gaussian Processes for Machine Learning” [RW2006]. It illustrates an
example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-
likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume
(ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the
CO2 concentration as a function of the time t.
The kernel is composed of several terms that are responsible for explaining different properties of the signal:
• a long term, smooth rising trend is to be explained by an RBF kernel. The RBF kernel with a large length-scale
enforces this component to be smooth; it is not enforced that the trend is rising which leaves this choice to the
GP. The speciﬁc length-scale and the amplitude are free hyperparameters.
• a seasonal component, which is to be explained by the periodic ExpSineSquared kernel with a ﬁxed periodicity
of 1 year. The length-scale of this periodic component, controlling its smoothness, is a free parameter. In order
to allow decaying away from exact periodicity, the product with an RBF kernel is taken. The length-scale of this
RBF component controls the decay time and is a further free parameter.
• smaller, medium term irregularities are to be explained by a RationalQuadratic kernel component, whose length-
scale and alpha parameter, which determines the diffuseness of the length-scales, are to be determined. Ac-
cording to [RW2006], these irregularities can better be explained by a RationalQuadratic than an RBF kernel
component, probably because it can accommodate several length-scales.
• a “noise” term, consisting of an RBF kernel contribution, which shall explain the correlated noise components
such as local weather phenomena, and a WhiteKernel contribution for the white noise. The relative amplitudes
and the RBF’s length scale are further free parameters.
Maximizing the log-marginal-likelihood after subtracting the target’s mean yields the following kernel with an LML
of -83.214:
34.4**2 * RBF(length_scale=41.8)
+ 3.27**2 * RBF(length_scale=180) * ExpSineSquared(length_scale=1.44,
periodicity=1)
+ 0.446**2 * RationalQuadratic(alpha=17.7, length_scale=0.957)
+ 0.197**2 * RBF(length_scale=0.138) + WhiteKernel(noise_level=0.0336)
Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The
periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long
decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an
amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the
overall noise level is very small, indicating that the data can be very well explained by the model. The ﬁgure shows
also that the model makes very conﬁdent predictions until around 2015.
17.4. Gaussian process regression (GPR) on Mauna Loa CO2 data.
877
scikit-learn user guide, Release 0.18.2
Out:
GPML kernel: 66**2 * RBF(length_scale=67) + 2.4**2 * RBF(length_scale=90) *
˓→ExpSineSquared(length_scale=1.3, periodicity=1) + 0.66**2 *
˓→RationalQuadratic(alpha=0.78, length_scale=1.2) + 0.18**2 * RBF(length_scale=0.134)
˓→+ WhiteKernel(noise_level=0.0361)
Log-marginal-likelihood: -87.034
Learned kernel: 34.5**2 * RBF(length_scale=41.8) + 3.27**2 * RBF(length_scale=180) *
˓→ExpSineSquared(length_scale=1.44, periodicity=1) + 0.446**2 *
˓→RationalQuadratic(alpha=17.6, length_scale=0.957) + 0.197**2 * RBF(length_scale=0.
˓→138) + WhiteKernel(noise_level=0.0336)
Log-marginal-likelihood: -83.214
print(__doc__)
# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
#
# License: BSD 3 clause
import numpy as np
878
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
from matplotlib import pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels \
import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared
from sklearn.datasets import fetch_mldata
data = fetch_mldata('mauna-loa-atmospheric-co2').data
X = data[:, [1]]
y = data[:, 0]
# Kernel with parameters given in GPML book
k1 = 66.0**2 * RBF(length_scale=67.0)
# long term smooth rising trend
k2 = 2.4**2 * RBF(length_scale=90.0) \
* ExpSineSquared(length_scale=1.3, periodicity=1.0)
# seasonal component
# medium term irregularity
k3 = 0.66**2 \
* RationalQuadratic(length_scale=1.2, alpha=0.78)
k4 = 0.18**2 * RBF(length_scale=0.134) \
+ WhiteKernel(noise_level=0.19**2)
# noise terms
kernel_gpml = k1 + k2 + k3 + k4
gp = GaussianProcessRegressor(kernel=kernel_gpml, alpha=0,
optimizer=None, normalize_y=True)
gp.fit(X, y)
print("GPML kernel: %s" % gp.kernel_)
print("Log-marginal-likelihood: %.3f"
% gp.log_marginal_likelihood(gp.kernel_.theta))
# Kernel with optimized parameters
k1 = 50.0**2 * RBF(length_scale=50.0)
# long term smooth rising trend
k2 = 2.0**2 * RBF(length_scale=100.0) \
* ExpSineSquared(length_scale=1.0, periodicity=1.0,
periodicity_bounds="fixed")
# seasonal component
# medium term irregularities
k3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)
k4 = 0.1**2 * RBF(length_scale=0.1) \
+ WhiteKernel(noise_level=0.1**2,
noise_level_bounds=(1e-3, np.inf))
# noise terms
kernel = k1 + k2 + k3 + k4
gp = GaussianProcessRegressor(kernel=kernel, alpha=0,
normalize_y=True)
gp.fit(X, y)
print("\nLearned kernel: %s" % gp.kernel_)
print("Log-marginal-likelihood: %.3f"
% gp.log_marginal_likelihood(gp.kernel_.theta))
X_ = np.linspace(X.min(), X.max() + 30, 1000)[:, np.newaxis]
y_pred, y_std = gp.predict(X_, return_std=True)
# Illustration
plt.scatter(X, y, c='k')
plt.plot(X_, y_pred)
plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,
17.4. Gaussian process regression (GPR) on Mauna Loa CO2 data.
879
scikit-learn user guide, Release 0.18.2
alpha=0.5, color='k')
plt.xlim(X_.min(), X_.max())
plt.xlabel("Year")
plt.ylabel(r"CO$_2$ in ppm")
plt.title(r"Atmospheric CO$_2$ concentration at Mauna Loa")
plt.tight_layout()
plt.show()
Total running time of the script: (0 minutes 11.150 seconds)
Download Python source code: plot_gpr_co2.py
Download IPython notebook: plot_gpr_co2.ipynb
17.5 Illustration of prior and posterior Gaussian process for different
kernels
This example illustrates the prior and posterior of a GPR with different kernels. Mean, standard deviation, and 10
samples are shown for both prior and posterior.
•
880
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
•
•
•
17.5. Illustration of prior and posterior Gaussian process for different kernels
881
scikit-learn user guide, Release 0.18.2
•
print(__doc__)
# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
#
# License: BSD 3 clause
import numpy as np
from matplotlib import pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,
ExpSineSquared, DotProduct,
ConstantKernel)
kernels = [1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0)),
1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1),
1.0 * ExpSineSquared(length_scale=1.0, periodicity=3.0,
length_scale_bounds=(0.1, 10.0),
periodicity_bounds=(1.0, 10.0)),
ConstantKernel(0.1, (0.01, 10.0))
* (DotProduct(sigma_0=1.0, sigma_0_bounds=(0.0, 10.0)) ** 2),
1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0),
nu=1.5)]
for fig_index, kernel in enumerate(kernels):
# Specify Gaussian Process
gp = GaussianProcessRegressor(kernel=kernel)
# Plot prior
plt.figure(fig_index, figsize=(8, 8))
plt.subplot(2, 1, 1)
X_ = np.linspace(0, 5, 100)
y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)
plt.plot(X_, y_mean, 'k', lw=3, zorder=9)
plt.fill_between(X_, y_mean - y_std, y_mean + y_std,
alpha=0.5, color='k')
y_samples = gp.sample_y(X_[:, np.newaxis], 10)
882
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
plt.plot(X_, y_samples, lw=1)
plt.xlim(0, 5)
plt.ylim(-3, 3)
plt.title("Prior (kernel:
%s)" % kernel, fontsize=12)
# Generate data and fit GP
rng = np.random.RandomState(4)
X = rng.uniform(0, 5, 10)[:, np.newaxis]
y = np.sin((X[:, 0] - 2.5) ** 2)
gp.fit(X, y)
# Plot posterior
plt.subplot(2, 1, 2)
X_ = np.linspace(0, 5, 100)
y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)
plt.plot(X_, y_mean, 'k', lw=3, zorder=9)
plt.fill_between(X_, y_mean - y_std, y_mean + y_std,
alpha=0.5, color='k')
y_samples = gp.sample_y(X_[:, np.newaxis], 10)
plt.plot(X_, y_samples, lw=1)
plt.scatter(X[:, 0], y, c='r', s=50, zorder=10)
plt.xlim(0, 5)
plt.ylim(-3, 3)
plt.title("Posterior (kernel: %s)\n Log-Likelihood: %.3f"
% (gp.kernel_, gp.log_marginal_likelihood(gp.kernel_.theta)),
fontsize=12)
plt.tight_layout()
plt.show()
Total running time of the script: (0 minutes 2.178 seconds)
Download Python source code: plot_gpr_prior_posterior.py
Download IPython notebook: plot_gpr_prior_posterior.ipynb
17.6 Iso-probability lines for Gaussian Processes classiﬁcation
(GPC)
A two-dimensional classiﬁcation example showing iso-probability lines for the predicted probabilities.
17.6. Iso-probability lines for Gaussian Processes classiﬁcation (GPC)
883
scikit-learn user guide, Release 0.18.2
Out:
Learned kernel: 0.0256**2 * DotProduct(sigma_0=5.72) ** 2
print(__doc__)
# Author: Vincent Dubourg <vincent.dubourg@gmail.com>
# Adapted to GaussianProcessClassifier:
#
Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
# License: BSD 3 clause
import numpy as np
from matplotlib import pyplot as plt
from matplotlib import cm
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import DotProduct, ConstantKernel as C
# A few constants
lim = 8
884
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
def g(x):
"""The function to predict (classification will then consist in predicting
whether g(x) <= 0 or not)"""
return 5. - x[:, 1] - .5 * x[:, 0] ** 2.
# Design of experiments
X = np.array([[-4.61611719, -6.00099547],
[4.10469096, 5.32782448],
[0.00000000, -0.50000000],
[-6.17289014, -4.6984743],
[1.3109306, -6.93271427],
[-5.03823144, 3.10584743],
[-2.87600388, 6.74310541],
[5.21301203, 4.26386883]])
# Observations
y = np.array(g(X) > 0, dtype=int)
# Instanciate and fit Gaussian Process Model
kernel = C(0.1, (1e-5, np.inf)) * DotProduct(sigma_0=0.1) ** 2
gp = GaussianProcessClassifier(kernel=kernel)
gp.fit(X, y)
print("Learned kernel: %s " % gp.kernel_)
# Evaluate real function and the predicted probability
res = 50
x1, x2 = np.meshgrid(np.linspace(- lim, lim, res),
np.linspace(- lim, lim, res))
xx = np.vstack([x1.reshape(x1.size), x2.reshape(x2.size)]).T
y_true = g(xx)
y_prob = gp.predict_proba(xx)[:, 1]
y_true = y_true.reshape((res, res))
y_prob = y_prob.reshape((res, res))
# Plot the probabilistic classification iso-values
fig = plt.figure(1)
ax = fig.gca()
ax.axes.set_aspect('equal')
plt.xticks([])
plt.yticks([])
ax.set_xticklabels([])
ax.set_yticklabels([])
plt.xlabel('$x_1$')
plt.ylabel('$x_2$')
cax = plt.imshow(y_prob, cmap=cm.gray_r, alpha=0.8,
extent=(-lim, lim, -lim, lim))
norm = plt.matplotlib.colors.Normalize(vmin=0., vmax=0.9)
cb = plt.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)
cb.set_label('${\\rm \mathbb{P}}\left[\widehat{G}(\mathbf{x}) \leq 0\\right]$')
plt.clim(0, 1)
plt.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)
plt.plot(X[y > 0, 0], X[y > 0, 1], 'b.', markersize=12)
17.6. Iso-probability lines for Gaussian Processes classiﬁcation (GPC)
885
scikit-learn user guide, Release 0.18.2
cs = plt.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')
cs = plt.contour(x1, x2, y_prob, [0.666], colors='b',
linestyles='solid')
plt.clabel(cs, fontsize=11)
cs = plt.contour(x1, x2, y_prob, [0.5], colors='k',
linestyles='dashed')
plt.clabel(cs, fontsize=11)
cs = plt.contour(x1, x2, y_prob, [0.334], colors='r',
linestyles='solid')
plt.clabel(cs, fontsize=11)
plt.show()
Total running time of the script: (0 minutes 0.325 seconds)
Download Python source code: plot_gpc_isoprobability.py
Download IPython notebook: plot_gpc_isoprobability.ipynb
17.7 Probabilistic predictions with Gaussian process classiﬁcation
(GPC)
This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparam-
eters. The ﬁrst ﬁgure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the
hyperparameters corresponding to the maximum log-marginal-likelihood (LML).
While the hyperparameters chosen by optimizing LML have a considerable larger LML, they perform slightly worse
according to the log-loss on test data. The ﬁgure shows that this is because they exhibit a steep change of the class
probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the
class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by
GPC.
The second ﬁgure shows the log-marginal-likelihood for different choices of the kernel’s hyperparameters, highlighting
the two choices of the hyperparameters used in the ﬁrst ﬁgure by black dots.
•
886
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
•
Out:
Log Marginal Likelihood (initial): -17.598
Log Marginal Likelihood (optimized): -3.875
Accuracy: 1.000 (initial) 1.000 (optimized)
Log-loss: 0.214 (initial) 0.319 (optimized)
print(__doc__)
# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
#
# License: BSD 3 clause
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics.classification import accuracy_score, log_loss
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
# Generate data
train_size = 50
rng = np.random.RandomState(0)
X = rng.uniform(0, 5, 100)[:, np.newaxis]
y = np.array(X[:, 0] > 2.5, dtype=int)
# Specify Gaussian Processes with fixed and optimized hyperparameters
gp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0),
optimizer=None)
gp_fix.fit(X[:train_size], y[:train_size])
gp_opt = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))
gp_opt.fit(X[:train_size], y[:train_size])
print("Log Marginal Likelihood (initial): %.3f"
% gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta))
17.7. Probabilistic predictions with Gaussian process classiﬁcation (GPC)
887
scikit-learn user guide, Release 0.18.2
print("Log Marginal Likelihood (optimized): %.3f"
% gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta))
print("Accuracy: %.3f (initial) %.3f (optimized)"
% (accuracy_score(y[:train_size], gp_fix.predict(X[:train_size])),
accuracy_score(y[:train_size], gp_opt.predict(X[:train_size]))))
print("Log-loss: %.3f (initial) %.3f (optimized)"
% (log_loss(y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),
log_loss(y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1])))
# Plot posteriors
plt.figure(0)
plt.scatter(X[:train_size, 0], y[:train_size], c='k', label="Train data")
plt.scatter(X[train_size:, 0], y[train_size:], c='g', label="Test data")
X_ = np.linspace(0, 5, 100)
plt.plot(X_, gp_fix.predict_proba(X_[:, np.newaxis])[:, 1], 'r',
label="Initial kernel: %s" % gp_fix.kernel_)
plt.plot(X_, gp_opt.predict_proba(X_[:, np.newaxis])[:, 1], 'b',
label="Optimized kernel: %s" % gp_opt.kernel_)
plt.xlabel("Feature")
plt.ylabel("Class 1 probability")
plt.xlim(0, 5)
plt.ylim(-0.25, 1.5)
plt.legend(loc="best")
# Plot LML landscape
plt.figure(1)
theta0 = np.logspace(0, 8, 30)
theta1 = np.logspace(-1, 1, 29)
Theta0, Theta1 = np.meshgrid(theta0, theta1)
LML = [[gp_opt.log_marginal_likelihood(np.log([Theta0[i, j], Theta1[i, j]]))
for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]
LML = np.array(LML).T
plt.plot(np.exp(gp_fix.kernel_.theta)[0], np.exp(gp_fix.kernel_.theta)[1],
'ko', zorder=10)
plt.plot(np.exp(gp_opt.kernel_.theta)[0], np.exp(gp_opt.kernel_.theta)[1],
'ko', zorder=10)
plt.pcolor(Theta0, Theta1, LML)
plt.xscale("log")
plt.yscale("log")
plt.colorbar()
plt.xlabel("Magnitude")
plt.ylabel("Length-scale")
plt.title("Log-marginal-likelihood")
plt.show()
Total running time of the script: (0 minutes 6.006 seconds)
Download Python source code: plot_gpc.py
Download IPython notebook: plot_gpc.ipynb
888
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
17.8 Gaussian process regression (GPR) with noise-level estimation
This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data.
An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML. The
ﬁrst corresponds to a model with a high noise level and a large length scale, which explains all variations in the data
by noise. The second one has a smaller noise level and shorter length scale, which explains most of the variation by
the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial
value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is
thus important to repeat the optimization several times for different initializations.
•
•
•
print(__doc__)
17.8. Gaussian process regression (GPR) with noise-level estimation
889
scikit-learn user guide, Release 0.18.2
# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
#
# License: BSD 3 clause
import numpy as np
from matplotlib import pyplot as plt
from matplotlib.colors import LogNorm
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
rng = np.random.RandomState(0)
X = rng.uniform(0, 5, 20)[:, np.newaxis]
y = 0.5 * np.sin(3 * X[:, 0]) + rng.normal(0, 0.5, X.shape[0])
# First run
plt.figure(0)
kernel = 1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) \
+ WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))
gp = GaussianProcessRegressor(kernel=kernel,
alpha=0.0).fit(X, y)
X_ = np.linspace(0, 5, 100)
y_mean, y_cov = gp.predict(X_[:, np.newaxis], return_cov=True)
plt.plot(X_, y_mean, 'k', lw=3, zorder=9)
plt.fill_between(X_, y_mean - np.sqrt(np.diag(y_cov)),
y_mean + np.sqrt(np.diag(y_cov)),
alpha=0.5, color='k')
plt.plot(X_, 0.5*np.sin(3*X_), 'r', lw=3, zorder=9)
plt.scatter(X[:, 0], y, c='r', s=50, zorder=10)
plt.title("Initial: %s\nOptimum: %s\nLog-Marginal-Likelihood: %s"
% (kernel, gp.kernel_,
gp.log_marginal_likelihood(gp.kernel_.theta)))
plt.tight_layout()
# Second run
plt.figure(1)
kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \
+ WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e+1))
gp = GaussianProcessRegressor(kernel=kernel,
alpha=0.0).fit(X, y)
X_ = np.linspace(0, 5, 100)
y_mean, y_cov = gp.predict(X_[:, np.newaxis], return_cov=True)
plt.plot(X_, y_mean, 'k', lw=3, zorder=9)
plt.fill_between(X_, y_mean - np.sqrt(np.diag(y_cov)),
y_mean + np.sqrt(np.diag(y_cov)),
alpha=0.5, color='k')
plt.plot(X_, 0.5*np.sin(3*X_), 'r', lw=3, zorder=9)
plt.scatter(X[:, 0], y, c='r', s=50, zorder=10)
plt.title("Initial: %s\nOptimum: %s\nLog-Marginal-Likelihood: %s"
% (kernel, gp.kernel_,
gp.log_marginal_likelihood(gp.kernel_.theta)))
plt.tight_layout()
# Plot LML landscape
plt.figure(2)
theta0 = np.logspace(-2, 3, 49)
890
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
theta1 = np.logspace(-2, 0, 50)
Theta0, Theta1 = np.meshgrid(theta0, theta1)
LML = [[gp.log_marginal_likelihood(np.log([0.36, Theta0[i, j], Theta1[i, j]]))
for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]
LML = np.array(LML).T
vmin, vmax = (-LML).min(), (-LML).max()
vmax = 50
plt.contour(Theta0, Theta1, -LML,
levels=np.logspace(np.log10(vmin), np.log10(vmax), 50),
norm=LogNorm(vmin=vmin, vmax=vmax))
plt.colorbar()
plt.xscale("log")
plt.yscale("log")
plt.xlabel("Length-scale")
plt.ylabel("Noise-level")
plt.title("Log-marginal-likelihood")
plt.tight_layout()
plt.show()
Total running time of the script: (0 minutes 10.482 seconds)
Download Python source code: plot_gpr_noisy.py
Download IPython notebook: plot_gpr_noisy.ipynb
17.9 Gaussian Processes regression: basic introductory example
A simple one-dimensional regression example computed in two different ways:
1. A noise-free case
2. A noisy case with known noise-level per datapoint
In both cases, the kernel’s parameters are estimated using the maximum likelihood principle.
The ﬁgures illustrate the interpolating property of the Gaussian Process model as well as its probabilistic nature in the
form of a pointwise 95% conﬁdence interval.
Note that the parameter alpha is applied as a Tikhonov regularization of the assumed covariance between the training
points.
•
17.9. Gaussian Processes regression: basic introductory example
891
scikit-learn user guide, Release 0.18.2
•
print(__doc__)
# Author: Vincent Dubourg <vincent.dubourg@gmail.com>
#
Jake Vanderplas <vanderplas@astro.washington.edu>
#
Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>s
# License: BSD 3 clause
import numpy as np
from matplotlib import pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
np.random.seed(1)
def f(x):
"""The function to predict."""
return x * np.sin(x)
# ----------------------------------------------------------------------
#
First the noiseless case
X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T
# Observations
y = f(X).ravel()
# Mesh the input space for evaluations of the real function, the prediction and
# its MSE
x = np.atleast_2d(np.linspace(0, 10, 1000)).T
# Instanciate a Gaussian Process model
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
# Fit to data using Maximum Likelihood Estimation of the parameters
gp.fit(X, y)
# Make the prediction on the meshed x-axis (ask for MSE as well)
y_pred, sigma = gp.predict(x, return_std=True)
# Plot the function, the prediction and the 95% confidence interval based on
# the MSE
892
Chapter 17. Gaussian Process for Machine Learning
scikit-learn user guide, Release 0.18.2
fig = plt.figure()
plt.plot(x, f(x), 'r:', label=u'$f(x) = x\,\sin(x)$')
plt.plot(X, y, 'r.', markersize=10, label=u'Observations')
plt.plot(x, y_pred, 'b-', label=u'Prediction')
plt.fill(np.concatenate([x, x[::-1]]),
np.concatenate([y_pred - 1.9600 * sigma,
(y_pred + 1.9600 * sigma)[::-1]]),
alpha=.5, fc='b', ec='None', label='95% confidence interval')
plt.xlabel('$x$')
plt.ylabel('$f(x)$')
plt.ylim(-10, 20)
plt.legend(loc='upper left')
# ----------------------------------------------------------------------
# now the noisy case
X = np.linspace(0.1, 9.9, 20)
X = np.atleast_2d(X).T
# Observations and noise
y = f(X).ravel()
dy = 0.5 + 1.0 * np.random.random(y.shape)
noise = np.random.normal(0, dy)
y += noise
# Instanciate a Gaussian Process model
gp = GaussianProcessRegressor(kernel=kernel, alpha=(dy / y) ** 2,
n_restarts_optimizer=10)
# Fit to data using Maximum Likelihood Estimation of the parameters
gp.fit(X, y)
# Make the prediction on the meshed x-axis (ask for MSE as well)
y_pred, sigma = gp.predict(x, return_std=True)
# Plot the function, the prediction and the 95% confidence interval based on
# the MSE
fig = plt.figure()
plt.plot(x, f(x), 'r:', label=u'$f(x) = x\,\sin(x)$')
plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label=u'Observations')
plt.plot(x, y_pred, 'b-', label=u'Prediction')
plt.fill(np.concatenate([x, x[::-1]]),
np.concatenate([y_pred - 1.9600 * sigma,
(y_pred + 1.9600 * sigma)[::-1]]),
alpha=.5, fc='b', ec='None', label='95% confidence interval')
plt.xlabel('$x$')
plt.ylabel('$f(x)$')
plt.ylim(-10, 20)
plt.legend(loc='upper left')
plt.show()
Total running time of the script: (0 minutes 1.011 seconds)
Download Python source code: plot_gpr_noisy_targets.py
Download IPython notebook: plot_gpr_noisy_targets.ipynb
17.9. Gaussian Processes regression: basic introductory example
893
scikit-learn user guide, Release 0.18.2
894
Chapter 17. Gaussian Process for Machine Learning
CHAPTER
EIGHTEEN
GENERALIZED LINEAR MODELS
Examples concerning the sklearn.linear_model module.
18.1 Lasso path using LARS
Computes Lasso Path along the regularization parameter using the LARS algorithm on the diabetes dataset. Each
color represents a different feature of the coefﬁcient vector, and this is displayed as a function of the regularization
parameter.
Out:
895
scikit-learn user guide, Release 0.18.2
Computing regularization path using the LARS ...
.
print(__doc__)
# Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>
#
Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn import datasets
diabetes = datasets.load_diabetes()
X = diabetes.data
y = diabetes.target
print("Computing regularization path using the LARS ...")
alphas, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True)
xx = np.sum(np.abs(coefs.T), axis=1)
xx /= xx[-1]
plt.plot(xx, coefs.T)
ymin, ymax = plt.ylim()
plt.vlines(xx, ymin, ymax, linestyle='dashed')
plt.xlabel('|coef| / max|coef|')
plt.ylabel('Coefficients')
plt.title('LASSO Path')
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.110 seconds)
Download Python source code: plot_lasso_lars.py
Download IPython notebook: plot_lasso_lars.ipynb
18.2 Plot Ridge coefﬁcients as a function of the regularization
Shows the effect of collinearity in the coefﬁcients of an estimator.
Ridge Regression is the estimator used in this example. Each color represents a different feature of the coefﬁcient
vector, and this is displayed as a function of the regularization parameter.
This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such
matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is
useful to set a certain regularization (alpha) to reduce this variation (noise).
896
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
When alpha is very large, the regularization effect dominates the squared loss function and the coefﬁcients tend to
zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares,
coefﬁcients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained
between both.
# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>
# License: BSD 3 clause
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
# X is the 10x10 Hilbert matrix
X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])
y = np.ones(10)
Compute paths
n_alphas = 200
alphas = np.logspace(-10, -2, n_alphas)
clf = linear_model.Ridge(fit_intercept=False)
coefs = []
for a in alphas:
clf.set_params(alpha=a)
clf.fit(X, y)
coefs.append(clf.coef_)
Display results
ax = plt.gca()
ax.plot(alphas, coefs)
ax.set_xscale('log')
ax.set_xlim(ax.get_xlim()[::-1])
# reverse axis
plt.xlabel('alpha')
plt.ylabel('weights')
plt.title('Ridge coefficients as a function of the regularization')
plt.axis('tight')
plt.show()
18.2. Plot Ridge coefﬁcients as a function of the regularization
897
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.227 seconds)
Download Python source code: plot_ridge_path.py
Download IPython notebook: plot_ridge_path.ipynb
18.3 Path with L1- Logistic Regression
Computes path on IRIS dataset.
print(__doc__)
# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn import datasets
from sklearn.svm import l1_min_c
iris = datasets.load_iris()
898
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
X = iris.data
y = iris.target
X = X[y != 2]
y = y[y != 2]
X -= np.mean(X, 0)
Demo path functions
cs = l1_min_c(X, y, loss='log') * np.logspace(0, 3)
print("Computing regularization path ...")
start = datetime.now()
clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)
coefs_ = []
for c in cs:
clf.set_params(C=c)
clf.fit(X, y)
coefs_.append(clf.coef_.ravel().copy())
print("This took ", datetime.now() - start)
coefs_ = np.array(coefs_)
plt.plot(np.log10(cs), coefs_)
ymin, ymax = plt.ylim()
plt.xlabel('log(C)')
plt.ylabel('Coefficients')
plt.title('Logistic Regression Path')
plt.axis('tight')
plt.show()
18.3. Path with L1- Logistic Regression
899
scikit-learn user guide, Release 0.18.2
Out:
Computing regularization path ...
This took
0:00:00.050203
Total running time of the script: (0 minutes 0.110 seconds)
Download Python source code: plot_logistic_path.py
Download IPython notebook: plot_logistic_path.ipynb
18.4 SGD: Maximum margin separating hyperplane
Plot the maximum margin separating hyperplane within a two-class separable dataset using a linear Support Vector
Machines classiﬁer trained using SGD.
900
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import SGDClassifier
from sklearn.datasets.samples_generator import make_blobs
# we create 50 separable points
X, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)
# fit the model
clf = SGDClassifier(loss="hinge", alpha=0.01, n_iter=200, fit_intercept=True)
clf.fit(X, Y)
# plot the line, the points, and the nearest vectors to the plane
xx = np.linspace(-1, 5, 10)
yy = np.linspace(-1, 5, 10)
X1, X2 = np.meshgrid(xx, yy)
Z = np.empty(X1.shape)
for (i, j), val in np.ndenumerate(X1):
x1 = val
x2 = X2[i, j]
p = clf.decision_function([[x1, x2]])
Z[i, j] = p[0]
levels = [-1.0, 0.0, 1.0]
18.4. SGD: Maximum margin separating hyperplane
901
scikit-learn user guide, Release 0.18.2
linestyles = ['dashed', 'solid', 'dashed']
colors = 'k'
plt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.069 seconds)
Download Python source code: plot_sgd_separating_hyperplane.py
Download IPython notebook: plot_sgd_separating_hyperplane.ipynb
18.5 SGD: convex loss functions
A plot that compares the various convex loss functions supported by sklearn.linear_model.SGDClassifier
.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
902
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
def modified_huber_loss(y_true, y_pred):
z = y_pred * y_true
loss = -4 * z
loss[z >= -1] = (1 - z[z >= -1]) ** 2
loss[z >= 1.] = 0
return loss
xmin, xmax = -4, 4
xx = np.linspace(xmin, xmax, 100)
lw = 2
plt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color='gold', lw=lw,
label="Zero-one loss")
plt.plot(xx, np.where(xx < 1, 1 - xx, 0), color='teal', lw=lw,
label="Hinge loss")
plt.plot(xx, -np.minimum(xx, 0), color='yellowgreen', lw=lw,
label="Perceptron loss")
plt.plot(xx, np.log2(1 + np.exp(-xx)), color='cornflowerblue', lw=lw,
label="Log loss")
plt.plot(xx, np.where(xx < 1, 1 - xx, 0) ** 2, color='orange', lw=lw,
label="Squared hinge loss")
plt.plot(xx, modified_huber_loss(xx, 1), color='darkorchid', lw=lw,
linestyle='--', label="Modified Huber loss")
plt.ylim((0, 8))
plt.legend(loc="upper right")
plt.xlabel(r"Decision function $f(x)$")
plt.ylabel("$L(y, f(x))$")
plt.show()
Total running time of the script: (0 minutes 0.068 seconds)
Download Python source code: plot_sgd_loss_functions.py
Download IPython notebook: plot_sgd_loss_functions.ipynb
18.6 Plot Ridge coefﬁcients as a function of the L2 regularization
Ridge Regression is the estimator used in this example. Each color in the left plot represents one different dimension
of the coefﬁcient vector, and this is displayed as a function of the regularization parameter. The right plot shows
how exact the solution is. This example illustrates how a well deﬁned solution is found by Ridge regression and
how regularization affects the coefﬁcients and their values. The plot on the right shows how the difference of the
coefﬁcients from the estimator changes as a function of regularization.
In this example the dependent variable Y is set as a function of the input features: y = X*w + c. The coefﬁcient vector
w is randomly sampled from a normal distribution, whereas the bias term c is set to a constant.
As alpha tends toward zero the coefﬁcients found by Ridge regression stabilize towards the randomly sampled vector
w. For big alpha (strong regularisation) the coefﬁcients are smaller (eventually converging at 0) leading to a simpler
and biased solution. These dependencies can be observed on the left plot.
The right plot shows the mean squared error between the coefﬁcients found by the model and the chosen vector w.
Less regularised models retrieve the exact coefﬁcients (error is equal to 0), stronger regularised models increase the
error.
Please note that in this example the data is non-noisy, hence it is possible to extract the exact coefﬁcients.
18.6. Plot Ridge coefﬁcients as a function of the L2 regularization
903
scikit-learn user guide, Release 0.18.2
# Author: Kornel Kielczewski -- <kornel.k@plusnet.pl>
print(__doc__)
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_regression
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
clf = Ridge()
X, y, w = make_regression(n_samples=10, n_features=10, coef=True,
random_state=1, bias=3.5)
coefs = []
errors = []
alphas = np.logspace(-6, 6, 200)
# Train the model with different regularisation strengths
for a in alphas:
clf.set_params(alpha=a)
clf.fit(X, y)
coefs.append(clf.coef_)
errors.append(mean_squared_error(clf.coef_, w))
# Display results
plt.figure(figsize=(20, 6))
plt.subplot(121)
ax = plt.gca()
ax.plot(alphas, coefs)
ax.set_xscale('log')
plt.xlabel('alpha')
plt.ylabel('weights')
plt.title('Ridge coefficients as a function of the regularization')
plt.axis('tight')
plt.subplot(122)
ax = plt.gca()
ax.plot(alphas, errors)
ax.set_xscale('log')
plt.xlabel('alpha')
904
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
plt.ylabel('error')
plt.title('Coefficient error as a function of the regularization')
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.326 seconds)
Download Python source code: plot_ridge_coeffs.py
Download IPython notebook: plot_ridge_coeffs.ipynb
18.7 Ordinary Least Squares and Ridge Regression Variance
Due to the few points in each dimension and the straight line that linear regression uses to follow these points as well
as it can, noise on the observations will cause great variance as shown in the ﬁrst plot. Every line’s slope can vary
quite a bit for each prediction due to the noise induced in the observations.
Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the
value of the regression coefﬁcients. Despite the few data points in each dimension, the slope of the prediction is much
more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression
•
•
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
X_train = np.c_[.5, 1].T
18.7. Ordinary Least Squares and Ridge Regression Variance
905
scikit-learn user guide, Release 0.18.2
y_train = [.5, 1]
X_test = np.c_[0, 2].T
np.random.seed(0)
classifiers = dict(ols=linear_model.LinearRegression(),
ridge=linear_model.Ridge(alpha=.1))
fignum = 1
for name, clf in classifiers.items():
fig = plt.figure(fignum, figsize=(4, 3))
plt.clf()
plt.title(name)
ax = plt.axes([.12, .12, .8, .8])
for _ in range(6):
this_X = .1 * np.random.normal(size=(2, 1)) + X_train
clf.fit(this_X, y_train)
ax.plot(X_test, clf.predict(X_test), color='.5')
ax.scatter(this_X, y_train, s=3, c='.5', marker='o', zorder=10)
clf.fit(X_train, y_train)
ax.plot(X_test, clf.predict(X_test), linewidth=2, color='blue')
ax.scatter(X_train, y_train, s=30, c='r', marker='+', zorder=10)
ax.set_xticks(())
ax.set_yticks(())
ax.set_ylim((0, 1.6))
ax.set_xlabel('X')
ax.set_ylabel('y')
ax.set_xlim(0, 2)
fignum += 1
plt.show()
Total running time of the script: (0 minutes 0.266 seconds)
Download Python source code: plot_ols_ridge_variance.py
Download IPython notebook: plot_ols_ridge_variance.ipynb
18.8 Logistic function
Shown in the plot is how the logistic regression would, in this synthetic dataset, classify values as either 0 or 1, i.e.
class one or two, using the logistic curve.
906
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Code source: Gael Varoquaux
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
# this is our test set, it's just a straight line with some
# Gaussian noise
xmin, xmax = -5, 5
n_samples = 100
np.random.seed(0)
X = np.random.normal(size=n_samples)
y = (X > 0).astype(np.float)
X[X > 0] *= 4
X += .3 * np.random.normal(size=n_samples)
X = X[:, np.newaxis]
# run the classifier
clf = linear_model.LogisticRegression(C=1e5)
clf.fit(X, y)
# and plot the result
plt.figure(1, figsize=(4, 3))
plt.clf()
plt.scatter(X.ravel(), y, color='black', zorder=20)
X_test = np.linspace(-5, 10, 300)
def model(x):
return 1 / (1 + np.exp(-x))
loss = model(X_test * clf.coef_ + clf.intercept_).ravel()
plt.plot(X_test, loss, color='red', linewidth=3)
18.8. Logistic function
907
scikit-learn user guide, Release 0.18.2
ols = linear_model.LinearRegression()
ols.fit(X, y)
plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)
plt.axhline(.5, color='.5')
plt.ylabel('y')
plt.xlabel('X')
plt.xticks(range(-5, 10))
plt.yticks([0, 0.5, 1])
plt.ylim(-.25, 1.25)
plt.xlim(-4, 10)
plt.legend(('Logistic Regression Model', 'Linear Regression Model'),
loc="lower right", fontsize='small')
plt.show()
Total running time of the script: (0 minutes 0.107 seconds)
Download Python source code: plot_logistic.py
Download IPython notebook: plot_logistic.ipynb
18.9 Polynomial interpolation
This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge
regression. Concretely, from n_samples 1d points, it sufﬁces to build the Vandermonde matrix, which is n_samples x
n_degree+1 and has the following form:
[[1, x_1, x_1 ** 2, x_1 ** 3, ...], [1, x_2, x_2 ** 2, x_2 ** 3, ...], ...]
Intuitively, this matrix can be interpreted as a matrix of pseudo features (the points raised to some power). The matrix
is akin to (but different from) the matrix induced by a polynomial kernel.
This example shows that you can do non-linear regression with a linear model, using a pipeline to add non-linear
features. Kernel methods extend this idea and can induce very high (even inﬁnite) dimensional feature spaces.
908
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Author: Mathieu Blondel
#
Jake Vanderplas
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
def f(x):
""" function to approximate by polynomial interpolation"""
return x * np.sin(x)
# generate points used to plot
x_plot = np.linspace(0, 10, 100)
# generate points and keep a subset of them
x = np.linspace(0, 10, 100)
rng = np.random.RandomState(0)
rng.shuffle(x)
18.9. Polynomial interpolation
909
scikit-learn user guide, Release 0.18.2
x = np.sort(x[:20])
y = f(x)
# create matrix versions of these arrays
X = x[:, np.newaxis]
X_plot = x_plot[:, np.newaxis]
colors = ['teal', 'yellowgreen', 'gold']
lw = 2
plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,
label="ground truth")
plt.scatter(x, y, color='navy', s=30, marker='o', label="training points")
for count, degree in enumerate([3, 4, 5]):
model = make_pipeline(PolynomialFeatures(degree), Ridge())
model.fit(X, y)
y_plot = model.predict(X_plot)
plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw,
label="degree %d" % degree)
plt.legend(loc='lower left')
plt.show()
Total running time of the script: (0 minutes 0.072 seconds)
Download Python source code: plot_polynomial_interpolation.py
Download IPython notebook: plot_polynomial_interpolation.ipynb
18.10 Linear Regression Example
This example uses the only the ﬁrst feature of the diabetes dataset, in order to illustrate a two-dimensional plot of
this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a
straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the
responses predicted by the linear approximation.
The coefﬁcients, the residual sum of squares and the variance score are also calculated.
910
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
Out:
Coefficients:
[ 938.23786125]
Mean squared error: 2548.07
Variance score: 0.47
print(__doc__)
# Code source: Jaques Grobler
# License: BSD 3 clause
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
# Load the diabetes dataset
diabetes = datasets.load_diabetes()
18.10. Linear Regression Example
911
scikit-learn user guide, Release 0.18.2
# Use only one feature
diabetes_X = diabetes.data[:, np.newaxis, 2]
# Split the data into training/testing sets
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]
# Split the targets into training/testing sets
diabetes_y_train = diabetes.target[:-20]
diabetes_y_test = diabetes.target[-20:]
# Create linear regression object
regr = linear_model.LinearRegression()
# Train the model using the training sets
regr.fit(diabetes_X_train, diabetes_y_train)
# The coefficients
print('Coefficients: \n', regr.coef_)
# The mean squared error
print("Mean squared error: %.2f"
% np.mean((regr.predict(diabetes_X_test) - diabetes_y_test) ** 2))
# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % regr.score(diabetes_X_test, diabetes_y_test))
# Plot outputs
plt.scatter(diabetes_X_test, diabetes_y_test,
color='black')
plt.plot(diabetes_X_test, regr.predict(diabetes_X_test), color='blue',
linewidth=3)
plt.xticks(())
plt.yticks(())
plt.show()
Total running time of the script: (0 minutes 0.097 seconds)
Download Python source code: plot_ols.py
Download IPython notebook: plot_ols.ipynb
18.11 Logistic Regression 3-class Classiﬁer
Show below is a logistic-regression classiﬁers decision boundaries on the iris dataset. The datapoints are colored
according to their labels.
912
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model, datasets
# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]
# we only take the first two features.
Y = iris.target
h = .02
# step size in the mesh
logreg = linear_model.LogisticRegression(C=1e5)
# we create an instance of Neighbours Classifier and fit the data.
logreg.fit(X, Y)
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
18.11. Logistic Regression 3-class Classiﬁer
913
scikit-learn user guide, Release 0.18.2
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())
plt.show()
Total running time of the script: (0 minutes 0.087 seconds)
Download Python source code: plot_iris_logistic.py
Download IPython notebook: plot_iris_logistic.ipynb
18.12 SGD: Weighted samples
Plot decision function of a weighted dataset, where the size of points is proportional to its weight.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
914
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
# we create 20 points
np.random.seed(0)
X = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]
y = [1] * 10 + [-1] * 10
sample_weight = 100 * np.abs(np.random.randn(20))
# and assign a bigger weight to the last 10 samples
sample_weight[:10] *= 10
# plot the weighted data points
xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))
plt.figure()
plt.scatter(X[:, 0], X[:, 1], c=y, s=sample_weight, alpha=0.9,
cmap=plt.cm.bone)
## fit the unweighted model
clf = linear_model.SGDClassifier(alpha=0.01, n_iter=100)
clf.fit(X, y)
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
no_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=['solid'])
## fit the weighted model
clf = linear_model.SGDClassifier(alpha=0.01, n_iter=100)
clf.fit(X, y, sample_weight=sample_weight)
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
samples_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=['dashed'])
plt.legend([no_weights.collections[0], samples_weights.collections[0]],
["no weights", "with weights"], loc="lower left")
plt.xticks(())
plt.yticks(())
plt.show()
Total running time of the script: (0 minutes 0.110 seconds)
Download Python source code: plot_sgd_weighted_samples.py
Download IPython notebook: plot_sgd_weighted_samples.ipynb
18.13 Lasso on dense and sparse data
We show that linear_model.Lasso provides the same results for dense and sparse data and that in the case of sparse
data the speed is improved.
print(__doc__)
from time import time
from scipy import sparse
from scipy import linalg
from sklearn.datasets.samples_generator import make_regression
from sklearn.linear_model import Lasso
The two Lasso implementations on Dense data
18.13. Lasso on dense and sparse data
915
scikit-learn user guide, Release 0.18.2
print("--- Dense matrices")
X, y = make_regression(n_samples=200, n_features=5000, random_state=0)
X_sp = sparse.coo_matrix(X)
alpha = 1
sparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)
dense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)
t0 = time()
sparse_lasso.fit(X_sp, y)
print("Sparse Lasso done in %fs" % (time() - t0))
t0 = time()
dense_lasso.fit(X, y)
print("Dense Lasso done in %fs" % (time() - t0))
print("Distance between coefficients : %s"
% linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))
The two Lasso implementations on Sparse data
print("--- Sparse matrices")
Xs = X.copy()
Xs[Xs < 2.5] = 0.0
Xs = sparse.coo_matrix(Xs)
Xs = Xs.tocsc()
print("Matrix density : %s %%" % (Xs.nnz / float(X.size) * 100))
alpha = 0.1
sparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)
dense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)
t0 = time()
sparse_lasso.fit(Xs, y)
print("Sparse Lasso done in %fs" % (time() - t0))
t0 = time()
dense_lasso.fit(Xs.toarray(), y)
print("Dense Lasso done in %fs" % (time() - t0))
print("Distance between coefficients : %s"
% linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: lasso_dense_vs_sparse_data.py
Download IPython notebook: lasso_dense_vs_sparse_data.ipynb
18.14 Lasso and Elastic Net for Sparse Signals
Estimates Lasso and Elastic-Net regression models on a manually generated sparse signal corrupted with an additive
noise. Estimated coefﬁcients are compared with the ground-truth.
916
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score
generate some sparse data to play with
np.random.seed(42)
n_samples, n_features = 50, 200
X = np.random.randn(n_samples, n_features)
coef = 3 * np.random.randn(n_features)
inds = np.arange(n_features)
np.random.shuffle(inds)
coef[inds[10:]] = 0
# sparsify coef
y = np.dot(X, coef)
# add noise
y += 0.01 * np.random.normal((n_samples,))
# Split data in train set and test set
n_samples = X.shape[0]
X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]
X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]
Lasso
from sklearn.linear_model import Lasso
alpha = 0.1
lasso = Lasso(alpha=alpha)
y_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)
r2_score_lasso = r2_score(y_test, y_pred_lasso)
print(lasso)
print("r^2 on test data : %f" % r2_score_lasso)
Out:
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, positive=False, precompute=False, random_state=None,
selection='cyclic', tol=0.0001, warm_start=False)
r^2 on test data : 0.384710
ElasticNet
from sklearn.linear_model import ElasticNet
enet = ElasticNet(alpha=alpha, l1_ratio=0.7)
y_pred_enet = enet.fit(X_train, y_train).predict(X_test)
r2_score_enet = r2_score(y_test, y_pred_enet)
print(enet)
print("r^2 on test data : %f" % r2_score_enet)
plt.plot(enet.coef_, color='lightgreen', linewidth=2,
18.14. Lasso and Elastic Net for Sparse Signals
917
scikit-learn user guide, Release 0.18.2
label='Elastic net coefficients')
plt.plot(lasso.coef_, color='gold', linewidth=2,
label='Lasso coefficients')
plt.plot(coef, '--', color='navy', label='original coefficients')
plt.legend(loc='best')
plt.title("Lasso R^2: %f, Elastic Net R^2: %f"
% (r2_score_lasso, r2_score_enet))
plt.show()
Out:
ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.7,
max_iter=1000, normalize=False, positive=False, precompute=False,
random_state=None, selection='cyclic', tol=0.0001, warm_start=False)
r^2 on test data : 0.240176
Total running time of the script: (0 minutes 0.202 seconds)
Download Python source code: plot_lasso_and_elasticnet.py
Download IPython notebook: plot_lasso_and_elasticnet.ipynb
918
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
18.15 Sparsity Example: Fitting only features 1 and 2
Features 1 and 2 of the diabetes-dataset are ﬁtted and plotted below. It illustrates that although feature 2 has a strong
coefﬁcient on the full model, it does not give us much regarding y when compared to just feature 1
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets, linear_model
diabetes = datasets.load_diabetes()
indices = (0, 1)
X_train = diabetes.data[:-20, indices]
X_test = diabetes.data[-20:, indices]
y_train = diabetes.target[:-20]
y_test = diabetes.target[-20:]
ols = linear_model.LinearRegression()
ols.fit(X_train, y_train)
Plot the ﬁgure
def plot_figs(fig_num, elev, azim, X_train, clf):
fig = plt.figure(fig_num, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, elev=elev, azim=azim)
ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c='k', marker='+')
ax.plot_surface(np.array([[-.1, -.1], [.15, .15]]),
np.array([[-.1, .15], [-.1, .15]]),
clf.predict(np.array([[-.1, -.1, .15, .15],
[-.1, .15, -.1, .15]]).T
).reshape((2, 2)),
alpha=.5)
ax.set_xlabel('X_1')
ax.set_ylabel('X_2')
ax.set_zlabel('Y')
ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
#Generate the three different figures from different views
elev = 43.5
azim = -110
plot_figs(1, elev, azim, X_train, ols)
elev = -.5
azim = 0
plot_figs(2, elev, azim, X_train, ols)
18.15. Sparsity Example: Fitting only features 1 and 2
919
scikit-learn user guide, Release 0.18.2
elev = -.5
azim = 90
plot_figs(3, elev, azim, X_train, ols)
plt.show()
•
•
•
Total running time of the script: (0 minutes 0.329 seconds)
Download Python source code: plot_ols_3d.py
Download IPython notebook: plot_ols_3d.ipynb
18.16 Joint feature selection with multi-task Lasso
The multi-task lasso allows to ﬁt multiple regression problems jointly enforcing the selected features to be the same
across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features
vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one
time point are select for all time point. This makes feature selection by the Lasso more stable.
print(__doc__)
# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause
import matplotlib.pyplot as plt
920
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
import numpy as np
from sklearn.linear_model import MultiTaskLasso, Lasso
rng = np.random.RandomState(42)
# Generate some 2D coefficients with sine waves with random frequency and phase
n_samples, n_features, n_tasks = 100, 30, 40
n_relevant_features = 5
coef = np.zeros((n_tasks, n_features))
times = np.linspace(0, 2 * np.pi, n_tasks)
for k in range(n_relevant_features):
coef[:, k] = np.sin((1. + rng.randn(1)) * times + 3 * rng.randn(1))
X = rng.randn(n_samples, n_features)
Y = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks)
coef_lasso_ = np.array([Lasso(alpha=0.5).fit(X, y).coef_ for y in Y.T])
coef_multi_task_lasso_ = MultiTaskLasso(alpha=1.).fit(X, Y).coef_
Plot support and time series
fig = plt.figure(figsize=(8, 5))
plt.subplot(1, 2, 1)
plt.spy(coef_lasso_)
plt.xlabel('Feature')
plt.ylabel('Time (or Task)')
plt.text(10, 5, 'Lasso')
plt.subplot(1, 2, 2)
plt.spy(coef_multi_task_lasso_)
plt.xlabel('Feature')
plt.ylabel('Time (or Task)')
plt.text(10, 5, 'MultiTaskLasso')
fig.suptitle('Coefficient non-zero location')
feature_to_plot = 0
plt.figure()
lw = 2
plt.plot(coef[:, feature_to_plot], color='seagreen', linewidth=lw,
label='Ground truth')
plt.plot(coef_lasso_[:, feature_to_plot], color='cornflowerblue', linewidth=lw,
label='Lasso')
plt.plot(coef_multi_task_lasso_[:, feature_to_plot], color='gold', linewidth=lw,
label='MultiTaskLasso')
plt.legend(loc='upper center')
plt.axis('tight')
plt.ylim([-1.1, 1.1])
plt.show()
18.16. Joint feature selection with multi-task Lasso
921
scikit-learn user guide, Release 0.18.2
•
•
Total running time of the script: (0 minutes 0.314 seconds)
Download Python source code: plot_multi_task_lasso_support.py
Download IPython notebook: plot_multi_task_lasso_support.ipynb
18.17 Comparing various online solvers
An example showing how different online solvers perform on the hand-written digits dataset.
922
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
Out:
training SGD
training ASGD
training Perceptron
training Passive-Aggressive I
training Passive-Aggressive II
training SAG
# Author: Rob Zinkov <rob at zinkov dot com>
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier, Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import LogisticRegression
18.17. Comparing various online solvers
923
scikit-learn user guide, Release 0.18.2
heldout = [0.95, 0.90, 0.75, 0.50, 0.01]
rounds = 20
digits = datasets.load_digits()
X, y = digits.data, digits.target
classifiers = [
("SGD", SGDClassifier()),
("ASGD", SGDClassifier(average=True)),
("Perceptron", Perceptron()),
("Passive-Aggressive I", PassiveAggressiveClassifier(loss='hinge',
C=1.0)),
("Passive-Aggressive II", PassiveAggressiveClassifier(loss='squared_hinge',
C=1.0)),
("SAG", LogisticRegression(solver='sag', tol=1e-1, C=1.e4 / X.shape[0]))
]
xx = 1. - np.array(heldout)
for name, clf in classifiers:
print("training %s" % name)
rng = np.random.RandomState(42)
yy = []
for i in heldout:
yy_ = []
for r in range(rounds):
X_train, X_test, y_train, y_test = \
train_test_split(X, y, test_size=i, random_state=rng)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
yy_.append(1 - np.mean(y_pred == y_test))
yy.append(np.mean(yy_))
plt.plot(xx, yy, label=name)
plt.legend(loc="upper right")
plt.xlabel("Proportion train")
plt.ylabel("Test Error Rate")
plt.show()
Total running time of the script: (0 minutes 13.852 seconds)
Download Python source code: plot_sgd_comparison.py
Download IPython notebook: plot_sgd_comparison.ipynb
18.18 Robust linear model estimation using RANSAC
In this example we see how to robustly ﬁt a linear model to faulty data using the RANSAC algorithm.
924
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
Out:
Estimated coefficients (true, normal, RANSAC):
82.1903908408 [ 54.17236387] [ 82.08533159]
import numpy as np
from matplotlib import pyplot as plt
from sklearn import linear_model, datasets
n_samples = 1000
n_outliers = 50
X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,
n_informative=1, noise=10,
coef=True, random_state=0)
# Add outlier data
np.random.seed(0)
18.18. Robust linear model estimation using RANSAC
925
scikit-learn user guide, Release 0.18.2
X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))
y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)
# Fit line using all data
model = linear_model.LinearRegression()
model.fit(X, y)
# Robustly fit linear model with RANSAC algorithm
model_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression())
model_ransac.fit(X, y)
inlier_mask = model_ransac.inlier_mask_
outlier_mask = np.logical_not(inlier_mask)
# Predict data of estimated models
line_X = np.arange(-5, 5)
line_y = model.predict(line_X[:, np.newaxis])
line_y_ransac = model_ransac.predict(line_X[:, np.newaxis])
# Compare estimated coefficients
print("Estimated coefficients (true, normal, RANSAC):")
print(coef, model.coef_, model_ransac.estimator_.coef_)
lw = 2
plt.scatter(X[inlier_mask], y[inlier_mask], color='yellowgreen', marker='.',
label='Inliers')
plt.scatter(X[outlier_mask], y[outlier_mask], color='gold', marker='.',
label='Outliers')
plt.plot(line_X, line_y, color='navy', linestyle='-', linewidth=lw,
label='Linear regressor')
plt.plot(line_X, line_y_ransac, color='cornflowerblue', linestyle='-',
linewidth=lw, label='RANSAC regressor')
plt.legend(loc='lower right')
plt.show()
Total running time of the script: (0 minutes 0.130 seconds)
Download Python source code: plot_ransac.py
Download IPython notebook: plot_ransac.ipynb
18.19 HuberRegressor vs Ridge on dataset with strong outliers
Fit Ridge and HuberRegressor on a dataset with outliers.
The example shows that the predictions in ridge are strongly inﬂuenced by the outliers present in the dataset. The
Huber regressor is less inﬂuenced by the outliers since the model uses the linear loss for these. As the parameter
epsilon is increased for the Huber regressor, the decision function approaches that of the ridge.
926
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
# Authors: Manoj Kumar mks542@nyu.edu
# License: BSD 3 clause
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.linear_model import HuberRegressor, Ridge
# Generate toy data.
rng = np.random.RandomState(0)
X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4.0,
bias=100.0)
# Add four strong outliers to the dataset.
X_outliers = rng.normal(0, 0.5, size=(4, 1))
y_outliers = rng.normal(0, 2.0, size=4)
X_outliers[:2, :] += X.max() + X.mean() / 4.
X_outliers[2:, :] += X.min() - X.mean() / 4.
y_outliers[:2] += y.min() - y.mean() / 4.
y_outliers[2:] += y.max() + y.mean() / 4.
X = np.vstack((X, X_outliers))
y = np.concatenate((y, y_outliers))
plt.plot(X, y, 'b.')
18.19. HuberRegressor vs Ridge on dataset with strong outliers
927
scikit-learn user guide, Release 0.18.2
# Fit the huber regressor over a series of epsilon values.
colors = ['r-', 'b-', 'y-', 'm-']
x = np.linspace(X.min(), X.max(), 7)
epsilon_values = [1.35, 1.5, 1.75, 1.9]
for k, epsilon in enumerate(epsilon_values):
huber = HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,
epsilon=epsilon)
huber.fit(X, y)
coef_ = huber.coef_ * x + huber.intercept_
plt.plot(x, coef_, colors[k], label="huber loss, %s" % epsilon)
# Fit a ridge regressor to compare it to huber regressor.
ridge = Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True)
ridge.fit(X, y)
coef_ridge = ridge.coef_
coef_ = ridge.coef_ * x + ridge.intercept_
plt.plot(x, coef_, 'g-', label="ridge regression")
plt.title("Comparison of HuberRegressor vs Ridge")
plt.xlabel("X")
plt.ylabel("y")
plt.legend(loc=0)
plt.show()
Total running time of the script: (0 minutes 0.092 seconds)
Download Python source code: plot_huber_vs_ridge.py
Download IPython notebook: plot_huber_vs_ridge.ipynb
18.20 SGD: Penalties
Plot the contours of the three penalties.
All of the above are supported by sklearn.linear_model.stochastic_gradient.
928
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
from __future__ import division
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
def l1(xs):
return np.array([np.sqrt((1 - np.sqrt(x ** 2.0)) ** 2.0) for x in xs])
def l2(xs):
return np.array([np.sqrt(1.0 - x ** 2.0) for x in xs])
def el(xs, z):
return np.array([(2 - 2 * x - 2 * z + 4 * x * z -
(4 * z ** 2
- 8 * x * z ** 2
+ 8 * x ** 2 * z ** 2
- 16 * x ** 2 * z ** 3
+ 8 * x * z ** 3 + 4 * x ** 2 * z ** 4) ** (1. / 2)
- 2 * x * z ** 2) / (2 - 4 * z) for x in xs])
def cross(ext):
18.20. SGD: Penalties
929
scikit-learn user guide, Release 0.18.2
plt.plot([-ext, ext], [0, 0], "k-")
plt.plot([0, 0], [-ext, ext], "k-")
xs = np.linspace(0, 1, 100)
alpha = 0.501
# 0.5 division throuh zero
cross(1.2)
l1_color = "navy"
l2_color = "c"
elastic_net_color = "darkorange"
lw = 2
plt.plot(xs, l1(xs), color=l1_color, label="L1", lw=lw)
plt.plot(xs, -1.0 * l1(xs), color=l1_color, lw=lw)
plt.plot(-1 * xs, l1(xs), color=l1_color, lw=lw)
plt.plot(-1 * xs, -1.0 * l1(xs), color=l1_color, lw=lw)
plt.plot(xs, l2(xs), color=l2_color, label="L2", lw=lw)
plt.plot(xs, -1.0 * l2(xs), color=l2_color, lw=lw)
plt.plot(-1 * xs, l2(xs), color=l2_color, lw=lw)
plt.plot(-1 * xs, -1.0 * l2(xs), color=l2_color, lw=lw)
plt.plot(xs, el(xs, alpha), color=elastic_net_color, label="Elastic Net", lw=lw)
plt.plot(xs, -1.0 * el(xs, alpha), color=elastic_net_color, lw=lw)
plt.plot(-1 * xs, el(xs, alpha), color=elastic_net_color, lw=lw)
plt.plot(-1 * xs, -1.0 * el(xs, alpha), color=elastic_net_color, lw=lw)
plt.xlabel(r"$w_0$")
plt.ylabel(r"$w_1$")
plt.legend()
plt.axis("equal")
plt.show()
Total running time of the script: (0 minutes 0.080 seconds)
Download Python source code: plot_sgd_penalties.py
Download IPython notebook: plot_sgd_penalties.ipynb
18.21 Bayesian Ridge Regression
Computes a Bayesian Ridge Regression on a synthetic dataset.
See Bayesian Ridge Regression for more information on the regressor.
Compared to the OLS (ordinary least squares) estimator, the coefﬁcient weights are slightly shifted toward zeros,
which stabilises them.
As the prior on the weights is a Gaussian prior, the histogram of the estimated weights is Gaussian.
The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.
print(__doc__)
import numpy as np
930
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.linear_model import BayesianRidge, LinearRegression
Generating simulated data with Gaussian weights
np.random.seed(0)
n_samples, n_features = 100, 100
X = np.random.randn(n_samples, n_features)
# Create Gaussian data
# Create weights with a precision lambda_ of 4.
lambda_ = 4.
w = np.zeros(n_features)
# Only keep 10 weights of interest
relevant_features = np.random.randint(0, n_features, 10)
for i in relevant_features:
w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))
# Create noise with a precision alpha of 50.
alpha_ = 50.
noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)
# Create the target
y = np.dot(X, w) + noise
Fit the Bayesian Ridge Regression and an OLS for comparison
clf = BayesianRidge(compute_score=True)
clf.fit(X, y)
ols = LinearRegression()
ols.fit(X, y)
Plot true weights, estimated weights and histogram of the weights
lw = 2
plt.figure(figsize=(6, 5))
plt.title("Weights of the model")
plt.plot(clf.coef_, color='lightgreen', linewidth=lw,
label="Bayesian Ridge estimate")
plt.plot(w, color='gold', linewidth=lw, label="Ground truth")
plt.plot(ols.coef_, color='navy', linestyle='--', label="OLS estimate")
plt.xlabel("Features")
plt.ylabel("Values of the weights")
plt.legend(loc="best", prop=dict(size=12))
plt.figure(figsize=(6, 5))
plt.title("Histogram of the weights")
plt.hist(clf.coef_, bins=n_features, color='gold', log=True)
plt.scatter(clf.coef_[relevant_features], 5 * np.ones(len(relevant_features)),
color='navy', label="Relevant features")
plt.ylabel("Features")
plt.xlabel("Values of the weights")
plt.legend(loc="upper left")
plt.figure(figsize=(6, 5))
plt.title("Marginal log-likelihood")
plt.plot(clf.scores_, color='navy', linewidth=lw)
plt.ylabel("Score")
plt.xlabel("Iterations")
18.21. Bayesian Ridge Regression
931
scikit-learn user guide, Release 0.18.2
plt.show()
•
•
•
Total running time of the script: (0 minutes 0.345 seconds)
Download Python source code: plot_bayesian_ridge.py
Download IPython notebook: plot_bayesian_ridge.ipynb
932
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
18.22 Automatic Relevance Determination Regression (ARD)
Fit regression model with Bayesian Ridge Regression.
See Bayesian Ridge Regression for more information on the regressor.
Compared to the OLS (ordinary least squares) estimator, the coefﬁcient weights are slightly shifted toward zeros,
which stabilises them.
The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights.
The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.linear_model import ARDRegression, LinearRegression
Generating simulated data with Gaussian weights
# Parameters of the example
np.random.seed(0)
n_samples, n_features = 100, 100
# Create Gaussian data
X = np.random.randn(n_samples, n_features)
# Create weights with a precision lambda_ of 4.
lambda_ = 4.
w = np.zeros(n_features)
# Only keep 10 weights of interest
relevant_features = np.random.randint(0, n_features, 10)
for i in relevant_features:
w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))
# Create noise with a precision alpha of 50.
alpha_ = 50.
noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)
# Create the target
y = np.dot(X, w) + noise
Fit the ARD Regression
clf = ARDRegression(compute_score=True)
clf.fit(X, y)
ols = LinearRegression()
ols.fit(X, y)
Plot the true weights, the estimated weights and the histogram of the weights
plt.figure(figsize=(6, 5))
plt.title("Weights of the model")
plt.plot(clf.coef_, color='darkblue', linestyle='-', linewidth=2,
label="ARD estimate")
plt.plot(ols.coef_, color='yellowgreen', linestyle=':', linewidth=2,
label="OLS estimate")
plt.plot(w, color='orange', linestyle='-', linewidth=2, label="Ground truth")
plt.xlabel("Features")
18.22. Automatic Relevance Determination Regression (ARD)
933
scikit-learn user guide, Release 0.18.2
plt.ylabel("Values of the weights")
plt.legend(loc=1)
plt.figure(figsize=(6, 5))
plt.title("Histogram of the weights")
plt.hist(clf.coef_, bins=n_features, color='navy', log=True)
plt.scatter(clf.coef_[relevant_features], 5 * np.ones(len(relevant_features)),
color='gold', marker='o', label="Relevant features")
plt.ylabel("Features")
plt.xlabel("Values of the weights")
plt.legend(loc=1)
plt.figure(figsize=(6, 5))
plt.title("Marginal log-likelihood")
plt.plot(clf.scores_, color='navy', linewidth=2)
plt.ylabel("Score")
plt.xlabel("Iterations")
plt.show()
•
•
934
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
•
Total running time of the script: (0 minutes 0.406 seconds)
Download Python source code: plot_ard.py
Download IPython notebook: plot_ard.ipynb
18.23 Orthogonal Matching Pursuit
Using orthogonal matching pursuit for recovering a sparse signal from a noisy measurement encoded with a dictionary
print(__doc__)
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import OrthogonalMatchingPursuit
from sklearn.linear_model import OrthogonalMatchingPursuitCV
from sklearn.datasets import make_sparse_coded_signal
n_components, n_features = 512, 100
n_nonzero_coefs = 17
# generate the data
###################
# y = Xw
# |x|_0 = n_nonzero_coefs
y, X, w = make_sparse_coded_signal(n_samples=1,
n_components=n_components,
n_features=n_features,
n_nonzero_coefs=n_nonzero_coefs,
random_state=0)
idx, = w.nonzero()
# distort the clean signal
y_noisy = y + 0.05 * np.random.randn(len(y))
# plot the sparse signal
18.23. Orthogonal Matching Pursuit
935
scikit-learn user guide, Release 0.18.2
plt.figure(figsize=(7, 7))
plt.subplot(4, 1, 1)
plt.xlim(0, 512)
plt.title("Sparse signal")
plt.stem(idx, w[idx])
# plot the noise-free reconstruction
omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)
omp.fit(X, y)
coef = omp.coef_
idx_r, = coef.nonzero()
plt.subplot(4, 1, 2)
plt.xlim(0, 512)
936
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
plt.title("Recovered signal from noise-free measurements")
plt.stem(idx_r, coef[idx_r])
# plot the noisy reconstruction
omp.fit(X, y_noisy)
coef = omp.coef_
idx_r, = coef.nonzero()
plt.subplot(4, 1, 3)
plt.xlim(0, 512)
plt.title("Recovered signal from noisy measurements")
plt.stem(idx_r, coef[idx_r])
# plot the noisy reconstruction with number of non-zeros set by CV
18.23. Orthogonal Matching Pursuit
937
scikit-learn user guide, Release 0.18.2
omp_cv = OrthogonalMatchingPursuitCV()
omp_cv.fit(X, y_noisy)
coef = omp_cv.coef_
idx_r, = coef.nonzero()
plt.subplot(4, 1, 4)
plt.xlim(0, 512)
plt.title("Recovered signal from noisy measurements with CV")
plt.stem(idx_r, coef[idx_r])
plt.subplots_adjust(0.06, 0.04, 0.94, 0.90, 0.20, 0.38)
plt.suptitle('Sparse signal recovery with Orthogonal Matching Pursuit',
fontsize=16)
plt.show()
938
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.550 seconds)
Download Python source code: plot_omp.py
Download IPython notebook: plot_omp.ipynb
18.24 Plot multi-class SGD on the iris dataset
Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all
(OVA) classiﬁers are represented by the dashed lines.
18.24. Plot multi-class SGD on the iris dataset
939
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.linear_model import SGDClassifier
# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]
# we only take the first two features. We could
# avoid this ugly slicing by using a two-dim dataset
y = iris.target
colors = "bry"
# shuffle
idx = np.arange(X.shape[0])
np.random.seed(13)
np.random.shuffle(idx)
X = X[idx]
y = y[idx]
# standardize
mean = X.mean(axis=0)
std = X.std(axis=0)
X = (X - mean) / std
940
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
h = .02
# step size in the mesh
clf = SGDClassifier(alpha=0.001, n_iter=100).fit(X, y)
# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
plt.axis('tight')
# Plot also the training points
for i, color in zip(clf.classes_, colors):
idx = np.where(y == i)
plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],
cmap=plt.cm.Paired)
plt.title("Decision surface of multi-class SGD")
plt.axis('tight')
# Plot the three one-against-all classifiers
xmin, xmax = plt.xlim()
ymin, ymax = plt.ylim()
coef = clf.coef_
intercept = clf.intercept_
def plot_hyperplane(c, color):
def line(x0):
return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]
plt.plot([xmin, xmax], [line(xmin), line(xmax)],
ls="--", color=color)
for i, color in zip(clf.classes_, colors):
plot_hyperplane(i, color)
plt.legend()
plt.show()
Total running time of the script: (0 minutes 0.143 seconds)
Download Python source code: plot_sgd_iris.py
Download IPython notebook: plot_sgd_iris.ipynb
18.25 Theil-Sen Regression
Computes a Theil-Sen Regression on a synthetic dataset.
See Theil-Sen estimator: generalized-median-based estimator for more information on the regressor.
18.25. Theil-Sen Regression
941
scikit-learn user guide, Release 0.18.2
Compared to the OLS (ordinary least squares) estimator, the Theil-Sen estimator is robust against outliers. It has
a breakdown point of about 29.3% in case of a simple linear regression which means that it can tolerate arbitrary
corrupted data (outliers) of up to 29.3% in the two-dimensional case.
The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible com-
binations of p subsample points. If an intercept is ﬁtted, p must be greater than or equal to n_features + 1. The ﬁnal
slope and intercept is then deﬁned as the spatial median of these slopes and intercepts.
In certain cases Theil-Sen performs better than RANSAC which is also a robust method. This is illustrated in the second
example below where outliers with respect to the x-axis perturb RANSAC. Tuning the residual_threshold
parameter of RANSAC remedies this but in general a priori knowledge about the data and the nature of the outliers
is needed. Due to the computational complexity of Theil-Sen it is recommended to use it only for small problems in
terms of number of samples and features. For larger problems the max_subpopulation parameter restricts the
magnitude of all possible combinations of p subsample points to a randomly chosen subset and therefore also limits the
runtime. Therefore, Theil-Sen is applicable to larger problems with the drawback of losing some of its mathematical
properties since it then works on a random subset.
# Author: Florian Wilhelm -- <florian.wilhelm@gmail.com>
# License: BSD 3 clause
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, TheilSenRegressor
from sklearn.linear_model import RANSACRegressor
print(__doc__)
estimators = [('OLS', LinearRegression()),
('Theil-Sen', TheilSenRegressor(random_state=42)),
('RANSAC', RANSACRegressor(random_state=42)), ]
colors = {'OLS': 'turquoise', 'Theil-Sen': 'gold', 'RANSAC': 'lightgreen'}
lw = 2
Outliers only in the y direction
np.random.seed(0)
n_samples = 200
# Linear model y = 3*x + N(2, 0.1**2)
x = np.random.randn(n_samples)
w = 3.
c = 2.
noise = 0.1 * np.random.randn(n_samples)
y = w * x + c + noise
# 10% outliers
y[-20:] += -20 * x[-20:]
X = x[:, np.newaxis]
plt.scatter(x, y, color='indigo', marker='x', s=40)
line_x = np.array([-3, 3])
for name, estimator in estimators:
t0 = time.time()
estimator.fit(X, y)
elapsed_time = time.time() - t0
y_pred = estimator.predict(line_x.reshape(2, 1))
plt.plot(line_x, y_pred, color=colors[name], linewidth=lw,
label='%s (fit time: %.2fs)' % (name, elapsed_time))
942
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
plt.axis('tight')
plt.legend(loc='upper left')
plt.title("Corrupt y")
Outliers in the X direction
np.random.seed(0)
# Linear model y = 3*x + N(2, 0.1**2)
x = np.random.randn(n_samples)
noise = 0.1 * np.random.randn(n_samples)
y = 3 * x + 2 + noise
# 10% outliers
x[-20:] = 9.9
y[-20:] += 22
X = x[:, np.newaxis]
plt.figure()
plt.scatter(x, y, color='indigo', marker='x', s=40)
line_x = np.array([-3, 10])
for name, estimator in estimators:
t0 = time.time()
estimator.fit(X, y)
elapsed_time = time.time() - t0
y_pred = estimator.predict(line_x.reshape(2, 1))
18.25. Theil-Sen Regression
943
scikit-learn user guide, Release 0.18.2
plt.plot(line_x, y_pred, color=colors[name], linewidth=lw,
label='%s (fit time: %.2fs)' % (name, elapsed_time))
plt.axis('tight')
plt.legend(loc='upper left')
plt.title("Corrupt x")
plt.show()
Total running time of the script: (0 minutes 1.785 seconds)
Download Python source code: plot_theilsen.py
Download IPython notebook: plot_theilsen.ipynb
18.26 L1 Penalty and Sparsity in Logistic Regression
Comparison of the sparsity (percentage of zero coefﬁcients) of solutions when L1 and L2 penalty are used for different
values of C. We can see that large values of C give more freedom to the model. Conversely, smaller values of C
constrain the model more. In the L1 penalty case, this leads to sparser solutions.
We classify 8x8 images of digits into two classes: 0-4 against 5-9. The visualization shows coefﬁcients of the models
for varying C.
944
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
Out:
C=100.00
Sparsity with L1 penalty: 6.25%
score with L1 penalty: 0.9104
Sparsity with L2 penalty: 4.69%
score with L2 penalty: 0.9098
C=1.00
Sparsity with L1 penalty: 10.94%
score with L1 penalty: 0.9104
Sparsity with L2 penalty: 4.69%
score with L2 penalty: 0.9093
C=0.01
Sparsity with L1 penalty: 85.94%
score with L1 penalty: 0.8614
Sparsity with L2 penalty: 4.69%
score with L2 penalty: 0.8915
print(__doc__)
# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
18.26. L1 Penalty and Sparsity in Logistic Regression
945
scikit-learn user guide, Release 0.18.2
#
Mathieu Blondel <mathieu@mblondel.org>
#
Andreas Mueller <amueller@ais.uni-bonn.de>
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
digits = datasets.load_digits()
X, y = digits.data, digits.target
X = StandardScaler().fit_transform(X)
# classify small against large digits
y = (y > 4).astype(np.int)
# Set regularization parameter
for i, C in enumerate((100, 1, 0.01)):
# turn down tolerance for short training time
clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)
clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)
clf_l1_LR.fit(X, y)
clf_l2_LR.fit(X, y)
coef_l1_LR = clf_l1_LR.coef_.ravel()
coef_l2_LR = clf_l2_LR.coef_.ravel()
# coef_l1_LR contains zeros due to the
# L1 sparsity inducing norm
sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100
sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100
print("C=%.2f" % C)
print("Sparsity with L1 penalty: %.2f%%" % sparsity_l1_LR)
print("score with L1 penalty: %.4f" % clf_l1_LR.score(X, y))
print("Sparsity with L2 penalty: %.2f%%" % sparsity_l2_LR)
print("score with L2 penalty: %.4f" % clf_l2_LR.score(X, y))
l1_plot = plt.subplot(3, 2, 2 * i + 1)
l2_plot = plt.subplot(3, 2, 2 * (i + 1))
if i == 0:
l1_plot.set_title("L1 penalty")
l2_plot.set_title("L2 penalty")
l1_plot.imshow(np.abs(coef_l1_LR.reshape(8, 8)), interpolation='nearest',
cmap='binary', vmax=1, vmin=0)
l2_plot.imshow(np.abs(coef_l2_LR.reshape(8, 8)), interpolation='nearest',
cmap='binary', vmax=1, vmin=0)
plt.text(-8, 3, "C = %.2f" % C)
l1_plot.set_xticks(())
l1_plot.set_yticks(())
l2_plot.set_xticks(())
946
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
l2_plot.set_yticks(())
plt.show()
Total running time of the script: (0 minutes 0.664 seconds)
Download Python source code: plot_logistic_l1_l2_sparsity.py
Download IPython notebook: plot_logistic_l1_l2_sparsity.ipynb
18.27 Plot multinomial and One-vs-Rest Logistic Regression
Plot decision surface of multinomial and One-vs-Rest Logistic Regression. The hyperplanes corresponding to the
three One-vs-Rest (OVR) classiﬁers are represented by the dashed lines.
•
•
Out:
training score : 0.995 (multinomial)
training score : 0.976 (ovr)
18.27. Plot multinomial and One-vs-Rest Logistic Regression
947
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.linear_model import LogisticRegression
# make 3-class dataset for classification
centers = [[-5, 0], [0, 1.5], [5, -1]]
X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)
transformation = [[0.4, 0.2], [-0.4, 1.2]]
X = np.dot(X, transformation)
for multi_class in ('multinomial', 'ovr'):
clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,
multi_class=multi_class).fit(X, y)
# print the training scores
print("training score : %.3f (%s)" % (clf.score(X, y), multi_class))
# create a mesh to plot in
h = .02
# step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure()
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
plt.title("Decision surface of LogisticRegression (%s)" % multi_class)
plt.axis('tight')
# Plot also the training points
colors = "bry"
for i, color in zip(clf.classes_, colors):
idx = np.where(y == i)
plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)
# Plot the three one-against-all classifiers
xmin, xmax = plt.xlim()
ymin, ymax = plt.ylim()
coef = clf.coef_
intercept = clf.intercept_
def plot_hyperplane(c, color):
def line(x0):
return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]
plt.plot([xmin, xmax], [line(xmin), line(xmax)],
ls="--", color=color)
for i, color in zip(clf.classes_, colors):
948
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
plot_hyperplane(i, color)
plt.show()
Total running time of the script: (0 minutes 0.398 seconds)
Download Python source code: plot_logistic_multinomial.py
Download IPython notebook: plot_logistic_multinomial.ipynb
18.28 Robust linear estimator ﬁtting
Here a sine function is ﬁt with a polynomial of order 3, for values close to zero.
Robust ﬁtting is demoed in different situations:
• No measurement errors, only modelling errors (ﬁtting a sine with a polynomial)
• Measurement errors in X
• Measurement errors in y
The median absolute deviation to non corrupt new data is used to judge the quality of the prediction.
What we can see that:
• RANSAC is good for strong outliers in the y direction
• TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs
worse than OLS.
• The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does
not attempt to completely ﬁlter the outliers but lessen their effect.
•
•
18.28. Robust linear estimator ﬁtting
949
scikit-learn user guide, Release 0.18.2
•
•
•
from matplotlib import pyplot as plt
import numpy as np
from sklearn.linear_model import (
LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor)
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
np.random.seed(42)
X = np.random.normal(size=400)
y = np.sin(X)
# Make sure that it X is 2D
X = X[:, np.newaxis]
X_test = np.random.normal(size=200)
y_test = np.sin(X_test)
X_test = X_test[:, np.newaxis]
950
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
y_errors = y.copy()
y_errors[::3] = 3
X_errors = X.copy()
X_errors[::3] = 3
y_errors_large = y.copy()
y_errors_large[::3] = 10
X_errors_large = X.copy()
X_errors_large[::3] = 10
estimators = [('OLS', LinearRegression()),
('Theil-Sen', TheilSenRegressor(random_state=42)),
('RANSAC', RANSACRegressor(random_state=42)),
('HuberRegressor', HuberRegressor())]
colors = {'OLS': 'turquoise', 'Theil-Sen': 'gold', 'RANSAC': 'lightgreen',
˓→'HuberRegressor': 'black'}
linestyle = {'OLS': '-', 'Theil-Sen': '-.', 'RANSAC': '--', 'HuberRegressor': '--'}
lw = 3
x_plot = np.linspace(X.min(), X.max())
for title, this_X, this_y in [
('Modeling Errors Only', X, y),
('Corrupt X, Small Deviants', X_errors, y),
('Corrupt y, Small Deviants', X, y_errors),
('Corrupt X, Large Deviants', X_errors_large, y),
('Corrupt y, Large Deviants', X, y_errors_large)]:
plt.figure(figsize=(5, 4))
plt.plot(this_X[:, 0], this_y, 'b+')
for name, estimator in estimators:
model = make_pipeline(PolynomialFeatures(3), estimator)
model.fit(this_X, this_y)
mse = mean_squared_error(model.predict(X_test), y_test)
y_plot = model.predict(x_plot[:, np.newaxis])
plt.plot(x_plot, y_plot, color=colors[name], linestyle=linestyle[name],
linewidth=lw, label='%s: error = %.3f' % (name, mse))
legend_title = 'Error of Mean\nAbsolute Deviation\nto Non-corrupt Data'
legend = plt.legend(loc='upper right', frameon=False, title=legend_title,
prop=dict(size='x-small'))
plt.xlim(-4, 10.2)
plt.ylim(-2, 10.2)
plt.title(title)
plt.show()
Total running time of the script: (0 minutes 5.835 seconds)
Download Python source code: plot_robust_fit.py
Download IPython notebook: plot_robust_fit.ipynb
18.29 Lasso and Elastic Net
Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent.
18.29. Lasso and Elastic Net
951
scikit-learn user guide, Release 0.18.2
The coefﬁcients can be forced to be positive.
•
•
•
Out:
Computing regularization path using the lasso...
Computing regularization path using the positive lasso...
Computing regularization path using the elastic net...
Computing regularization path using the positive elastic net...
952
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause
from itertools import cycle
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import lasso_path, enet_path
from sklearn import datasets
diabetes = datasets.load_diabetes()
X = diabetes.data
y = diabetes.target
X /= X.std(axis=0)
# Standardize data (easier to set the l1_ratio parameter)
# Compute paths
eps = 5e-3
# the smaller it is the longer is the path
print("Computing regularization path using the lasso...")
alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)
print("Computing regularization path using the positive lasso...")
alphas_positive_lasso, coefs_positive_lasso, _ = lasso_path(
X, y, eps, positive=True, fit_intercept=False)
print("Computing regularization path using the elastic net...")
alphas_enet, coefs_enet, _ = enet_path(
X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)
print("Computing regularization path using the positive elastic net...")
alphas_positive_enet, coefs_positive_enet, _ = enet_path(
X, y, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False)
# Display results
plt.figure(1)
ax = plt.gca()
colors = cycle(['b', 'r', 'g', 'c', 'k'])
neg_log_alphas_lasso = -np.log10(alphas_lasso)
neg_log_alphas_enet = -np.log10(alphas_enet)
for coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):
l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)
l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle='--', c=c)
plt.xlabel('-Log(alpha)')
plt.ylabel('coefficients')
plt.title('Lasso and Elastic-Net Paths')
plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')
plt.axis('tight')
plt.figure(2)
ax = plt.gca()
neg_log_alphas_positive_lasso = -np.log10(alphas_positive_lasso)
18.29. Lasso and Elastic Net
953
scikit-learn user guide, Release 0.18.2
for coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors):
l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)
l2 = plt.plot(neg_log_alphas_positive_lasso, coef_pl, linestyle='--', c=c)
plt.xlabel('-Log(alpha)')
plt.ylabel('coefficients')
plt.title('Lasso and positive Lasso')
plt.legend((l1[-1], l2[-1]), ('Lasso', 'positive Lasso'), loc='lower left')
plt.axis('tight')
plt.figure(3)
ax = plt.gca()
neg_log_alphas_positive_enet = -np.log10(alphas_positive_enet)
for (coef_e, coef_pe, c) in zip(coefs_enet, coefs_positive_enet, colors):
l1 = plt.plot(neg_log_alphas_enet, coef_e, c=c)
l2 = plt.plot(neg_log_alphas_positive_enet, coef_pe, linestyle='--', c=c)
plt.xlabel('-Log(alpha)')
plt.ylabel('coefficients')
plt.title('Elastic-Net and positive Elastic-Net')
plt.legend((l1[-1], l2[-1]), ('Elastic-Net', 'positive Elastic-Net'),
loc='lower left')
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.377 seconds)
Download Python source code: plot_lasso_coordinate_descent_path.py
Download IPython notebook: plot_lasso_coordinate_descent_path.ipynb
18.30 Lasso model selection: Cross-Validation / AIC / BIC
Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an
optimal value of the regularization parameter alpha of the Lasso estimator.
Results obtained with LassoLarsIC are based on AIC/BIC criteria.
Information-criterion based model selection is very fast, but it relies on a proper estimation of degrees of freedom, are
derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated
by this model. They also tend to break when the problem is badly conditioned (more features than samples).
For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented
by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms
give roughly the same results. They differ with regards to their execution speed and sources of numerical errors.
Lars computes a path solution only for each kink in the path. As a result, it is very efﬁcient when there are only of few
kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting
any meta parameter. On the opposite, coordinate descent compute the path points on a pre-speciﬁed grid (here we use
the default). Thus it is more efﬁcient if the number of grid points is smaller than the number of kinks in the path. Such
a strategy can be interesting if the number of features is really large and there are enough samples to select a large
amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the
coordinate descent algorithm will only sample the path on a grid.
Note how the optimal value of alpha varies for each fold. This illustrates why nested-cross validation is necessary
when trying to evaluate the performance of a method for which a parameter is chosen by cross-validation: this choice
954
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
of parameter may not be optimal for unseen data.
print(__doc__)
# Author: Olivier Grisel, Gael Varoquaux, Alexandre Gramfort
# License: BSD 3 clause
import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC
from sklearn import datasets
diabetes = datasets.load_diabetes()
X = diabetes.data
y = diabetes.target
rng = np.random.RandomState(42)
X = np.c_[X, rng.randn(X.shape[0], 14)]
# add some bad features
# normalize data as done by Lars to allow for comparison
X /= np.sqrt(np.sum(X ** 2, axis=0))
LassoLarsIC: least angle regression with BIC/AIC criterion
model_bic = LassoLarsIC(criterion='bic')
t1 = time.time()
model_bic.fit(X, y)
t_bic = time.time() - t1
alpha_bic_ = model_bic.alpha_
model_aic = LassoLarsIC(criterion='aic')
model_aic.fit(X, y)
alpha_aic_ = model_aic.alpha_
def plot_ic_criterion(model, name, color):
alpha_ = model.alpha_
alphas_ = model.alphas_
criterion_ = model.criterion_
plt.plot(-np.log10(alphas_), criterion_, '--', color=color,
linewidth=3, label='%s criterion' % name)
plt.axvline(-np.log10(alpha_), color=color, linewidth=3,
label='alpha: %s estimate' % name)
plt.xlabel('-log(alpha)')
plt.ylabel('criterion')
plt.figure()
plot_ic_criterion(model_aic, 'AIC', 'b')
plot_ic_criterion(model_bic, 'BIC', 'r')
plt.legend()
plt.title('Information-criterion for model selection (training time %.3fs)'
% t_bic)
18.30. Lasso model selection: Cross-Validation / AIC / BIC
955
scikit-learn user guide, Release 0.18.2
LassoCV: coordinate descent
# Compute paths
print("Computing regularization path using the coordinate descent lasso...")
t1 = time.time()
model = LassoCV(cv=20).fit(X, y)
t_lasso_cv = time.time() - t1
# Display results
m_log_alphas = -np.log10(model.alphas_)
plt.figure()
ymin, ymax = 2300, 3800
plt.plot(m_log_alphas, model.mse_path_, ':')
plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',
label='Average across the folds', linewidth=2)
plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',
label='alpha: CV estimate')
plt.legend()
plt.xlabel('-log(alpha)')
plt.ylabel('Mean square error')
plt.title('Mean square error on each fold: coordinate descent '
'(train time: %.2fs)' % t_lasso_cv)
plt.axis('tight')
956
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
plt.ylim(ymin, ymax)
Out:
Computing regularization path using the coordinate descent lasso...
LassoLarsCV: least angle regression
# Compute paths
print("Computing regularization path using the Lars lasso...")
t1 = time.time()
model = LassoLarsCV(cv=20).fit(X, y)
t_lasso_lars_cv = time.time() - t1
# Display results
m_log_alphas = -np.log10(model.cv_alphas_)
plt.figure()
plt.plot(m_log_alphas, model.cv_mse_path_, ':')
plt.plot(m_log_alphas, model.cv_mse_path_.mean(axis=-1), 'k',
label='Average across the folds', linewidth=2)
plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',
label='alpha CV')
plt.legend()
18.30. Lasso model selection: Cross-Validation / AIC / BIC
957
scikit-learn user guide, Release 0.18.2
plt.xlabel('-log(alpha)')
plt.ylabel('Mean square error')
plt.title('Mean square error on each fold: Lars (train time: %.2fs)'
% t_lasso_lars_cv)
plt.axis('tight')
plt.ylim(ymin, ymax)
plt.show()
Out:
Computing regularization path using the Lars lasso...
Total running time of the script: (0 minutes 1.109 seconds)
Download Python source code: plot_lasso_model_selection.py
Download IPython notebook: plot_lasso_model_selection.ipynb
18.31 Sparse recovery: feature selection for sparse linear models
Given a small number of observations, we want to recover which features of X are relevant to explain y. For this sparse
linear models can outperform standard statistical tests if the true model is sparse, i.e. if a small fraction of the features
are relevant.
958
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
As detailed in the compressive sensing notes, the ability of L1-based approach to identify the relevant variables de-
pends on the sparsity of the ground truth, the number of samples, the number of features, the conditioning of the
design matrix on the signal subspace, the amount of noise, and the absolute value of the smallest non-zero coefﬁcient
[Wainwright2006] (http://statistics.berkeley.edu/sites/default/ﬁles/tech-reports/709.pdf).
Here we keep all parameters constant and vary the conditioning of the design matrix. For a well-conditioned design
matrix (small mutual incoherence) we are exactly in compressive sensing conditions (i.i.d Gaussian sensing matrix),
and L1-recovery with the Lasso performs very well. For an ill-conditioned matrix (high mutual incoherence), regres-
sors are very correlated, and the Lasso randomly selects one. However, randomized-Lasso can recover the ground
truth well.
In each situation, we ﬁrst vary the alpha parameter setting the sparsity of the estimated model and look at the stability
scores of the randomized Lasso. This analysis, knowing the ground truth, shows an optimal regime in which relevant
features stand out from the irrelevant ones. If alpha is chosen too small, non-relevant variables enter the model. On
the opposite, if alpha is selected too large, the Lasso is equivalent to stepwise regression, and thus brings no advantage
over a univariate F-test.
In a second time, we set alpha and compare the performance of different feature selection methods, using the area
under curve (AUC) of the precision-recall.
•
•
18.31. Sparse recovery: feature selection for sparse linear models
959
scikit-learn user guide, Release 0.18.2
•
•
print(__doc__)
# Author: Alexandre Gramfort and Gael Varoquaux
# License: BSD 3 clause
import warnings
import matplotlib.pyplot as plt
import numpy as np
from scipy import linalg
from sklearn.linear_model import (RandomizedLasso, lasso_stability_path,
LassoLarsCV)
from sklearn.feature_selection import f_regression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import auc, precision_recall_curve
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.utils.extmath import pinvh
from sklearn.exceptions import ConvergenceWarning
def mutual_incoherence(X_relevant, X_irelevant):
"""Mutual incoherence, as defined by formula (26a) of [Wainwright2006].
"""
projector = np.dot(np.dot(X_irelevant.T, X_relevant),
pinvh(np.dot(X_relevant.T, X_relevant)))
return np.max(np.abs(projector).sum(axis=1))
960
Chapter 18. Generalized Linear Models
scikit-learn user guide, Release 0.18.2
for conditioning in (1, 1e-4):
###########################################################################
# Simulate regression data with a correlated design
n_features = 501
n_relevant_features = 3
noise_level = .2
coef_min = .2
# The Donoho-Tanner phase transition is around n_samples=25: below we
# will completely fail to recover in the well-conditioned case
n_samples = 25
block_size = n_relevant_features
rng = np.random.RandomState(42)
# The coefficients of our model
coef = np.zeros(n_features)
coef[:n_relevant_features] = coef_min + rng.rand(n_relevant_features)
# The correlation of our design: variables correlated by blocs of 3
corr = np.zeros((n_features, n_features))
for i in range(0, n_features, block_size):
corr[i:i + block_size, i:i + block_size] = 1 - conditioning
corr.flat[::n_features + 1] = 1
corr = linalg.cholesky(corr)
# Our design
X = rng.normal(size=(n_samples, n_features))
X = np.dot(X, corr)
# Keep [Wainwright2006] (26c) constant
X[:n_relevant_features] /= np.abs(
linalg.svdvals(X[:n_relevant_features])).max()
X = StandardScaler().fit_transform(X.copy())
# The output variable
y = np.dot(X, coef)
y /= np.std(y)
# We scale the added noise as a function of the average correlation
# between the design and the output variable
y += noise_level * rng.normal(size=n_samples)
mi = mutual_incoherence(X[:, :n_relevant_features],
X[:, n_relevant_features:])
###########################################################################
# Plot stability selection path, using a high eps for early stopping
# of the path, to save computation time
alpha_grid, scores_path = lasso_stability_path(X, y, random_state=42,
eps=0.05)
plt.figure()
# We plot the path as a function of alpha/alpha_max to the power 1/3: the
# power 1/3 scales the path less brutally than the log, and enables to
# see the progression along the path
hg = plt.plot(alpha_grid[1:] ** .333, scores_path[coef != 0].T[1:], 'r')
hb = plt.plot(alpha_grid[1:] ** .333, scores_path[coef == 0].T[1:], 'k')
ymin, ymax = plt.ylim()
plt.xlabel(r'$(\alpha / \alpha_{max})^{1/3}$')
plt.ylabel('Stability score: proportion of times selected')
18.31. Sparse recovery: feature selection for sparse linear models
961
scikit-learn user guide, Release 0.18.2
plt.title('Stability Scores Path - Mutual incoherence: %.1f' % mi)
plt.axis('tight')
plt.legend((hg[0], hb[0]), ('relevant features', 'irrelevant features'),
loc='best')
###########################################################################
# Plot the estimated stability scores for a given alpha
# Use 6-fold cross-validation rather than the default 3-fold: it leads to
# a better choice of alpha:
# Stop the user warnings outputs- they are not necessary for the example
# as it is specifically set up to be challenging.
with warnings.catch_warnings():
warnings.simplefilter('ignore', UserWarning)
warnings.simplefilter('ignore', ConvergenceWarning)
lars_cv = LassoLarsCV(cv=6).fit(X, y)
# Run the RandomizedLasso: we use a paths going down to .1*alpha_max
# to avoid exploring the regime in which very noisy variables enter
# the model
alphas = np.linspace(lars_cv.alphas_[0], .1 * lars_cv.alphas_[0], 6)
clf = RandomizedLasso(alpha=alphas, random_state=42).fit(X, y)
trees = ExtraTreesRegressor(100).fit(X, y)
# Compare with F-score
F, _ = f_regression(X, y)
plt.figure()
for name, score in [('F-test', F),
('Stability selection', clf.scores_),
('Lasso coefs', np.abs(lars_cv.coef_)),
('Trees', trees.feature_importances_),
]:
precision, recall, thresholds = precision_recall_curve(coef != 0,
score)
plt.semilogy(np.maximum(score / np.max(score), 1e-4),
label="%s. AUC: %.3f" % (name, auc(recall, precision)))
plt.plot(np.where(coef != 0)[0], [2e-4] * n_relevant_features, 'mo',
label="Ground truth")
plt.xlabel("Features")
plt.ylabel("Score")
# Plot only the 100 first coefficients
plt.xlim(0, 100)
plt.legend(loc='best')
plt.title('Feature selection scores - Mutual incoherence: %.1f'
% mi)
plt.show()
Total running time of the script: (0 minutes 8.912 seconds)
Download Python source code: plot_sparse_recovery.py
Download IPython notebook: plot_sparse_recovery.ipynb
962
Chapter 18. Generalized Linear Models
CHAPTER
NINETEEN
MANIFOLD LEARNING
Examples concerning the sklearn.manifold module.
19.1 Swiss Roll reduction with LLE
An illustration of Swiss Roll reduction with locally linear embedding
Out:
Computing LLE embedding
Done. Reconstruction error: 9.45483e-08
963
scikit-learn user guide, Release 0.18.2
# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>
# License: BSD 3 clause (C) INRIA 2011
print(__doc__)
import matplotlib.pyplot as plt
# This import is needed to modify the way figure behaves
from mpl_toolkits.mplot3d import Axes3D
Axes3D
#----------------------------------------------------------------------
# Locally linear embedding of the swiss roll
from sklearn import manifold, datasets
X, color = datasets.samples_generator.make_swiss_roll(n_samples=1500)
print("Computing LLE embedding")
X_r, err = manifold.locally_linear_embedding(X, n_neighbors=12,
n_components=2)
print("Done. Reconstruction error: %g" % err)
#----------------------------------------------------------------------
# Plot result
fig = plt.figure()
try:
# compatibility matplotlib < 1.0
ax = fig.add_subplot(211, projection='3d')
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
except:
ax = fig.add_subplot(211)
ax.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax.set_title("Original data")
ax = fig.add_subplot(212)
ax.scatter(X_r[:, 0], X_r[:, 1], c=color, cmap=plt.cm.Spectral)
plt.axis('tight')
plt.xticks([]), plt.yticks([])
plt.title('Projected data')
plt.show()
Total running time of the script: (0 minutes 0.472 seconds)
Download Python source code: plot_swissroll.py
Download IPython notebook: plot_swissroll.ipynb
19.2 Multi-dimensional scaling
An illustration of the metric and non-metric MDS on generated noisy data.
The reconstructed points using the metric MDS and non metric MDS are slightly shifted to avoid overlapping.
964
Chapter 19. Manifold learning
scikit-learn user guide, Release 0.18.2
# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>
# License: BSD
print(__doc__)
import numpy as np
from matplotlib import pyplot as plt
from matplotlib.collections import LineCollection
from sklearn import manifold
from sklearn.metrics import euclidean_distances
from sklearn.decomposition import PCA
n_samples = 20
seed = np.random.RandomState(seed=3)
X_true = seed.randint(0, 20, 2 * n_samples).astype(np.float)
X_true = X_true.reshape((n_samples, 2))
# Center the data
X_true -= X_true.mean()
similarities = euclidean_distances(X_true)
# Add noise to the similarities
noise = np.random.rand(n_samples, n_samples)
noise = noise + noise.T
noise[np.arange(noise.shape[0]), np.arange(noise.shape[0])] = 0
19.2. Multi-dimensional scaling
965
scikit-learn user guide, Release 0.18.2
similarities += noise
mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,
dissimilarity="precomputed", n_jobs=1)
pos = mds.fit(similarities).embedding_
nmds = manifold.MDS(n_components=2, metric=False, max_iter=3000, eps=1e-12,
dissimilarity="precomputed", random_state=seed, n_jobs=1,
n_init=1)
npos = nmds.fit_transform(similarities, init=pos)
# Rescale the data
pos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((pos ** 2).sum())
npos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((npos ** 2).sum())
# Rotate the data
clf = PCA(n_components=2)
X_true = clf.fit_transform(X_true)
pos = clf.fit_transform(pos)
npos = clf.fit_transform(npos)
fig = plt.figure(1)
ax = plt.axes([0., 0., 1., 1.])
s = 100
plt.scatter(X_true[:, 0], X_true[:, 1], color='navy', s=s, lw=0,
label='True Position')
plt.scatter(pos[:, 0], pos[:, 1], color='turquoise', s=s, lw=0, label='MDS')
plt.scatter(npos[:, 0], npos[:, 1], color='darkorange', s=s, lw=0, label='NMDS')
plt.legend(scatterpoints=1, loc='best', shadow=False)
similarities = similarities.max() / similarities * 100
similarities[np.isinf(similarities)] = 0
# Plot the edges
start_idx, end_idx = np.where(pos)
# a sequence of (*line0*, *line1*, *line2*), where::
#
linen = (x0, y0), (x1, y1), ... (xm, ym)
segments = [[X_true[i, :], X_true[j, :]]
for i in range(len(pos)) for j in range(len(pos))]
values = np.abs(similarities)
lc = LineCollection(segments,
zorder=0, cmap=plt.cm.Blues,
norm=plt.Normalize(0, values.max()))
lc.set_array(similarities.flatten())
lc.set_linewidths(0.5 * np.ones(len(segments)))
ax.add_collection(lc)
plt.show()
Total running time of the script: (0 minutes 0.191 seconds)
Download Python source code: plot_mds.py
Download IPython notebook: plot_mds.ipynb
966
Chapter 19. Manifold learning
scikit-learn user guide, Release 0.18.2
19.3 Comparison of Manifold Learning methods
An illustration of dimensionality reduction on the S-curve dataset with various manifold learning methods.
For a discussion and comparison of these algorithms, see the manifold module page
For a similar example, where the methods are applied to a sphere dataset, see Manifold Learning methods on a severed
sphere
Note that the purpose of the MDS is to ﬁnd a low-dimensional representation of the data (here 2D) in which the
distances respect well the distances in the original high-dimensional space, unlike other manifold-learning algorithms,
it does not seeks an isotropic representation of the data in the low-dimensional space.
Out:
standard: 0.2 sec
ltsa: 0.35 sec
hessian: 0.48 sec
modified: 0.39 sec
Isomap: 0.47 sec
MDS: 2.4 sec
SpectralEmbedding: 0.21 sec
t-SNE: 3.8 sec
# Author: Jake Vanderplas -- <vanderplas@astro.washington.edu>
print(__doc__)
from time import time
19.3. Comparison of Manifold Learning methods
967
scikit-learn user guide, Release 0.18.2
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
from sklearn import manifold, datasets
# Next line to silence pyflakes. This import is needed.
Axes3D
n_points = 1000
X, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)
n_neighbors = 10
n_components = 2
fig = plt.figure(figsize=(15, 8))
plt.suptitle("Manifold Learning with %i points, %i neighbors"
% (1000, n_neighbors), fontsize=14)
try:
# compatibility matplotlib < 1.0
ax = fig.add_subplot(251, projection='3d')
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax.view_init(4, -72)
except:
ax = fig.add_subplot(251, projection='3d')
plt.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)
methods = ['standard', 'ltsa', 'hessian', 'modified']
labels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']
for i, method in enumerate(methods):
t0 = time()
Y = manifold.LocallyLinearEmbedding(n_neighbors, n_components,
eigen_solver='auto',
method=method).fit_transform(X)
t1 = time()
print("%s: %.2g sec" % (methods[i], t1 - t0))
ax = fig.add_subplot(252 + i)
plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
plt.title("%s (%.2g sec)" % (labels[i], t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
t0 = time()
Y = manifold.Isomap(n_neighbors, n_components).fit_transform(X)
t1 = time()
print("Isomap: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(257)
plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
plt.title("Isomap (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
t0 = time()
968
Chapter 19. Manifold learning
scikit-learn user guide, Release 0.18.2
mds = manifold.MDS(n_components, max_iter=100, n_init=1)
Y = mds.fit_transform(X)
t1 = time()
print("MDS: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(258)
plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
plt.title("MDS (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
t0 = time()
se = manifold.SpectralEmbedding(n_components=n_components,
n_neighbors=n_neighbors)
Y = se.fit_transform(X)
t1 = time()
print("SpectralEmbedding: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(259)
plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
plt.title("SpectralEmbedding (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
t0 = time()
tsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)
Y = tsne.fit_transform(X)
t1 = time()
print("t-SNE: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(2, 5, 10)
plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
plt.title("t-SNE (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 9.038 seconds)
Download Python source code: plot_compare_methods.py
Download IPython notebook: plot_compare_methods.ipynb
19.4 Manifold Learning methods on a severed sphere
An application of the different Manifold learning techniques on a spherical data-set. Here one can see the use of
dimensionality reduction in order to gain some intuition regarding the manifold learning methods. Regarding the
dataset, the poles are cut from the sphere, as well as a thin slice down its side. This enables the manifold learning
techniques to ‘spread it open’ whilst projecting it onto two dimensions.
For a similar example, where the methods are applied to the S-curve dataset, see Comparison of Manifold Learning
methods
Note that the purpose of the MDS is to ﬁnd a low-dimensional representation of the data (here 2D) in which the
distances respect well the distances in the original high-dimensional space, unlike other manifold-learning algorithms,
19.4. Manifold Learning methods on a severed sphere
969
scikit-learn user guide, Release 0.18.2
it does not seeks an isotropic representation of the data in the low-dimensional space. Here the manifold problem
matches fairly that of representing a ﬂat map of the Earth, as with map projection
Out:
standard: 0.15 sec
ltsa: 0.23 sec
hessian: 0.35 sec
modified: 0.28 sec
ISO: 0.26 sec
MDS: 1.1 sec
Spectral Embedding: 0.17 sec
t-SNE: 2.4 sec
# Author: Jaques Grobler <jaques.grobler@inria.fr>
# License: BSD 3 clause
print(__doc__)
from time import time
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
from sklearn import manifold
from sklearn.utils import check_random_state
# Next line to silence pyflakes.
970
Chapter 19. Manifold learning
scikit-learn user guide, Release 0.18.2
Axes3D
# Variables for manifold learning.
n_neighbors = 10
n_samples = 1000
# Create our sphere.
random_state = check_random_state(0)
p = random_state.rand(n_samples) * (2 * np.pi - 0.55)
t = random_state.rand(n_samples) * np.pi
# Sever the poles from the sphere.
indices = ((t < (np.pi - (np.pi / 8))) & (t > ((np.pi / 8))))
colors = p[indices]
x, y, z = np.sin(t[indices]) * np.cos(p[indices]), \
np.sin(t[indices]) * np.sin(p[indices]), \
np.cos(t[indices])
# Plot our dataset.
fig = plt.figure(figsize=(15, 8))
plt.suptitle("Manifold Learning with %i points, %i neighbors"
% (1000, n_neighbors), fontsize=14)
ax = fig.add_subplot(251, projection='3d')
ax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)
try:
# compatibility matplotlib < 1.0
ax.view_init(40, -10)
except:
pass
sphere_data = np.array([x, y, z]).T
# Perform Locally Linear Embedding Manifold learning
methods = ['standard', 'ltsa', 'hessian', 'modified']
labels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']
for i, method in enumerate(methods):
t0 = time()
trans_data = manifold\
.LocallyLinearEmbedding(n_neighbors, 2,
method=method).fit_transform(sphere_data).T
t1 = time()
print("%s: %.2g sec" % (methods[i], t1 - t0))
ax = fig.add_subplot(252 + i)
plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)
plt.title("%s (%.2g sec)" % (labels[i], t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
# Perform Isomap Manifold learning.
t0 = time()
trans_data = manifold.Isomap(n_neighbors, n_components=2)\
.fit_transform(sphere_data).T
t1 = time()
print("%s: %.2g sec" % ('ISO', t1 - t0))
19.4. Manifold Learning methods on a severed sphere
971
scikit-learn user guide, Release 0.18.2
ax = fig.add_subplot(257)
plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)
plt.title("%s (%.2g sec)" % ('Isomap', t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
# Perform Multi-dimensional scaling.
t0 = time()
mds = manifold.MDS(2, max_iter=100, n_init=1)
trans_data = mds.fit_transform(sphere_data).T
t1 = time()
print("MDS: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(258)
plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)
plt.title("MDS (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
# Perform Spectral Embedding.
t0 = time()
se = manifold.SpectralEmbedding(n_components=2,
n_neighbors=n_neighbors)
trans_data = se.fit_transform(sphere_data).T
t1 = time()
print("Spectral Embedding: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(259)
plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)
plt.title("Spectral Embedding (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
# Perform t-distributed stochastic neighbor embedding.
t0 = time()
tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)
trans_data = tsne.fit_transform(sphere_data).T
t1 = time()
print("t-SNE: %.2g sec" % (t1 - t0))
ax = fig.add_subplot(2, 5, 10)
plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)
plt.title("t-SNE (%.2g sec)" % (t1 - t0))
ax.xaxis.set_major_formatter(NullFormatter())
ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 5.666 seconds)
Download Python source code: plot_manifold_sphere.py
Download IPython notebook: plot_manifold_sphere.ipynb
972
Chapter 19. Manifold learning
scikit-learn user guide, Release 0.18.2
19.5 Manifold learning on handwritten digits: Locally Linear Embed-
ding, Isomap...
An illustration of various embeddings on the digits dataset.
The RandomTreesEmbedding, from the sklearn.ensemble module, is not technically a manifold embedding
method, as it learn a high-dimensional representation on which we apply a dimensionality reduction method. However,
it is often useful to cast a dataset into a representation in which the classes are linearly-separable.
t-SNE will be initialized with the embedding that is generated by PCA in this example, which is not the default setting.
It ensures global stability of the embedding, i.e., the embedding does not depend on random initialization.
•
•
•
19.5. Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
973
scikit-learn user guide, Release 0.18.2
•
•
•
•
974
Chapter 19. Manifold learning
scikit-learn user guide, Release 0.18.2
•
•
•
•
19.5. Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
975
scikit-learn user guide, Release 0.18.2
•
•
Out:
Computing random projection
Computing PCA projection
Computing Linear Discriminant Analysis projection
Computing Isomap embedding
Done.
Computing LLE embedding
Done. Reconstruction error: 1.63544e-06
Computing modified LLE embedding
Done. Reconstruction error: 0.360832
Computing Hessian LLE embedding
Done. Reconstruction error: 0.212807
Computing LTSA embedding
Done. Reconstruction error: 0.212806
Computing MDS embedding
Done. Stress: 150446492.243191
Computing Totally Random Trees embedding
Computing Spectral embedding
Computing t-SNE embedding
# Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>
#
Olivier Grisel <olivier.grisel@ensta.org>
976
Chapter 19. Manifold learning
scikit-learn user guide, Release 0.18.2
#
Mathieu Blondel <mathieu@mblondel.org>
#
Gael Varoquaux
# License: BSD 3 clause (C) INRIA 2011
print(__doc__)
from time import time
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import offsetbox
from sklearn import (manifold, datasets, decomposition, ensemble,
discriminant_analysis, random_projection)
digits = datasets.load_digits(n_class=6)
X = digits.data
y = digits.target
n_samples, n_features = X.shape
n_neighbors = 30
#----------------------------------------------------------------------
# Scale and visualize the embedding vectors
def plot_embedding(X, title=None):
x_min, x_max = np.min(X, 0), np.max(X, 0)
X = (X - x_min) / (x_max - x_min)
plt.figure()
ax = plt.subplot(111)
for i in range(X.shape[0]):
plt.text(X[i, 0], X[i, 1], str(digits.target[i]),
color=plt.cm.Set1(y[i] / 10.),
fontdict={'weight': 'bold', 'size': 9})
if hasattr(offsetbox, 'AnnotationBbox'):
# only print thumbnails with matplotlib > 1.0
shown_images = np.array([[1., 1.]])
# just something big
for i in range(digits.data.shape[0]):
dist = np.sum((X[i] - shown_images) ** 2, 1)
if np.min(dist) < 4e-3:
# don't show points that are too close
continue
shown_images = np.r_[shown_images, [X[i]]]
imagebox = offsetbox.AnnotationBbox(
offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),
X[i])
ax.add_artist(imagebox)
plt.xticks([]), plt.yticks([])
if title is not None:
plt.title(title)
#----------------------------------------------------------------------
# Plot images of the digits
n_img_per_row = 20
img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))
for i in range(n_img_per_row):
ix = 10 * i + 1
for j in range(n_img_per_row):
19.5. Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
977
scikit-learn user guide, Release 0.18.2
iy = 10 * j + 1
img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))
plt.imshow(img, cmap=plt.cm.binary)
plt.xticks([])
plt.yticks([])
plt.title('A selection from the 64-dimensional digits dataset')
#----------------------------------------------------------------------
# Random 2D projection using a random unitary matrix
print("Computing random projection")
rp = random_projection.SparseRandomProjection(n_components=2, random_state=42)
X_projected = rp.fit_transform(X)
plot_embedding(X_projected, "Random Projection of the digits")
#----------------------------------------------------------------------
# Projection on to the first 2 principal components
print("Computing PCA projection")
t0 = time()
X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)
plot_embedding(X_pca,
"Principal Components projection of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# Projection on to the first 2 linear discriminant components
print("Computing Linear Discriminant Analysis projection")
X2 = X.copy()
X2.flat[::X.shape[1] + 1] += 0.01
# Make X invertible
t0 = time()
X_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2).fit_
˓→transform(X2, y)
plot_embedding(X_lda,
"Linear Discriminant projection of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# Isomap projection of the digits dataset
print("Computing Isomap embedding")
t0 = time()
X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)
print("Done.")
plot_embedding(X_iso,
"Isomap projection of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# Locally linear embedding of the digits dataset
print("Computing LLE embedding")
clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
method='standard')
t0 = time()
978
Chapter 19. Manifold learning
scikit-learn user guide, Release 0.18.2
X_lle = clf.fit_transform(X)
print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
plot_embedding(X_lle,
"Locally Linear Embedding of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# Modified Locally linear embedding of the digits dataset
print("Computing modified LLE embedding")
clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
method='modified')
t0 = time()
X_mlle = clf.fit_transform(X)
print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
plot_embedding(X_mlle,
"Modified Locally Linear Embedding of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# HLLE embedding of the digits dataset
print("Computing Hessian LLE embedding")
clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
method='hessian')
t0 = time()
X_hlle = clf.fit_transform(X)
print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
plot_embedding(X_hlle,
"Hessian Locally Linear Embedding of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# LTSA embedding of the digits dataset
print("Computing LTSA embedding")
clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
method='ltsa')
t0 = time()
X_ltsa = clf.fit_transform(X)
print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
plot_embedding(X_ltsa,
"Local Tangent Space Alignment of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# MDS
embedding of the digits dataset
print("Computing MDS embedding")
clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)
t0 = time()
X_mds = clf.fit_transform(X)
print("Done. Stress: %f" % clf.stress_)
plot_embedding(X_mds,
"MDS embedding of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# Random Trees embedding of the digits dataset
19.5. Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
979
scikit-learn user guide, Release 0.18.2
print("Computing Totally Random Trees embedding")
hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,
max_depth=5)
t0 = time()
X_transformed = hasher.fit_transform(X)
pca = decomposition.TruncatedSVD(n_components=2)
X_reduced = pca.fit_transform(X_transformed)
plot_embedding(X_reduced,
"Random forest embedding of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# Spectral embedding of the digits dataset
print("Computing Spectral embedding")
embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,
eigen_solver="arpack")
t0 = time()
X_se = embedder.fit_transform(X)
plot_embedding(X_se,
"Spectral embedding of the digits (time %.2fs)" %
(time() - t0))
#----------------------------------------------------------------------
# t-SNE embedding of the digits dataset
print("Computing t-SNE embedding")
tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)
t0 = time()
X_tsne = tsne.fit_transform(X)
plot_embedding(X_tsne,
"t-SNE embedding of the digits (time %.2fs)" %
(time() - t0))
plt.show()
Total running time of the script: (0 minutes 25.460 seconds)
Download Python source code: plot_lle_digits.py
Download IPython notebook: plot_lle_digits.ipynb
980
Chapter 19. Manifold learning
CHAPTER
TWENTY
GAUSSIAN MIXTURE MODELS
Examples concerning the sklearn.mixture module.
20.1 Density Estimation for a Gaussian mixture
Plot the density estimation of a mixture of two Gaussians. Data is generated from two Gaussians with different centers
and covariance matrices.
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
981
scikit-learn user guide, Release 0.18.2
from sklearn import mixture
n_samples = 300
# generate random sample, two components
np.random.seed(0)
# generate spherical data centered on (20, 20)
shifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])
# generate zero centered stretched Gaussian data
C = np.array([[0., -0.7], [3.5, .7]])
stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)
# concatenate the two datasets into the final training set
X_train = np.vstack([shifted_gaussian, stretched_gaussian])
# fit a Gaussian Mixture Model with two components
clf = mixture.GaussianMixture(n_components=2, covariance_type='full')
clf.fit(X_train)
# display predicted scores by the model as a contour plot
x = np.linspace(-20., 30.)
y = np.linspace(-20., 40.)
X, Y = np.meshgrid(x, y)
XX = np.array([X.ravel(), Y.ravel()]).T
Z = -clf.score_samples(XX)
Z = Z.reshape(X.shape)
CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),
levels=np.logspace(0, 3, 10))
CB = plt.colorbar(CS, shrink=0.8, extend='both')
plt.scatter(X_train[:, 0], X_train[:, 1], .8)
plt.title('Negative log-likelihood predicted by a GMM')
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.213 seconds)
Download Python source code: plot_gmm_pdf.py
Download IPython notebook: plot_gmm_pdf.ipynb
20.2 Gaussian Mixture Model Ellipsoids
Plot the conﬁdence ellipsoids of a mixture of two Gaussians obtained with Expectation Maximisation
(GaussianMixture class) and Variational Inference (BayesianGaussianMixture class models with a
Dirichlet process prior).
Both models have access to ﬁve components with which to ﬁt the data. Note that the Expectation Maximisation
model will necessarily use all ﬁve components while the Variational Inference model will effectively only use as many
as are needed for a good ﬁt. Here we can see that the Expectation Maximisation model splits some components
arbitrarily, because it is trying to ﬁt too many components, while the Dirichlet Process model adapts it number of state
automatically.
This example doesn’t show it, as we’re in a low-dimensional space, but another advantage of the Dirichlet process
982
Chapter 20. Gaussian Mixture Models
scikit-learn user guide, Release 0.18.2
model is that it can ﬁt full covariance matrices effectively even when there are less examples per cluster than there are
dimensions in the data, due to regularization properties of the inference algorithm.
import itertools
import numpy as np
from scipy import linalg
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn import mixture
color_iter = itertools.cycle(['navy', 'c', 'cornflowerblue', 'gold',
'darkorange'])
def plot_results(X, Y_, means, covariances, index, title):
splot = plt.subplot(2, 1, 1 + index)
for i, (mean, covar, color) in enumerate(zip(
means, covariances, color_iter)):
v, w = linalg.eigh(covar)
v = 2. * np.sqrt(2.) * np.sqrt(v)
u = w[0] / linalg.norm(w[0])
# as the DP will not use every component it has access to
# unless it needs it, we shouldn't plot the redundant
# components.
20.2. Gaussian Mixture Model Ellipsoids
983
scikit-learn user guide, Release 0.18.2
if not np.any(Y_ == i):
continue
plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
# Plot an ellipse to show the Gaussian component
angle = np.arctan(u[1] / u[0])
angle = 180. * angle / np.pi
# convert to degrees
ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)
ell.set_clip_box(splot.bbox)
ell.set_alpha(0.5)
splot.add_artist(ell)
plt.xlim(-9., 5.)
plt.ylim(-3., 6.)
plt.xticks(())
plt.yticks(())
plt.title(title)
# Number of samples per component
n_samples = 500
# Generate random sample, two components
np.random.seed(0)
C = np.array([[0., -0.1], [1.7, .4]])
X = np.r_[np.dot(np.random.randn(n_samples, 2), C),
.7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]
# Fit a Gaussian mixture with EM using five components
gmm = mixture.GaussianMixture(n_components=5, covariance_type='full').fit(X)
plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0,
'Gaussian Mixture')
# Fit a Dirichlet process Gaussian mixture using five components
dpgmm = mixture.BayesianGaussianMixture(n_components=5,
covariance_type='full').fit(X)
plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 1,
'Bayesian Gaussian Mixture with a Dirichlet process prior')
plt.show()
Total running time of the script: (0 minutes 0.443 seconds)
Download Python source code: plot_gmm.py
Download IPython notebook: plot_gmm.ipynb
20.3 Gaussian Mixture Model Selection
This example shows that model selection can be performed with Gaussian Mixture Models using information-theoretic
criteria (BIC). Model selection concerns both the covariance type and the number of components in the model. In that
case, AIC also provides the right result (not shown to save time), but BIC is better suited if the problem is to identify
the right model. Unlike Bayesian procedures, such inferences are prior-free.
In that case, the model with 2 components and full covariance (which corresponds to the true generative model) is
selected.
984
Chapter 20. Gaussian Mixture Models
scikit-learn user guide, Release 0.18.2
import numpy as np
import itertools
from scipy import linalg
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn import mixture
print(__doc__)
# Number of samples per component
n_samples = 500
# Generate random sample, two components
np.random.seed(0)
C = np.array([[0., -0.1], [1.7, .4]])
X = np.r_[np.dot(np.random.randn(n_samples, 2), C),
.7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]
lowest_bic = np.infty
bic = []
n_components_range = range(1, 7)
cv_types = ['spherical', 'tied', 'diag', 'full']
for cv_type in cv_types:
for n_components in n_components_range:
20.3. Gaussian Mixture Model Selection
985
scikit-learn user guide, Release 0.18.2
# Fit a Gaussian mixture with EM
gmm = mixture.GaussianMixture(n_components=n_components,
covariance_type=cv_type)
gmm.fit(X)
bic.append(gmm.bic(X))
if bic[-1] < lowest_bic:
lowest_bic = bic[-1]
best_gmm = gmm
bic = np.array(bic)
color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',
'darkorange'])
clf = best_gmm
bars = []
# Plot the BIC scores
spl = plt.subplot(2, 1, 1)
for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):
xpos = np.array(n_components_range) + .2 * (i - 2)
bars.append(plt.bar(xpos, bic[i * len(n_components_range):
(i + 1) * len(n_components_range)],
width=.2, color=color))
plt.xticks(n_components_range)
plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])
plt.title('BIC score per model')
xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\
.2 * np.floor(bic.argmin() / len(n_components_range))
plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)
spl.set_xlabel('Number of components')
spl.legend([b[0] for b in bars], cv_types)
# Plot the winner
splot = plt.subplot(2, 1, 2)
Y_ = clf.predict(X)
for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,
color_iter)):
v, w = linalg.eigh(cov)
if not np.any(Y_ == i):
continue
plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
# Plot an ellipse to show the Gaussian component
angle = np.arctan2(w[0][1], w[0][0])
angle = 180. * angle / np.pi
# convert to degrees
v = 2. * np.sqrt(2.) * np.sqrt(v)
ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)
ell.set_clip_box(splot.bbox)
ell.set_alpha(.5)
splot.add_artist(ell)
plt.xticks(())
plt.yticks(())
plt.title('Selected GMM: full model, 2 components')
plt.subplots_adjust(hspace=.35, bottom=.02)
plt.show()
Total running time of the script: (0 minutes 0.536 seconds)
986
Chapter 20. Gaussian Mixture Models
scikit-learn user guide, Release 0.18.2
Download Python source code: plot_gmm_selection.py
Download IPython notebook: plot_gmm_selection.ipynb
20.4 GMM covariances
Demonstration of several covariances types for Gaussian mixture models.
See Gaussian mixture models for more information on the estimator.
Although GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the
dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this
comparison valid.
We plot predicted labels on both training and held out test data using a variety of GMM covariance types on the
iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of
performance. Although one would expect full covariance to perform best in general, it is prone to overﬁtting on small
datasets and does not generalize well to held out test data.
On the plots, train data is shown as dots, while test data is shown as crosses. The iris dataset is four-dimensional. Only
the ﬁrst two dimensions are shown here, and thus some points are separated in other dimensions.
20.4. GMM covariances
987
scikit-learn user guide, Release 0.18.2
# Author: Ron Weiss <ronweiss@gmail.com>, Gael Varoquaux
# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>
# License: BSD 3 clause
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import StratifiedKFold
print(__doc__)
colors = ['navy', 'turquoise', 'darkorange']
988
Chapter 20. Gaussian Mixture Models
scikit-learn user guide, Release 0.18.2
def make_ellipses(gmm, ax):
for n, color in enumerate(colors):
if gmm.covariance_type == 'full':
covariances = gmm.covariances_[n][:2, :2]
elif gmm.covariance_type == 'tied':
covariances = gmm.covariances_[:2, :2]
elif gmm.covariance_type == 'diag':
covariances = np.diag(gmm.covariances_[n][:2])
elif gmm.covariance_type == 'spherical':
covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]
v, w = np.linalg.eigh(covariances)
u = w[0] / np.linalg.norm(w[0])
angle = np.arctan2(u[1], u[0])
angle = 180 * angle / np.pi
# convert to degrees
v = 2. * np.sqrt(2.) * np.sqrt(v)
ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],
180 + angle, color=color)
ell.set_clip_box(ax.bbox)
ell.set_alpha(0.5)
ax.add_artist(ell)
iris = datasets.load_iris()
# Break up the dataset into non-overlapping training (75%) and testing
# (25%) sets.
skf = StratifiedKFold(n_splits=4)
# Only take the first fold.
train_index, test_index = next(iter(skf.split(iris.data, iris.target)))
X_train = iris.data[train_index]
y_train = iris.target[train_index]
X_test = iris.data[test_index]
y_test = iris.target[test_index]
n_classes = len(np.unique(y_train))
# Try GMMs using different types of covariances.
estimators = dict((cov_type, GaussianMixture(n_components=n_classes,
covariance_type=cov_type, max_iter=20, random_state=0))
for cov_type in ['spherical', 'diag', 'tied', 'full'])
n_estimators = len(estimators)
plt.figure(figsize=(3 * n_estimators // 2, 6))
plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,
left=.01, right=.99)
for index, (name, estimator) in enumerate(estimators.items()):
# Since we have class labels for the training data, we can
# initialize the GMM parameters in a supervised manner.
estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)
for i in range(n_classes)])
# Train the other parameters using the EM algorithm.
estimator.fit(X_train)
20.4. GMM covariances
989
scikit-learn user guide, Release 0.18.2
h = plt.subplot(2, n_estimators // 2, index + 1)
make_ellipses(estimator, h)
for n, color in enumerate(colors):
data = iris.data[iris.target == n]
plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,
label=iris.target_names[n])
# Plot the test data with crosses
for n, color in enumerate(colors):
data = X_test[y_test == n]
plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)
y_train_pred = estimator.predict(X_train)
train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100
plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,
transform=h.transAxes)
y_test_pred = estimator.predict(X_test)
test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100
plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,
transform=h.transAxes)
plt.xticks(())
plt.yticks(())
plt.title(name)
plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))
plt.show()
Total running time of the script: (0 minutes 0.303 seconds)
Download Python source code: plot_gmm_covariances.py
Download IPython notebook: plot_gmm_covariances.ipynb
20.5 Gaussian Mixture Model Sine Curve
This example demonstrates the behavior of Gaussian mixture models ﬁt on data that was not sampled from a mixture
of Gaussian random variables. The dataset is formed by 100 points loosely spaced following a noisy sine curve. There
is therefore no ground truth value for the number of Gaussian components.
The ﬁrst model is a classical Gaussian Mixture Model with 10 components ﬁt with the Expectation-Maximization
algorithm.
The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process prior ﬁt with variational inference.
The low value of the concentration prior makes the model favor a lower number of active components. This models
“decides” to focus its modeling power on the big picture of the structure of the dataset: groups of points with alternating
directions modeled by non-diagonal covariance matrices. Those alternating directions roughly capture the alternating
nature of the original sine signal.
The third model is also a Bayesian Gaussian mixture model with a Dirichlet process prior but this time the value of the
concentration prior is higher giving the model more liberty to model the ﬁne-grained structure of the data. The result
is a mixture with a larger number of active components that is similar to the ﬁrst model where we arbitrarily decided
to ﬁx the number of components to 10.
990
Chapter 20. Gaussian Mixture Models
scikit-learn user guide, Release 0.18.2
Which model is the best is a matter of subjective judgement: do we want to favor models that only capture the big
picture to summarize and explain most of the structure of the data while ignoring the details or do we prefer models
that closely follow the high density regions of the signal?
The last two panels show how we can sample from the last two models. The resulting samples distributions do not
look exactly like the original data distribution. The difference primarily stems from the approximation error we made
by using a model that assumes that the data was generated by a ﬁnite number of Gaussian components instead of a
continuous noisy sine curve.
import itertools
import numpy as np
from scipy import linalg
import matplotlib.pyplot as plt
import matplotlib as mpl
20.5. Gaussian Mixture Model Sine Curve
991
scikit-learn user guide, Release 0.18.2
from sklearn import mixture
print(__doc__)
color_iter = itertools.cycle(['navy', 'c', 'cornflowerblue', 'gold',
'darkorange'])
def plot_results(X, Y, means, covariances, index, title):
splot = plt.subplot(5, 1, 1 + index)
for i, (mean, covar, color) in enumerate(zip(
means, covariances, color_iter)):
v, w = linalg.eigh(covar)
v = 2. * np.sqrt(2.) * np.sqrt(v)
u = w[0] / linalg.norm(w[0])
# as the DP will not use every component it has access to
# unless it needs it, we shouldn't plot the redundant
# components.
if not np.any(Y == i):
continue
plt.scatter(X[Y == i, 0], X[Y == i, 1], .8, color=color)
# Plot an ellipse to show the Gaussian component
angle = np.arctan(u[1] / u[0])
angle = 180. * angle / np.pi
# convert to degrees
ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)
ell.set_clip_box(splot.bbox)
ell.set_alpha(0.5)
splot.add_artist(ell)
plt.xlim(-6., 4. * np.pi - 6.)
plt.ylim(-5., 5.)
plt.title(title)
plt.xticks(())
plt.yticks(())
def plot_samples(X, Y, n_components, index, title):
plt.subplot(5, 1, 4 + index)
for i, color in zip(range(n_components), color_iter):
# as the DP will not use every component it has access to
# unless it needs it, we shouldn't plot the redundant
# components.
if not np.any(Y == i):
continue
plt.scatter(X[Y == i, 0], X[Y == i, 1], .8, color=color)
plt.xlim(-6., 4. * np.pi - 6.)
plt.ylim(-5., 5.)
plt.title(title)
plt.xticks(())
plt.yticks(())
# Parameters
n_samples = 100
992
Chapter 20. Gaussian Mixture Models
scikit-learn user guide, Release 0.18.2
# Generate random sample following a sine curve
np.random.seed(0)
X = np.zeros((n_samples, 2))
step = 4. * np.pi / n_samples
for i in range(X.shape[0]):
x = i * step - 6.
X[i, 0] = x + np.random.normal(0, 0.1)
X[i, 1] = 3. * (np.sin(x) + np.random.normal(0, .2))
plt.figure(figsize=(10, 10))
plt.subplots_adjust(bottom=.04, top=0.95, hspace=.2, wspace=.05,
left=.03, right=.97)
# Fit a Gaussian mixture with EM using ten components
gmm = mixture.GaussianMixture(n_components=10, covariance_type='full',
max_iter=100).fit(X)
plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0,
'Expectation-maximization')
dpgmm = mixture.BayesianGaussianMixture(
n_components=10, covariance_type='full', weight_concentration_prior=1e-2,
weight_concentration_prior_type='dirichlet_process',
mean_precision_prior=1e-2, covariance_prior=1e0 * np.eye(2),
init_params="random", max_iter=100, random_state=2).fit(X)
plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 1,
"Bayesian Gaussian mixture models with a Dirichlet process prior "
r"for $\gamma_0=0.01$.")
X_s, y_s = dpgmm.sample(n_samples=2000)
plot_samples(X_s, y_s, dpgmm.n_components, 0,
"Gaussian mixture with a Dirichlet process prior "
r"for $\gamma_0=0.01$ sampled with $2000$ samples.")
dpgmm = mixture.BayesianGaussianMixture(
n_components=10, covariance_type='full', weight_concentration_prior=1e+2,
weight_concentration_prior_type='dirichlet_process',
mean_precision_prior=1e-2, covariance_prior=1e0 * np.eye(2),
init_params="kmeans", max_iter=100, random_state=2).fit(X)
plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 2,
"Bayesian Gaussian mixture models with a Dirichlet process prior "
r"for $\gamma_0=100$")
X_s, y_s = dpgmm.sample(n_samples=2000)
plot_samples(X_s, y_s, dpgmm.n_components, 1,
"Gaussian mixture with a Dirichlet process prior "
r"for $\gamma_0=100$ sampled with $2000$ samples.")
plt.show()
Total running time of the script: (0 minutes 0.758 seconds)
Download Python source code: plot_gmm_sin.py
Download IPython notebook: plot_gmm_sin.ipynb
20.5. Gaussian Mixture Model Sine Curve
993
scikit-learn user guide, Release 0.18.2
20.6 Concentration Prior Type Analysis of Variation Bayesian Gaus-
sian Mixture
This
example
plots
the
ellipsoids
obtained
from
a
toy
dataset
(mixture
of
three
Gaussians)
ﬁt-
ted
by
the
BayesianGaussianMixture
class
models
with
a
Dirichlet
distribution
prior
(weight_concentration_prior_type='dirichlet_distribution')
and
a
Dirichlet
process
prior (weight_concentration_prior_type='dirichlet_process').
On each ﬁgure, we plot the
results for three different values of the weight concentration prior.
The BayesianGaussianMixture class can adapt its number of mixture componentsautomatically. The param-
eter weight_concentration_prior has a direct link with the resulting number of components with non-zero
weights. Specifying a low value for the concentration prior will make the model put most of the weight on few com-
ponents set the remaining components weights very close to zero. High values of the concentration prior will allow a
larger number of components to be active in the mixture.
The Dirichlet process prior allows to deﬁne an inﬁnite number of components and automatically selects the correct
number of components: it activates a component only if it is necessary.
On the contrary the classical ﬁnite mixture model with a Dirichlet distribution prior will favor more uniformly weighted
components and therefore tends to divide natural clusters into unnecessary sub-components.
•
•
# Author: Thierry Guillemot <thierry.guillemot.work@gmail.com>
# License: BSD 3 clause
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from sklearn.mixture import BayesianGaussianMixture
print(__doc__)
994
Chapter 20. Gaussian Mixture Models
scikit-learn user guide, Release 0.18.2
def plot_ellipses(ax, weights, means, covars):
for n in range(means.shape[0]):
eig_vals, eig_vecs = np.linalg.eigh(covars[n])
unit_eig_vec = eig_vecs[0] / np.linalg.norm(eig_vecs[0])
angle = np.arctan2(unit_eig_vec[1], unit_eig_vec[0])
# Ellipse needs degrees
angle = 180 * angle / np.pi
# eigenvector normalization
eig_vals = 2 * np.sqrt(2) * np.sqrt(eig_vals)
ell = mpl.patches.Ellipse(means[n], eig_vals[0], eig_vals[1],
180 + angle)
ell.set_clip_box(ax.bbox)
ell.set_alpha(weights[n])
ell.set_facecolor('#56B4E9')
ax.add_artist(ell)
def plot_results(ax1, ax2, estimator, X, y, title, plot_title=False):
ax1.set_title(title)
ax1.scatter(X[:, 0], X[:, 1], s=5, marker='o', color=colors[y], alpha=0.8)
ax1.set_xlim(-2., 2.)
ax1.set_ylim(-3., 3.)
ax1.set_xticks(())
ax1.set_yticks(())
plot_ellipses(ax1, estimator.weights_, estimator.means_,
estimator.covariances_)
ax2.get_xaxis().set_tick_params(direction='out')
ax2.yaxis.grid(True, alpha=0.7)
for k, w in enumerate(estimator.weights_):
ax2.bar(k - .45, w, width=0.9, color='#56B4E9', zorder=3)
ax2.text(k, w + 0.007, "%.1f%%" % (w * 100.),
horizontalalignment='center')
ax2.set_xlim(-.6, 2 * n_components - .4)
ax2.set_ylim(0., 1.1)
ax2.tick_params(axis='y', which='both', left='off',
right='off', labelleft='off')
ax2.tick_params(axis='x', which='both', top='off')
if plot_title:
ax1.set_ylabel('Estimated Mixtures')
ax2.set_ylabel('Weight of each component')
# Parameters of the dataset
random_state, n_components, n_features = 2, 3, 2
colors = np.array(['#0072B2', '#F0E442', '#D55E00'])
covars = np.array([[[.7, .0], [.0, .1]],
[[.5, .0], [.0, .1]],
[[.5, .0], [.0, .1]]])
samples = np.array([200, 500, 200])
means = np.array([[.0, -.70],
[.0, .0],
[.0, .70]])
# mean_precision_prior= 0.8 to minimize the influence of the prior
estimators = [
("Finite mixture with a Dirichlet distribution\nprior and "
20.6. Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture
995
scikit-learn user guide, Release 0.18.2
r"$\gamma_0=$", BayesianGaussianMixture(
weight_concentration_prior_type="dirichlet_distribution",
n_components=2 * n_components, reg_covar=0, init_params='random',
max_iter=1500, mean_precision_prior=.8,
random_state=random_state), [0.001, 1, 1000]),
("Infinite mixture with a Dirichlet process\n prior and" r"$\gamma_0=$",
BayesianGaussianMixture(
weight_concentration_prior_type="dirichlet_process",
n_components=2 * n_components, reg_covar=0, init_params='random',
max_iter=1500, mean_precision_prior=.8,
random_state=random_state), [1, 1000, 100000])]
# Generate data
rng = np.random.RandomState(random_state)
X = np.vstack([
rng.multivariate_normal(means[j], covars[j], samples[j])
for j in range(n_components)])
y = np.concatenate([j * np.ones(samples[j], dtype=int)
for j in range(n_components)])
# Plot results in two different figures
for (title, estimator, concentrations_prior) in estimators:
plt.figure(figsize=(4.7 * 3, 8))
plt.subplots_adjust(bottom=.04, top=0.90, hspace=.05, wspace=.05,
left=.03, right=.99)
gs = gridspec.GridSpec(3, len(concentrations_prior))
for k, concentration in enumerate(concentrations_prior):
estimator.weight_concentration_prior = concentration
estimator.fit(X)
plot_results(plt.subplot(gs[0:2, k]), plt.subplot(gs[2, k]), estimator,
X, y, r"%s$%.1e$" % (title, concentration),
plot_title=k == 0)
plt.show()
Total running time of the script: (0 minutes 18.124 seconds)
Download Python source code: plot_concentration_prior.py
Download IPython notebook: plot_concentration_prior.ipynb
996
Chapter 20. Gaussian Mixture Models
CHAPTER
TWENTYONE
MODEL SELECTION
Examples related to the sklearn.model_selection module.
21.1 Plotting Validation Curves
In this plot you can see the training scores and validation scores of an SVM for different values of the kernel parameter
gamma. For very low values of gamma, you can see that both the training score and the validation score are low.
This is called underﬁtting. Medium values of gamma will result in high values for both scores, i.e. the classiﬁer is
performing fairly well. If gamma is too high, the classiﬁer will overﬁt, which means that the training score is good but
the validation score is poor.
997
scikit-learn user guide, Release 0.18.2
print(__doc__)
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_digits
from sklearn.svm import SVC
from sklearn.model_selection import validation_curve
digits = load_digits()
X, y = digits.data, digits.target
param_range = np.logspace(-6, -1, 5)
train_scores, test_scores = validation_curve(
SVC(), X, y, param_name="gamma", param_range=param_range,
cv=10, scoring="accuracy", n_jobs=1)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)
plt.title("Validation Curve with SVM")
plt.xlabel("$\gamma$")
plt.ylabel("Score")
plt.ylim(0.0, 1.1)
lw = 2
plt.semilogx(param_range, train_scores_mean, label="Training score",
color="darkorange", lw=lw)
plt.fill_between(param_range, train_scores_mean - train_scores_std,
train_scores_mean + train_scores_std, alpha=0.2,
color="darkorange", lw=lw)
plt.semilogx(param_range, test_scores_mean, label="Cross-validation score",
color="navy", lw=lw)
plt.fill_between(param_range, test_scores_mean - test_scores_std,
test_scores_mean + test_scores_std, alpha=0.2,
color="navy", lw=lw)
plt.legend(loc="best")
plt.show()
Total running time of the script: (0 minutes 34.015 seconds)
Download Python source code: plot_validation_curve.py
Download IPython notebook: plot_validation_curve.ipynb
21.2 Underﬁtting vs. Overﬁtting
This example demonstrates the problems of underﬁtting and overﬁtting and how we can use linear regression with
polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate,
which is a part of the cosine function. In addition, the samples from the real function and the approximations of
different models are displayed. The models have polynomial features of different degrees. We can see that a linear
function (polynomial with degree 1) is not sufﬁcient to ﬁt the training samples. This is called underﬁtting. A
polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model
will overﬁt the training data, i.e. it learns the noise of the training data. We evaluate quantitatively overﬁtting /
underﬁtting by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher,
the less likely the model generalizes correctly from the training data.
998
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
np.random.seed(0)
n_samples = 30
degrees = [1, 4, 15]
true_fun = lambda X: np.cos(1.5 * np.pi * X)
X = np.sort(np.random.rand(n_samples))
y = true_fun(X) + np.random.randn(n_samples) * 0.1
plt.figure(figsize=(14, 5))
for i in range(len(degrees)):
ax = plt.subplot(1, len(degrees), i + 1)
plt.setp(ax, xticks=(), yticks=())
polynomial_features = PolynomialFeatures(degree=degrees[i],
include_bias=False)
linear_regression = LinearRegression()
pipeline = Pipeline([("polynomial_features", polynomial_features),
("linear_regression", linear_regression)])
pipeline.fit(X[:, np.newaxis], y)
# Evaluate the models using crossvalidation
scores = cross_val_score(pipeline, X[:, np.newaxis], y,
scoring="neg_mean_squared_error", cv=10)
X_test = np.linspace(0, 1, 100)
plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")
plt.plot(X_test, true_fun(X_test), label="True function")
plt.scatter(X, y, label="Samples")
plt.xlabel("x")
plt.ylabel("y")
plt.xlim((0, 1))
plt.ylim((-2, 2))
21.2. Underﬁtting vs. Overﬁtting
999
scikit-learn user guide, Release 0.18.2
plt.legend(loc="best")
plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(
degrees[i], -scores.mean(), scores.std()))
plt.show()
Total running time of the script: (0 minutes 0.324 seconds)
Download Python source code: plot_underfitting_overfitting.py
Download IPython notebook: plot_underfitting_overfitting.ipynb
21.3 Train error vs Test error
Illustration of how the performance of an estimator on unseen data (test data) is not the same as the performance on
training data. As the regularization increases the performance on train decreases while the performance on test is
optimal within a range of values of the regularization parameter. The example with an Elastic-Net regression model
and the performance is measured using the explained variance a.k.a. R^2.
print(__doc__)
# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause
import numpy as np
from sklearn import linear_model
Generate sample data
n_samples_train, n_samples_test, n_features = 75, 150, 500
np.random.seed(0)
coef = np.random.randn(n_features)
coef[50:] = 0.0
# only the top 10 features are impacting the model
X = np.random.randn(n_samples_train + n_samples_test, n_features)
y = np.dot(X, coef)
# Split train and test data
X_train, X_test = X[:n_samples_train], X[n_samples_train:]
y_train, y_test = y[:n_samples_train], y[n_samples_train:]
Compute train and test errors
alphas = np.logspace(-5, 1, 60)
enet = linear_model.ElasticNet(l1_ratio=0.7)
train_errors = list()
test_errors = list()
for alpha in alphas:
enet.set_params(alpha=alpha)
enet.fit(X_train, y_train)
train_errors.append(enet.score(X_train, y_train))
test_errors.append(enet.score(X_test, y_test))
i_alpha_optim = np.argmax(test_errors)
alpha_optim = alphas[i_alpha_optim]
print("Optimal regularization parameter : %s" % alpha_optim)
# Estimate the coef_ on full data with optimal regularization parameter
1000
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
enet.set_params(alpha=alpha_optim)
coef_ = enet.fit(X, y).coef_
Out:
Optimal regularization parameter : 0.000335292414925
Plot results functions
import matplotlib.pyplot as plt
plt.subplot(2, 1, 1)
plt.semilogx(alphas, train_errors, label='Train')
plt.semilogx(alphas, test_errors, label='Test')
plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',
linewidth=3, label='Optimum on test')
plt.legend(loc='lower left')
plt.ylim([0, 1.2])
plt.xlabel('Regularization parameter')
plt.ylabel('Performance')
# Show estimated coef_ vs true coef
plt.subplot(2, 1, 2)
plt.plot(coef, label='True coef')
plt.plot(coef_, label='Estimated coef')
plt.legend()
plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)
plt.show()
21.3. Train error vs Test error
1001
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 2.489 seconds)
Download Python source code: plot_train_error_vs_test_error.py
Download IPython notebook: plot_train_error_vs_test_error.ipynb
21.4 Receiver Operating Characteristic (ROC) with cross validation
Example of Receiver Operating Characteristic (ROC) metric to evaluate classiﬁer output quality using cross-validation.
ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the
top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not
very realistic, but it does mean that a larger area under the curve (AUC) is usually better.
The “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing
the false positive rate.
This example shows the ROC response of different datasets, created from K-fold cross-validation. Taking all of these
curves, it is possible to calculate the mean area under curve, and see the variance of the curve when the training set
is split into different subsets. This roughly shows how the classiﬁer output is affected by changes in the training data,
and how different the splits generated by K-fold cross-validation are from one another.
Note:
1002
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
See also sklearn.metrics.auc_score, sklearn.model_selection.cross_val_score, Receiver
Operating Characteristic (ROC),
print(__doc__)
import numpy as np
from scipy import interp
import matplotlib.pyplot as plt
from itertools import cycle
from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import StratifiedKFold
Data IO and generation
# import some data to play with
iris = datasets.load_iris()
X = iris.data
y = iris.target
X, y = X[y != 2], y[y != 2]
n_samples, n_features = X.shape
# Add noisy features
random_state = np.random.RandomState(0)
X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
Classiﬁcation and ROC analysis
# Run classifier with cross-validation and plot ROC curves
cv = StratifiedKFold(n_splits=6)
classifier = svm.SVC(kernel='linear', probability=True,
random_state=random_state)
mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)
colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])
lw = 2
i = 0
for (train, test), color in zip(cv.split(X, y), colors):
probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])
# Compute ROC curve and area the curve
fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
mean_tpr += interp(mean_fpr, fpr, tpr)
mean_tpr[0] = 0.0
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, lw=lw, color=color,
label='ROC fold %d (area = %0.2f)' % (i, roc_auc))
i += 1
plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',
label='Luck')
mean_tpr /= cv.get_n_splits(X, y)
mean_tpr[-1] = 1.0
21.4. Receiver Operating Characteristic (ROC) with cross validation
1003
scikit-learn user guide, Release 0.18.2
mean_auc = auc(mean_fpr, mean_tpr)
plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',
label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()
Total running time of the script: (0 minutes 0.275 seconds)
Download Python source code: plot_roc_crossval.py
Download IPython notebook: plot_roc_crossval.ipynb
21.5 Parameter estimation using grid search with cross-validation
This
examples
shows
how
a
classiﬁer
is
optimized
by
cross-validation,
which
is
done
using
the
sklearn.model_selection.GridSearchCV object on a development set that comprises only half of the
available labeled data.
1004
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set
that was not used during the model selection step.
More details on tools available for model selection can be found in the sections on Cross-validation: evaluating
estimator performance and Tuning the hyper-parameters of an estimator.
from __future__ import print_function
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.svm import SVC
print(__doc__)
# Loading the Digits dataset
digits = datasets.load_digits()
# To apply an classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
X = digits.images.reshape((n_samples, -1))
y = digits.target
# Split the dataset in two equal parts
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.5, random_state=0)
# Set the parameters by cross-validation
tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
'C': [1, 10, 100, 1000]},
{'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]
scores = ['precision', 'recall']
for score in scores:
print("# Tuning hyper-parameters for %s" % score)
print()
clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,
scoring='%s_macro' % score)
clf.fit(X_train, y_train)
print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
print("%0.3f (+/-%0.03f) for %r"
% (mean, std * 2, params))
print()
print("Detailed classification report:")
print()
21.5. Parameter estimation using grid search with cross-validation
1005
scikit-learn user guide, Release 0.18.2
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
print(classification_report(y_true, y_pred))
print()
# Note the problem is too easy: the hyperparameter plateau is too flat and the
# output model is the same for precision and recall with ties in quality.
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: grid_search_digits.py
Download IPython notebook: grid_search_digits.ipynb
21.6 Confusion matrix
Example of confusion matrix usage to evaluate the quality of the output of a classiﬁer on the iris data set. The diagonal
elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal
elements are those that are mislabeled by the classiﬁer. The higher the diagonal values of the confusion matrix the
better, indicating many correct predictions.
The ﬁgures show the confusion matrix with and without normalization by class support size (number of elements in
each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation
of which class is being misclassiﬁed.
Here the results are not as good as they could be as our choice for the regularization parameter C was not the best. In
real life applications this parameter is usually chosen using Tuning the hyper-parameters of an estimator.
•
1006
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
•
Out:
Confusion matrix, without normalization
[[13
0
0]
[ 0 10
6]
[ 0
0
9]]
Normalized confusion matrix
[[ 1.
0.
0.
]
[ 0.
0.62
0.38]
[ 0.
0.
1.
]]
print(__doc__)
import itertools
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
# import some data to play with
iris = datasets.load_iris()
X = iris.data
y = iris.target
class_names = iris.target_names
# Split the data into a training set and a test set
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
# Run classifier, using a model that is too regularized (C too low) to see
# the impact on the results
classifier = svm.SVC(kernel='linear', C=0.01)
y_pred = classifier.fit(X_train, y_train).predict(X_test)
def plot_confusion_matrix(cm, classes,
normalize=False,
21.6. Confusion matrix
1007
scikit-learn user guide, Release 0.18.2
title='Confusion matrix',
cmap=plt.cm.Blues):
"""
This function prints and plots the confusion matrix.
Normalization can be applied by setting `normalize=True`.
"""
plt.imshow(cm, interpolation='nearest', cmap=cmap)
plt.title(title)
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)
if normalize:
cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
print("Normalized confusion matrix")
else:
print('Confusion matrix, without normalization')
print(cm)
thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
plt.text(j, i, cm[i, j],
horizontalalignment="center",
color="white" if cm[i, j] > thresh else "black")
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, y_pred)
np.set_printoptions(precision=2)
# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=class_names,
title='Confusion matrix, without normalization')
# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,
title='Normalized confusion matrix')
plt.show()
Total running time of the script: (0 minutes 0.302 seconds)
Download Python source code: plot_confusion_matrix.py
Download IPython notebook: plot_confusion_matrix.ipynb
1008
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
21.7 Comparing randomized search and grid search for hyperparam-
eter estimation
Compare randomized search and grid search for optimizing hyperparameters of a random forest. All parameters that
inﬂuence the learning are searched simultaneously (except for the number of estimators, which poses a time / quality
tradeoff).
The randomized search and the grid search explore exactly the same space of parameters. The result in parameter
settings is quite similar, while the run time for randomized search is drastically lower.
The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not
carry over to a held-out test set.
Note that in practice, one would not search over this many different parameters simultaneously using grid search, but
pick only the ones deemed most important.
print(__doc__)
import numpy as np
from time import time
from scipy.stats import randint as sp_randint
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier
# get some data
digits = load_digits()
X, y = digits.data, digits.target
# build a classifier
clf = RandomForestClassifier(n_estimators=20)
# Utility function to report best scores
def report(results, n_top=3):
for i in range(1, n_top + 1):
candidates = np.flatnonzero(results['rank_test_score'] == i)
for candidate in candidates:
print("Model with rank: {0}".format(i))
print("Mean validation score: {0:.3f} (std: {1:.3f})".format(
results['mean_test_score'][candidate],
results['std_test_score'][candidate]))
print("Parameters: {0}".format(results['params'][candidate]))
print("")
# specify parameters and distributions to sample from
param_dist = {"max_depth": [3, None],
"max_features": sp_randint(1, 11),
"min_samples_split": sp_randint(1, 11),
"min_samples_leaf": sp_randint(1, 11),
"bootstrap": [True, False],
"criterion": ["gini", "entropy"]}
# run randomized search
21.7. Comparing randomized search and grid search for hyperparameter estimation
1009
scikit-learn user guide, Release 0.18.2
n_iter_search = 20
random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
n_iter=n_iter_search)
start = time()
random_search.fit(X, y)
print("RandomizedSearchCV took %.2f seconds for %d candidates"
" parameter settings." % ((time() - start), n_iter_search))
report(random_search.cv_results_)
# use a full grid over all parameters
param_grid = {"max_depth": [3, None],
"max_features": [1, 3, 10],
"min_samples_split": [1, 3, 10],
"min_samples_leaf": [1, 3, 10],
"bootstrap": [True, False],
"criterion": ["gini", "entropy"]}
# run grid search
grid_search = GridSearchCV(clf, param_grid=param_grid)
start = time()
grid_search.fit(X, y)
print("GridSearchCV took %.2f seconds for %d candidate parameter settings."
% (time() - start, len(grid_search.cv_results_['params'])))
report(grid_search.cv_results_)
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: randomized_search.py
Download IPython notebook: randomized_search.ipynb
21.8 Nested versus non-nested cross-validation
This example compares non-nested and nested cross-validation strategies on a classiﬁer of the iris data set. Nested
cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV
estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters
that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.
Model selection without nested CV uses the same data to tune model parameters and evaluate model performance.
Information may thus “leak” into the model and overﬁt the data. The magnitude of this effect is primarily dependent
on the size of the dataset and the stability of the model. See Cawley and Talbot 1 for an analysis of these issues.
To avoid this problem, nested CV effectively uses a series of train/validation/test set splits. In the inner loop, the
score is approximately maximized by ﬁtting a model to each training set, and then directly maximized in selecting
(hyper)parameters over the validation set. In the outer loop, generalization error is estimated by averaging test set
scores over several dataset splits.
The example below uses a support vector classiﬁer with a non-linear kernel to build a model with optimized hyperpa-
rameters by grid search. We compare the performance of non-nested and nested CV strategies by taking the difference
between their scores.
1 Cawley, G.C.; Talbot, N.L.C. On over-ﬁtting in model selection and subsequent selection bias in performance evaluation. J. Mach. Learn. Res
2010,11, 2079-2107.
1010
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
See Also:
• Cross-validation: evaluating estimator performance
• Tuning the hyper-parameters of an estimator
References:
Out:
Average difference of 0.007742 with std. dev. of 0.007688.
from sklearn.datasets import load_iris
from matplotlib import pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, KFold
import numpy as np
21.8. Nested versus non-nested cross-validation
1011
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Number of random trials
NUM_TRIALS = 30
# Load the dataset
iris = load_iris()
X_iris = iris.data
y_iris = iris.target
# Set up possible values of parameters to optimize over
p_grid = {"C": [1, 10, 100],
"gamma": [.01, .1]}
# We will use a Support Vector Classifier with "rbf" kernel
svr = SVC(kernel="rbf")
# Arrays to store scores
non_nested_scores = np.zeros(NUM_TRIALS)
nested_scores = np.zeros(NUM_TRIALS)
# Loop for each trial
for i in range(NUM_TRIALS):
# Choose cross-validation techniques for the inner and outer loops,
# independently of the dataset.
# E.g "LabelKFold", "LeaveOneOut", "LeaveOneLabelOut", etc.
inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)
outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)
# Non_nested parameter search and scoring
clf = GridSearchCV(estimator=svr, param_grid=p_grid, cv=inner_cv)
clf.fit(X_iris, y_iris)
non_nested_scores[i] = clf.best_score_
# Nested CV with parameter optimization
nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)
nested_scores[i] = nested_score.mean()
score_difference = non_nested_scores - nested_scores
print("Average difference of {0:6f} with std. dev. of {1:6f}."
.format(score_difference.mean(), score_difference.std()))
# Plot scores on each trial for nested and non-nested CV
plt.figure()
plt.subplot(211)
non_nested_scores_line, = plt.plot(non_nested_scores, color='r')
nested_line, = plt.plot(nested_scores, color='b')
plt.ylabel("score", fontsize="14")
plt.legend([non_nested_scores_line, nested_line],
["Non-Nested CV", "Nested CV"],
bbox_to_anchor=(0, .4, .5, 0))
plt.title("Non-Nested and Nested Cross Validation on Iris Dataset",
x=.5, y=1.1, fontsize="15")
# Plot bar chart of the difference.
plt.subplot(212)
1012
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
difference_plot = plt.bar(range(NUM_TRIALS), score_difference)
plt.xlabel("Individual Trial #")
plt.legend([difference_plot],
["Non-Nested CV - Nested CV Score"],
bbox_to_anchor=(0, 1, .8, 0))
plt.ylabel("score difference", fontsize="14")
plt.show()
Total running time of the script: (0 minutes 13.767 seconds)
Download Python source code: plot_nested_cross_validation_iris.py
Download IPython notebook: plot_nested_cross_validation_iris.ipynb
21.9 Sample pipeline for text feature extraction and evaluation
The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached
and reused for the document classiﬁcation example.
You can adjust the number of categories by giving their names to the dataset loader or setting them to None to get the
20 of them.
Here is a sample output of a run on a quad-core machine:
Loading 20 newsgroups dataset for categories:
['alt.atheism', 'talk.religion.misc']
1427 documents
2 categories
Performing grid search...
pipeline: ['vect', 'tfidf', 'clf']
parameters:
{'clf__alpha': (1.0000000000000001e-05, 9.9999999999999995e-07),
'clf__n_iter': (10, 50, 80),
'clf__penalty': ('l2', 'elasticnet'),
'tfidf__use_idf': (True, False),
'vect__max_n': (1, 2),
'vect__max_df': (0.5, 0.75, 1.0),
'vect__max_features': (None, 5000, 10000, 50000)}
done in 1737.030s
Best score: 0.940
Best parameters set:
clf__alpha: 9.9999999999999995e-07
clf__n_iter: 50
clf__penalty: 'elasticnet'
tfidf__use_idf: True
vect__max_n: 2
vect__max_df: 0.75
vect__max_features: 50000
# Author: Olivier Grisel <olivier.grisel@ensta.org>
#
Peter Prettenhofer <peter.prettenhofer@gmail.com>
#
Mathieu Blondel <mathieu@mblondel.org>
# License: BSD 3 clause
21.9. Sample pipeline for text feature extraction and evaluation
1013
scikit-learn user guide, Release 0.18.2
from __future__ import print_function
from pprint import pprint
from time import time
import logging
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
print(__doc__)
# Display progress logs on stdout
logging.basicConfig(level=logging.INFO,
format='%(asctime)s %(levelname)s %(message)s')
Load some categories from the training set
categories = [
'alt.atheism',
'talk.religion.misc',
]
# Uncomment the following to do the analysis on all the categories
#categories = None
print("Loading 20 newsgroups dataset for categories:")
print(categories)
data = fetch_20newsgroups(subset='train', categories=categories)
print("%d documents" % len(data.filenames))
print("%d categories" % len(data.target_names))
print()
deﬁne a pipeline combining a text feature extractor with a simple classiﬁer
pipeline = Pipeline([
('vect', CountVectorizer()),
('tfidf', TfidfTransformer()),
('clf', SGDClassifier()),
])
# uncommenting more parameters will give better exploring power but will
# increase processing time in a combinatorial way
parameters = {
'vect__max_df': (0.5, 0.75, 1.0),
#'vect__max_features': (None, 5000, 10000, 50000),
'vect__ngram_range': ((1, 1), (1, 2)),
# unigrams or bigrams
#'tfidf__use_idf': (True, False),
#'tfidf__norm': ('l1', 'l2'),
'clf__alpha': (0.00001, 0.000001),
'clf__penalty': ('l2', 'elasticnet'),
#'clf__n_iter': (10, 50, 80),
}
if __name__ == "__main__":
1014
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
# multiprocessing requires the fork to happen in a __main__ protected
# block
# find the best parameters for both the feature extraction and the
# classifier
grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)
print("Performing grid search...")
print("pipeline:", [name for name, _ in pipeline.steps])
print("parameters:")
pprint(parameters)
t0 = time()
grid_search.fit(data.data, data.target)
print("done in %0.3fs" % (time() - t0))
print()
print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
print("\t%s: %r" % (param_name, best_parameters[param_name]))
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: grid_search_text_feature_extraction.py
Download IPython notebook: grid_search_text_feature_extraction.ipynb
21.10 Precision-Recall
Example of Precision-Recall metric to evaluate classiﬁer output quality.
In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant
results are returned. A high area under the curve represents both high recall and high precision, where high precision
relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that
the classiﬁer is returning accurate results (high precision), as well as returning a majority of all positive results (high
recall).
A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when
compared to the training labels. A system with high precision but low recall is just the opposite, returning very few
results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high
precision and high recall will return many results, with all results labeled correctly.
Precision (𝑃) is deﬁned as the number of true positives (𝑇𝑝) over the number of true positives plus the number of false
positives (𝐹𝑝).
𝑃=
𝑇𝑝
𝑇𝑝+𝐹𝑝
Recall (𝑅) is deﬁned as the number of true positives (𝑇𝑝) over the number of true positives plus the number of false
negatives (𝐹𝑛).
𝑅=
𝑇𝑝
𝑇𝑝+𝐹𝑛
These quantities are also related to the (𝐹1) score, which is deﬁned as the harmonic mean of precision and recall.
𝐹1 = 2 𝑃×𝑅
𝑃+𝑅
21.10. Precision-Recall
1015
scikit-learn user guide, Release 0.18.2
It is important to note that the precision may not decrease with recall. The deﬁnition of precision (
𝑇𝑝
𝑇𝑝+𝐹𝑝) shows that
lowering the threshold of a classiﬁer may increase the denominator, by increasing the number of results returned. If
the threshold was previously set too high, the new results may all be true positives, which will increase precision. If the
previous threshold was about right or too low, further lowering the threshold will introduce false positives, decreasing
precision.
Recall is deﬁned as
𝑇𝑝
𝑇𝑝+𝐹𝑛, where 𝑇𝑝+ 𝐹𝑛does not depend on the classiﬁer threshold. This means that lowering
the classiﬁer threshold may increase recall, by increasing the number of true positive results. It is also possible that
lowering the threshold may leave recall unchanged, while the precision ﬂuctuates.
The relationship between recall and precision can be observed in the stairstep area of the plot - at the edges of these
steps a small change in the threshold considerably reduces precision, with only a minor gain in recall. See the corner
at recall = .59, precision = .8 for an example of this phenomenon.
Precision-recall curves are typically used in binary classiﬁcation to study the output of a classiﬁer. In order to extend
Precision-recall curve and average precision to multi-class or multi-label classiﬁcation, it is necessary to binarize the
output. One curve can be drawn per label, but one can also draw a precision-recall curve by considering each element
of the label indicator matrix as a binary prediction (micro-averaging).
Note:
See also sklearn.metrics.average_precision_score, sklearn.metrics.recall_score,
sklearn.metrics.precision_score, sklearn.metrics.f1_score
1016
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
print(__doc__)
import matplotlib.pyplot as plt
import numpy as np
from itertools import cycle
from sklearn import svm, datasets
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
# import some data to play with
iris = datasets.load_iris()
X = iris.data
y = iris.target
# setup plot details
colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])
lw = 2
# Binarize the output
y = label_binarize(y, classes=[0, 1, 2])
n_classes = y.shape[1]
# Add noisy features
random_state = np.random.RandomState(0)
n_samples, n_features = X.shape
X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
# Split into training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
random_state=random_state)
# Run classifier
classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,
random_state=random_state))
y_score = classifier.fit(X_train, y_train).decision_function(X_test)
# Compute Precision-Recall and plot curve
precision = dict()
recall = dict()
average_precision = dict()
for i in range(n_classes):
precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],
y_score[:, i])
average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])
# Compute micro-average ROC curve and ROC area
precision["micro"], recall["micro"], _ = precision_recall_curve(y_test.ravel(),
y_score.ravel())
average_precision["micro"] = average_precision_score(y_test, y_score,
average="micro")
# Plot Precision-Recall curve
plt.clf()
21.10. Precision-Recall
1017
scikit-learn user guide, Release 0.18.2
plt.plot(recall[0], precision[0], lw=lw, color='navy',
label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title('Precision-Recall example: AUC={0:0.2f}'.format(average_precision[0]))
plt.legend(loc="lower left")
plt.show()
# Plot Precision-Recall curve for each class
plt.clf()
plt.plot(recall["micro"], precision["micro"], color='gold', lw=lw,
label='micro-average Precision-recall curve (area = {0:0.2f})'
''.format(average_precision["micro"]))
for i, color in zip(range(n_classes), colors):
plt.plot(recall[i], precision[i], color=color, lw=lw,
label='Precision-recall curve of class {0} (area = {1:0.2f})'
''.format(i, average_precision[i]))
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Extension of Precision-Recall curve to multi-class')
plt.legend(loc="lower right")
plt.show()
Total running time of the script: (0 minutes 0.263 seconds)
Download Python source code: plot_precision_recall.py
Download IPython notebook: plot_precision_recall.ipynb
21.11 Receiver Operating Characteristic (ROC)
Example of Receiver Operating Characteristic (ROC) metric to evaluate classiﬁer output quality.
ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the
top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not
very realistic, but it does mean that a larger area under the curve (AUC) is usually better.
The “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing
the false positive rate.
21.11.1 Multiclass settings
ROC curves are typically used in binary classiﬁcation to study the output of a classiﬁer. In order to extend ROC curve
and ROC area to multi-class or multi-label classiﬁcation, it is necessary to binarize the output. One ROC curve can
be drawn per label, but one can also draw a ROC curve by considering each element of the label indicator matrix as a
binary prediction (micro-averaging).
Another evaluation measure for multi-class classiﬁcation is macro-averaging, which gives equal weight to the classi-
ﬁcation of each label.
1018
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
Note:
See also sklearn.metrics.roc_auc_score, Receiver Operating Characteristic (ROC) with cross valida-
tion.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle
from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from scipy import interp
# Import some data to play with
iris = datasets.load_iris()
X = iris.data
y = iris.target
# Binarize the output
y = label_binarize(y, classes=[0, 1, 2])
n_classes = y.shape[1]
# Add noisy features to make the problem harder
random_state = np.random.RandomState(0)
n_samples, n_features = X.shape
X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
# shuffle and split training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
random_state=0)
# Learn to predict each class against the other
classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,
random_state=random_state))
y_score = classifier.fit(X_train, y_train).decision_function(X_test)
# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
roc_auc[i] = auc(fpr[i], tpr[i])
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
Plot of a ROC curve for a speciﬁc class
plt.figure()
lw = 2
21.11. Receiver Operating Characteristic (ROC)
1019
scikit-learn user guide, Release 0.18.2
plt.plot(fpr[2], tpr[2], color='darkorange',
lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()
Plot ROC curves for the multiclass problem
# Compute macro-average ROC curve and ROC area
# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
mean_tpr += interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= n_classes
1020
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])
# Plot all ROC curves
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
label='micro-average ROC curve (area = {0:0.2f})'
''.format(roc_auc["micro"]),
color='deeppink', linestyle=':', linewidth=4)
plt.plot(fpr["macro"], tpr["macro"],
label='macro-average ROC curve (area = {0:0.2f})'
''.format(roc_auc["macro"]),
color='navy', linestyle=':', linewidth=4)
colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
for i, color in zip(range(n_classes), colors):
plt.plot(fpr[i], tpr[i], color=color, lw=lw,
label='ROC curve of class {0} (area = {1:0.2f})'
''.format(i, roc_auc[i]))
plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Some extension of Receiver operating characteristic to multi-class')
plt.legend(loc="lower right")
plt.show()
21.11. Receiver Operating Characteristic (ROC)
1021
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.231 seconds)
Download Python source code: plot_roc.py
Download IPython notebook: plot_roc.ipynb
21.12 Plotting Learning Curves
On the left side the learning curve of a naive Bayes classiﬁer is shown for the digits dataset. Note that the training
score and the cross-validation score are both not very good at the end. However, the shape of the curve can be found
in more complex datasets very often: the training score is very high at the beginning and decreases and the cross-
validation score is very low at the beginning and increases. On the right side we see the learning curve of an SVM
with RBF kernel. We can see clearly that the training score is still around the maximum and the validation score could
be increased with more training samples.
1022
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
•
•
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.datasets import load_digits
from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
"""
Generate a simple plot of the test and training learning curve.
Parameters
----------
estimator : object type that implements the "fit" and "predict" methods
An object of that type which is cloned for each validation.
title : string
Title for the chart.
X : array-like, shape (n_samples, n_features)
Training vector, where n_samples is the number of samples and
n_features is the number of features.
21.12. Plotting Learning Curves
1023
scikit-learn user guide, Release 0.18.2
y : array-like, shape (n_samples) or (n_samples, n_features), optional
Target relative to X for classification or regression;
None for unsupervised learning.
ylim : tuple, shape (ymin, ymax), optional
Defines minimum and maximum yvalues plotted.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy.
Possible inputs for cv are:
- None, to use the default 3-fold cross-validation,
- integer, to specify the number of folds.
- An object to be used as a cross-validation generator.
- An iterable yielding train/test splits.
For integer/None inputs, if ``y`` is binary or multiclass,
:class:`StratifiedKFold` used. If the estimator is not a classifier
or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.
Refer :ref:`User Guide <cross_validation>` for the various
cross-validators that can be used here.
n_jobs : integer, optional
Number of jobs to run in parallel (default 1).
"""
plt.figure()
plt.title(title)
if ylim is not None:
plt.ylim(*ylim)
plt.xlabel("Training examples")
plt.ylabel("Score")
train_sizes, train_scores, test_scores = learning_curve(
estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)
plt.grid()
plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
train_scores_mean + train_scores_std, alpha=0.1,
color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
test_scores_mean + test_scores_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
label="Cross-validation score")
plt.legend(loc="best")
return plt
digits = load_digits()
X, y = digits.data, digits.target
title = "Learning Curves (Naive Bayes)"
1024
Chapter 21. Model Selection
scikit-learn user guide, Release 0.18.2
# Cross validation with 100 iterations to get smoother mean test and train
# score curves, each time with 20% data randomly selected as a validation set.
cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
estimator = GaussianNB()
plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)
title = "Learning Curves (SVM, RBF kernel, $\gamma=0.001$)"
# SVC is more expensive so we do a lower number of CV iterations:
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)
estimator = SVC(gamma=0.001)
plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)
plt.show()
Total running time of the script: (0 minutes 11.760 seconds)
Download Python source code: plot_learning_curve.py
Download IPython notebook: plot_learning_curve.ipynb
21.12. Plotting Learning Curves
1025
scikit-learn user guide, Release 0.18.2
1026
Chapter 21. Model Selection
CHAPTER
TWENTYTWO
NEAREST NEIGHBORS
Examples concerning the sklearn.neighbors module.
22.1 Nearest Neighbors regression
Demonstrate the resolution of a regression problem using a k-Nearest Neighbor and the interpolation of the target
using both barycenter and constant weights.
print(__doc__)
# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
#
Fabian Pedregosa <fabian.pedregosa@inria.fr>
#
# License: BSD 3 clause (C) INRIA
Generate sample data
import numpy as np
import matplotlib.pyplot as plt
from sklearn import neighbors
np.random.seed(0)
X = np.sort(5 * np.random.rand(40, 1), axis=0)
T = np.linspace(0, 5, 500)[:, np.newaxis]
y = np.sin(X).ravel()
# Add noise to targets
y[::5] += 1 * (0.5 - np.random.rand(8))
Fit regression model
n_neighbors = 5
for i, weights in enumerate(['uniform', 'distance']):
knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)
y_ = knn.fit(X, y).predict(T)
plt.subplot(2, 1, i + 1)
plt.scatter(X, y, c='k', label='data')
plt.plot(T, y_, c='g', label='prediction')
plt.axis('tight')
plt.legend()
plt.title("KNeighborsRegressor (k = %i, weights = '%s')" % (n_neighbors,
1027
scikit-learn user guide, Release 0.18.2
weights))
plt.show()
Total running time of the script: (0 minutes 0.127 seconds)
Download Python source code: plot_regression.py
Download IPython notebook: plot_regression.ipynb
22.2 Nearest Neighbors Classiﬁcation
Sample usage of Nearest Neighbors classiﬁcation. It will plot the decision boundaries for each class.
1028
Chapter 22. Nearest Neighbors
scikit-learn user guide, Release 0.18.2
•
•
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
n_neighbors = 15
# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]
# we only take the first two features. We could
# avoid this ugly slicing by using a two-dim dataset
y = iris.target
h = .02
# step size in the mesh
# Create color maps
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
for weights in ['uniform', 'distance']:
# we create an instance of Neighbours Classifier and fit the data.
clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
clf.fit(X, y)
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
22.2. Nearest Neighbors Classiﬁcation
1029
scikit-learn user guide, Release 0.18.2
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("3-Class classification (k = %i, weights = '%s')"
% (n_neighbors, weights))
plt.show()
Total running time of the script: (0 minutes 0.403 seconds)
Download Python source code: plot_classification.py
Download IPython notebook: plot_classification.ipynb
22.3 Nearest Centroid Classiﬁcation
Sample usage of Nearest Centroid classiﬁcation. It will plot the decision boundaries for each class.
•
1030
Chapter 22. Nearest Neighbors
scikit-learn user guide, Release 0.18.2
•
Out:
None 0.813333333333
0.2 0.82
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import datasets
from sklearn.neighbors import NearestCentroid
n_neighbors = 15
# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]
# we only take the first two features. We could
# avoid this ugly slicing by using a two-dim dataset
y = iris.target
h = .02
# step size in the mesh
# Create color maps
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
for shrinkage in [None, .2]:
# we create an instance of Neighbours Classifier and fit the data.
clf = NearestCentroid(shrink_threshold=shrinkage)
clf.fit(X, y)
y_pred = clf.predict(X)
print(shrinkage, np.mean(y == y_pred))
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
22.3. Nearest Centroid Classiﬁcation
1031
scikit-learn user guide, Release 0.18.2
np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)
plt.title("3-Class classification (shrink_threshold=%r)"
% shrinkage)
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.182 seconds)
Download Python source code: plot_nearest_centroid.py
Download IPython notebook: plot_nearest_centroid.ipynb
22.4 Kernel Density Estimation
This example shows how kernel density estimation (KDE), a powerful non-parametric density estimation technique,
can be used to learn a generative model for a dataset. With this generative model in place, new samples can be drawn.
These new samples reﬂect the underlying model of the data.
1032
Chapter 22. Nearest Neighbors
scikit-learn user guide, Release 0.18.2
Out:
best bandwidth: 3.79269019073
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.neighbors import KernelDensity
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
# load the data
digits = load_digits()
data = digits.data
# project the 64-dimensional data to a lower dimension
pca = PCA(n_components=15, whiten=False)
data = pca.fit_transform(digits.data)
# use grid search cross-validation to optimize the bandwidth
22.4. Kernel Density Estimation
1033
scikit-learn user guide, Release 0.18.2
params = {'bandwidth': np.logspace(-1, 1, 20)}
grid = GridSearchCV(KernelDensity(), params)
grid.fit(data)
print("best bandwidth: {0}".format(grid.best_estimator_.bandwidth))
# use the best estimator to compute the kernel density estimate
kde = grid.best_estimator_
# sample 44 new points from the data
new_data = kde.sample(44, random_state=0)
new_data = pca.inverse_transform(new_data)
# turn data into a 4x11 grid
new_data = new_data.reshape((4, 11, -1))
real_data = digits.data[:44].reshape((4, 11, -1))
# plot real digits and resampled digits
fig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))
for j in range(11):
ax[4, j].set_visible(False)
for i in range(4):
im = ax[i, j].imshow(real_data[i, j].reshape((8, 8)),
cmap=plt.cm.binary, interpolation='nearest')
im.set_clim(0, 16)
im = ax[i + 5, j].imshow(new_data[i, j].reshape((8, 8)),
cmap=plt.cm.binary, interpolation='nearest')
im.set_clim(0, 16)
ax[0, 5].set_title('Selection from the input data')
ax[5, 5].set_title('"New" digits drawn from the kernel density model')
plt.show()
Total running time of the script: (0 minutes 15.624 seconds)
Download Python source code: plot_digits_kde_sampling.py
Download IPython notebook: plot_digits_kde_sampling.ipynb
22.5 Kernel Density Estimate of Species Distributions
This shows an example of a neighbors-based query (in particular a kernel density estimate) on geospatial data, using
a Ball Tree built upon the Haversine distance metric – i.e. distances over points in latitude/longitude. The dataset
is provided by Phillips et. al. (2006). If available, the example uses basemap to plot the coast lines and national
boundaries of South America.
This example does not perform any learning over the data (see Species distribution modeling for an example of classi-
ﬁcation based on the attributes in this dataset). It simply shows the kernel density estimate of observed data points in
geospatial coordinates.
The two species are:
• “Bradypus variegatus” , the Brown-throated Sloth.
• “Microryzomys minutus” , also known as the Forest Small Rice Rat, a rodent that lives in Peru, Colombia,
Ecuador, Peru, and Venezuela.
1034
Chapter 22. Nearest Neighbors
scikit-learn user guide, Release 0.18.2
22.5.1 References
• “Maximum entropy modeling of species geographic distributions” S. J. Phillips, R. P. Anderson, R. E. Schapire
- Ecological Modelling, 190:231-259, 2006.
Out:
- computing KDE in spherical coordinates
- plot coastlines from coverage
- computing KDE in spherical coordinates
- plot coastlines from coverage
# Author: Jake Vanderplas <jakevdp@cs.washington.edu>
#
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_species_distributions
from sklearn.datasets.species_distributions import construct_grids
from sklearn.neighbors import KernelDensity
22.5. Kernel Density Estimate of Species Distributions
1035
scikit-learn user guide, Release 0.18.2
# if basemap is available, we'll use it.
# otherwise, we'll improvise later...
try:
from mpl_toolkits.basemap import Basemap
basemap = True
except ImportError:
basemap = False
# Get matrices/arrays of species IDs and locations
data = fetch_species_distributions()
species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']
Xtrain = np.vstack([data['train']['dd lat'],
data['train']['dd long']]).T
ytrain = np.array([d.decode('ascii').startswith('micro')
for d in data['train']['species']], dtype='int')
Xtrain *= np.pi / 180.
# Convert lat/long to radians
# Set up the data grid for the contour plot
xgrid, ygrid = construct_grids(data)
X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])
land_reference = data.coverages[6][::5, ::5]
land_mask = (land_reference > -9999).ravel()
xy = np.vstack([Y.ravel(), X.ravel()]).T
xy = xy[land_mask]
xy *= np.pi / 180.
# Plot map of South America with distributions of each species
fig = plt.figure()
fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)
for i in range(2):
plt.subplot(1, 2, i + 1)
# construct a kernel density estimate of the distribution
print(" - computing KDE in spherical coordinates")
kde = KernelDensity(bandwidth=0.04, metric='haversine',
kernel='gaussian', algorithm='ball_tree')
kde.fit(Xtrain[ytrain == i])
# evaluate only on the land: -9999 indicates ocean
Z = -9999 + np.zeros(land_mask.shape[0])
Z[land_mask] = np.exp(kde.score_samples(xy))
Z = Z.reshape(X.shape)
# plot contours of the density
levels = np.linspace(0, Z.max(), 25)
plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)
if basemap:
print(" - plot coastlines using basemap")
m = Basemap(projection='cyl', llcrnrlat=Y.min(),
urcrnrlat=Y.max(), llcrnrlon=X.min(),
urcrnrlon=X.max(), resolution='c')
m.drawcoastlines()
m.drawcountries()
1036
Chapter 22. Nearest Neighbors
scikit-learn user guide, Release 0.18.2
else:
print(" - plot coastlines from coverage")
plt.contour(X, Y, land_reference,
levels=[-9999], colors="k",
linestyles="solid")
plt.xticks([])
plt.yticks([])
plt.title(species_names[i])
plt.show()
Total running time of the script: (0 minutes 10.640 seconds)
Download Python source code: plot_species_kde.py
Download IPython notebook: plot_species_kde.ipynb
22.6 Simple 1D Kernel Density Estimation
This example uses the sklearn.neighbors.KernelDensity class to demonstrate the principles of Kernel
Density Estimation in one dimension.
The ﬁrst plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a
histogram can be thought of as a scheme in which a unit “block” is stacked above each point on a regular grid. As
the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about
the underlying shape of the density distribution. If we instead center each block on the point it represents, we get the
estimate shown in the bottom left panel. This is a kernel density estimation with a “top hat” kernel. This idea can be
generalized to other kernel shapes: the bottom-right panel of the ﬁrst ﬁgure shows a Gaussian kernel density estimate
over the same distribution.
Scikit-learn implements efﬁcient kernel density estimation using either a Ball Tree or KD Tree structure, through the
sklearn.neighbors.KernelDensity estimator. The available kernels are shown in the second ﬁgure of this
example.
The third ﬁgure compares kernel density estimates for a distribution of 100 samples in 1 dimension. Though this
example uses 1D distributions, kernel density estimation is easily and efﬁciently extensible to higher dimensions as
well.
•
22.6. Simple 1D Kernel Density Estimation
1037
scikit-learn user guide, Release 0.18.2
•
•
# Author: Jake Vanderplas <jakevdp@cs.washington.edu>
#
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from sklearn.neighbors import KernelDensity
#----------------------------------------------------------------------
# Plot the progression of histograms to kernels
np.random.seed(1)
N = 20
X = np.concatenate((np.random.normal(0, 1, int(0.3 * N)),
np.random.normal(5, 1, int(0.7 * N))))[:, np.newaxis]
X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]
bins = np.linspace(-5, 10, 10)
fig, ax = plt.subplots(2, 2, sharex=True, sharey=True)
fig.subplots_adjust(hspace=0.05, wspace=0.05)
# histogram 1
ax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', normed=True)
ax[0, 0].text(-3.5, 0.31, "Histogram")
# histogram 2
ax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc='#AAAAFF', normed=True)
ax[0, 1].text(-3.5, 0.31, "Histogram, bins shifted")
1038
Chapter 22. Nearest Neighbors
scikit-learn user guide, Release 0.18.2
# tophat KDE
kde = KernelDensity(kernel='tophat', bandwidth=0.75).fit(X)
log_dens = kde.score_samples(X_plot)
ax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')
ax[1, 0].text(-3.5, 0.31, "Tophat Kernel Density")
# Gaussian KDE
kde = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(X)
log_dens = kde.score_samples(X_plot)
ax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')
ax[1, 1].text(-3.5, 0.31, "Gaussian Kernel Density")
for axi in ax.ravel():
axi.plot(X[:, 0], np.zeros(X.shape[0]) - 0.01, '+k')
axi.set_xlim(-4, 9)
axi.set_ylim(-0.02, 0.34)
for axi in ax[:, 0]:
axi.set_ylabel('Normalized Density')
for axi in ax[1, :]:
axi.set_xlabel('x')
#----------------------------------------------------------------------
# Plot all available kernels
X_plot = np.linspace(-6, 6, 1000)[:, None]
X_src = np.zeros((1, 1))
fig, ax = plt.subplots(2, 3, sharex=True, sharey=True)
fig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)
def format_func(x, loc):
if x == 0:
return '0'
elif x == 1:
return 'h'
elif x == -1:
return '-h'
else:
return '%ih' % x
for i, kernel in enumerate(['gaussian', 'tophat', 'epanechnikov',
'exponential', 'linear', 'cosine']):
axi = ax.ravel()[i]
log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(X_plot)
axi.fill(X_plot[:, 0], np.exp(log_dens), '-k', fc='#AAAAFF')
axi.text(-2.6, 0.95, kernel)
axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))
axi.xaxis.set_major_locator(plt.MultipleLocator(1))
axi.yaxis.set_major_locator(plt.NullLocator())
axi.set_ylim(0, 1.05)
axi.set_xlim(-2.9, 2.9)
ax[0, 1].set_title('Available Kernels')
22.6. Simple 1D Kernel Density Estimation
1039
scikit-learn user guide, Release 0.18.2
#----------------------------------------------------------------------
# Plot a 1D density example
N = 100
np.random.seed(1)
X = np.concatenate((np.random.normal(0, 1, int(0.3 * N)),
np.random.normal(5, 1, int(0.7 * N))))[:, np.newaxis]
X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]
true_dens = (0.3 * norm(0, 1).pdf(X_plot[:, 0])
+ 0.7 * norm(5, 1).pdf(X_plot[:, 0]))
fig, ax = plt.subplots()
ax.fill(X_plot[:, 0], true_dens, fc='black', alpha=0.2,
label='input distribution')
for kernel in ['gaussian', 'tophat', 'epanechnikov']:
kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)
log_dens = kde.score_samples(X_plot)
ax.plot(X_plot[:, 0], np.exp(log_dens), '-',
label="kernel = '{0}'".format(kernel))
ax.text(6, 0.38, "N={0} points".format(N))
ax.legend(loc='upper left')
ax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), '+k')
ax.set_xlim(-4, 9)
ax.set_ylim(-0.02, 0.4)
plt.show()
Total running time of the script: (0 minutes 0.989 seconds)
Download Python source code: plot_kde_1d.py
Download IPython notebook: plot_kde_1d.ipynb
22.7 Hyper-parameters of Approximate Nearest Neighbors
This example demonstrates the behaviour of the accuracy of the nearest neighbor queries of Locality Sensitive Hashing
Forest as the number of candidates and the number of estimators (trees) vary.
In the ﬁrst plot, accuracy is measured with the number of candidates. Here, the term “number of candidates” refers to
maximum bound for the number of distinct points retrieved from each tree to calculate the distances. Nearest neighbors
are selected from this pool of candidates. Number of estimators is maintained at three ﬁxed levels (1, 5, 10).
In the second plot, the number of candidates is ﬁxed at 50.
Number of trees is varied and the accuracy
is plotted against those values.
To measure the accuracy, the true nearest neighbors are required, therefore
sklearn.neighbors.NearestNeighbors is used to compute the exact neighbors.
from __future__ import division
print(__doc__)
# Author: Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>
#
# License: BSD 3 clause
1040
Chapter 22. Nearest Neighbors
scikit-learn user guide, Release 0.18.2
import numpy as np
from sklearn.datasets.samples_generator import make_blobs
from sklearn.neighbors import LSHForest
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
# Initialize size of the database, iterations and required neighbors.
n_samples = 10000
n_features = 100
n_queries = 30
rng = np.random.RandomState(42)
# Generate sample data
X, _ = make_blobs(n_samples=n_samples + n_queries,
n_features=n_features, centers=10,
random_state=0)
X_index = X[:n_samples]
X_query = X[n_samples:]
# Get exact neighbors
nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute',
metric='cosine').fit(X_index)
neighbors_exact = nbrs.kneighbors(X_query, return_distance=False)
# Set `n_candidate` values
n_candidates_values = np.linspace(10, 500, 5).astype(np.int)
n_estimators_for_candidate_value = [1, 5, 10]
n_iter = 10
stds_accuracies = np.zeros((len(n_estimators_for_candidate_value),
n_candidates_values.shape[0]),
dtype=float)
accuracies_c = np.zeros((len(n_estimators_for_candidate_value),
n_candidates_values.shape[0]), dtype=float)
# LSH Forest is a stochastic index: perform several iteration to estimate
# expected accuracy and standard deviation displayed as error bars in
# the plots
for j, value in enumerate(n_estimators_for_candidate_value):
for i, n_candidates in enumerate(n_candidates_values):
accuracy_c = []
for seed in range(n_iter):
lshf = LSHForest(n_estimators=value,
n_candidates=n_candidates, n_neighbors=1,
random_state=seed)
# Build the LSH Forest index
lshf.fit(X_index)
# Get neighbors
neighbors_approx = lshf.kneighbors(X_query,
return_distance=False)
accuracy_c.append(np.sum(np.equal(neighbors_approx,
neighbors_exact)) /
n_queries)
stds_accuracies[j, i] = np.std(accuracy_c)
accuracies_c[j, i] = np.mean(accuracy_c)
# Set `n_estimators` values
n_estimators_values = [1, 5, 10, 20, 30, 40, 50]
22.7. Hyper-parameters of Approximate Nearest Neighbors
1041
scikit-learn user guide, Release 0.18.2
accuracies_trees = np.zeros(len(n_estimators_values), dtype=float)
# Calculate average accuracy for each value of `n_estimators`
for i, n_estimators in enumerate(n_estimators_values):
lshf = LSHForest(n_estimators=n_estimators, n_neighbors=1)
# Build the LSH Forest index
lshf.fit(X_index)
# Get neighbors
neighbors_approx = lshf.kneighbors(X_query, return_distance=False)
accuracies_trees[i] = np.sum(np.equal(neighbors_approx,
neighbors_exact))/n_queries
Plot the accuracy variation with n_candidates
plt.figure()
colors = ['c', 'm', 'y']
for i, n_estimators in enumerate(n_estimators_for_candidate_value):
label = 'n_estimators = %d ' % n_estimators
plt.plot(n_candidates_values, accuracies_c[i, :],
'o-', c=colors[i], label=label)
plt.errorbar(n_candidates_values, accuracies_c[i, :],
stds_accuracies[i, :], c=colors[i])
plt.legend(loc='upper left', prop=dict(size='small'))
plt.ylim([0, 1.2])
plt.xlim(min(n_candidates_values), max(n_candidates_values))
plt.ylabel("Accuracy")
plt.xlabel("n_candidates")
plt.grid(which='both')
plt.title("Accuracy variation with n_candidates")
# Plot the accuracy variation with `n_estimators`
plt.figure()
plt.scatter(n_estimators_values, accuracies_trees, c='k')
plt.plot(n_estimators_values, accuracies_trees, c='g')
plt.ylim([0, 1.2])
plt.xlim(min(n_estimators_values), max(n_estimators_values))
plt.ylabel("Accuracy")
plt.xlabel("n_estimators")
plt.grid(which='both')
plt.title("Accuracy variation with n_estimators")
plt.show()
•
1042
Chapter 22. Nearest Neighbors
scikit-learn user guide, Release 0.18.2
•
Total running time of the script: (0 minutes 21.120 seconds)
Download Python source code: plot_approximate_nearest_neighbors_hyperparameters.py
Download IPython notebook: plot_approximate_nearest_neighbors_hyperparameters.ipynb
22.8 Scalability of Approximate Nearest Neighbors
This example studies the scalability proﬁle of approximate 10-neighbors queries using the LSHForest with
n_estimators=20 and n_candidates=200 when varying the number of samples in the dataset.
The ﬁrst plot demonstrates the relationship between query time and index size of LSHForest. Query time is compared
with the brute force method in exact nearest neighbor search for the same index sizes. The brute force queries have
a very predictable linear scalability with the index (full scan). LSHForest index have sub-linear scalability proﬁle but
can be slower for small datasets.
The second plot shows the speedup when using approximate queries vs brute force exact queries. The speedup tends
to increase with the dataset size but should reach a plateau typically when doing queries on datasets with millions
of samples and a few hundreds of dimensions. Higher dimensional datasets tends to beneﬁt more from LSHForest
indexing.
The break even point (speedup = 1) depends on the dimensionality and structure of the indexed data and the parameters
of the LSHForest index.
The precision of approximate queries should decrease slowly with the dataset size. The speed of the decrease depends
mostly on the LSHForest parameters and the dimensionality of the data.
from __future__ import division
print(__doc__)
# Authors: Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>
#
Olivier Grisel <olivier.grisel@ensta.org>
#
# License: BSD 3 clause
import time
import numpy as np
from sklearn.datasets.samples_generator import make_blobs
from sklearn.neighbors import LSHForest
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
22.8. Scalability of Approximate Nearest Neighbors
1043
scikit-learn user guide, Release 0.18.2
# Parameters of the study
n_samples_min = int(1e3)
n_samples_max = int(1e5)
n_features = 100
n_centers = 100
n_queries = 100
n_steps = 6
n_iter = 5
# Initialize the range of `n_samples`
n_samples_values = np.logspace(np.log10(n_samples_min),
np.log10(n_samples_max),
n_steps).astype(np.int)
# Generate some structured data
rng = np.random.RandomState(42)
all_data, _ = make_blobs(n_samples=n_samples_max + n_queries,
n_features=n_features, centers=n_centers, shuffle=True,
random_state=0)
queries = all_data[:n_queries]
index_data = all_data[n_queries:]
# Metrics to collect for the plots
average_times_exact = []
average_times_approx = []
std_times_approx = []
accuracies = []
std_accuracies = []
average_speedups = []
std_speedups = []
# Calculate the average query time
for n_samples in n_samples_values:
X = index_data[:n_samples]
# Initialize LSHForest for queries of a single neighbor
lshf = LSHForest(n_estimators=20, n_candidates=200,
n_neighbors=10).fit(X)
nbrs = NearestNeighbors(algorithm='brute', metric='cosine',
n_neighbors=10).fit(X)
time_approx = []
time_exact = []
accuracy = []
for i in range(n_iter):
# pick one query at random to study query time variability in LSHForest
query = queries[[rng.randint(0, n_queries)]]
t0 = time.time()
exact_neighbors = nbrs.kneighbors(query, return_distance=False)
time_exact.append(time.time() - t0)
t0 = time.time()
approx_neighbors = lshf.kneighbors(query, return_distance=False)
time_approx.append(time.time() - t0)
accuracy.append(np.in1d(approx_neighbors, exact_neighbors).mean())
average_time_exact = np.mean(time_exact)
1044
Chapter 22. Nearest Neighbors
scikit-learn user guide, Release 0.18.2
average_time_approx = np.mean(time_approx)
speedup = np.array(time_exact) / np.array(time_approx)
average_speedup = np.mean(speedup)
mean_accuracy = np.mean(accuracy)
std_accuracy = np.std(accuracy)
print("Index size: %d, exact: %0.3fs, LSHF: %0.3fs, speedup: %0.1f, "
"accuracy: %0.2f +/-%0.2f" %
(n_samples, average_time_exact, average_time_approx, average_speedup,
mean_accuracy, std_accuracy))
accuracies.append(mean_accuracy)
std_accuracies.append(std_accuracy)
average_times_exact.append(average_time_exact)
average_times_approx.append(average_time_approx)
std_times_approx.append(np.std(time_approx))
average_speedups.append(average_speedup)
std_speedups.append(np.std(speedup))
# Plot average query time against n_samples
plt.figure()
plt.errorbar(n_samples_values, average_times_approx, yerr=std_times_approx,
fmt='o-', c='r', label='LSHForest')
plt.plot(n_samples_values, average_times_exact, c='b',
label="NearestNeighbors(algorithm='brute', metric='cosine')")
plt.legend(loc='upper left', prop=dict(size='small'))
plt.ylim(0, None)
plt.ylabel("Average query time in seconds")
plt.xlabel("n_samples")
plt.grid(which='both')
plt.title("Impact of index size on response time for first "
"nearest neighbors queries")
# Plot average query speedup versus index size
plt.figure()
plt.errorbar(n_samples_values, average_speedups, yerr=std_speedups,
fmt='o-', c='r')
plt.ylim(0, None)
plt.ylabel("Average speedup")
plt.xlabel("n_samples")
plt.grid(which='both')
plt.title("Speedup of the approximate NN queries vs brute force")
# Plot average precision versus index size
plt.figure()
plt.errorbar(n_samples_values, accuracies, std_accuracies, fmt='o-', c='c')
plt.ylim(0, 1.1)
plt.ylabel("precision@10")
plt.xlabel("n_samples")
plt.grid(which='both')
plt.title("precision of 10-nearest-neighbors queries with index size")
plt.show()
22.8. Scalability of Approximate Nearest Neighbors
1045
scikit-learn user guide, Release 0.18.2
•
•
•
Out:
Index size: 1000, exact: 0.001s, LSHF: 0.017s, speedup: 0.1, accuracy: 1.00 +/-0.00
Index size: 2511, exact: 0.003s, LSHF: 0.018s, speedup: 0.2, accuracy: 1.00 +/-0.00
Index size: 6309, exact: 0.007s, LSHF: 0.020s, speedup: 0.3, accuracy: 1.00 +/-0.00
Index size: 15848, exact: 0.018s, LSHF: 0.022s, speedup: 0.9, accuracy: 1.00 +/-0.00
Index size: 39810, exact: 0.045s, LSHF: 0.020s, speedup: 2.2, accuracy: 1.00 +/-0.00
Index size: 100000, exact: 0.132s, LSHF: 0.026s, speedup: 5.1, accuracy: 0.96 +/-0.05
Total running time of the script: (0 minutes 5.955 seconds)
Download Python source code: plot_approximate_nearest_neighbors_scalability.py
Download IPython notebook: plot_approximate_nearest_neighbors_scalability.ipynb
1046
Chapter 22. Nearest Neighbors
CHAPTER
TWENTYTHREE
NEURAL NETWORKS
Examples concerning the sklearn.neural_network module.
23.1 Visualization of MLP weights on MNIST
Sometimes looking at the learned coefﬁcients of a neural network can provide insight into the learning behavior. For
example if weights look unstructured, maybe some were not used at all, or if very large coefﬁcients exist, maybe
regularization was too low or the learning rate too high.
This example shows how to plot some of the ﬁrst layer weights in a MLPClassiﬁer trained on the MNIST dataset.
The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the ﬁrst
layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the
weight matrix as a 28x28 pixel image.
To make the example run faster, we use very few hidden units, and train only for a very short time. Training longer
would result in weights with a much smoother spatial appearance.
1047
scikit-learn user guide, Release 0.18.2
Out:
Iteration 1, loss = 0.32212731
Iteration 2, loss = 0.15738787
Iteration 3, loss = 0.11647274
Iteration 4, loss = 0.09631113
Iteration 5, loss = 0.08074513
Iteration 6, loss = 0.07163224
Iteration 7, loss = 0.06351392
Iteration 8, loss = 0.05694146
Iteration 9, loss = 0.05213487
Iteration 10, loss = 0.04708320
Training set score: 0.985733
Test set score: 0.971000
print(__doc__)
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_mldata
from sklearn.neural_network import MLPClassifier
1048
Chapter 23. Neural Networks
scikit-learn user guide, Release 0.18.2
mnist = fetch_mldata("MNIST original")
# rescale the data, use the traditional train/test split
X, y = mnist.data / 255., mnist.target
X_train, X_test = X[:60000], X[60000:]
y_train, y_test = y[:60000], y[60000:]
# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
#
solver='sgd', verbose=10, tol=1e-4, random_state=1)
mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
solver='sgd', verbose=10, tol=1e-4, random_state=1,
learning_rate_init=.1)
mlp.fit(X_train, y_train)
print("Training set score: %f" % mlp.score(X_train, y_train))
print("Test set score: %f" % mlp.score(X_test, y_test))
fig, axes = plt.subplots(4, 4)
# use global min / max to ensure all weights are shown on the same scale
vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()
for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):
ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,
vmax=.5 * vmax)
ax.set_xticks(())
ax.set_yticks(())
plt.show()
Total running time of the script: (0 minutes 17.925 seconds)
Download Python source code: plot_mnist_filters.py
Download IPython notebook: plot_mnist_filters.ipynb
23.2 Restricted Boltzmann Machine features for digit classiﬁcation
For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like
handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (BernoulliRBM) can perform
effective non-linear feature extraction.
In order to learn good latent representations from a small dataset, we artiﬁcially generate more labeled data by per-
turbing the training data with linear shifts of 1 pixel in each direction.
This example shows how to build a classiﬁcation pipeline with a BernoulliRBM feature extractor and a
LogisticRegression classiﬁer. The hyperparameters of the entire model (learning rate, hidden layer size, regu-
larization) were optimized by grid search, but the search is not reproduced here because of runtime constraints.
Logistic regression on raw pixel values is presented for comparison. The example shows that the features extracted by
the BernoulliRBM help improve the classiﬁcation accuracy.
from __future__ import print_function
print(__doc__)
# Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve
# License: BSD
import numpy as np
23.2. Restricted Boltzmann Machine features for digit classiﬁcation
1049
scikit-learn user guide, Release 0.18.2
import matplotlib.pyplot as plt
from scipy.ndimage import convolve
from sklearn import linear_model, datasets, metrics
from sklearn.model_selection import train_test_split
from sklearn.neural_network import BernoulliRBM
from sklearn.pipeline import Pipeline
Setting up
def nudge_dataset(X, Y):
"""
This produces a dataset 5 times bigger than the original one,
by moving the 8x8 images in X around by 1px to left, right, down, up
"""
direction_vectors = [
[[0, 1, 0],
[0, 0, 0],
[0, 0, 0]],
[[0, 0, 0],
[1, 0, 0],
[0, 0, 0]],
[[0, 0, 0],
[0, 0, 1],
[0, 0, 0]],
[[0, 0, 0],
[0, 0, 0],
[0, 1, 0]]]
shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',
weights=w).ravel()
X = np.concatenate([X] +
[np.apply_along_axis(shift, 1, X, vector)
for vector in direction_vectors])
Y = np.concatenate([Y for _ in range(5)], axis=0)
return X, Y
# Load Data
digits = datasets.load_digits()
X = np.asarray(digits.data, 'float32')
X, Y = nudge_dataset(X, digits.target)
X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)
# 0-1 scaling
X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
test_size=0.2,
random_state=0)
# Models we will use
logistic = linear_model.LogisticRegression()
rbm = BernoulliRBM(random_state=0, verbose=True)
classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])
Training
1050
Chapter 23. Neural Networks
scikit-learn user guide, Release 0.18.2
# Hyper-parameters. These were set by cross-validation,
# using a GridSearchCV. Here we are not performing cross-validation to
# save time.
rbm.learning_rate = 0.06
rbm.n_iter = 20
# More components tend to give better prediction performance, but larger
# fitting time
rbm.n_components = 100
logistic.C = 6000.0
# Training RBM-Logistic Pipeline
classifier.fit(X_train, Y_train)
# Training Logistic regression
logistic_classifier = linear_model.LogisticRegression(C=100.0)
logistic_classifier.fit(X_train, Y_train)
Out:
[BernoulliRBM] Iteration 1, pseudo-likelihood = -25.39, time = 0.40s
[BernoulliRBM] Iteration 2, pseudo-likelihood = -23.77, time = 0.52s
[BernoulliRBM] Iteration 3, pseudo-likelihood = -22.94, time = 0.51s
[BernoulliRBM] Iteration 4, pseudo-likelihood = -21.91, time = 0.51s
[BernoulliRBM] Iteration 5, pseudo-likelihood = -21.69, time = 0.51s
[BernoulliRBM] Iteration 6, pseudo-likelihood = -21.06, time = 0.51s
[BernoulliRBM] Iteration 7, pseudo-likelihood = -20.89, time = 0.50s
[BernoulliRBM] Iteration 8, pseudo-likelihood = -20.64, time = 0.50s
[BernoulliRBM] Iteration 9, pseudo-likelihood = -20.36, time = 0.50s
[BernoulliRBM] Iteration 10, pseudo-likelihood = -20.09, time = 0.50s
[BernoulliRBM] Iteration 11, pseudo-likelihood = -20.08, time = 0.50s
[BernoulliRBM] Iteration 12, pseudo-likelihood = -19.82, time = 0.50s
[BernoulliRBM] Iteration 13, pseudo-likelihood = -19.64, time = 0.50s
[BernoulliRBM] Iteration 14, pseudo-likelihood = -19.61, time = 0.50s
[BernoulliRBM] Iteration 15, pseudo-likelihood = -19.57, time = 0.51s
[BernoulliRBM] Iteration 16, pseudo-likelihood = -19.41, time = 0.50s
[BernoulliRBM] Iteration 17, pseudo-likelihood = -19.30, time = 0.50s
[BernoulliRBM] Iteration 18, pseudo-likelihood = -19.25, time = 0.50s
[BernoulliRBM] Iteration 19, pseudo-likelihood = -19.27, time = 0.50s
[BernoulliRBM] Iteration 20, pseudo-likelihood = -19.01, time = 0.50s
Evaluation
print()
print("Logistic regression using RBM features:\n%s\n" % (
metrics.classification_report(
Y_test,
classifier.predict(X_test))))
print("Logistic regression using raw pixel features:\n%s\n" % (
metrics.classification_report(
Y_test,
logistic_classifier.predict(X_test))))
Out:
Logistic regression using RBM features:
precision
recall
f1-score
support
23.2. Restricted Boltzmann Machine features for digit classiﬁcation
1051
scikit-learn user guide, Release 0.18.2
0
0.99
0.99
0.99
174
1
0.92
0.95
0.93
184
2
0.95
0.98
0.97
166
3
0.97
0.91
0.94
194
4
0.97
0.95
0.96
186
5
0.93
0.93
0.93
181
6
0.98
0.97
0.97
207
7
0.95
1.00
0.97
154
8
0.90
0.88
0.89
182
9
0.91
0.93
0.92
169
avg / total
0.95
0.95
0.95
1797
Logistic regression using raw pixel features:
precision
recall
f1-score
support
0
0.85
0.94
0.89
174
1
0.57
0.55
0.56
184
2
0.72
0.85
0.78
166
3
0.76
0.74
0.75
194
4
0.85
0.82
0.84
186
5
0.74
0.75
0.75
181
6
0.93
0.88
0.91
207
7
0.86
0.90
0.88
154
8
0.68
0.55
0.61
182
9
0.71
0.74
0.72
169
avg / total
0.77
0.77
0.77
1797
Plotting
plt.figure(figsize=(4.2, 4))
for i, comp in enumerate(rbm.components_):
plt.subplot(10, 10, i + 1)
plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,
interpolation='nearest')
plt.xticks(())
plt.yticks(())
plt.suptitle('100 components extracted by RBM', fontsize=16)
plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)
plt.show()
1052
Chapter 23. Neural Networks
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 36.804 seconds)
Download Python source code: plot_rbm_logistic_classification.py
Download IPython notebook: plot_rbm_logistic_classification.ipynb
23.3 Compare Stochastic learning strategies for MLPClassiﬁer
This example visualizes some training loss curves for different stochastic learning strategies, including SGD and
Adam. Because of time-constraints, we use several small datasets, for which L-BFGS might be more suitable. The
general trend shown in these examples seems to carry over to larger datasets, however.
Note that those results can be highly dependent on the value of learning_rate_init.
23.3. Compare Stochastic learning strategies for MLPClassiﬁer
1053
scikit-learn user guide, Release 0.18.2
Out:
learning on dataset iris
training: constant learning-rate
Training set score: 0.980000
Training set loss: 0.096922
training: constant with momentum
Training set score: 0.980000
Training set loss: 0.050260
training: constant with Nesterov's momentum
Training set score: 0.980000
Training set loss: 0.050277
training: inv-scaling learning-rate
Training set score: 0.360000
Training set loss: 0.979983
training: inv-scaling with momentum
Training set score: 0.860000
Training set loss: 0.504017
training: inv-scaling with Nesterov's momentum
Training set score: 0.860000
Training set loss: 0.504760
training: adam
Training set score: 0.980000
Training set loss: 0.046248
learning on dataset digits
training: constant learning-rate
Training set score: 0.956038
Training set loss: 0.243802
1054
Chapter 23. Neural Networks
scikit-learn user guide, Release 0.18.2
training: constant with momentum
Training set score: 0.992766
Training set loss: 0.041297
training: constant with Nesterov's momentum
Training set score: 0.993879
Training set loss: 0.042898
training: inv-scaling learning-rate
Training set score: 0.638843
Training set loss: 1.855465
training: inv-scaling with momentum
Training set score: 0.912632
Training set loss: 0.290584
training: inv-scaling with Nesterov's momentum
Training set score: 0.909293
Training set loss: 0.318387
training: adam
Training set score: 0.991653
Training set loss: 0.045934
learning on dataset circles
training: constant learning-rate
Training set score: 0.830000
Training set loss: 0.681498
training: constant with momentum
Training set score: 0.940000
Training set loss: 0.163712
training: constant with Nesterov's momentum
Training set score: 0.940000
Training set loss: 0.163012
training: inv-scaling learning-rate
Training set score: 0.500000
Training set loss: 0.692855
training: inv-scaling with momentum
Training set score: 0.510000
Training set loss: 0.688376
training: inv-scaling with Nesterov's momentum
Training set score: 0.500000
Training set loss: 0.688593
training: adam
Training set score: 0.930000
Training set loss: 0.159988
learning on dataset moons
training: constant learning-rate
Training set score: 0.850000
Training set loss: 0.342245
training: constant with momentum
Training set score: 0.850000
Training set loss: 0.345580
training: constant with Nesterov's momentum
Training set score: 0.850000
Training set loss: 0.336284
training: inv-scaling learning-rate
Training set score: 0.500000
Training set loss: 0.689729
training: inv-scaling with momentum
Training set score: 0.830000
Training set loss: 0.512595
23.3. Compare Stochastic learning strategies for MLPClassiﬁer
1055
scikit-learn user guide, Release 0.18.2
training: inv-scaling with Nesterov's momentum
Training set score: 0.830000
Training set loss: 0.513034
training: adam
Training set score: 0.850000
Training set loss: 0.334243
print(__doc__)
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn import datasets
# different learning rate schedules and momentum parameters
params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,
'learning_rate_init': 0.2},
{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
'nesterovs_momentum': False, 'learning_rate_init': 0.2},
{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
'nesterovs_momentum': True, 'learning_rate_init': 0.2},
{'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,
'learning_rate_init': 0.2},
{'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
'nesterovs_momentum': True, 'learning_rate_init': 0.2},
{'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
'nesterovs_momentum': False, 'learning_rate_init': 0.2},
{'solver': 'adam', 'learning_rate_init': 0.01}]
labels = ["constant learning-rate", "constant with momentum",
"constant with Nesterov's momentum",
"inv-scaling learning-rate", "inv-scaling with momentum",
"inv-scaling with Nesterov's momentum", "adam"]
plot_args = [{'c': 'red', 'linestyle': '-'},
{'c': 'green', 'linestyle': '-'},
{'c': 'blue', 'linestyle': '-'},
{'c': 'red', 'linestyle': '--'},
{'c': 'green', 'linestyle': '--'},
{'c': 'blue', 'linestyle': '--'},
{'c': 'black', 'linestyle': '-'}]
def plot_on_dataset(X, y, ax, name):
# for each dataset, plot learning for each learning strategy
print("\nlearning on dataset %s" % name)
ax.set_title(name)
X = MinMaxScaler().fit_transform(X)
mlps = []
if name == "digits":
# digits is larger but converges fairly quickly
max_iter = 15
else:
max_iter = 400
1056
Chapter 23. Neural Networks
scikit-learn user guide, Release 0.18.2
for label, param in zip(labels, params):
print("training: %s" % label)
mlp = MLPClassifier(verbose=0, random_state=0,
max_iter=max_iter, **param)
mlp.fit(X, y)
mlps.append(mlp)
print("Training set score: %f" % mlp.score(X, y))
print("Training set loss: %f" % mlp.loss_)
for mlp, label, args in zip(mlps, labels, plot_args):
ax.plot(mlp.loss_curve_, label=label, **args)
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
# load / generate some toy datasets
iris = datasets.load_iris()
digits = datasets.load_digits()
data_sets = [(iris.data, iris.target),
(digits.data, digits.target),
datasets.make_circles(noise=0.2, factor=0.5, random_state=1),
datasets.make_moons(noise=0.3, random_state=0)]
for ax, data, name in zip(axes.ravel(), data_sets, ['iris', 'digits',
'circles', 'moons']):
plot_on_dataset(*data, ax=ax, name=name)
fig.legend(ax.get_lines(), labels=labels, ncol=3, loc="upper center")
plt.show()
Total running time of the script: (0 minutes 5.879 seconds)
Download Python source code: plot_mlp_training_curves.py
Download IPython notebook: plot_mlp_training_curves.ipynb
23.4 Varying regularization in Multi-layer Perceptron
A comparison of different values for regularization parameter ‘alpha’ on synthetic datasets. The plot shows that
different alphas yield different decision functions.
Alpha is a parameter for regularization term, aka penalty term, that combats overﬁtting by constraining the size of the
weights. Increasing alpha may ﬁx high variance (a sign of overﬁtting) by encouraging smaller weights, resulting in
a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may ﬁx high bias (a sign of
underﬁtting) by encouraging larger weights, potentially resulting in a more complicated decision boundary.
23.4. Varying regularization in Multi-layer Perceptron
1057
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Author: Issam H. Laradji
# License: BSD 3 clause
import numpy as np
from matplotlib import pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
h = .02
# step size in the mesh
alphas = np.logspace(-5, 3, 5)
names = []
for i in alphas:
names.append('alpha ' + str(i))
classifiers = []
for i in alphas:
classifiers.append(MLPClassifier(alpha=i, random_state=1))
X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
random_state=0, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)
datasets = [make_moons(noise=0.3, random_state=0),
make_circles(noise=0.2, factor=0.5, random_state=1),
linearly_separable]
1058
Chapter 23. Neural Networks
scikit-learn user guide, Release 0.18.2
figure = plt.figure(figsize=(17, 9))
i = 1
# iterate over datasets
for X, y in datasets:
# preprocess dataset, split into training and test part
X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
# just plot the dataset first
cm = plt.cm.RdBu
cm_bright = ListedColormap(['#FF0000', '#0000FF'])
ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
# Plot the training points
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
# and testing points
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())
i += 1
# iterate over classifiers
for name, clf in zip(names, classifiers):
ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
if hasattr(clf, "decision_function"):
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
else:
Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
# Put the result into a color plot
Z = Z.reshape(xx.shape)
ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
# Plot also the training points
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
# and testing points
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
alpha=0.6)
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())
ax.set_title(name)
ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
size=15, horizontalalignment='right')
i += 1
23.4. Varying regularization in Multi-layer Perceptron
1059
scikit-learn user guide, Release 0.18.2
figure.subplots_adjust(left=.02, right=.98)
plt.show()
Total running time of the script: (0 minutes 5.244 seconds)
Download Python source code: plot_mlp_alpha.py
Download IPython notebook: plot_mlp_alpha.ipynb
1060
Chapter 23. Neural Networks
CHAPTER
TWENTYFOUR
PREPROCESSING
Examples concerning the sklearn.preprocessing module.
24.1 Using FunctionTransformer to select columns
Shows how to use a function transformer in a pipeline. If you know your dataset’s ﬁrst principle component is irrelevant
for a classiﬁcation task, you can use the FunctionTransformer to select all but the ﬁrst column of the PCA transformed
data.
•
•
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
1061
scikit-learn user guide, Release 0.18.2
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import FunctionTransformer
def _generate_vector(shift=0.5, noise=15):
return np.arange(1000) + (np.random.rand(1000) - shift) * noise
def generate_dataset():
"""
This dataset is two lines with a slope ~ 1, where one has
a y offset of ~100
"""
return np.vstack((
np.vstack((
_generate_vector(),
_generate_vector() + 100,
)).T,
np.vstack((
_generate_vector(),
_generate_vector(),
)).T,
)), np.hstack((np.zeros(1000), np.ones(1000)))
def all_but_first_column(X):
return X[:, 1:]
def drop_first_component(X, y):
"""
Create a pipeline with PCA and the column selector and use it to
transform the dataset.
"""
pipeline = make_pipeline(
PCA(), FunctionTransformer(all_but_first_column),
)
X_train, X_test, y_train, y_test = train_test_split(X, y)
pipeline.fit(X_train, y_train)
return pipeline.transform(X_test), y_test
if __name__ == '__main__':
X, y = generate_dataset()
lw = 0
plt.figure()
plt.scatter(X[:, 0], X[:, 1], c=y, lw=lw)
plt.figure()
X_transformed, y_transformed = drop_first_component(*generate_dataset())
plt.scatter(
X_transformed[:, 0],
np.zeros(len(X_transformed)),
c=y_transformed,
lw=lw,
s=60
)
plt.show()
1062
Chapter 24. Preprocessing
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.144 seconds)
Download Python source code: plot_function_transformer.py
Download IPython notebook: plot_function_transformer.ipynb
24.2 Robust Scaling on Toy Data
Making sure that each Feature has approximately the same scale can be a crucial preprocessing step. However, when
data contains outliers, StandardScaler can often be mislead. In such cases, it is better to use a scaler that is robust
against outliers.
Here, we demonstrate this on a toy dataset, where one single datapoint is a large outlier.
Out:
Testset accuracy using standard scaler: 0.545
Testset accuracy using robust scaler:
0.705
from __future__ import print_function
print(__doc__)
# Code source: Thomas Unterthiner
# License: BSD 3 clause
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler
# Create training and test data
np.random.seed(42)
n_datapoints = 100
Cov = [[0.9, 0.0], [0.0, 20.0]]
mu1 = [100.0, -3.0]
mu2 = [101.0, -3.0]
X1 = np.random.multivariate_normal(mean=mu1, cov=Cov, size=n_datapoints)
24.2. Robust Scaling on Toy Data
1063
scikit-learn user guide, Release 0.18.2
X2 = np.random.multivariate_normal(mean=mu2, cov=Cov, size=n_datapoints)
Y_train = np.hstack([[-1]*n_datapoints, [1]*n_datapoints])
X_train = np.vstack([X1, X2])
X1 = np.random.multivariate_normal(mean=mu1, cov=Cov, size=n_datapoints)
X2 = np.random.multivariate_normal(mean=mu2, cov=Cov, size=n_datapoints)
Y_test = np.hstack([[-1]*n_datapoints, [1]*n_datapoints])
X_test = np.vstack([X1, X2])
X_train[0, 0] = -1000
# a fairly large outlier
# Scale data
standard_scaler = StandardScaler()
Xtr_s = standard_scaler.fit_transform(X_train)
Xte_s = standard_scaler.transform(X_test)
robust_scaler = RobustScaler()
Xtr_r = robust_scaler.fit_transform(X_train)
Xte_r = robust_scaler.transform(X_test)
# Plot data
fig, ax = plt.subplots(1, 3, figsize=(12, 4))
ax[0].scatter(X_train[:, 0], X_train[:, 1],
color=np.where(Y_train > 0, 'r', 'b'))
ax[1].scatter(Xtr_s[:, 0], Xtr_s[:, 1], color=np.where(Y_train > 0, 'r', 'b'))
ax[2].scatter(Xtr_r[:, 0], Xtr_r[:, 1], color=np.where(Y_train > 0, 'r', 'b'))
ax[0].set_title("Unscaled data")
ax[1].set_title("After standard scaling (zoomed in)")
ax[2].set_title("After robust scaling (zoomed in)")
# for the scaled data, we zoom in to the data center (outlier can't be seen!)
for a in ax[1:]:
a.set_xlim(-3, 3)
a.set_ylim(-3, 3)
plt.tight_layout()
plt.show()
# Classify using k-NN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(Xtr_s, Y_train)
acc_s = knn.score(Xte_s, Y_test)
print("Testset accuracy using standard scaler: %.3f" % acc_s)
knn.fit(Xtr_r, Y_train)
acc_r = knn.score(Xte_r, Y_test)
print("Testset accuracy using robust scaler:
%.3f" % acc_r)
Total running time of the script: (0 minutes 0.284 seconds)
Download Python source code: plot_robust_scaling.py
Download IPython notebook: plot_robust_scaling.ipynb
1064
Chapter 24. Preprocessing
CHAPTER
TWENTYFIVE
SEMI SUPERVISED CLASSIFICATION
Examples concerning the sklearn.semi_supervised module.
25.1 Label Propagation learning a complex structure
Example of LabelPropagation learning a complex internal structure to demonstrate “manifold learning”. The outer
circle should be labeled “red” and the inner circle “blue”. Because both label groups lie inside their own distinct
shape, we can see that the labels propagate correctly around the circle.
print(__doc__)
# Authors: Clay Woolam <clay@woolam.org>
#
Andreas Mueller <amueller@ais.uni-bonn.de>
# License: BSD
import numpy as np
import matplotlib.pyplot as plt
from sklearn.semi_supervised import label_propagation
from sklearn.datasets import make_circles
# generate ring with inner box
n_samples = 200
X, y = make_circles(n_samples=n_samples, shuffle=False)
outer, inner = 0, 1
labels = -np.ones(n_samples)
labels[0] = outer
labels[-1] = inner
Learn with LabelSpreading
label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=1.0)
label_spread.fit(X, labels)
Plot output labels
output_labels = label_spread.transduction_
plt.figure(figsize=(8.5, 4))
plt.subplot(1, 2, 1)
plt.scatter(X[labels == outer, 0], X[labels == outer, 1], color='navy',
marker='s', lw=0, label="outer labeled", s=10)
plt.scatter(X[labels == inner, 0], X[labels == inner, 1], color='c',
marker='s', lw=0, label='inner labeled', s=10)
plt.scatter(X[labels == -1, 0], X[labels == -1, 1], color='darkorange',
1065
scikit-learn user guide, Release 0.18.2
marker='.', label='unlabeled')
plt.legend(scatterpoints=1, shadow=False, loc='upper right')
plt.title("Raw data (2 classes=outer and inner)")
plt.subplot(1, 2, 2)
output_label_array = np.asarray(output_labels)
outer_numbers = np.where(output_label_array == outer)[0]
inner_numbers = np.where(output_label_array == inner)[0]
plt.scatter(X[outer_numbers, 0], X[outer_numbers, 1], color='navy',
marker='s', lw=0, s=10, label="outer learned")
plt.scatter(X[inner_numbers, 0], X[inner_numbers, 1], color='c',
marker='s', lw=0, s=10, label="inner learned")
plt.legend(scatterpoints=1, shadow=False, loc='upper right')
plt.title("Labels learned with Label Spreading (KNN)")
plt.subplots_adjust(left=0.07, bottom=0.07, right=0.93, top=0.92)
plt.show()
Total running time of the script: (0 minutes 0.128 seconds)
Download Python source code: plot_label_propagation_structure.py
Download IPython notebook: plot_label_propagation_structure.ipynb
25.2 Label Propagation digits: Demonstrating performance
This example demonstrates the power of semisupervised learning by training a Label Spreading model to classify
handwritten digits with sets of very few labels.
The handwritten digit dataset has 1797 total points. The model will be trained using all points, but only 30 will be
labeled. Results in the form of a confusion matrix and a series of metrics over each class will be very good.
At the end, the top 10 most uncertain predictions will be shown.
print(__doc__)
1066
Chapter 25. Semi Supervised Classiﬁcation
scikit-learn user guide, Release 0.18.2
# Authors: Clay Woolam <clay@woolam.org>
# License: BSD
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn import datasets
from sklearn.semi_supervised import label_propagation
from sklearn.metrics import confusion_matrix, classification_report
digits = datasets.load_digits()
rng = np.random.RandomState(0)
indices = np.arange(len(digits.data))
rng.shuffle(indices)
X = digits.data[indices[:330]]
y = digits.target[indices[:330]]
images = digits.images[indices[:330]]
n_total_samples = len(y)
n_labeled_points = 30
indices = np.arange(n_total_samples)
unlabeled_set = indices[n_labeled_points:]
# shuffle everything around
y_train = np.copy(y)
y_train[unlabeled_set] = -1
Learn with LabelSpreading
lp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=5)
lp_model.fit(X, y_train)
predicted_labels = lp_model.transduction_[unlabeled_set]
true_labels = y[unlabeled_set]
cm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)
print("Label Spreading model: %d labeled & %d unlabeled points (%d total)" %
(n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))
print(classification_report(true_labels, predicted_labels))
print("Confusion matrix")
print(cm)
# calculate uncertainty values for each transduced distribution
pred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)
# pick the top 10 most uncertain labels
uncertainty_index = np.argsort(pred_entropies)[-10:]
Out:
25.2. Label Propagation digits: Demonstrating performance
1067
scikit-learn user guide, Release 0.18.2
Label Spreading model: 30 labeled & 300 unlabeled points (330 total)
precision
recall
f1-score
support
0
1.00
1.00
1.00
23
1
0.58
0.54
0.56
28
2
0.96
0.93
0.95
29
3
0.00
0.00
0.00
28
4
0.91
0.80
0.85
25
5
0.96
0.79
0.87
33
6
0.97
0.97
0.97
36
7
0.89
1.00
0.94
34
8
0.48
0.83
0.61
29
9
0.54
0.77
0.64
35
avg / total
0.73
0.77
0.74
300
Confusion matrix
[[23
0
0
0
0
0
0
0
0]
[ 0 15
1
0
0
1
0 11
0]
[ 0
0 27
0
0
0
2
0
0]
[ 0
5
0 20
0
0
0
0
0]
[ 0
0
0
0 26
0
0
1
6]
[ 0
1
0
0
0 35
0
0
0]
[ 0
0
0
0
0
0 34
0
0]
[ 0
5
0
0
0
0
0 24
0]
[ 0
0
0
2
1
0
2
3 27]]
plot
f = plt.figure(figsize=(7, 5))
for index, image_index in enumerate(uncertainty_index):
image = images[image_index]
sub = f.add_subplot(2, 5, index + 1)
sub.imshow(image, cmap=plt.cm.gray_r)
plt.xticks([])
plt.yticks([])
sub.set_title('predict: %i\ntrue: %i' % (
lp_model.transduction_[image_index], y[image_index]))
f.suptitle('Learning with small amount of labeled data')
plt.show()
1068
Chapter 25. Semi Supervised Classiﬁcation
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.979 seconds)
Download Python source code: plot_label_propagation_digits.py
Download IPython notebook: plot_label_propagation_digits.ipynb
25.3 Decision boundary of label propagation versus SVM on the Iris
dataset
Comparison for decision boundary generated on iris dataset between Label Propagation and SVM.
This demonstrates Label Propagation learning a good boundary even with a small amount of labeled data.
25.3. Decision boundary of label propagation versus SVM on the Iris dataset
1069
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Authors: Clay Woolam <clay@woolam.org>
# License: BSD
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn import svm
from sklearn.semi_supervised import label_propagation
rng = np.random.RandomState(0)
iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target
# step size in the mesh
h = .02
y_30 = np.copy(y)
y_30[rng.rand(len(y)) < 0.3] = -1
y_50 = np.copy(y)
y_50[rng.rand(len(y)) < 0.5] = -1
# we create an instance of SVM and fit out data. We do not scale our
1070
Chapter 25. Semi Supervised Classiﬁcation
scikit-learn user guide, Release 0.18.2
# data since we want to plot the support vectors
ls30 = (label_propagation.LabelSpreading().fit(X, y_30),
y_30)
ls50 = (label_propagation.LabelSpreading().fit(X, y_50),
y_50)
ls100 = (label_propagation.LabelSpreading().fit(X, y), y)
rbf_svc = (svm.SVC(kernel='rbf').fit(X, y), y)
# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
np.arange(y_min, y_max, h))
# title for the plots
titles = ['Label Spreading 30% data',
'Label Spreading 50% data',
'Label Spreading 100% data',
'SVC with rbf kernel']
color_map = {-1: (1, 1, 1), 0: (0, 0, .9), 1: (1, 0, 0), 2: (.8, .6, 0)}
for i, (clf, y_train) in enumerate((ls30, ls50, ls100, rbf_svc)):
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
plt.subplot(2, 2, i + 1)
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
plt.axis('off')
# Plot also the training points
colors = [color_map[y] for y in y_train]
plt.scatter(X[:, 0], X[:, 1], c=colors, cmap=plt.cm.Paired)
plt.title(titles[i])
plt.text(.90, 0, "Unlabeled points are colored white")
plt.show()
Total running time of the script: (0 minutes 1.718 seconds)
Download Python source code: plot_label_propagation_versus_svm_iris.py
Download IPython notebook: plot_label_propagation_versus_svm_iris.ipynb
25.4 Label Propagation digits active learning
Demonstrates an active learning technique to learn handwritten digits using label propagation.
We start by training a label propagation model with only 10 labeled points, then we select the top ﬁve most uncertain
points to label. Next, we train with 15 labeled points (original 10 + 5 new ones). We repeat this process four times to
have a model trained with 30 labeled examples.
25.4. Label Propagation digits active learning
1071
scikit-learn user guide, Release 0.18.2
A plot will appear showing the top 5 most uncertain digits for each iteration of training. These may or may not contain
mistakes, but we will train the next model with their true labels.
Out:
Iteration 0 ______________________________________________________________________
Label Spreading model: 10 labeled & 320 unlabeled (330 total)
precision
recall
f1-score
support
0
0.00
0.00
0.00
24
1
0.49
0.90
0.63
29
2
0.88
0.97
0.92
31
3
0.00
0.00
0.00
28
4
0.00
0.00
0.00
27
5
0.89
0.49
0.63
35
6
0.86
0.95
0.90
40
7
0.75
0.92
0.83
36
8
0.54
0.79
0.64
33
9
0.41
0.86
0.56
37
avg / total
0.52
0.63
0.55
320
Confusion matrix
[[26
1
0
0
1
0
1]
[ 1 30
0
0
0
0
0]
[ 0
0 17
6
0
2 10]
1072
Chapter 25. Semi Supervised Classiﬁcation
scikit-learn user guide, Release 0.18.2
[ 2
0
0 38
0
0
0]
[ 0
3
0
0 33
0
0]
[ 7
0
0
0
0 26
0]
[ 0
0
2
0
0
3 32]]
Iteration 1 ______________________________________________________________________
Label Spreading model: 15 labeled & 315 unlabeled (330 total)
precision
recall
f1-score
support
0
1.00
1.00
1.00
23
1
0.61
0.59
0.60
29
2
0.91
0.97
0.94
31
3
1.00
0.56
0.71
27
4
0.79
0.88
0.84
26
5
0.89
0.46
0.60
35
6
0.86
0.95
0.90
40
7
0.97
0.92
0.94
36
8
0.54
0.84
0.66
31
9
0.70
0.81
0.75
37
avg / total
0.82
0.80
0.79
315
Confusion matrix
[[23
0
0
0
0
0
0
0
0
0]
[ 0 17
1
0
2
0
0
1
7
1]
[ 0
1 30
0
0
0
0
0
0
0]
[ 0
0
0 15
0
0
0
0 10
2]
[ 0
3
0
0 23
0
0
0
0
0]
[ 0
0
0
0
1 16
6
0
2 10]
[ 0
2
0
0
0
0 38
0
0
0]
[ 0
0
2
0
1
0
0 33
0
0]
[ 0
5
0
0
0
0
0
0 26
0]
[ 0
0
0
0
2
2
0
0
3 30]]
Iteration 2 ______________________________________________________________________
Label Spreading model: 20 labeled & 310 unlabeled (330 total)
precision
recall
f1-score
support
0
1.00
1.00
1.00
23
1
0.68
0.59
0.63
29
2
0.91
0.97
0.94
31
3
0.96
1.00
0.98
23
4
0.81
1.00
0.89
25
5
0.89
0.46
0.60
35
6
0.86
0.95
0.90
40
7
0.97
0.92
0.94
36
8
0.68
0.84
0.75
31
9
0.75
0.81
0.78
37
avg / total
0.85
0.84
0.83
310
Confusion matrix
[[23
0
0
0
0
0
0
0
0
0]
[ 0 17
1
0
2
0
0
1
7
1]
[ 0
1 30
0
0
0
0
0
0
0]
[ 0
0
0 23
0
0
0
0
0
0]
[ 0
0
0
0 25
0
0
0
0
0]
[ 0
0
0
1
1 16
6
0
2
9]
[ 0
2
0
0
0
0 38
0
0
0]
[ 0
0
2
0
1
0
0 33
0
0]
25.4. Label Propagation digits active learning
1073
scikit-learn user guide, Release 0.18.2
[ 0
5
0
0
0
0
0
0 26
0]
[ 0
0
0
0
2
2
0
0
3 30]]
Iteration 3 ______________________________________________________________________
Label Spreading model: 25 labeled & 305 unlabeled (330 total)
precision
recall
f1-score
support
0
1.00
1.00
1.00
23
1
0.70
0.85
0.77
27
2
1.00
0.90
0.95
31
3
1.00
1.00
1.00
23
4
1.00
1.00
1.00
25
5
0.96
0.74
0.83
34
6
1.00
0.95
0.97
40
7
0.90
1.00
0.95
35
8
0.83
0.81
0.82
31
9
0.75
0.83
0.79
36
avg / total
0.91
0.90
0.90
305
Confusion matrix
[[23
0
0
0
0
0
0
0
0
0]
[ 0 23
0
0
0
0
0
0
4
0]
[ 0
1 28
0
0
0
0
2
0
0]
[ 0
0
0 23
0
0
0
0
0
0]
[ 0
0
0
0 25
0
0
0
0
0]
[ 0
0
0
0
0 25
0
0
0
9]
[ 0
2
0
0
0
0 38
0
0
0]
[ 0
0
0
0
0
0
0 35
0
0]
[ 0
5
0
0
0
0
0
0 25
1]
[ 0
2
0
0
0
1
0
2
1 30]]
Iteration 4 ______________________________________________________________________
Label Spreading model: 30 labeled & 300 unlabeled (330 total)
precision
recall
f1-score
support
0
1.00
1.00
1.00
23
1
0.77
0.88
0.82
26
2
1.00
0.90
0.95
31
3
1.00
1.00
1.00
23
4
1.00
1.00
1.00
25
5
0.94
0.97
0.95
32
6
1.00
0.97
0.99
39
7
0.90
1.00
0.95
35
8
0.89
0.81
0.85
31
9
0.94
0.89
0.91
35
avg / total
0.94
0.94
0.94
300
Confusion matrix
[[23
0
0
0
0
0
0
0
0
0]
[ 0 23
0
0
0
0
0
0
3
0]
[ 0
1 28
0
0
0
0
2
0
0]
[ 0
0
0 23
0
0
0
0
0
0]
[ 0
0
0
0 25
0
0
0
0
0]
[ 0
0
0
0
0 31
0
0
0
1]
[ 0
1
0
0
0
0 38
0
0
0]
[ 0
0
0
0
0
0
0 35
0
0]
[ 0
5
0
0
0
0
0
0 25
1]
[ 0
0
0
0
0
2
0
2
0 31]]
1074
Chapter 25. Semi Supervised Classiﬁcation
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Authors: Clay Woolam <clay@woolam.org>
# License: BSD
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn import datasets
from sklearn.semi_supervised import label_propagation
from sklearn.metrics import classification_report, confusion_matrix
digits = datasets.load_digits()
rng = np.random.RandomState(0)
indices = np.arange(len(digits.data))
rng.shuffle(indices)
X = digits.data[indices[:330]]
y = digits.target[indices[:330]]
images = digits.images[indices[:330]]
n_total_samples = len(y)
n_labeled_points = 10
unlabeled_indices = np.arange(n_total_samples)[n_labeled_points:]
f = plt.figure()
for i in range(5):
y_train = np.copy(y)
y_train[unlabeled_indices] = -1
lp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=5)
lp_model.fit(X, y_train)
predicted_labels = lp_model.transduction_[unlabeled_indices]
true_labels = y[unlabeled_indices]
cm = confusion_matrix(true_labels, predicted_labels,
labels=lp_model.classes_)
print('Iteration %i %s' % (i, 70 * '_'))
print("Label Spreading model: %d labeled & %d unlabeled (%d total)"
% (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))
print(classification_report(true_labels, predicted_labels))
print("Confusion matrix")
print(cm)
# compute the entropies of transduced label distributions
pred_entropies = stats.distributions.entropy(
lp_model.label_distributions_.T)
# select five digit examples that the classifier is most uncertain about
25.4. Label Propagation digits active learning
1075
scikit-learn user guide, Release 0.18.2
uncertainty_index = uncertainty_index = np.argsort(pred_entropies)[-5:]
# keep track of indices that we get labels for
delete_indices = np.array([])
f.text(.05, (1 - (i + 1) * .183),
"model %d\n\nfit with\n%d labels" % ((i + 1), i * 5 + 10), size=10)
for index, image_index in enumerate(uncertainty_index):
image = images[image_index]
sub = f.add_subplot(5, 5, index + 1 + (5 * i))
sub.imshow(image, cmap=plt.cm.gray_r)
sub.set_title('predict: %i\ntrue: %i' % (
lp_model.transduction_[image_index], y[image_index]), size=10)
sub.axis('off')
# labeling 5 points, remote from labeled set
delete_index, = np.where(unlabeled_indices == image_index)
delete_indices = np.concatenate((delete_indices, delete_index))
unlabeled_indices = np.delete(unlabeled_indices, delete_indices)
n_labeled_points += 5
f.suptitle("Active learning with Label Propagation.\nRows show 5 most "
"uncertain labels to learn with the next model.")
plt.subplots_adjust(0.12, 0.03, 0.9, 0.8, 0.2, 0.45)
plt.show()
Total running time of the script: (0 minutes 1.908 seconds)
Download Python source code: plot_label_propagation_digits_active_learning.py
Download IPython notebook: plot_label_propagation_digits_active_learning.ipynb
1076
Chapter 25. Semi Supervised Classiﬁcation
CHAPTER
TWENTYSIX
SUPPORT VECTOR MACHINES
Examples concerning the sklearn.svm module.
26.1 Support Vector Regression (SVR) using linear and non-linear
kernels
Toy example of 1D regression using linear, polynomial and RBF kernels.
print(__doc__)
import numpy as np
from sklearn.svm import SVR
import matplotlib.pyplot as plt
Generate sample data
X = np.sort(5 * np.random.rand(40, 1), axis=0)
y = np.sin(X).ravel()
Add noise to targets
y[::5] += 3 * (0.5 - np.random.rand(8))
Fit regression model
svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)
svr_lin = SVR(kernel='linear', C=1e3)
svr_poly = SVR(kernel='poly', C=1e3, degree=2)
y_rbf = svr_rbf.fit(X, y).predict(X)
y_lin = svr_lin.fit(X, y).predict(X)
y_poly = svr_poly.fit(X, y).predict(X)
look at the results
lw = 2
plt.scatter(X, y, color='darkorange', label='data')
plt.hold('on')
plt.plot(X, y_rbf, color='navy', lw=lw, label='RBF model')
plt.plot(X, y_lin, color='c', lw=lw, label='Linear model')
plt.plot(X, y_poly, color='cornflowerblue', lw=lw, label='Polynomial model')
plt.xlabel('data')
plt.ylabel('target')
plt.title('Support Vector Regression')
1077
scikit-learn user guide, Release 0.18.2
plt.legend()
plt.show()
Total running time of the script: (0 minutes 0.778 seconds)
Download Python source code: plot_svm_regression.py
Download IPython notebook: plot_svm_regression.ipynb
26.2 Non-linear SVM
Perform binary classiﬁcation using non-linear SVC with RBF kernel. The target to predict is a XOR of the inputs.
The color map illustrates the decision function learned by the SVC.
1078
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
xx, yy = np.meshgrid(np.linspace(-3, 3, 500),
np.linspace(-3, 3, 500))
np.random.seed(0)
X = np.random.randn(300, 2)
Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)
# fit the model
clf = svm.NuSVC()
clf.fit(X, Y)
# plot the decision function for each datapoint on the grid
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.imshow(Z, interpolation='nearest',
extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',
origin='lower', cmap=plt.cm.PuOr_r)
contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,
linetypes='--')
plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired)
26.2. Non-linear SVM
1079
scikit-learn user guide, Release 0.18.2
plt.xticks(())
plt.yticks(())
plt.axis([-3, 3, -3, 3])
plt.show()
Total running time of the script: (0 minutes 1.233 seconds)
Download Python source code: plot_svm_nonlinear.py
Download IPython notebook: plot_svm_nonlinear.ipynb
26.3 SVM: Maximum margin separating hyperplane
Plot the maximum margin separating hyperplane within a two-class separable dataset using a Support Vector Machine
classiﬁer with linear kernel.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
# we create 40 separable points
np.random.seed(0)
1080
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20
# fit the model
clf = svm.SVC(kernel='linear')
clf.fit(X, Y)
# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - (clf.intercept_[0]) / w[1]
# plot the parallels to the separating hyperplane that pass through the
# support vectors
b = clf.support_vectors_[0]
yy_down = a * xx + (b[1] - a * b[0])
b = clf.support_vectors_[-1]
yy_up = a * xx + (b[1] - a * b[0])
# plot the line, the points, and the nearest vectors to the plane
plt.plot(xx, yy, 'k-')
plt.plot(xx, yy_down, 'k--')
plt.plot(xx, yy_up, 'k--')
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
s=80, facecolors='none')
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.061 seconds)
Download Python source code: plot_separating_hyperplane.py
Download IPython notebook: plot_separating_hyperplane.ipynb
26.4 SVM: Separating hyperplane for unbalanced classes
Find the optimal separating hyperplane using an SVC for classes that are unbalanced.
We ﬁrst ﬁnd the separating plane with a plain SVC and then plot (dashed) the separating hyperplane with automatically
correction for unbalanced classes.
Note:
This
example
will
also
work
by
replacing
SVC(kernel="linear")
with
SGDClassifier(loss="hinge"). Setting the loss parameter of the SGDClassifier equal to hinge will
yield behaviour such as that of a SVC with a linear kernel.
For example try instead of the SVC:
clf = SGDClassifier(n_iter=100, alpha=0.01)
26.4. SVM: Separating hyperplane for unbalanced classes
1081
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
#from sklearn.linear_model import SGDClassifier
# we create 40 separable points
rng = np.random.RandomState(0)
n_samples_1 = 1000
n_samples_2 = 100
X = np.r_[1.5 * rng.randn(n_samples_1, 2),
0.5 * rng.randn(n_samples_2, 2) + [2, 2]]
y = [0] * (n_samples_1) + [1] * (n_samples_2)
# fit the model and get the separating hyperplane
clf = svm.SVC(kernel='linear', C=1.0)
clf.fit(X, y)
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - clf.intercept_[0] / w[1]
# get the separating hyperplane using weighted classes
1082
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
wclf = svm.SVC(kernel='linear', class_weight={1: 10})
wclf.fit(X, y)
ww = wclf.coef_[0]
wa = -ww[0] / ww[1]
wyy = wa * xx - wclf.intercept_[0] / ww[1]
# plot separating hyperplanes and samples
h0 = plt.plot(xx, yy, 'k-', label='no weights')
h1 = plt.plot(xx, wyy, 'k--', label='with weights')
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.legend()
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.080 seconds)
Download Python source code: plot_separating_hyperplane_unbalanced.py
Download IPython notebook: plot_separating_hyperplane_unbalanced.ipynb
26.5 SVM-Anova: SVM with univariate feature selection
This example shows how to perform univariate feature selection before running a SVC (support vector classiﬁer) to
improve the classiﬁcation scores.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets, feature_selection
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
Import some data to play with
digits = datasets.load_digits()
y = digits.target
# Throw away data, to be in the curse of dimension settings
y = y[:200]
X = digits.data[:200]
n_samples = len(y)
X = X.reshape((n_samples, -1))
# add 200 non-informative features
X = np.hstack((X, 2 * np.random.random((n_samples, 200))))
Create a feature-selection transform and an instance of SVM that we combine together to have an full-blown estimator
transform = feature_selection.SelectPercentile(feature_selection.f_classif)
clf = Pipeline([('anova', transform), ('svc', svm.SVC(C=1.0))])
Plot the cross-validation score as a function of percentile of features
26.5. SVM-Anova: SVM with univariate feature selection
1083
scikit-learn user guide, Release 0.18.2
score_means = list()
score_stds = list()
percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)
for percentile in percentiles:
clf.set_params(anova__percentile=percentile)
# Compute cross-validation score using 1 CPU
this_scores = cross_val_score(clf, X, y, n_jobs=1)
score_means.append(this_scores.mean())
score_stds.append(this_scores.std())
plt.errorbar(percentiles, score_means, np.array(score_stds))
plt.title(
'Performance of the SVM-Anova varying the percentile of features selected')
plt.xlabel('Percentile')
plt.ylabel('Prediction rate')
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.644 seconds)
Download Python source code: plot_svm_anova.py
Download IPython notebook: plot_svm_anova.ipynb
1084
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
26.6 SVM with custom kernel
Simple usage of Support Vector Machines to classify a sample. It will plot the decision surface and the support vectors.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]
# we only take the first two features. We could
# avoid this ugly slicing by using a two-dim dataset
Y = iris.target
def my_kernel(X, Y):
"""
We create a custom kernel:
(2
0)
k(X, Y) = X
(
) Y.T
(0
1)
"""
26.6. SVM with custom kernel
1085
scikit-learn user guide, Release 0.18.2
M = np.array([[2, 0], [0, 1.0]])
return np.dot(np.dot(X, M), Y.T)
h = .02
# step size in the mesh
# we create an instance of SVM and fit out data.
clf = svm.SVC(kernel=my_kernel)
clf.fit(X, Y)
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)
plt.title('3-Class classification using Support Vector Machine with custom'
' kernel')
plt.axis('tight')
plt.show()
Total running time of the script: (0 minutes 0.181 seconds)
Download Python source code: plot_custom_kernel.py
Download IPython notebook: plot_custom_kernel.ipynb
26.7 SVM: Weighted samples
Plot decision function of a weighted dataset, where the size of points is proportional to its weight.
The sample weighting rescales the C parameter, which means that the classiﬁer puts more emphasis on getting these
points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making
the deformation of the decision boundary very visible.
1086
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
def plot_decision_function(classifier, sample_weight, axis, title):
# plot the decision function
xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))
Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
# plot the line, the points, and the nearest vectors to the plane
axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)
axis.scatter(X[:, 0], X[:, 1], c=y, s=100 * sample_weight, alpha=0.9,
cmap=plt.cm.bone)
axis.axis('off')
axis.set_title(title)
# we create 20 points
np.random.seed(0)
X = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]
y = [1] * 10 + [-1] * 10
sample_weight_last_ten = abs(np.random.randn(len(X)))
sample_weight_constant = np.ones(len(X))
# and bigger weights to some outliers
sample_weight_last_ten[15:] *= 5
sample_weight_last_ten[9] *= 15
# for reference, first fit without class weights
# fit the model
clf_weights = svm.SVC()
clf_weights.fit(X, y, sample_weight=sample_weight_last_ten)
26.7. SVM: Weighted samples
1087
scikit-learn user guide, Release 0.18.2
clf_no_weights = svm.SVC()
clf_no_weights.fit(X, y)
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
plot_decision_function(clf_no_weights, sample_weight_constant, axes[0],
"Constant weights")
plot_decision_function(clf_weights, sample_weight_last_ten, axes[1],
"Modified weights")
plt.show()
Total running time of the script: (0 minutes 0.507 seconds)
Download Python source code: plot_weighted_samples.py
Download IPython notebook: plot_weighted_samples.ipynb
26.8 SVM-Kernels
Three different types of SVM-Kernels are displayed below. The polynomial and RBF are especially useful when the
data-points are not linearly separable.
•
•
•
print(__doc__)
# Code source: Gaël Varoquaux
1088
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
# Our dataset and targets
X = np.c_[(.4, -.7),
(-1.5, -1),
(-1.4, -.9),
(-1.3, -1.2),
(-1.1, -.2),
(-1.2, -.4),
(-.5, 1.2),
(-1.5, 2.1),
(1, 1),
# --
(1.3, .8),
(1.2, .5),
(.2, -2),
(.5, -2.4),
(.2, -2.3),
(0, -2.7),
(1.3, 2.1)].T
Y = [0] * 8 + [1] * 8
# figure number
fignum = 1
# fit the model
for kernel in ('linear', 'poly', 'rbf'):
clf = svm.SVC(kernel=kernel, gamma=2)
clf.fit(X, Y)
# plot the line, the points, and the nearest vectors to the plane
plt.figure(fignum, figsize=(4, 3))
plt.clf()
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
facecolors='none', zorder=10)
plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)
plt.axis('tight')
x_min = -3
x_max = 3
y_min = -3
y_max = 3
XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])
# Put the result into a color plot
Z = Z.reshape(XX.shape)
plt.figure(fignum, figsize=(4, 3))
plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)
plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],
levels=[-.5, 0, .5])
26.8. SVM-Kernels
1089
scikit-learn user guide, Release 0.18.2
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())
fignum = fignum + 1
plt.show()
Total running time of the script: (0 minutes 0.321 seconds)
Download Python source code: plot_svm_kernels.py
Download IPython notebook: plot_svm_kernels.ipynb
26.9 SVM Margins Example
The plots below illustrate the effect the parameter C has on the separation line. A large value of C basically tells our
model that we do not have that much faith in our data’s distribution, and will only consider points close to line of
separation.
A small value of C includes more/all the observations, allowing the margins to be calculated using all the data in the
area.
•
•
print(__doc__)
# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
# we create 40 separable points
np.random.seed(0)
1090
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20
# figure number
fignum = 1
# fit the model
for name, penalty in (('unreg', 1), ('reg', 0.05)):
clf = svm.SVC(kernel='linear', C=penalty)
clf.fit(X, Y)
# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - (clf.intercept_[0]) / w[1]
# plot the parallels to the separating hyperplane that pass through the
# support vectors
margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))
yy_down = yy + a * margin
yy_up = yy - a * margin
# plot the line, the points, and the nearest vectors to the plane
plt.figure(fignum, figsize=(4, 3))
plt.clf()
plt.plot(xx, yy, 'k-')
plt.plot(xx, yy_down, 'k--')
plt.plot(xx, yy_up, 'k--')
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
facecolors='none', zorder=10)
plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)
plt.axis('tight')
x_min = -4.8
x_max = 4.2
y_min = -6
y_max = 6
XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])
# Put the result into a color plot
Z = Z.reshape(XX.shape)
plt.figure(fignum, figsize=(4, 3))
plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())
fignum = fignum + 1
plt.show()
26.9. SVM Margins Example
1091
scikit-learn user guide, Release 0.18.2
Total running time of the script: (0 minutes 0.152 seconds)
Download Python source code: plot_svm_margin.py
Download IPython notebook: plot_svm_margin.ipynb
26.10 Plot different SVM classiﬁers in the iris dataset
Comparison of different linear SVM classiﬁers on a 2D projection of the iris dataset. We only consider the ﬁrst 2
features of this dataset:
• Sepal length
• Sepal width
This example shows how to plot the decision surface for four SVM classiﬁers with different kernels.
The linear models LinearSVC() and SVC(kernel='linear') yield slightly different decision boundaries.
This can be a consequence of the following differences:
• LinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss.
• LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVC uses the One-
vs-One multiclass reduction.
Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models
(polynomial or Gaussian RBF) have more ﬂexible non-linear decision boundaries with shapes that depend on the kind
of kernel and its parameters.
Note:
while plotting the decision function of classiﬁers for toy 2D datasets can help get an intuitive understanding
of their respective expressive power, be aware that those intuitions don’t always generalize to more realistic high-
dimensional problems.
1092
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]
# we only take the first two features. We could
# avoid this ugly slicing by using a two-dim dataset
y = iris.target
h = .02
# step size in the mesh
# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0
# SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)
lin_svc = svm.LinearSVC(C=C).fit(X, y)
# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
26.10. Plot different SVM classiﬁers in the iris dataset
1093
scikit-learn user guide, Release 0.18.2
np.arange(y_min, y_max, h))
# title for the plots
titles = ['SVC with linear kernel',
'LinearSVC (linear kernel)',
'SVC with RBF kernel',
'SVC with polynomial (degree 3) kernel']
for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
plt.subplot(2, 2, i + 1)
plt.subplots_adjust(wspace=0.4, hspace=0.4)
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())
plt.title(titles[i])
plt.show()
Total running time of the script: (0 minutes 0.679 seconds)
Download Python source code: plot_iris.py
Download IPython notebook: plot_iris.ipynb
26.11 One-class SVM with non-linear kernel (RBF)
An example using a one-class SVM for novelty detection.
One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new
data as similar or different to the training set.
1094
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager
from sklearn import svm
xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))
# Generate train data
X = 0.3 * np.random.randn(100, 2)
X_train = np.r_[X + 2, X - 2]
# Generate some regular novel observations
X = 0.3 * np.random.randn(20, 2)
X_test = np.r_[X + 2, X - 2]
# Generate some abnormal novel observations
X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
# fit the model
clf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.1)
clf.fit(X_train)
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)
y_pred_outliers = clf.predict(X_outliers)
n_error_train = y_pred_train[y_pred_train == -1].size
n_error_test = y_pred_test[y_pred_test == -1].size
n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size
26.11. One-class SVM with non-linear kernel (RBF)
1095
scikit-learn user guide, Release 0.18.2
# plot the line, the points, and the nearest vectors to the plane
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.title("Novelty Detection")
plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)
a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')
s = 40
b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s)
b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s)
c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s)
plt.axis('tight')
plt.xlim((-5, 5))
plt.ylim((-5, 5))
plt.legend([a.collections[0], b1, b2, c],
["learned frontier", "training observations",
"new regular observations", "new abnormal observations"],
loc="upper left",
prop=matplotlib.font_manager.FontProperties(size=11))
plt.xlabel(
"error train: %d/200 ; errors novel regular: %d/40 ; "
"errors novel abnormal: %d/40"
% (n_error_train, n_error_test, n_error_outliers))
plt.show()
Total running time of the script: (0 minutes 0.291 seconds)
Download Python source code: plot_oneclass.py
Download IPython notebook: plot_oneclass.ipynb
26.12 Scaling the regularization parameter for SVCs
The following example illustrates the effect of scaling the regularization parameter when using Support Vector Ma-
chines for classiﬁcation. For SVC classiﬁcation, we are interested in a risk minimization for the equation:
𝐶
∑︁
𝑖=1,𝑛
ℒ(𝑓(𝑥𝑖), 𝑦𝑖) + Ω(𝑤)
where
• 𝐶is used to set the amount of regularization
• ℒis a loss function of our samples and our model parameters.
• Ωis a penalty function of our model parameters
If we consider the loss function to be the individual error per sample, then the data-ﬁt term, or the sum of the error for
each sample, will increase as we add more samples. The penalization term, however, will not increase.
When using, for example, cross validation, to set the amount of regularization with C, there will be a different amount
of samples between the main problem and the smaller problems within the folds of the cross validation.
Since our loss function is dependent on the amount of samples, the latter will inﬂuence the selected value of C. The
question that arises is How do we optimally adjust C to account for the different amount of training samples?
1096
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
The ﬁgures below are used to illustrate the effect of scaling our C to compensate for the change in the number of
samples, in the case of using an l1 penalty, as well as the l2 penalty.
26.12.1 l1-penalty case
In the l1 case, theory says that prediction consistency (i.e. that under given hypothesis, the estimator learned predicts
as well as a model knowing the true distribution) is not possible because of the bias of the l1. It does say, however,
that model consistency, in terms of ﬁnding the right set of non-zero parameters as well as their signs, can be achieved
by scaling C1.
26.12.2 l2-penalty case
The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the
number of samples grow.
26.12.3 Simulations
The two ﬁgures below plot the values of C on the x-axis and the corresponding cross-validation scores on the y-axis,
for several different fractions of a generated data-set.
In the l1 penalty case, the cross-validation-error correlates best with the test-error, when scaling our C with the number
of samples, n, which can be seen in the ﬁrst ﬁgure.
For the l2 penalty case, the best result comes from the case where C is not scaled.
Note:
Two separate datasets are used for the two different plots. The reason behind this is the l1 case works better on
sparse data, while l2 is better suited to the non-sparse case.
•
26.12. Scaling the regularization parameter for SVCs
1097
scikit-learn user guide, Release 0.18.2
•
print(__doc__)
# Author: Andreas Mueller <amueller@ais.uni-bonn.de>
#
Jaques Grobler <jaques.grobler@inria.fr>
# License: BSD 3 clause
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import LinearSVC
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GridSearchCV
from sklearn.utils import check_random_state
from sklearn import datasets
rnd = check_random_state(1)
# set up dataset
n_samples = 100
n_features = 300
# l1 data (only 5 informative features)
X_1, y_1 = datasets.make_classification(n_samples=n_samples,
n_features=n_features, n_informative=5,
random_state=1)
# l2 data: non sparse, but less features
y_2 = np.sign(.5 - rnd.rand(n_samples))
X_2 = rnd.randn(n_samples, n_features // 5) + y_2[:, np.newaxis]
X_2 += 5 * rnd.randn(n_samples, n_features // 5)
clf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False,
tol=1e-3),
np.logspace(-2.3, -1.3, 10), X_1, y_1),
1098
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
(LinearSVC(penalty='l2', loss='squared_hinge', dual=True,
tol=1e-4),
np.logspace(-4.5, -2, 10), X_2, y_2)]
colors = ['navy', 'cyan', 'darkorange']
lw = 2
for fignum, (clf, cs, X, y) in enumerate(clf_sets):
# set up the plot for each regressor
plt.figure(fignum, figsize=(9, 10))
for k, train_size in enumerate(np.linspace(0.3, 0.7, 3)[::-1]):
param_grid = dict(C=cs)
# To get nice curve, we need a large number of iterations to
# reduce the variance
grid = GridSearchCV(clf, refit=False, param_grid=param_grid,
cv=ShuffleSplit(train_size=train_size,
n_splits=250, random_state=1))
grid.fit(X, y)
scores = grid.cv_results_['mean_test_score']
scales = [(1, 'No scaling'),
((n_samples * train_size), '1/n_samples'),
]
for subplotnum, (scaler, name) in enumerate(scales):
plt.subplot(2, 1, subplotnum + 1)
plt.xlabel('C')
plt.ylabel('CV Score')
grid_cs = cs * float(scaler)
# scale the C's
plt.semilogx(grid_cs, scores, label="fraction %.2f" %
train_size, color=colors[k], lw=lw)
plt.title('scaling=%s, penalty=%s, loss=%s' %
(name, clf.penalty, clf.loss))
plt.legend(loc="best")
plt.show()
Total running time of the script: (1 minutes 2.825 seconds)
Download Python source code: plot_svm_scale_c.py
Download IPython notebook: plot_svm_scale_c.ipynb
26.13 RBF SVM parameters
This example illustrates the effect of the parameters gamma and C of the Radial Basis Function (RBF) kernel SVM.
Intuitively, the gamma parameter deﬁnes how far the inﬂuence of a single training example reaches, with low values
meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of
inﬂuence of samples selected by the model as support vectors.
The C parameter trades off misclassiﬁcation of training examples against simplicity of the decision surface. A low C
makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the
model freedom to select more samples as support vectors.
The ﬁrst plot is a visualization of the decision function for a variety of parameter values on a simpliﬁed classiﬁcation
26.13. RBF SVM parameters
1099
scikit-learn user guide, Release 0.18.2
problem involving only 2 input features and 2 possible target classes (binary classiﬁcation). Note that this kind of plot
is not possible to do for problems with more features or target classes.
The second plot is a heatmap of the classiﬁer’s cross-validation accuracy as a function of C and gamma. For this
example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from 10−3 to 103
is usually sufﬁcient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a
subsequent search.
Note that the heat map plot has a special colorbar with a midpoint value close to the score values of the best performing
models so as to make it easy to tell them appart in the blink of an eye.
The behavior of the model is very sensitive to the gamma parameter. If gamma is too large, the radius of the area of
inﬂuence of the support vectors only includes the support vector itself and no amount of regularization with C will be
able to prevent overﬁtting.
When gamma is very small, the model is too constrained and cannot capture the complexity or “shape” of the data.
The region of inﬂuence of any selected support vector would include the whole training set. The resulting model will
behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two
classes.
For intermediate values, we can see on the second plot that good models can be found on a diagonal of C and gamma.
Smooth models (lower gamma values) can be made more complex by selecting a larger number of support vectors
(larger C values) hence the diagonal of good performing models.
Finally one can also observe that for some intermediate values of gamma we get equally performing models when C
becomes very large: it is not necessary to regularize by limiting the number of support vectors. The radius of the RBF
kernel alone acts as a good structural regularizer. In practice though it might still be interesting to limit the number of
support vectors with a lower value of C so as to favor models that use less memory and that are faster to predict.
We should also note that small differences in scores results from the random splits of the cross-validation procedure.
Those spurious variations can be smoothed out by increasing the number of CV iterations n_splits at the expense
of compute time. Increasing the value number of C_range and gamma_range steps will increase the resolution of
the hyper-parameter heat map.
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import GridSearchCV
# Utility function to move the midpoint of a colormap to be around
# the values of interest.
class MidpointNormalize(Normalize):
def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
self.midpoint = midpoint
Normalize.__init__(self, vmin, vmax, clip)
def __call__(self, value, clip=None):
x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
return np.ma.masked_array(np.interp(value, x, y))
1100
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
Load and prepare data set
dataset for grid search
iris = load_iris()
X = iris.data
y = iris.target
# Dataset for decision function visualization: we only keep the first two
# features in X and sub-sample the dataset to keep only 2 classes and
# make it a binary classification problem.
X_2d = X[:, :2]
X_2d = X_2d[y > 0]
y_2d = y[y > 0]
y_2d -= 1
# It is usually a good idea to scale the data for SVM training.
# We are cheating a bit in this example in scaling all of the data,
# instead of fitting the transformation on the training set and
# just applying it on the test set.
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_2d = scaler.fit_transform(X_2d)
Train classiﬁers
For an initial search, a logarithmic grid with basis 10 is often helpful. Using a basis of 2, a ﬁner tuning can be achieved
but at a much higher cost.
C_range = np.logspace(-2, 10, 13)
gamma_range = np.logspace(-9, 3, 13)
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
grid.fit(X, y)
print("The best parameters are %s with a score of %0.2f"
% (grid.best_params_, grid.best_score_))
# Now we need to fit a classifier for all parameters in the 2d version
# (we use a smaller set of parameters here because it takes a while to train)
C_2d_range = [1e-2, 1, 1e2]
gamma_2d_range = [1e-1, 1, 1e1]
classifiers = []
for C in C_2d_range:
for gamma in gamma_2d_range:
clf = SVC(C=C, gamma=gamma)
clf.fit(X_2d, y_2d)
classifiers.append((C, gamma, clf))
Out:
The best parameters are {'C': 1.0, 'gamma': 0.10000000000000001} with a score of 0.97
visualization
draw visualization of parameter effects
26.13. RBF SVM parameters
1101
scikit-learn user guide, Release 0.18.2
plt.figure(figsize=(8, 6))
xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))
for (k, (C, gamma, clf)) in enumerate(classifiers):
# evaluate decision function in a grid
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
# visualize decision function for these parameters
plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)
plt.title("gamma=10^%d, C=10^%d" % (np.log10(gamma), np.log10(C)),
size='medium')
# visualize parameter's effect on decision function
plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r)
plt.xticks(())
plt.yticks(())
plt.axis('tight')
scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),
len(gamma_range))
# Draw heatmap of the validation accuracy as a function of gamma and C
#
# The score are encoded as colors with the hot colormap which varies from dark
# red to bright yellow. As the most interesting scores are all located in the
# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so
# as to make it easier to visualize the small variations of score values in the
# interesting range while not brutally collapsing all the low score values to
# the same color.
plt.figure(figsize=(8, 6))
plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)
plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,
norm=MidpointNormalize(vmin=0.2, midpoint=0.92))
plt.xlabel('gamma')
plt.ylabel('C')
plt.colorbar()
plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)
plt.yticks(np.arange(len(C_range)), C_range)
plt.title('Validation accuracy')
plt.show()
•
1102
Chapter 26. Support Vector Machines
scikit-learn user guide, Release 0.18.2
•
Total running time of the script: (0 minutes 7.772 seconds)
Download Python source code: plot_rbf_parameters.py
Download IPython notebook: plot_rbf_parameters.ipynb
26.13. RBF SVM parameters
1103
scikit-learn user guide, Release 0.18.2
1104
Chapter 26. Support Vector Machines
CHAPTER
TWENTYSEVEN
WORKING WITH TEXT DOCUMENTS
Examples concerning the sklearn.feature_extraction.text module.
27.1 FeatureHasher and DictVectorizer Comparison
Compares FeatureHasher and DictVectorizer by using both to vectorize text documents.
The example demonstrates syntax and speed only; it doesn’t actually do anything useful with the extracted vectors.
See the example scripts {document_classiﬁcation_20newsgroups,clustering}.py for actual learning on text documents.
A discrepancy between the number of terms reported for DictVectorizer and for FeatureHasher is to be expected due
to hash collisions.
# Author: Lars Buitinck
# License: BSD 3 clause
from __future__ import print_function
from collections import defaultdict
import re
import sys
from time import time
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction import DictVectorizer, FeatureHasher
def n_nonzero_columns(X):
"""Returns the number of non-zero columns in a CSR matrix X."""
return len(np.unique(X.nonzero()[1]))
def tokens(doc):
"""Extract tokens from doc.
This uses a simple regex to break strings into tokens. For a more
principled approach, see CountVectorizer or TfidfVectorizer.
"""
return (tok.lower() for tok in re.findall(r"\w+", doc))
def token_freqs(doc):
"""Extract a dict mapping tokens from doc to their frequencies."""
1105
scikit-learn user guide, Release 0.18.2
freq = defaultdict(int)
for tok in tokens(doc):
freq[tok] += 1
return freq
categories = [
'alt.atheism',
'comp.graphics',
'comp.sys.ibm.pc.hardware',
'misc.forsale',
'rec.autos',
'sci.space',
'talk.religion.misc',
]
# Uncomment the following line to use a larger set (11k+ documents)
#categories = None
print(__doc__)
print("Usage: %s [n_features_for_hashing]" % sys.argv[0])
print("
The default number of features is 2**18.")
print()
try:
n_features = int(sys.argv[1])
except IndexError:
n_features = 2 ** 18
except ValueError:
print("not a valid number of features: %r" % sys.argv[1])
sys.exit(1)
print("Loading 20 newsgroups training data")
raw_data = fetch_20newsgroups(subset='train', categories=categories).data
data_size_mb = sum(len(s.encode('utf-8')) for s in raw_data) / 1e6
print("%d documents - %0.3fMB" % (len(raw_data), data_size_mb))
print()
print("DictVectorizer")
t0 = time()
vectorizer = DictVectorizer()
vectorizer.fit_transform(token_freqs(d) for d in raw_data)
duration = time() - t0
print("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))
print("Found %d unique terms" % len(vectorizer.get_feature_names()))
print()
print("FeatureHasher on frequency dicts")
t0 = time()
hasher = FeatureHasher(n_features=n_features)
X = hasher.transform(token_freqs(d) for d in raw_data)
duration = time() - t0
print("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))
print("Found %d unique terms" % n_nonzero_columns(X))
print()
print("FeatureHasher on raw tokens")
t0 = time()
1106
Chapter 27. Working with text documents
scikit-learn user guide, Release 0.18.2
hasher = FeatureHasher(n_features=n_features, input_type="string")
X = hasher.transform(tokens(d) for d in raw_data)
duration = time() - t0
print("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))
print("Found %d unique terms" % n_nonzero_columns(X))
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: hashing_vs_dict_vectorizer.py
Download IPython notebook: hashing_vs_dict_vectorizer.ipynb
27.2 Classiﬁcation of text documents: using a MLComp dataset
This is an example showing how the scikit-learn can be used to classify documents by topics using a bag-of-words
approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.
The dataset used in this example is the 20 newsgroups dataset and should be downloaded from the http://mlcomp.org
(free registration required):
http://mlcomp.org/datasets/379
Once downloaded unzip the archive somewhere on your ﬁlesystem. For instance in:
% mkdir -p ~/data/mlcomp
% cd
~/data/mlcomp
% unzip /path/to/dataset-379-20news-18828_XXXXX.zip
You should get a folder ~/data/mlcomp/379 with a ﬁle named metadata and subfolders raw, train and
test holding the text documents organized by newsgroups.
Then set the MLCOMP_DATASETS_HOME environment variable pointing to the root folder holding the uncompressed
archive:
% export MLCOMP_DATASETS_HOME="~/data/mlcomp"
Then you are ready to run this example using your favorite python shell:
% ipython examples/mlcomp_sparse_document_classification.py
# Author: Olivier Grisel <olivier.grisel@ensta.org>
# License: BSD 3 clause
from __future__ import print_function
from time import time
import sys
import os
import numpy as np
import scipy.sparse as sp
import matplotlib.pyplot as plt
from sklearn.datasets import load_mlcomp
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
27.2. Classiﬁcation of text documents: using a MLComp dataset
1107
scikit-learn user guide, Release 0.18.2
from sklearn.naive_bayes import MultinomialNB
print(__doc__)
if 'MLCOMP_DATASETS_HOME' not in os.environ:
print("MLCOMP_DATASETS_HOME not set; please follow the above instructions")
sys.exit(0)
# Load the training set
print("Loading 20 newsgroups training set... ")
news_train = load_mlcomp('20news-18828', 'train')
print(news_train.DESCR)
print("%d documents" % len(news_train.filenames))
print("%d categories" % len(news_train.target_names))
print("Extracting features from the dataset using a sparse vectorizer")
t0 = time()
vectorizer = TfidfVectorizer(encoding='latin1')
X_train = vectorizer.fit_transform((open(f).read()
for f in news_train.filenames))
print("done in %fs" % (time() - t0))
print("n_samples: %d, n_features: %d" % X_train.shape)
assert sp.issparse(X_train)
y_train = news_train.target
print("Loading 20 newsgroups test set... ")
news_test = load_mlcomp('20news-18828', 'test')
t0 = time()
print("done in %fs" % (time() - t0))
print("Predicting the labels of the test set...")
print("%d documents" % len(news_test.filenames))
print("%d categories" % len(news_test.target_names))
print("Extracting features from the dataset using the same vectorizer")
t0 = time()
X_test = vectorizer.transform((open(f).read() for f in news_test.filenames))
y_test = news_test.target
print("done in %fs" % (time() - t0))
print("n_samples: %d, n_features: %d" % X_test.shape)
Benchmark classiﬁers
def benchmark(clf_class, params, name):
print("parameters:", params)
t0 = time()
clf = clf_class(**params).fit(X_train, y_train)
print("done in %fs" % (time() - t0))
if hasattr(clf, 'coef_'):
print("Percentage of non zeros coef: %f"
% (np.mean(clf.coef_ != 0) * 100))
print("Predicting the outcomes of the testing set")
t0 = time()
pred = clf.predict(X_test)
print("done in %fs" % (time() - t0))
1108
Chapter 27. Working with text documents
scikit-learn user guide, Release 0.18.2
print("Classification report on test set for classifier:")
print(clf)
print()
print(classification_report(y_test, pred,
target_names=news_test.target_names))
cm = confusion_matrix(y_test, pred)
print("Confusion matrix:")
print(cm)
# Show confusion matrix
plt.matshow(cm)
plt.title('Confusion matrix of the %s classifier' % name)
plt.colorbar()
print("Testbenching a linear classifier...")
parameters = {
'loss': 'hinge',
'penalty': 'l2',
'n_iter': 50,
'alpha': 0.00001,
'fit_intercept': True,
}
benchmark(SGDClassifier, parameters, 'SGD')
print("Testbenching a MultinomialNB classifier...")
parameters = {'alpha': 0.01}
benchmark(MultinomialNB, parameters, 'MultinomialNB')
plt.show()
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: mlcomp_sparse_document_classification.py
Download IPython notebook: mlcomp_sparse_document_classification.ipynb
27.3 Clustering text documents using k-means
This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words
approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.
Two feature extraction methods can be used in this example:
• TﬁdfVectorizer uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices
and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted
using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.
• HashingVectorizer hashes word occurrences to a ﬁxed dimensional space, possibly with collisions. The word
count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which
seems to be important for k-means to work in high dimensional space.
HashingVectorizer does not provide IDF weighting as this is a stateless model (the ﬁt method does nothing).
When IDF weighting is needed it can be added by pipelining its output to a TﬁdfTransformer instance.
27.3. Clustering text documents using k-means
1109
scikit-learn user guide, Release 0.18.2
Two algorithms are demoed: ordinary k-means and its more scalable cousin minibatch k-means.
Additionally, latent semantic analysis can also be used to reduce dimensionality and discover latent patterns in the
data.
It can be noted that k-means (and minibatch k-means) are very sensitive to feature scaling and that in this case the IDF
weighting helps improve the quality of the clustering by quite a lot as measured against the “ground truth” provided
by the class label assignments of the 20 newsgroups dataset.
This improvement is not visible in the Silhouette Coefﬁcient which is small for both as this measure seem to suffer
from the phenomenon called “Concentration of Measure” or “Curse of Dimensionality” for high dimensional datasets
such as text data. Other measures such as V-measure and Adjusted Rand Index are information theoretic based eval-
uation scores: as they are only based on cluster assignments rather than distances, hence not affected by the curse of
dimensionality.
Note: as k-means is optimizing a non-convex objective function, it will likely end up in a local optimum. Several runs
with independent random init might be necessary to get a good convergence.
# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
#
Lars Buitinck
# License: BSD 3 clause
from __future__ import print_function
from sklearn.datasets import fetch_20newsgroups
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn import metrics
from sklearn.cluster import KMeans, MiniBatchKMeans
import logging
from optparse import OptionParser
import sys
from time import time
import numpy as np
# Display progress logs on stdout
logging.basicConfig(level=logging.INFO,
format='%(asctime)s %(levelname)s %(message)s')
# parse commandline arguments
op = OptionParser()
op.add_option("--lsa",
dest="n_components", type="int",
help="Preprocess documents with latent semantic analysis.")
op.add_option("--no-minibatch",
action="store_false", dest="minibatch", default=True,
help="Use ordinary k-means algorithm (in batch mode).")
op.add_option("--no-idf",
action="store_false", dest="use_idf", default=True,
help="Disable Inverse Document Frequency feature weighting.")
op.add_option("--use-hashing",
1110
Chapter 27. Working with text documents
scikit-learn user guide, Release 0.18.2
action="store_true", default=False,
help="Use a hashing feature vectorizer")
op.add_option("--n-features", type=int, default=10000,
help="Maximum number of features (dimensions)"
" to extract from text.")
op.add_option("--verbose",
action="store_true", dest="verbose", default=False,
help="Print progress reports inside k-means algorithm.")
print(__doc__)
op.print_help()
(opts, args) = op.parse_args()
if len(args) > 0:
op.error("this script takes no arguments.")
sys.exit(1)
Load some categories from the training set
categories = [
'alt.atheism',
'talk.religion.misc',
'comp.graphics',
'sci.space',
]
# Uncomment the following to do the analysis on all the categories
#categories = None
print("Loading 20 newsgroups dataset for categories:")
print(categories)
dataset = fetch_20newsgroups(subset='all', categories=categories,
shuffle=True, random_state=42)
print("%d documents" % len(dataset.data))
print("%d categories" % len(dataset.target_names))
print()
labels = dataset.target
true_k = np.unique(labels).shape[0]
print("Extracting features from the training dataset using a sparse vectorizer")
t0 = time()
if opts.use_hashing:
if opts.use_idf:
# Perform an IDF normalization on the output of HashingVectorizer
hasher = HashingVectorizer(n_features=opts.n_features,
stop_words='english', non_negative=True,
norm=None, binary=False)
vectorizer = make_pipeline(hasher, TfidfTransformer())
else:
vectorizer = HashingVectorizer(n_features=opts.n_features,
stop_words='english',
non_negative=False, norm='l2',
binary=False)
else:
vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,
min_df=2, stop_words='english',
27.3. Clustering text documents using k-means
1111
scikit-learn user guide, Release 0.18.2
use_idf=opts.use_idf)
X = vectorizer.fit_transform(dataset.data)
print("done in %fs" % (time() - t0))
print("n_samples: %d, n_features: %d" % X.shape)
print()
if opts.n_components:
print("Performing dimensionality reduction using LSA")
t0 = time()
# Vectorizer results are normalized, which makes KMeans behave as
# spherical k-means for better results. Since LSA/SVD results are
# not normalized, we have to redo the normalization.
svd = TruncatedSVD(opts.n_components)
normalizer = Normalizer(copy=False)
lsa = make_pipeline(svd, normalizer)
X = lsa.fit_transform(X)
print("done in %fs" % (time() - t0))
explained_variance = svd.explained_variance_ratio_.sum()
print("Explained variance of the SVD step: {}%".format(
int(explained_variance * 100)))
print()
Do the actual clustering
if opts.minibatch:
km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,
init_size=1000, batch_size=1000, verbose=opts.verbose)
else:
km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,
verbose=opts.verbose)
print("Clustering sparse data with %s" % km)
t0 = time()
km.fit(X)
print("done in %0.3fs" % (time() - t0))
print()
print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels, km.labels_))
print("Completeness: %0.3f" % metrics.completeness_score(labels, km.labels_))
print("V-measure: %0.3f" % metrics.v_measure_score(labels, km.labels_))
print("Adjusted Rand-Index: %.3f"
% metrics.adjusted_rand_score(labels, km.labels_))
print("Silhouette Coefficient: %0.3f"
% metrics.silhouette_score(X, km.labels_, sample_size=1000))
print()
if not opts.use_hashing:
print("Top terms per cluster:")
if opts.n_components:
original_space_centroids = svd.inverse_transform(km.cluster_centers_)
1112
Chapter 27. Working with text documents
scikit-learn user guide, Release 0.18.2
order_centroids = original_space_centroids.argsort()[:, ::-1]
else:
order_centroids = km.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()
for i in range(true_k):
print("Cluster %d:" % i, end='')
for ind in order_centroids[i, :10]:
print(' %s' % terms[ind], end='')
print()
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: document_clustering.py
Download IPython notebook: document_clustering.ipynb
27.4 Classiﬁcation of text documents using sparse features
This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words ap-
proach. This example uses a scipy.sparse matrix to store the features and demonstrates various classiﬁers that can
efﬁciently handle sparse matrices.
The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.
The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classiﬁer.
# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
#
Olivier Grisel <olivier.grisel@ensta.org>
#
Mathieu Blondel <mathieu@mblondel.org>
#
Lars Buitinck
# License: BSD 3 clause
from __future__ import print_function
import logging
import numpy as np
from optparse import OptionParser
import sys
from time import time
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import RidgeClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.extmath import density
27.4. Classiﬁcation of text documents using sparse features
1113
scikit-learn user guide, Release 0.18.2
from sklearn import metrics
# Display progress logs on stdout
logging.basicConfig(level=logging.INFO,
format='%(asctime)s %(levelname)s %(message)s')
# parse commandline arguments
op = OptionParser()
op.add_option("--report",
action="store_true", dest="print_report",
help="Print a detailed classification report.")
op.add_option("--chi2_select",
action="store", type="int", dest="select_chi2",
help="Select some number of features using a chi-squared test")
op.add_option("--confusion_matrix",
action="store_true", dest="print_cm",
help="Print the confusion matrix.")
op.add_option("--top10",
action="store_true", dest="print_top10",
help="Print ten most discriminative terms per class"
" for every classifier.")
op.add_option("--all_categories",
action="store_true", dest="all_categories",
help="Whether to use all categories or not.")
op.add_option("--use_hashing",
action="store_true",
help="Use a hashing vectorizer.")
op.add_option("--n_features",
action="store", type=int, default=2 ** 16,
help="n_features when using the hashing vectorizer.")
op.add_option("--filtered",
action="store_true",
help="Remove newsgroup information that is easily overfit: "
"headers, signatures, and quoting.")
(opts, args) = op.parse_args()
if len(args) > 0:
op.error("this script takes no arguments.")
sys.exit(1)
print(__doc__)
op.print_help()
print()
Load some categories from the training set
if opts.all_categories:
categories = None
else:
categories = [
'alt.atheism',
'talk.religion.misc',
'comp.graphics',
'sci.space',
]
1114
Chapter 27. Working with text documents
scikit-learn user guide, Release 0.18.2
if opts.filtered:
remove = ('headers', 'footers', 'quotes')
else:
remove = ()
print("Loading 20 newsgroups dataset for categories:")
print(categories if categories else "all")
data_train = fetch_20newsgroups(subset='train', categories=categories,
shuffle=True, random_state=42,
remove=remove)
data_test = fetch_20newsgroups(subset='test', categories=categories,
shuffle=True, random_state=42,
remove=remove)
print('data loaded')
# order of labels in `target_names` can be different from `categories`
target_names = data_train.target_names
def size_mb(docs):
return sum(len(s.encode('utf-8')) for s in docs) / 1e6
data_train_size_mb = size_mb(data_train.data)
data_test_size_mb = size_mb(data_test.data)
print("%d documents - %0.3fMB (training set)" % (
len(data_train.data), data_train_size_mb))
print("%d documents - %0.3fMB (test set)" % (
len(data_test.data), data_test_size_mb))
print("%d categories" % len(categories))
print()
# split a training set and a test set
y_train, y_test = data_train.target, data_test.target
print("Extracting features from the training data using a sparse vectorizer")
t0 = time()
if opts.use_hashing:
vectorizer = HashingVectorizer(stop_words='english', non_negative=True,
n_features=opts.n_features)
X_train = vectorizer.transform(data_train.data)
else:
vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
stop_words='english')
X_train = vectorizer.fit_transform(data_train.data)
duration = time() - t0
print("done in %fs at %0.3fMB/s" % (duration, data_train_size_mb / duration))
print("n_samples: %d, n_features: %d" % X_train.shape)
print()
print("Extracting features from the test data using the same vectorizer")
t0 = time()
X_test = vectorizer.transform(data_test.data)
duration = time() - t0
print("done in %fs at %0.3fMB/s" % (duration, data_test_size_mb / duration))
print("n_samples: %d, n_features: %d" % X_test.shape)
27.4. Classiﬁcation of text documents using sparse features
1115
scikit-learn user guide, Release 0.18.2
print()
# mapping from integer feature name to original token string
if opts.use_hashing:
feature_names = None
else:
feature_names = vectorizer.get_feature_names()
if opts.select_chi2:
print("Extracting %d best features by a chi-squared test" %
opts.select_chi2)
t0 = time()
ch2 = SelectKBest(chi2, k=opts.select_chi2)
X_train = ch2.fit_transform(X_train, y_train)
X_test = ch2.transform(X_test)
if feature_names:
# keep selected feature names
feature_names = [feature_names[i] for i
in ch2.get_support(indices=True)]
print("done in %fs" % (time() - t0))
print()
if feature_names:
feature_names = np.asarray(feature_names)
def trim(s):
"""Trim string to fit on terminal (assuming 80-column display)"""
return s if len(s) <= 80 else s[:77] + "..."
Benchmark classiﬁers
def benchmark(clf):
print('_' * 80)
print("Training: ")
print(clf)
t0 = time()
clf.fit(X_train, y_train)
train_time = time() - t0
print("train time: %0.3fs" % train_time)
t0 = time()
pred = clf.predict(X_test)
test_time = time() - t0
print("test time:
%0.3fs" % test_time)
score = metrics.accuracy_score(y_test, pred)
print("accuracy:
%0.3f" % score)
if hasattr(clf, 'coef_'):
print("dimensionality: %d" % clf.coef_.shape[1])
print("density: %f" % density(clf.coef_))
if opts.print_top10 and feature_names is not None:
print("top 10 keywords per class:")
for i, label in enumerate(target_names):
top10 = np.argsort(clf.coef_[i])[-10:]
print(trim("%s: %s" % (label, " ".join(feature_names[top10]))))
1116
Chapter 27. Working with text documents
scikit-learn user guide, Release 0.18.2
print()
if opts.print_report:
print("classification report:")
print(metrics.classification_report(y_test, pred,
target_names=target_names))
if opts.print_cm:
print("confusion matrix:")
print(metrics.confusion_matrix(y_test, pred))
print()
clf_descr = str(clf).split('(')[0]
return clf_descr, score, train_time, test_time
results = []
for clf, name in (
(RidgeClassifier(tol=1e-2, solver="lsqr"), "Ridge Classifier"),
(Perceptron(n_iter=50), "Perceptron"),
(PassiveAggressiveClassifier(n_iter=50), "Passive-Aggressive"),
(KNeighborsClassifier(n_neighbors=10), "kNN"),
(RandomForestClassifier(n_estimators=100), "Random forest")):
print('=' * 80)
print(name)
results.append(benchmark(clf))
for penalty in ["l2", "l1"]:
print('=' * 80)
print("%s penalty" % penalty.upper())
# Train Liblinear model
results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
dual=False, tol=1e-3)))
# Train SGD model
results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
penalty=penalty)))
# Train SGD with Elastic Net penalty
print('=' * 80)
print("Elastic-Net penalty")
results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
penalty="elasticnet")))
# Train NearestCentroid without threshold
print('=' * 80)
print("NearestCentroid (aka Rocchio classifier)")
results.append(benchmark(NearestCentroid()))
# Train sparse Naive Bayes classifiers
print('=' * 80)
print("Naive Bayes")
results.append(benchmark(MultinomialNB(alpha=.01)))
results.append(benchmark(BernoulliNB(alpha=.01)))
print('=' * 80)
print("LinearSVC with L1-based feature selection")
# The smaller C, the stronger the regularization.
27.4. Classiﬁcation of text documents using sparse features
1117
scikit-learn user guide, Release 0.18.2
# The more regularization, the more sparsity.
results.append(benchmark(Pipeline([
('feature_selection', LinearSVC(penalty="l1", dual=False, tol=1e-3)),
('classification', LinearSVC())
])))
# make some plots
indices = np.arange(len(results))
results = [[x[i] for x in results] for i in range(4)]
clf_names, score, training_time, test_time = results
training_time = np.array(training_time) / np.max(training_time)
test_time = np.array(test_time) / np.max(test_time)
plt.figure(figsize=(12, 8))
plt.title("Score")
plt.barh(indices, score, .2, label="score", color='navy')
plt.barh(indices + .3, training_time, .2, label="training time",
color='c')
plt.barh(indices + .6, test_time, .2, label="test time", color='darkorange')
plt.yticks(())
plt.legend(loc='best')
plt.subplots_adjust(left=.25)
plt.subplots_adjust(top=.95)
plt.subplots_adjust(bottom=.05)
for i, c in zip(indices, clf_names):
plt.text(-.3, i, c)
plt.show()
Total running time of the script: (0 minutes 0.000 seconds)
Download Python source code: document_classification_20newsgroups.py
Download IPython notebook: document_classification_20newsgroups.ipynb
1118
Chapter 27. Working with text documents
CHAPTER
TWENTYEIGHT
DECISION TREES
Examples concerning the sklearn.tree module.
28.1 Decision Tree Regression
A 1D regression with decision tree.
The decision trees is used to ﬁt a sine curve with addition noisy observation. As a result, it learns local linear regres-
sions approximating the sine curve.
We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision
trees learn too ﬁne details of the training data and learn from the noise, i.e. they overﬁt.
1119
scikit-learn user guide, Release 0.18.2
print(__doc__)
# Import the necessary modules and libraries
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
# Create a random dataset
rng = np.random.RandomState(1)
X = np.sort(5 * rng.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(16))
# Fit regression model
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
regr_1.fit(X, y)
regr_2.fit(X, y)
# Predict
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)
# Plot the results
plt.figure()
1120
Chapter 28. Decision Trees
scikit-learn user guide, Release 0.18.2
plt.scatter(X, y, c="darkorange", label="data")
plt.plot(X_test, y_1, color="cornflowerblue", label="max_depth=2", linewidth=2)
plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("Decision Tree Regression")
plt.legend()
plt.show()
Total running time of the script: (0 minutes 0.139 seconds)
Download Python source code: plot_tree_regression.py
Download IPython notebook: plot_tree_regression.ipynb
28.2 Multi-output Decision Tree Regression
An example to illustrate multi-output regression with decision tree.
The decision trees is used to predict simultaneously the noisy x and y observations of a circle given a single underlying
feature. As a result, it learns local linear regressions approximating the circle.
We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision
trees learn too ﬁne details of the training data and learn from the noise, i.e. they overﬁt.
28.2. Multi-output Decision Tree Regression
1121
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
# Create a random dataset
rng = np.random.RandomState(1)
X = np.sort(200 * rng.rand(100, 1) - 100, axis=0)
y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T
y[::5, :] += (0.5 - rng.rand(20, 2))
# Fit regression model
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
regr_3 = DecisionTreeRegressor(max_depth=8)
regr_1.fit(X, y)
regr_2.fit(X, y)
regr_3.fit(X, y)
# Predict
X_test = np.arange(-100.0, 100.0, 0.01)[:, np.newaxis]
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)
y_3 = regr_3.predict(X_test)
# Plot the results
plt.figure()
s = 50
plt.scatter(y[:, 0], y[:, 1], c="navy", s=s, label="data")
plt.scatter(y_1[:, 0], y_1[:, 1], c="cornflowerblue", s=s, label="max_depth=2")
plt.scatter(y_2[:, 0], y_2[:, 1], c="c", s=s, label="max_depth=5")
plt.scatter(y_3[:, 0], y_3[:, 1], c="orange", s=s, label="max_depth=8")
plt.xlim([-6, 6])
plt.ylim([-6, 6])
plt.xlabel("target 1")
plt.ylabel("target 2")
plt.title("Multi-output Decision Tree Regression")
plt.legend()
plt.show()
Total running time of the script: (0 minutes 0.227 seconds)
Download Python source code: plot_tree_regression_multioutput.py
Download IPython notebook: plot_tree_regression_multioutput.ipynb
28.3 Plot the decision surface of a decision tree on the iris dataset
Plot the decision surface of a decision tree trained on pairs of features of the iris dataset.
See decision tree for more information on the estimator.
For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding
rules inferred from the training samples.
1122
Chapter 28. Decision Trees
scikit-learn user guide, Release 0.18.2
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
# Parameters
n_classes = 3
plot_colors = "bry"
plot_step = 0.02
# Load data
iris = load_iris()
for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],
[1, 2], [1, 3], [2, 3]]):
# We only take the two corresponding features
X = iris.data[:, pair]
y = iris.target
# Train
clf = DecisionTreeClassifier().fit(X, y)
# Plot the decision boundary
28.3. Plot the decision surface of a decision tree on the iris dataset
1123
scikit-learn user guide, Release 0.18.2
plt.subplot(2, 3, pairidx + 1)
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
np.arange(y_min, y_max, plot_step))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
plt.xlabel(iris.feature_names[pair[0]])
plt.ylabel(iris.feature_names[pair[1]])
plt.axis("tight")
# Plot the training points
for i, color in zip(range(n_classes), plot_colors):
idx = np.where(y == i)
plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],
cmap=plt.cm.Paired)
plt.axis("tight")
plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend()
plt.show()
Total running time of the script: (0 minutes 0.514 seconds)
Download Python source code: plot_iris.py
Download IPython notebook: plot_iris.ipynb
28.4 Understanding the decision tree structure
The decision tree structure can be analysed to gain further insight on the relation between the features and the target
to predict. In this example, we show how to retrieve:
• the binary tree structure;
• the depth of each node and whether or not it’s a leaf;
• the nodes that were reached by a sample using the decision_path method;
• the leaf that was reached by a sample using the apply method;
• the rules that were used to predict a sample;
• the decision path shared by a group of samples.
Out:
The binary tree structure has 5 nodes and has the following tree structure:
node=0 test node: go to node 1 if X[:, 3] <= 0.800000011921s else to node 2.
node=1 leaf node.
node=2 test node: go to node 3 if X[:, 2] <= 4.94999980927s else to node 4.
node=3 leaf node.
node=4 leaf node.
1124
Chapter 28. Decision Trees
scikit-learn user guide, Release 0.18.2
Rules used to predict sample 0:
decision id node 4 : (X[0, -2] (= 1.5) > -2.0)
The following samples [0, 1] share the node [0 2] in the tree
It is 40.0 % of all nodes.
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
estimator = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)
estimator.fit(X_train, y_train)
# The decision estimator has an attribute called tree_
which stores the entire
# tree structure and allows access to low level attributes. The binary tree
# tree_ is represented as a number of parallel arrays. The i-th element of each
# array holds information about the node `i`. Node 0 is the tree's root. NOTE:
# Some of the arrays only apply to either leaves or split nodes, resp. In this
# case the values of nodes of the other type are arbitrary!
#
# Among those arrays, we have:
#
- left_child, id of the left child of the node
#
- right_child, id of the right child of the node
#
- feature, feature used for splitting the node
#
- threshold, threshold value at the node
#
# Using those arrays, we can parse the tree structure:
n_nodes = estimator.tree_.node_count
children_left = estimator.tree_.children_left
children_right = estimator.tree_.children_right
feature = estimator.tree_.feature
threshold = estimator.tree_.threshold
# The tree structure can be traversed to compute various properties such
# as the depth of each node and whether or not it is a leaf.
node_depth = np.zeros(shape=n_nodes, dtype=np.int64)
is_leaves = np.zeros(shape=n_nodes, dtype=bool)
stack = [(0, -1)]
# seed is the root node id and its parent depth
while len(stack) > 0:
node_id, parent_depth = stack.pop()
node_depth[node_id] = parent_depth + 1
# If we have a test node
28.4. Understanding the decision tree structure
1125
scikit-learn user guide, Release 0.18.2
if (children_left[node_id] != children_right[node_id]):
stack.append((children_left[node_id], parent_depth + 1))
stack.append((children_right[node_id], parent_depth + 1))
else:
is_leaves[node_id] = True
print("The binary tree structure has %s nodes and has "
"the following tree structure:"
% n_nodes)
for i in range(n_nodes):
if is_leaves[i]:
print("%snode=%s leaf node." % (node_depth[i] * "\t", i))
else:
print("%snode=%s test node: go to node %s if X[:, %s] <= %ss else to "
"node %s."
% (node_depth[i] * "\t",
i,
children_left[i],
feature[i],
threshold[i],
children_right[i],
))
print()
# First let's retrieve the decision path of each sample. The decision_path
# method allows to retrieve the node indicator functions. A non zero element of
# indicator matrix at the position (i, j) indicates that the sample i goes
# through the node j.
node_indicator = estimator.decision_path(X_test)
# Similarly, we can also have the leaves ids reached by each sample.
leave_id = estimator.apply(X_test)
# Now, it's possible to get the tests that were used to predict a sample or
# a group of samples. First, let's make it for the sample.
sample_id = 0
node_index = node_indicator.indices[node_indicator.indptr[sample_id]:
node_indicator.indptr[sample_id + 1]]
print('Rules used to predict sample %s: ' % sample_id)
for node_id in node_index:
if leave_id[sample_id] != node_id:
continue
if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):
threshold_sign = "<="
else:
threshold_sign = ">"
print("decision id node %s : (X[%s, %s] (= %s) %s %s)"
% (node_id,
sample_id,
feature[node_id],
X_test[i, feature[node_id]],
threshold_sign,
1126
Chapter 28. Decision Trees
scikit-learn user guide, Release 0.18.2
threshold[node_id]))
# For a group of samples, we have the following common node.
sample_ids = [0, 1]
common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==
len(sample_ids))
common_node_id = np.arange(n_nodes)[common_nodes]
print("\nThe following samples %s share the node %s in the tree"
% (sample_ids, common_node_id))
print("It is %s %% of all nodes." % (100 * len(common_node_id) / n_nodes,))
Total running time of the script: (0 minutes 0.005 seconds)
Download Python source code: plot_unveil_tree_structure.py
Download IPython notebook: plot_unveil_tree_structure.ipynb
28.4. Understanding the decision tree structure
1127
scikit-learn user guide, Release 0.18.2
1128
Chapter 28. Decision Trees
CHAPTER
TWENTYNINE
API REFERENCE
This is the class and function reference of scikit-learn. Please refer to the full user guide for further details, as the class
and function raw speciﬁcations may not be enough to give full guidelines on their uses.
29.1 sklearn.base: Base classes and utility functions
Base classes for all estimators.
29.1.1 Base classes
base.BaseEstimator
Base class for all estimators in scikit-learn
base.ClassifierMixin
Mixin class for all classiﬁers in scikit-learn.
base.ClusterMixin
Mixin class for all cluster estimators in scikit-learn.
base.RegressorMixin
Mixin class for all regression estimators in scikit-learn.
base.TransformerMixin
Mixin class for all transformers in scikit-learn.
sklearn.base.BaseEstimator
class sklearn.base.BaseEstimator
Base class for all estimators in scikit-learn
Notes
All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit
keyword arguments (no *args or **kwargs).
Methods
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
get_params(deep=True)
1129
scikit-learn user guide, Release 0.18.2
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.base.BaseEstimator
• Feature Union with Heterogeneous Data Sources
sklearn.base.ClassiﬁerMixin
class sklearn.base.ClassifierMixin
Mixin class for all classiﬁers in scikit-learn.
Methods
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
1130
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.base.ClusterMixin
class sklearn.base.ClusterMixin
Mixin class for all cluster estimators in scikit-learn.
Methods
fit_predict(X[, y])
Performs clustering on X and returns cluster labels.
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
fit_predict(X, y=None)
Performs clustering on X and returns cluster labels.
ParametersX : ndarray, shape (n_samples, n_features)
Input data.
Returnsy : ndarray, shape (n_samples,)
cluster labels
sklearn.base.RegressorMixin
class sklearn.base.RegressorMixin
Mixin class for all regression estimators in scikit-learn.
Methods
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
29.1. sklearn.base: Base classes and utility functions
1131
scikit-learn user guide, Release 0.18.2
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
sklearn.base.TransformerMixin
class sklearn.base.TransformerMixin
Mixin class for all transformers in scikit-learn.
Methods
fit_transform(X[, y])
Fit to data, then transform it.
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
Examples using sklearn.base.TransformerMixin
• Feature Union with Heterogeneous Data Sources
29.1.2 Functions
base.clone(estimator[, safe])
Constructs a new estimator with the same parameters.
sklearn.base.clone
sklearn.base.clone(estimator, safe=True)
Constructs a new estimator with the same parameters.
Clone does a deep copy of the model in an estimator without actually copying attached data. It yields a new
estimator with the same parameters that has not been ﬁt on any data.
Parametersestimator: estimator object, or list, tuple or set of objects :
The estimator or group of estimators to be cloned
safe: boolean, optional :
1132
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
If safe is false, clone will fall back to a deepcopy on objects that are not estimators.
29.2 sklearn.cluster: Clustering
The sklearn.cluster module gathers popular unsupervised clustering algorithms.
User guide: See the Clustering section for further details.
29.2.1 Classes
cluster.AffinityPropagation([damping, ...])
Perform Afﬁnity Propagation Clustering of data.
cluster.AgglomerativeClustering([...])
Agglomerative Clustering
cluster.Birch([threshold, branching_factor, ...])
Implements the Birch clustering algorithm.
cluster.DBSCAN([eps, min_samples, metric, ...])
Perform DBSCAN clustering from vector array or distance
matrix.
cluster.FeatureAgglomeration([n_clusters, ...])
Agglomerate features.
cluster.KMeans([n_clusters, init, n_init, ...])
K-Means clustering
cluster.MiniBatchKMeans([n_clusters, init, ...])
Mini-Batch K-Means clustering
cluster.MeanShift([bandwidth, seeds, ...])
Mean shift clustering using a ﬂat kernel.
cluster.SpectralClustering([n_clusters, ...])
Apply clustering to a projection to the normalized lapla-
cian.
sklearn.cluster.AfﬁnityPropagation
class sklearn.cluster.AffinityPropagation(damping=0.5,
max_iter=200,
conver-
gence_iter=15,
copy=True,
preference=None,
afﬁnity=’euclidean’, verbose=False)
Perform Afﬁnity Propagation Clustering of data.
Read more in the User Guide.
Parametersdamping : ﬂoat, optional, default: 0.5
Damping factor between 0.5 and 1.
convergence_iter : int, optional, default: 15
Number of iterations with no change in the number of estimated clusters that stops the
convergence.
max_iter : int, optional, default: 200
Maximum number of iterations.
copy : boolean, optional, default: True
Make a copy of input data.
preference : array-like, shape (n_samples,) or ﬂoat, optional
Preferences for each point - points with larger values of preferences are more likely to
be chosen as exemplars. The number of exemplars, ie of clusters, is inﬂuenced by the
input preferences value. If the preferences are not passed as arguments, they will be set
to the median of the input similarities.
afﬁnity : string, optional, default=‘‘euclidean‘‘
29.2. sklearn.cluster: Clustering
1133
scikit-learn user guide, Release 0.18.2
Which afﬁnity to use. At the moment precomputed and euclidean are supported.
euclidean uses the negative squared euclidean distance between points.
verbose : boolean, optional, default: False
Whether to be verbose.
Attributescluster_centers_indices_ : array, shape (n_clusters,)
Indices of cluster centers
cluster_centers_ : array, shape (n_clusters, n_features)
Cluster centers (if afﬁnity != precomputed).
labels_ : array, shape (n_samples,)
Labels of each point
afﬁnity_matrix_ : array, shape (n_samples, n_samples)
Stores the afﬁnity matrix used in fit.
n_iter_ : int
Number of iterations taken to converge.
Notes
See examples/cluster/plot_afﬁnity_propagation.py for an example.
The algorithmic complexity of afﬁnity propagation is quadratic in the number of points.
References
Brendan J. Frey and Delbert Dueck, “Clustering by Passing Messages Between Data Points”, Science Feb. 2007
Methods
fit(X[, y])
Create afﬁnity matrix from negative euclidean dis-
tances, then apply afﬁnity propagation clustering.
fit_predict(X[, y])
Performs clustering on X and returns cluster labels.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict the closest cluster each sample in X belongs to.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, afﬁn-
ity=’euclidean’, verbose=False)
fit(X, y=None)
Create afﬁnity matrix from negative euclidean distances, then apply afﬁnity propagation clustering.
ParametersX: array-like, shape (n_samples, n_features) or (n_samples, n_samples) :
Data matrix or, if afﬁnity is precomputed, matrix of similarities / afﬁnities.
fit_predict(X, y=None)
Performs clustering on X and returns cluster labels.
1134
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ParametersX : ndarray, shape (n_samples, n_features)
Input data.
Returnsy : ndarray, shape (n_samples,)
cluster labels
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict the closest cluster each sample in X belongs to.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
New data to predict.
Returnslabels : array, shape (n_samples,)
Index of the cluster each sample belongs to.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.cluster.AffinityPropagation
• Demo of afﬁnity propagation clustering algorithm
• Comparing different clustering algorithms on toy datasets
sklearn.cluster.AgglomerativeClustering
class sklearn.cluster.AgglomerativeClustering(n_clusters=2,
afﬁnity=’euclidean’,
mem-
ory=Memory(cachedir=None),
connec-
tivity=None,
compute_full_tree=’auto’,
linkage=’ward’,
pooling_func=<function
mean>)
Agglomerative Clustering
Recursively merges the pair of clusters that minimally increases a given linkage distance.
Read more in the User Guide.
Parametersn_clusters : int, default=2
The number of clusters to ﬁnd.
29.2. sklearn.cluster: Clustering
1135
scikit-learn user guide, Release 0.18.2
connectivity : array-like or callable, optional
Connectivity matrix. Deﬁnes for each sample the neighboring samples following a
given structure of the data. This can be a connectivity matrix itself or a callable that
transforms the data into a connectivity matrix, such as derived from kneighbors_graph.
Default is None, i.e, the hierarchical clustering algorithm is unstructured.
afﬁnity : string or callable, default: “euclidean”
Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “co-
sine”, or ‘precomputed’. If linkage is “ward”, only “euclidean” is accepted.
memory : Instance of joblib.Memory or string (optional)
Used to cache the output of the computation of the tree. By default, no caching is done.
If a string is given, it is the path to the caching directory.
compute_full_tree : bool or ‘auto’ (optional)
Stop early the construction of the tree at n_clusters. This is useful to decrease compu-
tation time if the number of clusters is not small compared to the number of samples.
This option is useful only when specifying a connectivity matrix. Note also that when
varying the number of clusters and using caching, it may be advantageous to compute
the full tree.
linkage : {“ward”, “complete”, “average”}, optional, default: “ward”
Which linkage criterion to use. The linkage criterion determines which distance to use
between sets of observation. The algorithm will merge the pairs of cluster that minimize
this criterion.
•ward minimizes the variance of the clusters being merged.
•average uses the average of the distances of each observation of the two sets.
•complete or maximum linkage uses the maximum distances between all observations
of the two sets.
pooling_func : callable, default=np.mean
This combines the values of agglomerated features into a single value, and should accept
an array of shape [M, N] and the keyword argument axis=1, and reduce it to an array
of size [M].
Attributeslabels_ : array [n_samples]
cluster labels for each point
n_leaves_ : int
Number of leaves in the hierarchical tree.
n_components_ : int
The estimated number of connected components in the graph.
children_ : array-like, shape (n_nodes-1, 2)
The children of each non-leaf node. Values less than n_samples correspond to leaves
of the tree which are the original samples. A node i greater than or equal to n_samples
is a non-leaf node and has children children_[i - n_samples]. Alternatively at the i-th
iteration, children[i][0] and children[i][1] are merged to form node n_samples + i
1136
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Methods
fit(X[, y])
Fit the hierarchical clustering on the data
fit_predict(X[, y])
Performs clustering on X and returns cluster labels.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_clusters=2, afﬁnity=’euclidean’, memory=Memory(cachedir=None), connectivity=None,
compute_full_tree=’auto’, linkage=’ward’, pooling_func=<function mean>)
fit(X, y=None)
Fit the hierarchical clustering on the data
ParametersX : array-like, shape = [n_samples, n_features]
The samples a.k.a. observations.
Returnsself :
fit_predict(X, y=None)
Performs clustering on X and returns cluster labels.
ParametersX : ndarray, shape (n_samples, n_features)
Input data.
Returnsy : ndarray, shape (n_samples,)
cluster labels
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.cluster.AgglomerativeClustering
• Agglomerative clustering with and without structure
• Agglomerative clustering with different metrics
• Comparing different clustering algorithms on toy datasets
• Various Agglomerative Clustering on a 2D embedding of digits
• A demo of structured Ward hierarchical clustering on a raccoon face image
29.2. sklearn.cluster: Clustering
1137
scikit-learn user guide, Release 0.18.2
• Hierarchical clustering: structured vs unstructured ward
sklearn.cluster.Birch
class sklearn.cluster.Birch(threshold=0.5,
branching_factor=50,
n_clusters=3,
com-
pute_labels=True, copy=True)
Implements the Birch clustering algorithm.
Every new sample is inserted into the root of the Clustering Feature Tree. It is then clubbed together with the
subcluster that has the centroid closest to the new sample. This is done recursively till it ends up at the subcluster
of the leaf of the tree has the closest centroid.
Read more in the User Guide.
Parametersthreshold : ﬂoat, default 0.5
The radius of the subcluster obtained by merging a new sample and the closest subclus-
ter should be lesser than the threshold. Otherwise a new subcluster is started.
branching_factor : int, default 50
Maximum number of CF subclusters in each node. If a new samples enters such that the
number of subclusters exceed the branching_factor then the node has to be split. The
corresponding parent also has to be split and if the number of subclusters in the parent
is greater than the branching factor, then it has to be split recursively.
n_clusters : int, instance of sklearn.cluster model, default 3
Number of clusters after the ﬁnal clustering step, which treats the subclusters from the
leaves as new samples. If None, this ﬁnal clustering step is not performed and the sub-
clusters are returned as they are. If a model is provided, the model is ﬁt treating the
subclusters as new samples and the initial data is mapped to the label of the closest sub-
cluster. If an int is provided, the model ﬁt is AgglomerativeClustering with n_clusters
set to the int.
compute_labels : bool, default True
Whether or not to compute labels for each ﬁt.
copy : bool, default True
Whether or not to make a copy of the given data. If set to False, the initial data will be
overwritten.
Attributesroot_ : _CFNode
Root of the CFTree.
dummy_leaf_ : _CFNode
Start pointer to all the leaves.
subcluster_centers_ : ndarray,
Centroids of all subclusters read directly from the leaves.
subcluster_labels_ : ndarray,
Labels assigned to the centroids of the subclusters after they are clustered globally.
labels_ : ndarray, shape (n_samples,)
Array of labels assigned to the input data. if partial_ﬁt is used instead of ﬁt, they are
assigned to the last batch of data.
1138
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
•Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efﬁcient data clustering method for large
databases. http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf
•Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm https://code.google.com/
archive/p/jbirch
Examples
>>> from sklearn.cluster import Birch
>>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]
>>> brc = Birch(branching_factor=50, n_clusters=None, threshold=0.5,
... compute_labels=True)
>>> brc.fit(X)
Birch(branching_factor=50, compute_labels=True, copy=True, n_clusters=None,
threshold=0.5)
>>> brc.predict(X)
array([0, 0, 0, 1, 1, 1])
Methods
fit(X[, y])
Build a CF Tree for the input data.
fit_predict(X[, y])
Performs clustering on X and returns cluster labels.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
partial_fit([X, y])
Online learning.
predict(X)
Predict data using the centroids_ of subclusters.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Transform X into subcluster centroids dimension.
__init__(threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True)
fit(X, y=None)
Build a CF Tree for the input data.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Input data.
fit_predict(X, y=None)
Performs clustering on X and returns cluster labels.
ParametersX : ndarray, shape (n_samples, n_features)
Input data.
Returnsy : ndarray, shape (n_samples,)
cluster labels
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
29.2. sklearn.cluster: Clustering
1139
scikit-learn user guide, Release 0.18.2
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X=None, y=None)
Online learning. Prevents rebuilding of CFTree from scratch.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features), None
Input data. If X is not provided, only the global clustering step is done.
predict(X)
Predict data using the centroids_ of subclusters.
Avoid computation of the row norms of X.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Input data.
Returnslabels: ndarray, shape(n_samples) :
Labelled data.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Transform X into subcluster centroids dimension.
Each dimension represents the distance from the sample point to each cluster centroid.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Input data.
ReturnsX_trans : {array-like, sparse matrix}, shape (n_samples, n_clusters)
Transformed data.
1140
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.cluster.Birch
• Compare BIRCH and MiniBatchKMeans
• Comparing different clustering algorithms on toy datasets
sklearn.cluster.DBSCAN
class sklearn.cluster.DBSCAN(eps=0.5,
min_samples=5,
metric=’euclidean’,
algorithm=’auto’,
leaf_size=30, p=None, n_jobs=1)
Perform DBSCAN clustering from vector array or distance matrix.
DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density
and expands clusters from them. Good for data which contains clusters of similar density.
Read more in the User Guide.
Parameterseps : ﬂoat, optional
The maximum distance between two samples for them to be considered as in the same
neighborhood.
min_samples : int, optional
The number of samples (or total weight) in a neighborhood for a point to be considered
as a core point. This includes the point itself.
metric : string, or callable
The metric to use when calculating distance between instances in a feature array.
If metric is a string or callable, it must be one of the options allowed by met-
rics.pairwise.calculate_distance for its metric parameter. If metric is “precomputed”,
X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in
which case only “nonzero” elements may be considered neighbors for DBSCAN.
New in version 0.17: metric precomputed to accept precomputed sparse matrix.
algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
The algorithm to be used by the NearestNeighbors module to compute pointwise dis-
tances and ﬁnd nearest neighbors. See NearestNeighbors module documentation for
details.
leaf_size : int, optional (default = 30)
Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction
and query, as well as the memory required to store the tree. The optimal value depends
on the nature of the problem.
p : ﬂoat, optional
The power of the Minkowski metric to be used to calculate distance between points.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run. If -1, then the number of jobs is set to the number
of CPU cores.
Attributescore_sample_indices_ : array, shape = [n_core_samples]
Indices of core samples.
components_ : array, shape = [n_core_samples, n_features]
29.2. sklearn.cluster: Clustering
1141
scikit-learn user guide, Release 0.18.2
Copy of each core sample found by training.
labels_ : array, shape = [n_samples]
Cluster labels for each point in the dataset given to ﬁt(). Noisy samples are given the
label -1.
Notes
See examples/cluster/plot_dbscan.py for an example.
This implementation bulk-computes all neighborhood queries, which increases the memory complexity to
O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n).
Sparse neighborhoods can be precomputed using NearestNeighbors.radius_neighbors_graph
with mode='distance'.
References
Ester, M., H. P. Kriegel, J. Sander, and X. Xu, “A Density-Based Algorithm for Discovering Clusters in Large
Spatial Databases with Noise”. In: Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
Methods
fit(X[, y, sample_weight])
Perform DBSCAN clustering from features or distance
matrix.
fit_predict(X[, y, sample_weight])
Performs clustering on X and returns cluster labels.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(eps=0.5, min_samples=5, metric=’euclidean’, algorithm=’auto’, leaf_size=30, p=None,
n_jobs=1)
fit(X, y=None, sample_weight=None)
Perform DBSCAN clustering from features or distance matrix.
ParametersX : array or sparse (CSR) matrix of shape (n_samples, n_features), or array of shape
(n_samples, n_samples)
A feature array, or array of distances between samples if metric='precomputed'.
sample_weight : array, shape (n_samples,), optional
Weight of each sample, such that a sample with a weight of at least min_samples
is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor
from being core. Note that weights are absolute, and default to 1.
fit_predict(X, y=None, sample_weight=None)
Performs clustering on X and returns cluster labels.
ParametersX : array or sparse (CSR) matrix of shape (n_samples, n_features), or array of shape
(n_samples, n_samples)
A feature array, or array of distances between samples if metric='precomputed'.
1142
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sample_weight : array, shape (n_samples,), optional
Weight of each sample, such that a sample with a weight of at least min_samples
is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor
from being core. Note that weights are absolute, and default to 1.
Returnsy : ndarray, shape (n_samples,)
cluster labels
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.cluster.DBSCAN
• Comparing different clustering algorithms on toy datasets
• Demo of DBSCAN clustering algorithm
sklearn.cluster.FeatureAgglomeration
class sklearn.cluster.FeatureAgglomeration(n_clusters=2,
afﬁnity=’euclidean’,
mem-
ory=Memory(cachedir=None),
connectiv-
ity=None,
compute_full_tree=’auto’,
link-
age=’ward’, pooling_func=<function mean>)
Agglomerate features.
Similar to AgglomerativeClustering, but recursively merges features instead of samples.
Read more in the User Guide.
Parametersn_clusters : int, default 2
The number of clusters to ﬁnd.
connectivity : array-like or callable, optional
Connectivity matrix. Deﬁnes for each feature the neighboring features following a given
structure of the data. This can be a connectivity matrix itself or a callable that transforms
the data into a connectivity matrix, such as derived from kneighbors_graph. Default is
None, i.e, the hierarchical clustering algorithm is unstructured.
afﬁnity : string or callable, default “euclidean”
29.2. sklearn.cluster: Clustering
1143
scikit-learn user guide, Release 0.18.2
Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “co-
sine”, or ‘precomputed’. If linkage is “ward”, only “euclidean” is accepted.
memory : Instance of joblib.Memory or string, optional
Used to cache the output of the computation of the tree. By default, no caching is done.
If a string is given, it is the path to the caching directory.
compute_full_tree : bool or ‘auto’, optional, default “auto”
Stop early the construction of the tree at n_clusters. This is useful to decrease compu-
tation time if the number of clusters is not small compared to the number of features.
This option is useful only when specifying a connectivity matrix. Note also that when
varying the number of clusters and using caching, it may be advantageous to compute
the full tree.
linkage : {“ward”, “complete”, “average”}, optional, default “ward”
Which linkage criterion to use. The linkage criterion determines which distance to use
between sets of features. The algorithm will merge the pairs of cluster that minimize
this criterion.
•ward minimizes the variance of the clusters being merged.
•average uses the average of the distances of each feature of the two sets.
•complete or maximum linkage uses the maximum distances between all features of
the two sets.
pooling_func : callable, default np.mean
This combines the values of agglomerated features into a single value, and should accept
an array of shape [M, N] and the keyword argument axis=1, and reduce it to an array of
size [M].
Attributeslabels_ : array-like, (n_features,)
cluster labels for each feature.
n_leaves_ : int
Number of leaves in the hierarchical tree.
n_components_ : int
The estimated number of connected components in the graph.
children_ : array-like, shape (n_nodes-1, 2)
The children of each non-leaf node. Values less than n_features correspond to leaves
of the tree which are the original samples. A node i greater than or equal to n_features
is a non-leaf node and has children children_[i - n_features]. Alternatively at the i-th
iteration, children[i][0] and children[i][1] are merged to form node n_features + i
Methods
fit(X[, y])
Fit the hierarchical clustering on the data
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
inverse_transform(Xred)
Inverse the transformation.
Continued on next page
1144
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.13 – continued from previous page
pooling_func(a[, axis, dtype, out, keepdims])
Compute the arithmetic mean along the speciﬁed axis.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform a new matrix using the built clustering
__init__(n_clusters=2, afﬁnity=’euclidean’, memory=Memory(cachedir=None), connectivity=None,
compute_full_tree=’auto’, linkage=’ward’, pooling_func=<function mean>)
fit(X, y=None, **params)
Fit the hierarchical clustering on the data
ParametersX : array-like, shape = [n_samples, n_features]
The data
Returnsself :
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(Xred)
Inverse the transformation. Return a vector of size nb_features with the values of Xred assigned to each
group of features
ParametersXred : array-like, shape=[n_samples, n_clusters] or [n_clusters,]
The values to be assigned to each cluster of samples
ReturnsX : array, shape=[n_samples, n_features] or [n_features]
A vector of size n_samples with the values of Xred assigned to each of the cluster of
samples.
pooling_func(a,
axis=None,
dtype=None,
out=None,
keepdims=<class
numpy._globals._NoValue>)
Compute the arithmetic mean along the speciﬁed axis.
Returns the average of the array elements. The average is taken over the ﬂattened array by default, other-
wise over the speciﬁed axis. ﬂoat64 intermediate and return values are used for integer inputs.
29.2. sklearn.cluster: Clustering
1145
scikit-learn user guide, Release 0.18.2
Parametersa : array_like
Array containing numbers whose mean is desired. If a is not an array, a conversion is
attempted.
axis : None or int or tuple of ints, optional
Axis or axes along which the means are computed. The default is to compute the mean
of the ﬂattened array.
If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis
or all the axes as before.
dtype : data-type, optional
Type to use in computing the mean. For integer inputs, the default is ﬂoat64; for ﬂoating
point inputs, it is the same as the input dtype.
out : ndarray, optional
Alternate output array in which to place the result. The default is None; if provided, it
must have the same shape as the expected output, but the type will be cast if necessary.
See doc.ufuncs for details.
keepdims : bool, optional
If this is set to True, the axes which are reduced are left in the result as dimensions with
size one. With this option, the result will broadcast correctly against the original arr.
If the default value is passed, then keepdims will not be passed through to the mean
method of sub-classes of ndarray, however any non-default value will be. If the sub-
classes sum method does not implement keepdims any exceptions will be raised.
Returnsm : ndarray, see dtype parameter above
If out=None, returns a new array containing the mean values, otherwise a reference to
the output array is returned.
See also:
averageWeighted average
std, var, nanmean, nanstd, nanvar
Notes
The arithmetic mean is the sum of the elements along the axis divided by the number of elements.
Note that for ﬂoating-point input, the mean is computed using the same precision the input has. Depending
on the input data, this can cause the results to be inaccurate, especially for ﬂoat32 (see example below).
Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue.
Examples
>>> a = np.array([[1, 2], [3, 4]])
>>> np.mean(a)
2.5
>>> np.mean(a, axis=0)
array([ 2.,
3.])
1146
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> np.mean(a, axis=1)
array([ 1.5,
3.5])
In single precision, mean can be inaccurate:
>>> a = np.zeros((2, 512*512), dtype=np.float32)
>>> a[0, :] = 1.0
>>> a[1, :] = 0.1
>>> np.mean(a)
0.546875
Computing the mean in ﬂoat64 is more accurate:
>>> np.mean(a, dtype=np.float64)
0.55000000074505806
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transform a new matrix using the built clustering
ParametersX : array-like, shape = [n_samples, n_features] or [n_features]
A M by N array of M observations in N dimensions or a length M array of M one-
dimensional observations.
ReturnsY : array, shape = [n_samples, n_clusters] or [n_clusters]
The pooled values for each feature cluster.
Examples using sklearn.cluster.FeatureAgglomeration
• Feature agglomeration
• Feature agglomeration vs. univariate selection
sklearn.cluster.KMeans
class sklearn.cluster.KMeans(n_clusters=8,
init=’k-means++’,
n_init=10,
max_iter=300,
tol=0.0001,
precompute_distances=’auto’,
verbose=0,
ran-
dom_state=None, copy_x=True, n_jobs=1, algorithm=’auto’)
K-Means clustering
Read more in the User Guide.
Parametersn_clusters : int, optional, default: 8
The number of clusters to form as well as the number of centroids to generate.
max_iter : int, default: 300
Maximum number of iterations of the k-means algorithm for a single run.
29.2. sklearn.cluster: Clustering
1147
scikit-learn user guide, Release 0.18.2
n_init : int, default: 10
Number of time the k-means algorithm will be run with different centroid seeds. The
ﬁnal results will be the best output of n_init consecutive runs in terms of inertia.
init : {‘k-means++’, ‘random’ or an ndarray}
Method for initialization, defaults to ‘k-means++’:
‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to
speed up convergence. See section Notes in k_init for more details.
‘random’: choose k observations (rows) at random from data for the initial centroids.
If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial
centers.
algorithm : “auto”, “full” or “elkan”, default=”auto”
K-means algorithm to use. The classical EM-style algorithm is “full”. The “elkan”
variation is more efﬁcient by using the triangle inequality, but currently doesn’t support
sparse data. “auto” chooses “elkan” for dense data and “full” for sparse data.
precompute_distances : {‘auto’, True, False}
Precompute distances (faster but takes more memory).
‘auto’ : do not precompute distances if n_samples * n_clusters > 12 million. This
corresponds to about 100MB overhead per job using double precision.
True : always precompute distances
False : never precompute distances
tol : ﬂoat, default: 1e-4
Relative tolerance with regards to inertia to declare convergence
n_jobs : int
The number of jobs to use for the computation. This works by computing each of the
n_init runs in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
random_state : integer or numpy.RandomState, optional
The generator used to initialize the centers. If an integer is given, it ﬁxes the seed.
Defaults to the global numpy random number generator.
verbose : int, default 0
Verbosity mode.
copy_x : boolean, default True
When pre-computing distances it is more numerically accurate to center the data ﬁrst.
If copy_x is True, then the original data is not modiﬁed. If False, the original data is
modiﬁed, and put back before the function returns, but small numerical differences may
be introduced by subtracting and then adding the data mean.
Attributescluster_centers_ : array, [n_clusters, n_features]
Coordinates of cluster centers
1148
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
labels_ : :
Labels of each point
inertia_ : ﬂoat
Sum of distances of samples to their closest cluster center.
See also:
MiniBatchKMeansAlternative online implementation that does incremental updates of the centers positions
using mini-batches. For large scale learning (say n_samples > 10k) MiniBatchKMeans is probably much
faster than the default batch implementation.
Notes
The k-means problem is solved using Lloyd’s algorithm.
The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.
The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S.
Vassilvitskii, ‘How slow is the k-means method?’ SoCG2006)
In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in
local minima. That’s why it can be useful to restart it several times.
Examples
>>> from sklearn.cluster import KMeans
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...
[4, 2], [4, 4], [4, 0]])
>>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
>>> kmeans.labels_
array([0, 0, 0, 1, 1, 1], dtype=int32)
>>> kmeans.predict([[0, 0], [4, 4]])
array([0, 1], dtype=int32)
>>> kmeans.cluster_centers_
array([[ 1.,
2.],
[ 4.,
2.]])
Methods
fit(X[, y])
Compute k-means clustering.
fit_predict(X[, y])
Compute cluster centers and predict cluster index for
each sample.
fit_transform(X[, y])
Compute clustering and transform X to cluster-distance
space.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict the closest cluster each sample in X belongs to.
score(X[, y])
Opposite of the value of X on the K-means objective.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Transform X to a cluster-distance space.
29.2. sklearn.cluster: Clustering
1149
scikit-learn user guide, Release 0.18.2
__init__(n_clusters=8,
init=’k-means++’,
n_init=10,
max_iter=300,
tol=0.0001,
precom-
pute_distances=’auto’, verbose=0, random_state=None, copy_x=True, n_jobs=1, al-
gorithm=’auto’)
fit(X, y=None)
Compute k-means clustering.
ParametersX : array-like or sparse matrix, shape=(n_samples, n_features)
Training instances to cluster.
fit_predict(X, y=None)
Compute cluster centers and predict cluster index for each sample.
Convenience method; equivalent to calling ﬁt(X) followed by predict(X).
fit_transform(X, y=None)
Compute clustering and transform X to cluster-distance space.
Equivalent to ﬁt(X).transform(X), but more efﬁciently implemented.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict the closest cluster each sample in X belongs to.
In the vector quantization literature, cluster_centers_ is called the code book and each value returned by
predict is the index of the closest code in the code book.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
New data to predict.
Returnslabels : array, shape [n_samples,]
Index of the cluster each sample belongs to.
score(X, y=None)
Opposite of the value of X on the K-means objective.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
New data.
Returnsscore : ﬂoat
Opposite of the value of X on the K-means objective.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
1150
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
transform(X, y=None)
Transform X to a cluster-distance space.
In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the
array returned by transform will typically be dense.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
New data to transform.
ReturnsX_new : array, shape [n_samples, k]
X transformed in the new space.
Examples using sklearn.cluster.KMeans
• K-means Clustering
• Color Quantization using K-Means
• Vector Quantization Example
• Demonstration of k-means assumptions
• A demo of K-Means clustering on the handwritten digits data
• Selecting the number of clusters with silhouette analysis on KMeans clustering
• Empirical evaluation of the impact of k-means initialization
• Comparison of the K-Means and MiniBatchKMeans clustering algorithms
• Clustering text documents using k-means
sklearn.cluster.MiniBatchKMeans
class sklearn.cluster.MiniBatchKMeans(n_clusters=8,
init=’k-means++’,
max_iter=100,
batch_size=100,
verbose=0,
compute_labels=True,
random_state=None, tol=0.0, max_no_improvement=10,
init_size=None, n_init=3, reassignment_ratio=0.01)
Mini-Batch K-Means clustering
Read more in the User Guide.
Parametersn_clusters : int, optional, default: 8
The number of clusters to form as well as the number of centroids to generate.
max_iter : int, optional
Maximum number of iterations over the complete dataset before stopping independently
of any early stopping criterion heuristics.
max_no_improvement : int, default: 10
Control early stopping based on the consecutive number of mini batches that does not
yield an improvement on the smoothed inertia.
To disable convergence detection based on inertia, set max_no_improvement to None.
tol : ﬂoat, default: 0.0
29.2. sklearn.cluster: Clustering
1151
scikit-learn user guide, Release 0.18.2
Control early stopping based on the relative center changes as measured by a smoothed,
variance-normalized of the mean center squared position changes. This early stopping
heuristics is closer to the one used for the batch variant of the algorithms but induces a
slight computational and memory overhead over the inertia heuristic.
To disable convergence detection based on normalized center change, set tol to 0.0
(default).
batch_size : int, optional, default: 100
Size of the mini batches.
init_size : int, optional, default: 3 * batch_size
Number of samples to randomly sample for speeding up the initialization (sometimes at
the expense of accuracy): the only algorithm is initialized by running a batch KMeans
on a random subset of the data. This needs to be larger than n_clusters.
init : {‘k-means++’, ‘random’ or an ndarray}, default: ‘k-means++’
Method for initialization, defaults to ‘k-means++’:
‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to
speed up convergence. See section Notes in k_init for more details.
‘random’: choose k observations (rows) at random from data for the initial centroids.
If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial
centers.
n_init : int, default=3
Number of random initializations that are tried. In contrast to KMeans, the algorithm is
only run once, using the best of the n_init initializations as measured by inertia.
compute_labels : boolean, default=True
Compute label assignment and inertia for the complete dataset once the minibatch opti-
mization has converged in ﬁt.
random_state : integer or numpy.RandomState, optional
The generator used to initialize the centers. If an integer is given, it ﬁxes the seed.
Defaults to the global numpy random number generator.
reassignment_ratio : ﬂoat, default: 0.01
Control the fraction of the maximum number of counts for a center to be reassigned. A
higher value means that low count centers are more easily reassigned, which means that
the model will take longer to converge, but should converge in a better clustering.
verbose : boolean, optional
Verbosity mode.
Attributescluster_centers_ : array, [n_clusters, n_features]
Coordinates of cluster centers
labels_ : :
Labels of each point (if compute_labels is set to True).
inertia_ : ﬂoat
1152
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The value of the inertia criterion associated with the chosen partition (if compute_labels
is set to True). The inertia is deﬁned as the sum of square distances of samples to their
nearest neighbor.
See also:
KMeansThe classic implementation of the clustering method based on the Lloyd’s algorithm. It consumes the
whole set of input data at each iteration.
Notes
See http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf
Methods
fit(X[, y])
Compute the centroids on X by chunking it into mini-
batches.
fit_predict(X[, y])
Compute cluster centers and predict cluster index for
each sample.
fit_transform(X[, y])
Compute clustering and transform X to cluster-distance
space.
get_params([deep])
Get parameters for this estimator.
partial_fit(X[, y])
Update k means estimate on a single mini-batch X.
predict(X)
Predict the closest cluster each sample in X belongs to.
score(X[, y])
Opposite of the value of X on the K-means objective.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Transform X to a cluster-distance space.
__init__(n_clusters=8,
init=’k-means++’,
max_iter=100,
batch_size=100,
verbose=0,
compute_labels=True,
random_state=None,
tol=0.0,
max_no_improvement=10,
init_size=None, n_init=3, reassignment_ratio=0.01)
fit(X, y=None)
Compute the centroids on X by chunking it into mini-batches.
ParametersX : array-like or sparse matrix, shape=(n_samples, n_features)
Training instances to cluster.
fit_predict(X, y=None)
Compute cluster centers and predict cluster index for each sample.
Convenience method; equivalent to calling ﬁt(X) followed by predict(X).
fit_transform(X, y=None)
Compute clustering and transform X to cluster-distance space.
Equivalent to ﬁt(X).transform(X), but more efﬁciently implemented.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
29.2. sklearn.cluster: Clustering
1153
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y=None)
Update k means estimate on a single mini-batch X.
ParametersX : array-like, shape = [n_samples, n_features]
Coordinates of the data points to cluster.
predict(X)
Predict the closest cluster each sample in X belongs to.
In the vector quantization literature, cluster_centers_ is called the code book and each value returned by
predict is the index of the closest code in the code book.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
New data to predict.
Returnslabels : array, shape [n_samples,]
Index of the cluster each sample belongs to.
score(X, y=None)
Opposite of the value of X on the K-means objective.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
New data.
Returnsscore : ﬂoat
Opposite of the value of X on the K-means objective.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Transform X to a cluster-distance space.
In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the
array returned by transform will typically be dense.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
New data to transform.
ReturnsX_new : array, shape [n_samples, k]
X transformed in the new space.
Examples using sklearn.cluster.MiniBatchKMeans
• Biclustering documents with the Spectral Co-clustering algorithm
• Compare BIRCH and MiniBatchKMeans
• Comparing different clustering algorithms on toy datasets
1154
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Online learning of a dictionary of parts of faces
• Empirical evaluation of the impact of k-means initialization
• Comparison of the K-Means and MiniBatchKMeans clustering algorithms
• Faces dataset decompositions
• Clustering text documents using k-means
sklearn.cluster.MeanShift
class sklearn.cluster.MeanShift(bandwidth=None,
seeds=None,
bin_seeding=False,
min_bin_freq=1, cluster_all=True, n_jobs=1)
Mean shift clustering using a ﬂat kernel.
Mean shift clustering aims to discover “blobs” in a smooth density of samples. It is a centroid-based algo-
rithm, which works by updating candidates for centroids to be the mean of the points within a given region.
These candidates are then ﬁltered in a post-processing stage to eliminate near-duplicates to form the ﬁnal set of
centroids.
Seeding is performed using a binning technique for scalability.
Read more in the User Guide.
Parametersbandwidth : ﬂoat, optional
Bandwidth used in the RBF kernel.
If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see
the documentation for that function for hints on scalability (see also the Notes, below).
seeds : array, shape=[n_samples, n_features], optional
Seeds used to initialize kernels.
If not set, the seeds are calculated by cluster-
ing.get_bin_seeds with bandwidth as the grid size and default values for other parame-
ters.
bin_seeding : boolean, optional
If true, initial kernel locations are not locations of all points, but rather the location of
the discretized version of points, where points are binned onto a grid whose coarseness
corresponds to the bandwidth. Setting this option to True will speed up the algorithm
because fewer seeds will be initialized. default value: False Ignored if seeds argument
is not None.
min_bin_freq : int, optional
To speed up the algorithm, accept only those bins with at least min_bin_freq points as
seeds. If not deﬁned, set to 1.
cluster_all : boolean, default True
If true, then all points are clustered, even those orphans that are not within any kernel.
Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label
-1.
n_jobs : int
The number of jobs to use for the computation. This works by computing each of the
n_init runs in parallel.
29.2. sklearn.cluster: Clustering
1155
scikit-learn user guide, Release 0.18.2
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
Attributescluster_centers_ : array, [n_clusters, n_features]
Coordinates of cluster centers.
labels_ : :
Labels of each point.
Notes
Scalability:
Because this implementation uses a ﬂat kernel and a Ball Tree to look up members of each kernel, the complexity
will tend towards O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points.
In higher dimensions the complexity will tend towards O(T*n^2).
Scalability can be boosted by using fewer seeds, for example by using a higher value of min_bin_freq in the
get_bin_seeds function.
Note that the estimate_bandwidth function is much less scalable than the mean shift algorithm and will be the
bottleneck if it is used.
References
Dorin Comaniciu and Peter Meer, “Mean Shift: A robust approach toward feature space analysis”. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619.
Methods
fit(X[, y])
Perform clustering.
fit_predict(X[, y])
Performs clustering on X and returns cluster labels.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict the closest cluster each sample in X belongs to.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True,
n_jobs=1)
fit(X, y=None)
Perform clustering.
ParametersX : array-like, shape=[n_samples, n_features]
Samples to cluster.
fit_predict(X, y=None)
Performs clustering on X and returns cluster labels.
ParametersX : ndarray, shape (n_samples, n_features)
Input data.
Returnsy : ndarray, shape (n_samples,)
1156
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
cluster labels
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict the closest cluster each sample in X belongs to.
ParametersX : {array-like, sparse matrix}, shape=[n_samples, n_features]
New data to predict.
Returnslabels : array, shape [n_samples,]
Index of the cluster each sample belongs to.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.cluster.MeanShift
• Comparing different clustering algorithms on toy datasets
• A demo of the mean-shift clustering algorithm
sklearn.cluster.SpectralClustering
class sklearn.cluster.SpectralClustering(n_clusters=8,
eigen_solver=None,
ran-
dom_state=None,
n_init=10,
gamma=1.0,
afﬁn-
ity=’rbf’,
n_neighbors=10,
eigen_tol=0.0,
as-
sign_labels=’kmeans’,
degree=3,
coef0=1,
ker-
nel_params=None, n_jobs=1)
Apply clustering to a projection to the normalized laplacian.
In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex
or more generally when a measure of the center and spread of the cluster is not a suitable description of the
complete cluster. For instance when clusters are nested circles on the 2D plan.
If afﬁnity is the adjacency matrix of a graph, this method can be used to ﬁnd normalized graph cuts.
When calling fit, an afﬁnity matrix is constructed using either kernel function such the Gaussian (aka RBF)
kernel of the euclidean distanced d(X,X):
np.exp(-gamma * d(X,X) ** 2)
29.2. sklearn.cluster: Clustering
1157
scikit-learn user guide, Release 0.18.2
or a k-nearest neighbors connectivity matrix.
Alternatively, using precomputed, a user-provided afﬁnity matrix can be used.
Read more in the User Guide.
Parametersn_clusters : integer, optional
The dimension of the projection subspace.
afﬁnity : string, array-like or callable, default ‘rbf’
If a string, this may be one of ‘nearest_neighbors’, ‘precomputed’, ‘rbf’ or one of the
kernels supported by sklearn.metrics.pairwise_kernels.
Only kernels that produce similarity scores (non-negative values that increase with sim-
ilarity) should be used. This property is not checked by the clustering algorithm.
gamma : ﬂoat, default=1.0
Scaling factor of RBF, polynomial, exponential chi^2 and sigmoid afﬁnity kernel. Ig-
nored for affinity='nearest_neighbors'.
degree : ﬂoat, default=3
Degree of the polynomial kernel. Ignored by other kernels.
coef0 : ﬂoat, default=1
Zero coefﬁcient for polynomial and sigmoid kernels. Ignored by other kernels.
n_neighbors : integer
Number of neighbors to use when constructing the afﬁnity matrix using the nearest
neighbors method. Ignored for affinity='rbf'.
eigen_solver : {None, ‘arpack’, ‘lobpcg’, or ‘amg’}
The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It
can be faster on very large, sparse problems, but may also lead to instabilities
random_state : int seed, RandomState instance, or None (default)
A pseudo random number generator used for the initialization of the lobpcg eigen vec-
tors decomposition when eigen_solver == ‘amg’ and by the K-Means initialization.
n_init : int, optional, default: 10
Number of time the k-means algorithm will be run with different centroid seeds. The
ﬁnal results will be the best output of n_init consecutive runs in terms of inertia.
eigen_tol : ﬂoat, optional, default: 0.0
Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack
eigen_solver.
assign_labels : {‘kmeans’, ‘discretize’}, default: ‘kmeans’
The strategy to use to assign labels in the embedding space. There are two ways to
assign labels after the laplacian embedding. k-means can be applied and is a popular
choice. But it can also be sensitive to initialization. Discretization is another approach
which is less sensitive to random initialization.
kernel_params : dictionary of string to any, optional
Parameters (keyword arguments) and values for kernel passed as callable object. Ig-
nored by other kernels.
1158
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
n_jobs : int, optional (default = 1)
The number of parallel jobs to run. If -1, then the number of jobs is set to the number
of CPU cores.
Attributesafﬁnity_matrix_ : array-like, shape (n_samples, n_samples)
Afﬁnity matrix used for clustering. Available only if after calling fit.
labels_ : :
Labels of each point
Notes
If you have an afﬁnity matrix, such as a distance matrix, for which 0 means identical elements, and high values
means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm
by applying the Gaussian (RBF, heat) kernel:
np.exp(- dist_matrix ** 2 / (2. * delta ** 2))
Where delta is a free parameter representing the width of the Gaussian kernel.
Another alternative is to take a symmetric version of the k nearest neighbors connectivity matrix of the points.
If the pyamg package is installed, it is used: this greatly speeds up computation.
References
•Normalized cuts and image segmentation, 2000 Jianbo Shi, Jitendra Malik http://citeseer.ist.psu.edu/
viewdoc/summary?doi=10.1.1.160.2324
•A Tutorial on Spectral Clustering, 2007 Ulrike von Luxburg http://citeseerx.ist.psu.edu/viewdoc/
summary?doi=10.1.1.165.9323
•Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi http://www1.icsi.berkeley.edu/~stellayu/
publication/doc/2003kwayICCV.pdf
Methods
fit(X[, y])
Creates an afﬁnity matrix for X using the selected afﬁn-
ity, then applies spectral clustering to this afﬁnity ma-
trix.
fit_predict(X[, y])
Performs clustering on X and returns cluster labels.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_clusters=8, eigen_solver=None, random_state=None, n_init=10, gamma=1.0, afﬁn-
ity=’rbf’, n_neighbors=10, eigen_tol=0.0, assign_labels=’kmeans’, degree=3, coef0=1,
kernel_params=None, n_jobs=1)
fit(X, y=None)
Creates an afﬁnity matrix for X using the selected afﬁnity, then applies spectral clustering to this afﬁnity
matrix.
ParametersX : array-like or sparse matrix, shape (n_samples, n_features)
29.2. sklearn.cluster: Clustering
1159
scikit-learn user guide, Release 0.18.2
OR, if afﬁnity==‘precomputed‘, a precomputed afﬁnity matrix of shape (n_samples,
n_samples)
fit_predict(X, y=None)
Performs clustering on X and returns cluster labels.
ParametersX : ndarray, shape (n_samples, n_features)
Input data.
Returnsy : ndarray, shape (n_samples,)
cluster labels
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.cluster.SpectralClustering
• Comparing different clustering algorithms on toy datasets
29.2.2 Functions
cluster.estimate_bandwidth(X[, quantile, ...])
Estimate the bandwidth to use with the mean-shift algo-
rithm.
cluster.k_means(X, n_clusters[, init, ...])
K-means clustering algorithm.
cluster.ward_tree(X[, connectivity, ...])
Ward clustering based on a Feature matrix.
cluster.affinity_propagation(S[, ...])
Perform Afﬁnity Propagation Clustering of data
cluster.dbscan(X[, eps, min_samples, ...])
Perform DBSCAN clustering from vector array or distance
matrix.
cluster.mean_shift(X[, bandwidth, seeds, ...])
Perform mean shift clustering of data using a ﬂat kernel.
cluster.spectral_clustering(afﬁnity[, ...])
Apply clustering to a projection to the normalized lapla-
cian.
sklearn.cluster.estimate_bandwidth
sklearn.cluster.estimate_bandwidth(X,
quantile=0.3,
n_samples=None,
random_state=0,
n_jobs=1)
Estimate the bandwidth to use with the mean-shift algorithm.
1160
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
That this function takes time at least quadratic in n_samples. For large datasets, it’s wise to set that parameter
to a small value.
ParametersX : array-like, shape=[n_samples, n_features]
Input points.
quantile : ﬂoat, default 0.3
should be between [0, 1] 0.5 means that the median of all pairwise distances is used.
n_samples : int, optional
The number of samples to use. If not given, all samples are used.
random_state : int or RandomState
Pseudo-random number generator state used for random sampling.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run for neighbors search. If -1, then the number of jobs
is set to the number of CPU cores.
Returnsbandwidth : ﬂoat
The bandwidth parameter.
Examples using sklearn.cluster.estimate_bandwidth
• Comparing different clustering algorithms on toy datasets
• A demo of the mean-shift clustering algorithm
sklearn.cluster.k_means
sklearn.cluster.k_means(X,
n_clusters,
init=’k-means++’,
precompute_distances=’auto’,
n_init=10,
max_iter=300,
verbose=False,
tol=0.0001,
ran-
dom_state=None,
copy_x=True,
n_jobs=1,
algorithm=’auto’,
re-
turn_n_iter=False)
K-means clustering algorithm.
Read more in the User Guide.
ParametersX : array-like or sparse matrix, shape (n_samples, n_features)
The observations to cluster.
n_clusters : int
The number of clusters to form as well as the number of centroids to generate.
max_iter : int, optional, default 300
Maximum number of iterations of the k-means algorithm to run.
n_init : int, optional, default: 10
Number of time the k-means algorithm will be run with different centroid seeds. The
ﬁnal results will be the best output of n_init consecutive runs in terms of inertia.
init : {‘k-means++’, ‘random’, or ndarray, or a callable}, optional
29.2. sklearn.cluster: Clustering
1161
scikit-learn user guide, Release 0.18.2
Method for initialization, default to ‘k-means++’:
‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to
speed up convergence. See section Notes in k_init for more details.
‘random’: generate k centroids from a Gaussian with mean and variance estimated from
the data.
If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial
centers.
If a callable is passed, it should take arguments X, k and and a random state and return
an initialization.
algorithm : “auto”, “full” or “elkan”, default=”auto”
K-means algorithm to use. The classical EM-style algorithm is “full”. The “elkan”
variation is more efﬁcient by using the triangle inequality, but currently doesn’t support
sparse data. “auto” chooses “elkan” for dense data and “full” for sparse data.
precompute_distances : {‘auto’, True, False}
Precompute distances (faster but takes more memory).
‘auto’ : do not precompute distances if n_samples * n_clusters > 12 million. This
corresponds to about 100MB overhead per job using double precision.
True : always precompute distances
False : never precompute distances
tol : ﬂoat, optional
The relative increment in the results before declaring convergence.
verbose : boolean, optional
Verbosity mode.
random_state : integer or numpy.RandomState, optional
The generator used to initialize the centers. If an integer is given, it ﬁxes the seed.
Defaults to the global numpy random number generator.
copy_x : boolean, optional
When pre-computing distances it is more numerically accurate to center the data ﬁrst.
If copy_x is True, then the original data is not modiﬁed. If False, the original data is
modiﬁed, and put back before the function returns, but small numerical differences may
be introduced by subtracting and then adding the data mean.
n_jobs : int
The number of jobs to use for the computation. This works by computing each of the
n_init runs in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
return_n_iter : bool, optional
Whether or not to return the number of iterations.
Returnscentroid : ﬂoat ndarray with shape (k, n_features)
Centroids found at the last iteration of k-means.
1162
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
label : integer ndarray with shape (n_samples,)
label[i] is the code or index of the centroid the i’th observation is closest to.
inertia : ﬂoat
The ﬁnal value of the inertia criterion (sum of squared distances to the closest centroid
for all observations in the training set).
best_n_iter: int :
Number of iterations corresponding to the best results. Returned only if return_n_iter
is set to True.
sklearn.cluster.ward_tree
sklearn.cluster.ward_tree(X, connectivity=None, n_clusters=None, return_distance=False)
Ward clustering based on a Feature matrix.
Recursively merges the pair of clusters that minimally increases within-cluster variance.
The inertia matrix uses a Heapq-based representation.
This is the structured version, that takes into account some topological structure between samples.
Read more in the User Guide.
ParametersX : array, shape (n_samples, n_features)
feature matrix representing n_samples samples to be clustered
connectivity : sparse matrix (optional).
connectivity matrix. Deﬁnes for each sample the neighboring samples following a given
structure of the data. The matrix is assumed to be symmetric and only the upper trian-
gular half is used. Default is None, i.e, the Ward algorithm is unstructured.
n_clusters : int (optional)
Stop early the construction of the tree at n_clusters. This is useful to decrease compu-
tation time if the number of clusters is not small compared to the number of samples.
In this case, the complete tree is not computed, thus the ‘children’ output is of limited
use, and the ‘parents’ output should rather be used. This option is valid only when
specifying a connectivity matrix.
return_distance: bool (optional) :
If True, return the distance between the clusters.
Returnschildren : 2D array, shape (n_nodes-1, 2)
The children of each non-leaf node. Values less than n_samples correspond to leaves
of the tree which are the original samples. A node i greater than or equal to n_samples
is a non-leaf node and has children children_[i - n_samples]. Alternatively at the i-th
iteration, children[i][0] and children[i][1] are merged to form node n_samples + i
n_components : int
The number of connected components in the graph.
n_leaves : int
The number of leaves in the tree
parents : 1D array, shape (n_nodes, ) or None
29.2. sklearn.cluster: Clustering
1163
scikit-learn user guide, Release 0.18.2
The parent of each node. Only returned when a connectivity matrix is speciﬁed, else-
where ‘None’ is returned.
distances : 1D array, shape (n_nodes-1, )
Only returned if return_distance is set to True (for compatibility). The distances be-
tween the centers of the nodes. distances[i] corresponds to a weighted euclidean dis-
tance between the nodes children[i, 1] and children[i, 2]. If the nodes refer to leaves of
the tree, then distances[i] is their unweighted euclidean distance. Distances are updated
in the following way (from scipy.hierarchy.linkage):
The new entry 𝑑(𝑢, 𝑣) is computed as follows,
𝑑(𝑢, 𝑣) =
√︂
|𝑣| + |𝑠|
𝑇
𝑑(𝑣, 𝑠)2 + |𝑣| + |𝑡|
𝑇
𝑑(𝑣, 𝑡)2 −|𝑣|
𝑇𝑑(𝑠, 𝑡)2
where 𝑢is the newly joined cluster consisting of clusters 𝑠and 𝑡, 𝑣is an unused cluster
in the forest, 𝑇= |𝑣| + |𝑠| + |𝑡|, and | * | is the cardinality of its argument. This is also
known as the incremental algorithm.
sklearn.cluster.afﬁnity_propagation
sklearn.cluster.affinity_propagation(S,
preference=None,
convergence_iter=15,
max_iter=200,
damping=0.5,
copy=True,
ver-
bose=False, return_n_iter=False)
Perform Afﬁnity Propagation Clustering of data
Read more in the User Guide.
ParametersS : array-like, shape (n_samples, n_samples)
Matrix of similarities between points
preference : array-like, shape (n_samples,) or ﬂoat, optional
Preferences for each point - points with larger values of preferences are more likely to
be chosen as exemplars. The number of exemplars, i.e. of clusters, is inﬂuenced by the
input preferences value. If the preferences are not passed as arguments, they will be set
to the median of the input similarities (resulting in a moderate number of clusters). For
a smaller amount of clusters, this can be set to the minimum value of the similarities.
convergence_iter : int, optional, default: 15
Number of iterations with no change in the number of estimated clusters that stops the
convergence.
max_iter : int, optional, default: 200
Maximum number of iterations
damping : ﬂoat, optional, default: 0.5
Damping factor between 0.5 and 1.
copy : boolean, optional, default: True
If copy is False, the afﬁnity matrix is modiﬁed inplace by the algorithm, for memory
efﬁciency
verbose : boolean, optional, default: False
The verbosity level
return_n_iter : bool, default False
1164
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Whether or not to return the number of iterations.
Returnscluster_centers_indices : array, shape (n_clusters,)
index of clusters centers
labels : array, shape (n_samples,)
cluster labels for each point
n_iter : int
number of iterations run. Returned only if return_n_iter is set to True.
Notes
See examples/cluster/plot_afﬁnity_propagation.py for an example.
References
Brendan J. Frey and Delbert Dueck, “Clustering by Passing Messages Between Data Points”, Science Feb. 2007
Examples using sklearn.cluster.affinity_propagation
• Visualizing the stock market structure
sklearn.cluster.dbscan
sklearn.cluster.dbscan(X,
eps=0.5,
min_samples=5,
metric=’minkowski’,
algorithm=’auto’,
leaf_size=30, p=2, sample_weight=None, n_jobs=1)
Perform DBSCAN clustering from vector array or distance matrix.
Read more in the User Guide.
ParametersX : array or sparse (CSR) matrix of shape (n_samples, n_features), or array of shape
(n_samples, n_samples)
A feature array, or array of distances between samples if metric='precomputed'.
eps : ﬂoat, optional
The maximum distance between two samples for them to be considered as in the same
neighborhood.
min_samples : int, optional
The number of samples (or total weight) in a neighborhood for a point to be considered
as a core point. This includes the point itself.
metric : string, or callable
The metric to use when calculating distance between instances in a feature array.
If metric is a string or callable, it must be one of the options allowed by met-
rics.pairwise.pairwise_distances for its metric parameter. If metric is “precomputed”,
X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in
which case only “nonzero” elements may be considered neighbors for DBSCAN.
algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
29.2. sklearn.cluster: Clustering
1165
scikit-learn user guide, Release 0.18.2
The algorithm to be used by the NearestNeighbors module to compute pointwise dis-
tances and ﬁnd nearest neighbors. See NearestNeighbors module documentation for
details.
leaf_size : int, optional (default = 30)
Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction
and query, as well as the memory required to store the tree. The optimal value depends
on the nature of the problem.
p : ﬂoat, optional
The power of the Minkowski metric to be used to calculate distance between points.
sample_weight : array, shape (n_samples,), optional
Weight of each sample, such that a sample with a weight of at least min_samples
is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor
from being core. Note that weights are absolute, and default to 1.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run for neighbors search. If -1, then the number of jobs
is set to the number of CPU cores.
Returnscore_samples : array [n_core_samples]
Indices of core samples.
labels : array [n_samples]
Cluster labels for each point. Noisy samples are given the label -1.
Notes
See examples/cluster/plot_dbscan.py for an example.
This implementation bulk-computes all neighborhood queries, which increases the memory complexity to
O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n).
Sparse neighborhoods can be precomputed using NearestNeighbors.radius_neighbors_graph
with mode='distance'.
References
Ester, M., H. P. Kriegel, J. Sander, and X. Xu, “A Density-Based Algorithm for Discovering Clusters in Large
Spatial Databases with Noise”. In: Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
sklearn.cluster.mean_shift
sklearn.cluster.mean_shift(X,
bandwidth=None,
seeds=None,
bin_seeding=False,
min_bin_freq=1, cluster_all=True, max_iter=300, n_jobs=1)
Perform mean shift clustering of data using a ﬂat kernel.
Read more in the User Guide.
ParametersX : array-like, shape=[n_samples, n_features]
Input data.
1166
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
bandwidth : ﬂoat, optional
Kernel bandwidth.
If bandwidth is not given, it is determined using a heuristic based on the median of
all pairwise distances. This will take quadratic time in the number of samples. The
sklearn.cluster.estimate_bandwidth function can be used to do this more efﬁciently.
seeds : array-like, shape=[n_seeds, n_features] or None
Point used as initial kernel locations. If None and bin_seeding=False, each data point is
used as a seed. If None and bin_seeding=True, see bin_seeding.
bin_seeding : boolean, default=False
If true, initial kernel locations are not locations of all points, but rather the location of
the discretized version of points, where points are binned onto a grid whose coarseness
corresponds to the bandwidth. Setting this option to True will speed up the algorithm
because fewer seeds will be initialized. Ignored if seeds argument is not None.
min_bin_freq : int, default=1
To speed up the algorithm, accept only those bins with at least min_bin_freq points as
seeds.
cluster_all : boolean, default True
If true, then all points are clustered, even those orphans that are not within any kernel.
Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label
-1.
max_iter : int, default 300
Maximum number of iterations, per seed point before the clustering operation termi-
nates (for that seed point), if has not converged yet.
n_jobs : int
The number of jobs to use for the computation. This works by computing each of the
n_init runs in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
New in version 0.17: Parallel Execution using n_jobs.
Returnscluster_centers : array, shape=[n_clusters, n_features]
Coordinates of cluster centers.
labels : array, shape=[n_samples]
Cluster labels for each point.
Notes
See examples/cluster/plot_mean_shift.py for an example.
29.2. sklearn.cluster: Clustering
1167
scikit-learn user guide, Release 0.18.2
sklearn.cluster.spectral_clustering
sklearn.cluster.spectral_clustering(afﬁnity,
n_clusters=8,
n_components=None,
eigen_solver=None,
random_state=None,
n_init=10,
eigen_tol=0.0, assign_labels=’kmeans’)
Apply clustering to a projection to the normalized laplacian.
In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex
or more generally when a measure of the center and spread of the cluster is not a suitable description of the
complete cluster. For instance when clusters are nested circles on the 2D plan.
If afﬁnity is the adjacency matrix of a graph, this method can be used to ﬁnd normalized graph cuts.
Read more in the User Guide.
Parametersafﬁnity : array-like or sparse matrix, shape: (n_samples, n_samples)
The afﬁnity matrix describing the relationship of the samples to embed. Must be sym-
metric.
Possible examples:
•adjacency matrix of a graph,
•heat kernel of the pairwise distance matrix of the samples,
•symmetric k-nearest neighbours connectivity matrix of the samples.
n_clusters : integer, optional
Number of clusters to extract.
n_components : integer, optional, default is n_clusters
Number of eigen vectors to use for the spectral embedding
eigen_solver : {None, ‘arpack’, ‘lobpcg’, or ‘amg’}
The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It
can be faster on very large, sparse problems, but may also lead to instabilities
random_state : int seed, RandomState instance, or None (default)
A pseudo random number generator used for the initialization of the lobpcg eigen vec-
tors decomposition when eigen_solver == ‘amg’ and by the K-Means initialization.
n_init : int, optional, default: 10
Number of time the k-means algorithm will be run with different centroid seeds. The
ﬁnal results will be the best output of n_init consecutive runs in terms of inertia.
eigen_tol : ﬂoat, optional, default: 0.0
Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack
eigen_solver.
assign_labels : {‘kmeans’, ‘discretize’}, default: ‘kmeans’
The strategy to use to assign labels in the embedding space. There are two ways to
assign labels after the laplacian embedding. k-means can be applied and is a popular
choice. But it can also be sensitive to initialization. Discretization is another approach
which is less sensitive to random initialization. See the ‘Multiclass spectral clustering’
paper referenced below for more details on the discretization approach.
Returnslabels : array of integers, shape: n_samples
The labels of the clusters.
1168
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
The graph should contain only one connect component, elsewhere the results make little sense.
This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering.
References
•Normalized cuts and image segmentation, 2000 Jianbo Shi, Jitendra Malik http://citeseer.ist.psu.edu/
viewdoc/summary?doi=10.1.1.160.2324
•A Tutorial on Spectral Clustering, 2007 Ulrike von Luxburg http://citeseerx.ist.psu.edu/viewdoc/
summary?doi=10.1.1.165.9323
•Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi http://www1.icsi.berkeley.edu/~stellayu/
publication/doc/2003kwayICCV.pdf
Examples using sklearn.cluster.spectral_clustering
• Segmenting the picture of a raccoon face in regions
• Spectral clustering for image segmentation
29.3 sklearn.cluster.bicluster: Biclustering
Spectral biclustering algorithms.
Authors : Kemal Eren License: BSD 3 clause
User guide: See the Biclustering section for further details.
29.3.1 Classes
SpectralBiclustering([n_clusters, method, ...])
Spectral biclustering (Kluger, 2003).
SpectralCoclustering([n_clusters, ...])
Spectral Co-Clustering algorithm (Dhillon, 2001).
sklearn.cluster.bicluster.SpectralBiclustering
class sklearn.cluster.bicluster.SpectralBiclustering(n_clusters=3,
method=’bistochastic’,
n_components=6,
n_best=3,
svd_method=’randomized’,
n_svd_vecs=None,
mini_batch=False,
init=’k-
means++’, n_init=10, n_jobs=1,
random_state=None)
Spectral biclustering (Kluger, 2003).
Partitions rows and columns under the assumption that the data has an underlying checkerboard structure. For
instance, if there are two row partitions and three column partitions, each row will belong to three biclusters,
and each column will belong to two biclusters. The outer product of the corresponding row and column label
29.3. sklearn.cluster.bicluster: Biclustering
1169
scikit-learn user guide, Release 0.18.2
vectors gives this checkerboard structure.
Read more in the User Guide.
Parametersn_clusters : integer or tuple (n_row_clusters, n_column_clusters)
The number of row and column clusters in the checkerboard structure.
method : string, optional, default: ‘bistochastic’
Method of normalizing and converting singular vectors into biclusters. May be one
of ‘scale’, ‘bistochastic’, or ‘log’. The authors recommend using ‘log’. If the data is
sparse, however, log normalization will not work, which is why the default is ‘bistochas-
tic’. CAUTION: if method=’log’, the data must not be sparse.
n_components : integer, optional, default: 6
Number of singular vectors to check.
n_best : integer, optional, default: 3
Number of best singular vectors to which to project the data for clustering.
svd_method : string, optional, default: ‘randomized’
Selects the algorithm for ﬁnding singular vectors. May be ‘randomized’ or ‘arpack’.
If ‘randomized’, uses sklearn.utils.extmath.randomized_svd, which may be faster for
large matrices. If ‘arpack’, uses sklearn.utils.arpack.svds, which is more accurate, but
possibly slower in some cases.
n_svd_vecs : int, optional, default: None
Number of vectors to use in calculating the SVD. Corresponds to ncv when
svd_method=arpack and n_oversamples when svd_method is ‘randomized‘.
mini_batch : bool, optional, default: False
Whether to use mini-batch k-means, which is faster but may get different results.
init : {‘k-means++’, ‘random’ or an ndarray}
Method for initialization of k-means algorithm; defaults to ‘k-means++’.
n_init : int, optional, default: 10
Number of random initializations that are tried with the k-means algorithm.
If mini-batch k-means is used, the best initialization is chosen and the algorithm runs
once. Otherwise, the algorithm is run for each initialization and the best solution chosen.
n_jobs : int, optional, default: 1
The number of jobs to use for the computation. This works by breaking down the
pairwise matrix into n_jobs even slices and computing them in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
random_state : int seed, RandomState instance, or None (default)
A pseudo random number generator used by the K-Means initialization.
Attributesrows_ : array-like, shape (n_row_clusters, n_rows)
Results of the clustering. rows[i, r] is True if cluster i contains row r. Available only
after calling fit.
1170
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
columns_ : array-like, shape (n_column_clusters, n_columns)
Results of the clustering, like rows.
row_labels_ : array-like, shape (n_rows,)
Row partition labels.
column_labels_ : array-like, shape (n_cols,)
Column partition labels.
References
•Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray data: coclustering genes and conditions.
Methods
fit(X)
Creates a biclustering for X.
get_indices(i)
Row and column indices of the i’th bicluster.
get_params([deep])
Get parameters for this estimator.
get_shape(i)
Shape of the i’th bicluster.
get_submatrix(i, data)
Returns the submatrix corresponding to bicluster i.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_clusters=3,
method=’bistochastic’,
n_components=6,
n_best=3,
svd_method=’randomized’,
n_svd_vecs=None,
mini_batch=False,
init=’k-means++’,
n_init=10, n_jobs=1, random_state=None)
biclusters_
Convenient way to get row and column indicators together.
Returns the rows_ and columns_ members.
fit(X)
Creates a biclustering for X.
ParametersX : array-like, shape (n_samples, n_features)
get_indices(i)
Row and column indices of the i’th bicluster.
Only works if rows_ and columns_ attributes exist.
Returnsrow_ind : np.array, dtype=np.intp
Indices of rows in the dataset that belong to the bicluster.
col_ind : np.array, dtype=np.intp
Indices of columns in the dataset that belong to the bicluster.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
29.3. sklearn.cluster.bicluster: Biclustering
1171
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_shape(i)
Shape of the i’th bicluster.
Returnsshape : (int, int)
Number of rows and columns (resp.) in the bicluster.
get_submatrix(i, data)
Returns the submatrix corresponding to bicluster i.
Works with sparse matrices. Only works if rows_ and columns_ attributes exist.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sklearn.cluster.bicluster.SpectralCoclustering
class sklearn.cluster.bicluster.SpectralCoclustering(n_clusters=3,
svd_method=’randomized’,
n_svd_vecs=None,
mini_batch=False,
init=’k-
means++’, n_init=10, n_jobs=1,
random_state=None)
Spectral Co-Clustering algorithm (Dhillon, 2001).
Clusters rows and columns of an array X to solve the relaxed normalized cut of the bipartite graph created from
X as follows: the edge between row vertex i and column vertex j has weight X[i, j].
The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one
bicluster.
Supports sparse matrices, as long as they are nonnegative.
Read more in the User Guide.
Parametersn_clusters : integer, optional, default: 3
The number of biclusters to ﬁnd.
svd_method : string, optional, default: ‘randomized’
Selects the algorithm for ﬁnding singular vectors. May be ‘randomized’ or ‘arpack’.
If ‘randomized’, use sklearn.utils.extmath.randomized_svd, which may
be faster for large matrices.
If ‘arpack’, use sklearn.utils.arpack.svds,
which is more accurate, but possibly slower in some cases.
n_svd_vecs : int, optional, default: None
Number of vectors to use in calculating the SVD. Corresponds to ncv when
svd_method=arpack and n_oversamples when svd_method is ‘randomized‘.
mini_batch : bool, optional, default: False
Whether to use mini-batch k-means, which is faster but may get different results.
1172
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
init : {‘k-means++’, ‘random’ or an ndarray}
Method for initialization of k-means algorithm; defaults to ‘k-means++’.
n_init : int, optional, default: 10
Number of random initializations that are tried with the k-means algorithm.
If mini-batch k-means is used, the best initialization is chosen and the algorithm runs
once. Otherwise, the algorithm is run for each initialization and the best solution chosen.
n_jobs : int, optional, default: 1
The number of jobs to use for the computation. This works by breaking down the
pairwise matrix into n_jobs even slices and computing them in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
random_state : int seed, RandomState instance, or None (default)
A pseudo random number generator used by the K-Means initialization.
Attributesrows_ : array-like, shape (n_row_clusters, n_rows)
Results of the clustering. rows[i, r] is True if cluster i contains row r. Available only
after calling fit.
columns_ : array-like, shape (n_column_clusters, n_columns)
Results of the clustering, like rows.
row_labels_ : array-like, shape (n_rows,)
The bicluster label of each row.
column_labels_ : array-like, shape (n_cols,)
The bicluster label of each column.
References
•Dhillon, Inderjit S, 2001. Co-clustering documents and words using bipartite spectral graph partitioning.
Methods
fit(X)
Creates a biclustering for X.
get_indices(i)
Row and column indices of the i’th bicluster.
get_params([deep])
Get parameters for this estimator.
get_shape(i)
Shape of the i’th bicluster.
get_submatrix(i, data)
Returns the submatrix corresponding to bicluster i.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_clusters=3, svd_method=’randomized’, n_svd_vecs=None, mini_batch=False, init=’k-
means++’, n_init=10, n_jobs=1, random_state=None)
biclusters_
Convenient way to get row and column indicators together.
29.3. sklearn.cluster.bicluster: Biclustering
1173
scikit-learn user guide, Release 0.18.2
Returns the rows_ and columns_ members.
fit(X)
Creates a biclustering for X.
ParametersX : array-like, shape (n_samples, n_features)
get_indices(i)
Row and column indices of the i’th bicluster.
Only works if rows_ and columns_ attributes exist.
Returnsrow_ind : np.array, dtype=np.intp
Indices of rows in the dataset that belong to the bicluster.
col_ind : np.array, dtype=np.intp
Indices of columns in the dataset that belong to the bicluster.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_shape(i)
Shape of the i’th bicluster.
Returnsshape : (int, int)
Number of rows and columns (resp.) in the bicluster.
get_submatrix(i, data)
Returns the submatrix corresponding to bicluster i.
Works with sparse matrices. Only works if rows_ and columns_ attributes exist.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.4 sklearn.covariance: Covariance Estimators
The sklearn.covariance module includes methods and algorithms to robustly estimate the covariance of fea-
tures given a set of points. The precision matrix deﬁned as the inverse of the covariance is also estimated. Covariance
estimation is closely related to the theory of Gaussian Graphical Models.
User guide: See the Covariance estimation section for further details.
covariance.EmpiricalCovariance([...])
Maximum likelihood covariance estimator
Continued on next page
1174
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.22 – continued from previous page
covariance.EllipticEnvelope([...])
An object for detecting outliers in a Gaussian distributed
dataset.
covariance.GraphLasso([alpha, mode, tol, ...])
Sparse inverse covariance estimation with an l1-penalized
estimator.
covariance.GraphLassoCV([alphas, ...])
Sparse inverse covariance w/ cross-validated choice of the
l1 penalty
covariance.LedoitWolf([store_precision, ...])
LedoitWolf Estimator
covariance.MinCovDet([store_precision, ...])
Minimum Covariance Determinant (MCD): robust estima-
tor of covariance.
covariance.OAS([store_precision, ...])
Oracle Approximating Shrinkage Estimator
covariance.ShrunkCovariance([...])
Covariance estimator with shrinkage
29.4.1 sklearn.covariance.EmpiricalCovariance
class sklearn.covariance.EmpiricalCovariance(store_precision=True,
as-
sume_centered=False)
Maximum likelihood covariance estimator
Read more in the User Guide.
Parametersstore_precision : bool
Speciﬁes if the estimated precision is stored.
assume_centered : bool
If True, data are not centered before computation. Useful when working with data
whose mean is almost, but not exactly zero. If False (default), data are centered before
computation.
Attributescovariance_ : 2D ndarray, shape (n_features, n_features)
Estimated covariance matrix
precision_ : 2D ndarray, shape (n_features, n_features)
Estimated pseudo-inverse matrix. (stored only if store_precision is True)
Methods
error_norm(comp_cov[, norm, scaling, squared])
Computes the Mean Squared Error between two covari-
ance estimators.
fit(X[, y])
Fits the Maximum Likelihood Estimator covariance
model according to the given training data and parame-
ters.
get_params([deep])
Get parameters for this estimator.
get_precision()
Getter for the precision matrix.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given
observations.
score(X_test[, y])
Computes the log-likelihood of a Gaussian data set with
self.covariance_ as an estimator of its covariance ma-
trix.
set_params(\*\*params)
Set the parameters of this estimator.
29.4. sklearn.covariance: Covariance Estimators
1175
scikit-learn user guide, Release 0.18.2
__init__(store_precision=True, assume_centered=False)
error_norm(comp_cov, norm=’frobenius’, scaling=True, squared=True)
Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius
norm).
Parameterscomp_cov : array-like, shape = [n_features, n_features]
The covariance to compare with.
norm : str
The type of norm used to compute the error. Available error types: - ‘frobenius’ (de-
fault): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error
(comp_cov -self.covariance_).
scaling : bool
If True (default), the squared error norm is divided by n_features. If False, the squared
error norm is not rescaled.
squared : bool
Whether to compute the squared error norm or the error norm. If True (default), the
squared error norm is returned. If False, the error norm is returned.
ReturnsThe Mean Squared Error (in the sense of the Frobenius norm) between :
‘self‘ and ‘comp_cov‘ covariance estimators. :
fit(X, y=None)
Fits the Maximum Likelihood Estimator covariance model according to the given training data and param-
eters.
ParametersX : array-like, shape = [n_samples, n_features]
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : not used, present for API consistence purpose.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Getter for the precision matrix.
Returnsprecision_ : array-like,
The precision matrix associated to the current covariance object.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given observations.
1176
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Parametersobservations : array-like, shape = [n_observations, n_features]
The observations, the Mahalanobis distances of the which we compute. Observations
are assumed to be drawn from the same distribution than the data used in ﬁt.
Returnsmahalanobis_distance : array, shape = [n_observations,]
Squared Mahalanobis distances of the observations.
score(X_test, y=None)
Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance
matrix.
ParametersX_test : array-like, shape = [n_samples, n_features]
Test data of which we compute the likelihood, where n_samples is the number of sam-
ples and n_features is the number of features. X_test is assumed to be drawn from the
same distribution than the data used in ﬁt (including centering).
y : not used, present for API consistence purpose.
Returnsres : ﬂoat
The likelihood of the data set with self.covariance_ as an estimator of its covariance
matrix.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.covariance.EmpiricalCovariance
• Robust covariance estimation and Mahalanobis distances relevance
• Robust vs Empirical covariance estimate
29.4.2 sklearn.covariance.EllipticEnvelope
class sklearn.covariance.EllipticEnvelope(store_precision=True,
assume_centered=False,
support_fraction=None,
contamination=0.1,
random_state=None)
An object for detecting outliers in a Gaussian distributed dataset.
Read more in the User Guide.
Parametersstore_precision : bool
Specify if the estimated precision is stored.
assume_centered : Boolean
If True, the support of robust location and covariance estimates is computed, and a
covariance estimate is recomputed from it, without centering the data. Useful to work
with data whose mean is signiﬁcantly equal to zero but is not exactly zero. If False,
the robust location and covariance are directly computed with the FastMCD algorithm
without additional treatment.
29.4. sklearn.covariance: Covariance Estimators
1177
scikit-learn user guide, Release 0.18.2
support_fraction : ﬂoat, 0 < support_fraction < 1
The proportion of points to be included in the support of the raw MCD estimate. Default
is None, which implies that the minimum value of support_fraction will be used within
the algorithm: [n_sample + n_features + 1] / 2.
contamination : ﬂoat, 0. < contamination < 0.5
The amount of contamination of the data set, i.e. the proportion of outliers in the data
set.
Attributeslocation_ : array-like, shape (n_features,)
Estimated robust location
covariance_ : array-like, shape (n_features, n_features)
Estimated robust covariance matrix
precision_ : array-like, shape (n_features, n_features)
Estimated pseudo inverse matrix. (stored only if store_precision is True)
support_ : array-like, shape (n_samples,)
A mask of the observations that have been used to compute the robust estimates of
location and shape.
See also:
EmpiricalCovariance, MinCovDet
Notes
Outlier detection from covariance estimation may break or not perform well in high-dimensional settings. In
particular, one will always take care to work with n_samples > n_features ** 2.
References
Methods
correct_covariance(data)
Apply a correction to raw Minimum Covariance Deter-
minant estimates.
decision_function(X[, raw_values])
Compute the decision function of the given observa-
tions.
error_norm(comp_cov[, norm, scaling, squared])
Computes the Mean Squared Error between two covari-
ance estimators.
fit(X[, y])
get_params([deep])
Get parameters for this estimator.
get_precision()
Getter for the precision matrix.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given
observations.
predict(X)
Outlyingness of observations in X according to the ﬁtted
model.
reweight_covariance(data)
Re-weight raw Minimum Covariance Determinant esti-
mates.
Continued on next page
1178
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.24 – continued from previous page
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(store_precision=True,
assume_centered=False,
support_fraction=None,
contamina-
tion=0.1, random_state=None)
correct_covariance(data)
Apply a correction to raw Minimum Covariance Determinant estimates.
Correction using the empirical correction factor suggested by Rousseeuw and Van Driessen in
[Rouseeuw1984].
Parametersdata : array-like, shape (n_samples, n_features)
The data matrix, with p features and n samples. The data set must be the one which was
used to compute the raw estimates.
Returnscovariance_corrected : array-like, shape (n_features, n_features)
Corrected robust covariance estimate.
decision_function(X, raw_values=False)
Compute the decision function of the given observations.
ParametersX : array-like, shape (n_samples, n_features)
raw_values : bool
Whether or not to consider raw Mahalanobis distances as the decision function. Must
be False (default) for compatibility with the others outlier detection tools.
Returnsdecision : array-like, shape (n_samples, )
The values of the decision function for each observations. It is equal to the Mahalanobis
distances if raw_values is True. By default (raw_values=True), it is equal to the
cubic root of the shifted Mahalanobis distances. In that case, the threshold for being an
outlier is 0, which ensures a compatibility with other outlier detection tools such as the
One-Class SVM.
error_norm(comp_cov, norm=’frobenius’, scaling=True, squared=True)
Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius
norm).
Parameterscomp_cov : array-like, shape = [n_features, n_features]
The covariance to compare with.
norm : str
The type of norm used to compute the error. Available error types: - ‘frobenius’ (de-
fault): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error
(comp_cov -self.covariance_).
scaling : bool
If True (default), the squared error norm is divided by n_features. If False, the squared
error norm is not rescaled.
squared : bool
Whether to compute the squared error norm or the error norm. If True (default), the
squared error norm is returned. If False, the error norm is returned.
29.4. sklearn.covariance: Covariance Estimators
1179
scikit-learn user guide, Release 0.18.2
ReturnsThe Mean Squared Error (in the sense of the Frobenius norm) between :
‘self‘ and ‘comp_cov‘ covariance estimators. :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Getter for the precision matrix.
Returnsprecision_ : array-like,
The precision matrix associated to the current covariance object.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given observations.
Parametersobservations : array-like, shape = [n_observations, n_features]
The observations, the Mahalanobis distances of the which we compute. Observations
are assumed to be drawn from the same distribution than the data used in ﬁt.
Returnsmahalanobis_distance : array, shape = [n_observations,]
Squared Mahalanobis distances of the observations.
predict(X)
Outlyingness of observations in X according to the ﬁtted model.
ParametersX : array-like, shape = (n_samples, n_features)
Returnsis_outliers : array, shape = (n_samples, ), dtype = bool
For each observations, tells whether or not it should be considered as an outlier accord-
ing to the ﬁtted model.
threshold : ﬂoat,
The values of the less outlying point’s decision function.
reweight_covariance(data)
Re-weight raw Minimum Covariance Determinant estimates.
Re-weight observations using Rousseeuw’s method (equivalent to deleting outlying observations from the
data set before computing location and covariance estimates). [Rouseeuw1984]
Parametersdata : array-like, shape (n_samples, n_features)
The data matrix, with p features and n samples. The data set must be the one which was
used to compute the raw estimates.
Returnslocation_reweighted : array-like, shape (n_features, )
Re-weighted robust location estimate.
covariance_reweighted : array-like, shape (n_features, n_features)
Re-weighted robust covariance estimate.
1180
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
support_reweighted : array-like, type boolean, shape (n_samples,)
A mask of the observations that have been used to compute the re-weighted robust
location and covariance estimates.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.covariance.EllipticEnvelope
• Outlier detection on a real data set
• Outlier detection with several methods.
29.4.3 sklearn.covariance.GraphLasso
class sklearn.covariance.GraphLasso(alpha=0.01,
mode=’cd’,
tol=0.0001,
enet_tol=0.0001,
max_iter=100, verbose=False, assume_centered=False)
Sparse inverse covariance estimation with an l1-penalized estimator.
Read more in the User Guide.
Parametersalpha : positive ﬂoat, default 0.01
The regularization parameter: the higher alpha, the more regularization, the sparser the
inverse covariance.
mode : {‘cd’, ‘lars’}, default ‘cd’
The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse un-
derlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable.
tol : positive ﬂoat, default 1e-4
The tolerance to declare convergence: if the dual gap goes below this value, iterations
are stopped.
29.4. sklearn.covariance: Covariance Estimators
1181
scikit-learn user guide, Release 0.18.2
enet_tol : positive ﬂoat, optional
The tolerance for the elastic net solver used to calculate the descent direction. This
parameter controls the accuracy of the search direction for a given column update, not
of the overall parameter estimate. Only used for mode=’cd’.
max_iter : integer, default 100
The maximum number of iterations.
verbose : boolean, default False
If verbose is True, the objective function and dual gap are plotted at each iteration.
assume_centered : boolean, default False
If True, data are not centered before computation. Useful when working with data
whose mean is almost, but not exactly zero. If False, data are centered before computa-
tion.
Attributescovariance_ : array-like, shape (n_features, n_features)
Estimated covariance matrix
precision_ : array-like, shape (n_features, n_features)
Estimated pseudo inverse matrix.
n_iter_ : int
Number of iterations run.
See also:
graph_lasso, GraphLassoCV
Methods
error_norm(comp_cov[, norm, scaling, squared])
Computes the Mean Squared Error between two covari-
ance estimators.
fit(X[, y])
get_params([deep])
Get parameters for this estimator.
get_precision()
Getter for the precision matrix.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given
observations.
score(X_test[, y])
Computes the log-likelihood of a Gaussian data set with
self.covariance_ as an estimator of its covariance ma-
trix.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=0.01, mode=’cd’, tol=0.0001, enet_tol=0.0001, max_iter=100, verbose=False, as-
sume_centered=False)
error_norm(comp_cov, norm=’frobenius’, scaling=True, squared=True)
Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius
norm).
Parameterscomp_cov : array-like, shape = [n_features, n_features]
The covariance to compare with.
1182
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
norm : str
The type of norm used to compute the error. Available error types: - ‘frobenius’ (de-
fault): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error
(comp_cov -self.covariance_).
scaling : bool
If True (default), the squared error norm is divided by n_features. If False, the squared
error norm is not rescaled.
squared : bool
Whether to compute the squared error norm or the error norm. If True (default), the
squared error norm is returned. If False, the error norm is returned.
ReturnsThe Mean Squared Error (in the sense of the Frobenius norm) between :
‘self‘ and ‘comp_cov‘ covariance estimators. :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Getter for the precision matrix.
Returnsprecision_ : array-like,
The precision matrix associated to the current covariance object.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given observations.
Parametersobservations : array-like, shape = [n_observations, n_features]
The observations, the Mahalanobis distances of the which we compute. Observations
are assumed to be drawn from the same distribution than the data used in ﬁt.
Returnsmahalanobis_distance : array, shape = [n_observations,]
Squared Mahalanobis distances of the observations.
score(X_test, y=None)
Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance
matrix.
ParametersX_test : array-like, shape = [n_samples, n_features]
Test data of which we compute the likelihood, where n_samples is the number of sam-
ples and n_features is the number of features. X_test is assumed to be drawn from the
same distribution than the data used in ﬁt (including centering).
y : not used, present for API consistence purpose.
Returnsres : ﬂoat
The likelihood of the data set with self.covariance_ as an estimator of its covariance
matrix.
29.4. sklearn.covariance: Covariance Estimators
1183
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.4.4 sklearn.covariance.GraphLassoCV
class sklearn.covariance.GraphLassoCV(alphas=4,
n_reﬁnements=4,
cv=None,
tol=0.0001,
enet_tol=0.0001, max_iter=100, mode=’cd’, n_jobs=1,
verbose=False, assume_centered=False)
Sparse inverse covariance w/ cross-validated choice of the l1 penalty
Read more in the User Guide.
Parametersalphas : integer, or list positive ﬂoat, optional
If an integer is given, it ﬁxes the number of points on the grids of alpha to be used. If
a list is given, it gives the grid to be used. See the notes in the class docstring for more
details.
n_reﬁnements: strictly positive integer :
The number of times the grid is reﬁned. Not used if explicit values of alphas are passed.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
tol : positive ﬂoat, optional
The tolerance to declare convergence: if the dual gap goes below this value, iterations
are stopped.
enet_tol : positive ﬂoat, optional
The tolerance for the elastic net solver used to calculate the descent direction. This
parameter controls the accuracy of the search direction for a given column update, not
of the overall parameter estimate. Only used for mode=’cd’.
max_iter : integer, optional
Maximum number of iterations.
mode: {‘cd’, ‘lars’} :
The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse under-
lying graphs, where number of features is greater than number of samples. Elsewhere
prefer cd which is more numerically stable.
1184
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
n_jobs : int, optional
number of jobs to run in parallel (default 1).
verbose : boolean, optional
If verbose is True, the objective function and duality gap are printed at each iteration.
assume_centered : Boolean
If True, data are not centered before computation. Useful when working with data
whose mean is almost, but not exactly zero. If False, data are centered before computa-
tion.
Attributescovariance_ : numpy.ndarray, shape (n_features, n_features)
Estimated covariance matrix.
precision_ : numpy.ndarray, shape (n_features, n_features)
Estimated precision matrix (inverse covariance).
alpha_ : ﬂoat
Penalization parameter selected.
cv_alphas_ : list of ﬂoat
All penalization parameters explored.
‘grid_scores‘: 2D numpy.ndarray (n_alphas, n_folds) :
Log-likelihood score on left-out data across folds.
n_iter_ : int
Number of iterations run for the optimal alpha.
See also:
graph_lasso, GraphLasso
Notes
The search for the optimal penalization parameter (alpha) is done on an iteratively reﬁned grid: ﬁrst the cross-
validated scores on a grid are computed, then a new reﬁned grid is centered around the maximum, and so on.
One of the challenges which is faced here is that the solvers can fail to converge to a well-conditioned estimate.
The corresponding values of alpha then come out as missing values, but the optimum may be close to these
missing values.
Methods
error_norm(comp_cov[, norm, scaling, squared])
Computes the Mean Squared Error between two covari-
ance estimators.
fit(X[, y])
Fits the GraphLasso covariance model to X.
get_params([deep])
Get parameters for this estimator.
get_precision()
Getter for the precision matrix.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given
observations.
Continued on next page
29.4. sklearn.covariance: Covariance Estimators
1185
scikit-learn user guide, Release 0.18.2
Table 29.26 – continued from previous page
score(X_test[, y])
Computes the log-likelihood of a Gaussian data set with
self.covariance_ as an estimator of its covariance ma-
trix.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alphas=4,
n_reﬁnements=4,
cv=None,
tol=0.0001,
enet_tol=0.0001,
max_iter=100,
mode=’cd’, n_jobs=1, verbose=False, assume_centered=False)
error_norm(comp_cov, norm=’frobenius’, scaling=True, squared=True)
Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius
norm).
Parameterscomp_cov : array-like, shape = [n_features, n_features]
The covariance to compare with.
norm : str
The type of norm used to compute the error. Available error types: - ‘frobenius’ (de-
fault): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error
(comp_cov -self.covariance_).
scaling : bool
If True (default), the squared error norm is divided by n_features. If False, the squared
error norm is not rescaled.
squared : bool
Whether to compute the squared error norm or the error norm. If True (default), the
squared error norm is returned. If False, the error norm is returned.
ReturnsThe Mean Squared Error (in the sense of the Frobenius norm) between :
‘self‘ and ‘comp_cov‘ covariance estimators. :
fit(X, y=None)
Fits the GraphLasso covariance model to X.
ParametersX : ndarray, shape (n_samples, n_features)
Data from which to compute the covariance estimate
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Getter for the precision matrix.
Returnsprecision_ : array-like,
The precision matrix associated to the current covariance object.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given observations.
1186
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Parametersobservations : array-like, shape = [n_observations, n_features]
The observations, the Mahalanobis distances of the which we compute. Observations
are assumed to be drawn from the same distribution than the data used in ﬁt.
Returnsmahalanobis_distance : array, shape = [n_observations,]
Squared Mahalanobis distances of the observations.
score(X_test, y=None)
Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance
matrix.
ParametersX_test : array-like, shape = [n_samples, n_features]
Test data of which we compute the likelihood, where n_samples is the number of sam-
ples and n_features is the number of features. X_test is assumed to be drawn from the
same distribution than the data used in ﬁt (including centering).
y : not used, present for API consistence purpose.
Returnsres : ﬂoat
The likelihood of the data set with self.covariance_ as an estimator of its covariance
matrix.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.covariance.GraphLassoCV
• Visualizing the stock market structure
• Sparse inverse covariance estimation
29.4.5 sklearn.covariance.LedoitWolf
class sklearn.covariance.LedoitWolf(store_precision=True,
assume_centered=False,
block_size=1000)
LedoitWolf Estimator
Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefﬁcient is computed using O. Ledoit and
M. Wolf’s formula as described in “A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices”,
Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.
Read more in the User Guide.
Parametersstore_precision : bool, default=True
Specify if the estimated precision is stored.
assume_centered : bool, default=False
If True, data are not centered before computation. Useful when working with data
whose mean is almost, but not exactly zero. If False (default), data are centered before
computation.
29.4. sklearn.covariance: Covariance Estimators
1187
scikit-learn user guide, Release 0.18.2
block_size : int, default=1000
Size of the blocks into which the covariance matrix will be split during its Ledoit-Wolf
estimation. This is purely a memory optimization and does not affect results.
Attributescovariance_ : array-like, shape (n_features, n_features)
Estimated covariance matrix
precision_ : array-like, shape (n_features, n_features)
Estimated pseudo inverse matrix. (stored only if store_precision is True)
shrinkage_ : ﬂoat, 0 <= shrinkage <= 1
Coefﬁcient in the convex combination used for the computation of the shrunk estimate.
Notes
The regularised covariance is:
(1 - shrinkage)*cov
+ shrinkage*mu*np.identity(n_features)
where mu = trace(cov) / n_features and shrinkage is given by the Ledoit and Wolf formula (see References)
References
“A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices”, Ledoit and Wolf, Journal of Mul-
tivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.
Methods
error_norm(comp_cov[, norm, scaling, squared])
Computes the Mean Squared Error between two covari-
ance estimators.
fit(X[, y])
Fits the Ledoit-Wolf shrunk covariance model accord-
ing to the given training data and parameters.
get_params([deep])
Get parameters for this estimator.
get_precision()
Getter for the precision matrix.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given
observations.
score(X_test[, y])
Computes the log-likelihood of a Gaussian data set with
self.covariance_ as an estimator of its covariance ma-
trix.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(store_precision=True, assume_centered=False, block_size=1000)
error_norm(comp_cov, norm=’frobenius’, scaling=True, squared=True)
Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius
norm).
Parameterscomp_cov : array-like, shape = [n_features, n_features]
The covariance to compare with.
1188
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
norm : str
The type of norm used to compute the error. Available error types: - ‘frobenius’ (de-
fault): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error
(comp_cov -self.covariance_).
scaling : bool
If True (default), the squared error norm is divided by n_features. If False, the squared
error norm is not rescaled.
squared : bool
Whether to compute the squared error norm or the error norm. If True (default), the
squared error norm is returned. If False, the error norm is returned.
ReturnsThe Mean Squared Error (in the sense of the Frobenius norm) between :
‘self‘ and ‘comp_cov‘ covariance estimators. :
fit(X, y=None)
Fits the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.
ParametersX : array-like, shape = [n_samples, n_features]
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : not used, present for API consistence purpose.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Getter for the precision matrix.
Returnsprecision_ : array-like,
The precision matrix associated to the current covariance object.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given observations.
Parametersobservations : array-like, shape = [n_observations, n_features]
The observations, the Mahalanobis distances of the which we compute. Observations
are assumed to be drawn from the same distribution than the data used in ﬁt.
Returnsmahalanobis_distance : array, shape = [n_observations,]
Squared Mahalanobis distances of the observations.
29.4. sklearn.covariance: Covariance Estimators
1189
scikit-learn user guide, Release 0.18.2
score(X_test, y=None)
Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance
matrix.
ParametersX_test : array-like, shape = [n_samples, n_features]
Test data of which we compute the likelihood, where n_samples is the number of sam-
ples and n_features is the number of features. X_test is assumed to be drawn from the
same distribution than the data used in ﬁt (including centering).
y : not used, present for API consistence purpose.
Returnsres : ﬂoat
The likelihood of the data set with self.covariance_ as an estimator of its covariance
matrix.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.covariance.LedoitWolf
• Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood
• Ledoit-Wolf vs OAS estimation
• Model selection with Probabilistic PCA and Factor Analysis (FA)
29.4.6 sklearn.covariance.MinCovDet
class sklearn.covariance.MinCovDet(store_precision=True,
assume_centered=False,
sup-
port_fraction=None, random_state=None)
Minimum Covariance Determinant (MCD): robust estimator of covariance.
The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but
could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with
multi-modal data (the algorithm used to ﬁt a MinCovDet object is likely to fail in such a case). One should
consider projection pursuit methods to deal with multi-modal datasets.
Read more in the User Guide.
Parametersstore_precision : bool
Specify if the estimated precision is stored.
assume_centered : Boolean
If True, the support of the robust location and the covariance estimates is computed, and
a covariance estimate is recomputed from it, without centering the data. Useful to work
with data whose mean is signiﬁcantly equal to zero but is not exactly zero. If False,
the robust location and covariance are directly computed with the FastMCD algorithm
without additional treatment.
support_fraction : ﬂoat, 0 < support_fraction < 1
1190
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The proportion of points to be included in the support of the raw MCD estimate. Default
is None, which implies that the minimum value of support_fraction will be used within
the algorithm: [n_sample + n_features + 1] / 2
random_state : integer or numpy.RandomState, optional
The random generator used. If an integer is given, it ﬁxes the seed. Defaults to the
global numpy random number generator.
Attributesraw_location_ : array-like, shape (n_features,)
The raw robust estimated location before correction and re-weighting.
raw_covariance_ : array-like, shape (n_features, n_features)
The raw robust estimated covariance before correction and re-weighting.
raw_support_ : array-like, shape (n_samples,)
A mask of the observations that have been used to compute the raw robust estimates of
location and shape, before correction and re-weighting.
location_ : array-like, shape (n_features,)
Estimated robust location
covariance_ : array-like, shape (n_features, n_features)
Estimated robust covariance matrix
precision_ : array-like, shape (n_features, n_features)
Estimated pseudo inverse matrix. (stored only if store_precision is True)
support_ : array-like, shape (n_samples,)
A mask of the observations that have been used to compute the robust estimates of
location and shape.
dist_ : array-like, shape (n_samples,)
Mahalanobis distances of the training set (on which ﬁt is called) observations.
References
[Rouseeuw1984], [Rouseeuw1999], [Butler1993]
Methods
correct_covariance(data)
Apply a correction to raw Minimum Covariance Deter-
minant estimates.
error_norm(comp_cov[, norm, scaling, squared])
Computes the Mean Squared Error between two covari-
ance estimators.
fit(X[, y])
Fits a Minimum Covariance Determinant with the
FastMCD algorithm.
get_params([deep])
Get parameters for this estimator.
get_precision()
Getter for the precision matrix.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given
observations.
Continued on next page
29.4. sklearn.covariance: Covariance Estimators
1191
scikit-learn user guide, Release 0.18.2
Table 29.28 – continued from previous page
reweight_covariance(data)
Re-weight raw Minimum Covariance Determinant esti-
mates.
score(X_test[, y])
Computes the log-likelihood of a Gaussian data set with
self.covariance_ as an estimator of its covariance ma-
trix.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(store_precision=True,
assume_centered=False,
support_fraction=None,
ran-
dom_state=None)
correct_covariance(data)
Apply a correction to raw Minimum Covariance Determinant estimates.
Correction using the empirical correction factor suggested by Rousseeuw and Van Driessen in
[Rouseeuw1984].
Parametersdata : array-like, shape (n_samples, n_features)
The data matrix, with p features and n samples. The data set must be the one which was
used to compute the raw estimates.
Returnscovariance_corrected : array-like, shape (n_features, n_features)
Corrected robust covariance estimate.
error_norm(comp_cov, norm=’frobenius’, scaling=True, squared=True)
Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius
norm).
Parameterscomp_cov : array-like, shape = [n_features, n_features]
The covariance to compare with.
norm : str
The type of norm used to compute the error. Available error types: - ‘frobenius’ (de-
fault): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error
(comp_cov -self.covariance_).
scaling : bool
If True (default), the squared error norm is divided by n_features. If False, the squared
error norm is not rescaled.
squared : bool
Whether to compute the squared error norm or the error norm. If True (default), the
squared error norm is returned. If False, the error norm is returned.
ReturnsThe Mean Squared Error (in the sense of the Frobenius norm) between :
‘self‘ and ‘comp_cov‘ covariance estimators. :
fit(X, y=None)
Fits a Minimum Covariance Determinant with the FastMCD algorithm.
ParametersX : array-like, shape = [n_samples, n_features]
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : not used, present for API consistence purpose.
1192
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Getter for the precision matrix.
Returnsprecision_ : array-like,
The precision matrix associated to the current covariance object.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given observations.
Parametersobservations : array-like, shape = [n_observations, n_features]
The observations, the Mahalanobis distances of the which we compute. Observations
are assumed to be drawn from the same distribution than the data used in ﬁt.
Returnsmahalanobis_distance : array, shape = [n_observations,]
Squared Mahalanobis distances of the observations.
reweight_covariance(data)
Re-weight raw Minimum Covariance Determinant estimates.
Re-weight observations using Rousseeuw’s method (equivalent to deleting outlying observations from the
data set before computing location and covariance estimates). [Rouseeuw1984]
Parametersdata : array-like, shape (n_samples, n_features)
The data matrix, with p features and n samples. The data set must be the one which was
used to compute the raw estimates.
Returnslocation_reweighted : array-like, shape (n_features, )
Re-weighted robust location estimate.
covariance_reweighted : array-like, shape (n_features, n_features)
Re-weighted robust covariance estimate.
support_reweighted : array-like, type boolean, shape (n_samples,)
A mask of the observations that have been used to compute the re-weighted robust
location and covariance estimates.
score(X_test, y=None)
Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance
matrix.
ParametersX_test : array-like, shape = [n_samples, n_features]
29.4. sklearn.covariance: Covariance Estimators
1193
scikit-learn user guide, Release 0.18.2
Test data of which we compute the likelihood, where n_samples is the number of sam-
ples and n_features is the number of features. X_test is assumed to be drawn from the
same distribution than the data used in ﬁt (including centering).
y : not used, present for API consistence purpose.
Returnsres : ﬂoat
The likelihood of the data set with self.covariance_ as an estimator of its covariance
matrix.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.covariance.MinCovDet
• Robust covariance estimation and Mahalanobis distances relevance
• Robust vs Empirical covariance estimate
29.4.7 sklearn.covariance.OAS
class sklearn.covariance.OAS(store_precision=True, assume_centered=False)
Oracle Approximating Shrinkage Estimator
Read more in the User Guide.
OAS is a particular form of shrinkage described in “Shrinkage Algorithms for MMSE Covariance Estimation”
Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.
The formula used here does not correspond to the one given in the article. It has been taken from the Matlab
program available from the authors’ webpage (http://tbayes.eecs.umich.edu/yilun/covestimation). In the original
article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, this
operation is omitted in the author’s MATLAB program because for a large p, the value of 2/p is so small that it
doesn’t affect the value of the estimator.
Parametersstore_precision : bool, default=True
Specify if the estimated precision is stored.
assume_centered: bool, default=False :
If True, data are not centered before computation. Useful when working with data
whose mean is almost, but not exactly zero. If False (default), data are centered before
computation.
Attributescovariance_ : array-like, shape (n_features, n_features)
Estimated covariance matrix.
precision_ : array-like, shape (n_features, n_features)
Estimated pseudo inverse matrix. (stored only if store_precision is True)
shrinkage_ : ﬂoat, 0 <= shrinkage <= 1
1194
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
coefﬁcient in the convex combination used for the computation of the shrunk estimate.
Notes
The regularised covariance is:
(1 - shrinkage)*cov
+ shrinkage*mu*np.identity(n_features)
where mu = trace(cov) / n_features and shrinkage is given by the OAS formula (see References)
References
“Shrinkage Algorithms for MMSE Covariance Estimation” Chen et al., IEEE Trans. on Sign. Proc., Volume 58,
Issue 10, October 2010.
Methods
error_norm(comp_cov[, norm, scaling, squared])
Computes the Mean Squared Error between two covari-
ance estimators.
fit(X[, y])
Fits the Oracle Approximating Shrinkage covariance
model according to the given training data and parame-
ters.
get_params([deep])
Get parameters for this estimator.
get_precision()
Getter for the precision matrix.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given
observations.
score(X_test[, y])
Computes the log-likelihood of a Gaussian data set with
self.covariance_ as an estimator of its covariance ma-
trix.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(store_precision=True, assume_centered=False)
error_norm(comp_cov, norm=’frobenius’, scaling=True, squared=True)
Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius
norm).
Parameterscomp_cov : array-like, shape = [n_features, n_features]
The covariance to compare with.
norm : str
The type of norm used to compute the error. Available error types: - ‘frobenius’ (de-
fault): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error
(comp_cov -self.covariance_).
scaling : bool
If True (default), the squared error norm is divided by n_features. If False, the squared
error norm is not rescaled.
squared : bool
29.4. sklearn.covariance: Covariance Estimators
1195
scikit-learn user guide, Release 0.18.2
Whether to compute the squared error norm or the error norm. If True (default), the
squared error norm is returned. If False, the error norm is returned.
ReturnsThe Mean Squared Error (in the sense of the Frobenius norm) between :
‘self‘ and ‘comp_cov‘ covariance estimators. :
fit(X, y=None)
Fits the Oracle Approximating Shrinkage covariance model according to the given training data and pa-
rameters.
ParametersX : array-like, shape = [n_samples, n_features]
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : not used, present for API consistence purpose.
Returnsself: object :
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Getter for the precision matrix.
Returnsprecision_ : array-like,
The precision matrix associated to the current covariance object.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given observations.
Parametersobservations : array-like, shape = [n_observations, n_features]
The observations, the Mahalanobis distances of the which we compute. Observations
are assumed to be drawn from the same distribution than the data used in ﬁt.
Returnsmahalanobis_distance : array, shape = [n_observations,]
Squared Mahalanobis distances of the observations.
score(X_test, y=None)
Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance
matrix.
ParametersX_test : array-like, shape = [n_samples, n_features]
Test data of which we compute the likelihood, where n_samples is the number of sam-
ples and n_features is the number of features. X_test is assumed to be drawn from the
same distribution than the data used in ﬁt (including centering).
y : not used, present for API consistence purpose.
Returnsres : ﬂoat
1196
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The likelihood of the data set with self.covariance_ as an estimator of its covariance
matrix.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.covariance.OAS
• Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood
• Ledoit-Wolf vs OAS estimation
29.4.8 sklearn.covariance.ShrunkCovariance
class sklearn.covariance.ShrunkCovariance(store_precision=True,
assume_centered=False,
shrinkage=0.1)
Covariance estimator with shrinkage
Read more in the User Guide.
Parametersstore_precision : boolean, default True
Specify if the estimated precision is stored
shrinkage : ﬂoat, 0 <= shrinkage <= 1, default 0.1
Coefﬁcient in the convex combination used for the computation of the shrunk estimate.
assume_centered : boolean, default False
If True, data are not centered before computation. Useful when working with data
whose mean is almost, but not exactly zero. If False, data are centered before computa-
tion.
Attributescovariance_ : array-like, shape (n_features, n_features)
Estimated covariance matrix
precision_ : array-like, shape (n_features, n_features)
Estimated pseudo inverse matrix. (stored only if store_precision is True)
‘shrinkage‘ : ﬂoat, 0 <= shrinkage <= 1
Coefﬁcient in the convex combination used for the computation of the shrunk estimate.
Notes
The regularized covariance is given by
(1 - shrinkage)*cov
•shrinkage*mu*np.identity(n_features)
where mu = trace(cov) / n_features
29.4. sklearn.covariance: Covariance Estimators
1197
scikit-learn user guide, Release 0.18.2
Methods
error_norm(comp_cov[, norm, scaling, squared])
Computes the Mean Squared Error between two covari-
ance estimators.
fit(X[, y])
Fits the shrunk covariance model according to the given
training data and parameters.
get_params([deep])
Get parameters for this estimator.
get_precision()
Getter for the precision matrix.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given
observations.
score(X_test[, y])
Computes the log-likelihood of a Gaussian data set with
self.covariance_ as an estimator of its covariance ma-
trix.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(store_precision=True, assume_centered=False, shrinkage=0.1)
error_norm(comp_cov, norm=’frobenius’, scaling=True, squared=True)
Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius
norm).
Parameterscomp_cov : array-like, shape = [n_features, n_features]
The covariance to compare with.
norm : str
The type of norm used to compute the error. Available error types: - ‘frobenius’ (de-
fault): sqrt(tr(A^t.A)) - ‘spectral’: sqrt(max(eigenvalues(A^t.A)) where A is the error
(comp_cov -self.covariance_).
scaling : bool
If True (default), the squared error norm is divided by n_features. If False, the squared
error norm is not rescaled.
squared : bool
Whether to compute the squared error norm or the error norm. If True (default), the
squared error norm is returned. If False, the error norm is returned.
ReturnsThe Mean Squared Error (in the sense of the Frobenius norm) between :
‘self‘ and ‘comp_cov‘ covariance estimators. :
fit(X, y=None)
Fits the shrunk covariance model according to the given training data and parameters.
ParametersX : array-like, shape = [n_samples, n_features]
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : not used, present for API consistence purpose.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
1198
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Getter for the precision matrix.
Returnsprecision_ : array-like,
The precision matrix associated to the current covariance object.
mahalanobis(observations)
Computes the squared Mahalanobis distances of given observations.
Parametersobservations : array-like, shape = [n_observations, n_features]
The observations, the Mahalanobis distances of the which we compute. Observations
are assumed to be drawn from the same distribution than the data used in ﬁt.
Returnsmahalanobis_distance : array, shape = [n_observations,]
Squared Mahalanobis distances of the observations.
score(X_test, y=None)
Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance
matrix.
ParametersX_test : array-like, shape = [n_samples, n_features]
Test data of which we compute the likelihood, where n_samples is the number of sam-
ples and n_features is the number of features. X_test is assumed to be drawn from the
same distribution than the data used in ﬁt (including centering).
y : not used, present for API consistence purpose.
Returnsres : ﬂoat
The likelihood of the data set with self.covariance_ as an estimator of its covariance
matrix.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.covariance.ShrunkCovariance
• Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood
• Model selection with Probabilistic PCA and Factor Analysis (FA)
covariance.empirical_covariance(X[, ...])
Computes the Maximum likelihood covariance estimator
covariance.ledoit_wolf(X[,
assume_centered,
...])
Estimates the shrunk Ledoit-Wolf covariance matrix.
Continued on next page
29.4. sklearn.covariance: Covariance Estimators
1199
scikit-learn user guide, Release 0.18.2
Table 29.31 – continued from previous page
covariance.shrunk_covariance(emp_cov[, ...])
Calculates a covariance matrix shrunk on the diagonal
covariance.oas(X[, assume_centered])
Estimate covariance with the Oracle Approximating
Shrinkage algorithm.
covariance.graph_lasso(emp_cov, alpha[, ...])
l1-penalized covariance estimator
29.4.9 sklearn.covariance.empirical_covariance
sklearn.covariance.empirical_covariance(X, assume_centered=False)
Computes the Maximum likelihood covariance estimator
ParametersX : ndarray, shape (n_samples, n_features)
Data from which to compute the covariance estimate
assume_centered : Boolean
If True, data are not centered before computation. Useful when working with data
whose mean is almost, but not exactly zero. If False, data are centered before computa-
tion.
Returnscovariance : 2D ndarray, shape (n_features, n_features)
Empirical covariance (Maximum Likelihood Estimator).
Examples using sklearn.covariance.empirical_covariance
• Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood
29.4.10 sklearn.covariance.ledoit_wolf
sklearn.covariance.ledoit_wolf(X, assume_centered=False, block_size=1000)
Estimates the shrunk Ledoit-Wolf covariance matrix.
Read more in the User Guide.
ParametersX : array-like, shape (n_samples, n_features)
Data from which to compute the covariance estimate
assume_centered : boolean, default=False
If True, data are not centered before computation. Useful to work with data whose mean
is signiﬁcantly equal to zero but is not exactly zero. If False, data are centered before
computation.
block_size : int, default=1000
Size of the blocks into which the covariance matrix will be split. This is purely a mem-
ory optimization and does not affect results.
Returnsshrunk_cov : array-like, shape (n_features, n_features)
Shrunk covariance.
shrinkage : ﬂoat
Coefﬁcient in the convex combination used for the computation of the shrunk estimate.
1200
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
The regularized (shrunk) covariance is:
(1 - shrinkage)*cov
•shrinkage * mu * np.identity(n_features)
where mu = trace(cov) / n_features
Examples using sklearn.covariance.ledoit_wolf
• Sparse inverse covariance estimation
29.4.11 sklearn.covariance.shrunk_covariance
sklearn.covariance.shrunk_covariance(emp_cov, shrinkage=0.1)
Calculates a covariance matrix shrunk on the diagonal
Read more in the User Guide.
Parametersemp_cov : array-like, shape (n_features, n_features)
Covariance matrix to be shrunk
shrinkage : ﬂoat, 0 <= shrinkage <= 1
Coefﬁcient in the convex combination used for the computation of the shrunk estimate.
Returnsshrunk_cov : array-like
Shrunk covariance.
Notes
The regularized (shrunk) covariance is given by
(1 - shrinkage)*cov
•shrinkage*mu*np.identity(n_features)
where mu = trace(cov) / n_features
29.4.12 sklearn.covariance.oas
sklearn.covariance.oas(X, assume_centered=False)
Estimate covariance with the Oracle Approximating Shrinkage algorithm.
ParametersX : array-like, shape (n_samples, n_features)
Data from which to compute the covariance estimate.
assume_centered : boolean
If True, data are not centered before computation. Useful to work with data whose mean
is signiﬁcantly equal to zero but is not exactly zero. If False, data are centered before
computation.
Returnsshrunk_cov : array-like, shape (n_features, n_features)
29.4. sklearn.covariance: Covariance Estimators
1201
scikit-learn user guide, Release 0.18.2
Shrunk covariance.
shrinkage : ﬂoat
Coefﬁcient in the convex combination used for the computation of the shrunk estimate.
Notes
The regularised (shrunk) covariance is:
(1 - shrinkage)*cov
•shrinkage * mu * np.identity(n_features)
where mu = trace(cov) / n_features
The formula we used to implement the OAS does not correspond to the one given in the article. It has been
taken from the MATLAB program available from the author’s webpage (http://tbayes.eecs.umich.edu/yilun/
covestimation).
29.4.13 sklearn.covariance.graph_lasso
sklearn.covariance.graph_lasso(emp_cov,
alpha,
cov_init=None,
mode=’cd’,
tol=0.0001,
enet_tol=0.0001,
max_iter=100,
verbose=False,
re-
turn_costs=False,
eps=2.2204460492503131e-16,
re-
turn_n_iter=False)
l1-penalized covariance estimator
Read more in the User Guide.
Parametersemp_cov : 2D ndarray, shape (n_features, n_features)
Empirical covariance from which to compute the covariance estimate.
alpha : positive ﬂoat
The regularization parameter: the higher alpha, the more regularization, the sparser the
inverse covariance.
cov_init : 2D array (n_features, n_features), optional
The initial guess for the covariance.
mode : {‘cd’, ‘lars’}
The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse un-
derlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable.
tol : positive ﬂoat, optional
The tolerance to declare convergence: if the dual gap goes below this value, iterations
are stopped.
enet_tol : positive ﬂoat, optional
The tolerance for the elastic net solver used to calculate the descent direction. This
parameter controls the accuracy of the search direction for a given column update, not
of the overall parameter estimate. Only used for mode=’cd’.
max_iter : integer, optional
The maximum number of iterations.
1202
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
verbose : boolean, optional
If verbose is True, the objective function and dual gap are printed at each iteration.
return_costs : boolean, optional
If return_costs is True, the objective function and dual gap at each iteration are returned.
eps : ﬂoat, optional
The machine-precision regularization in the computation of the Cholesky diagonal fac-
tors. Increase this for very ill-conditioned systems.
return_n_iter : bool, optional
Whether or not to return the number of iterations.
Returnscovariance : 2D ndarray, shape (n_features, n_features)
The estimated covariance matrix.
precision : 2D ndarray, shape (n_features, n_features)
The estimated (sparse) precision matrix.
costs : list of (objective, dual_gap) pairs
The list of values of the objective function and the dual gap at each iteration. Returned
only if return_costs is True.
n_iter : int
Number of iterations. Returned only if return_n_iter is set to True.
See also:
GraphLasso, GraphLassoCV
Notes
The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics
paper. It is the same algorithm as in the R glasso package.
One possible difference with the glasso R package is that the diagonal coefﬁcients are not penalized.
29.5 sklearn.model_selection: Model Selection
User guide: See the Cross-validation: evaluating estimator performance, Tuning the hyper-parameters of an estima-
tor and Learning curve sections for further details.
29.5.1 Splitter Classes
model_selection.KFold([n_splits, shufﬂe, ...])
K-Folds cross-validator
model_selection.GroupKFold([n_splits])
K-fold iterator variant with non-overlapping groups.
model_selection.StratifiedKFold([n_splits,
...])
Stratiﬁed K-Folds cross-validator
model_selection.LeaveOneGroupOut()
Leave One Group Out cross-validator
Continued on next page
29.5. sklearn.model_selection: Model Selection
1203
scikit-learn user guide, Release 0.18.2
Table 29.32 – continued from previous page
model_selection.LeavePGroupsOut(n_groups)
Leave P Group(s) Out cross-validator
model_selection.LeaveOneOut()
Leave-One-Out cross-validator
model_selection.LeavePOut(p)
Leave-P-Out cross-validator
model_selection.ShuffleSplit([n_splits, ...])
Random permutation cross-validator
model_selection.GroupShuffleSplit([...])
Shufﬂe-Group(s)-Out cross-validation iterator
model_selection.StratifiedShuffleSplit([...])Stratiﬁed ShufﬂeSplit cross-validator
model_selection.PredefinedSplit(test_fold)
Predeﬁned split cross-validator
model_selection.TimeSeriesSplit([n_splits])
Time Series cross-validator
sklearn.model_selection.KFold
class sklearn.model_selection.KFold(n_splits=3, shufﬂe=False, random_state=None)
K-Folds cross-validator
Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shufﬂing
by default).
Each fold is then used once as a validation while the k - 1 remaining folds form the training set.
Read more in the User Guide.
Parametersn_splits : int, default=3
Number of folds. Must be at least 2.
shufﬂe : boolean, optional
Whether to shufﬂe the data before splitting into batches.
random_state : None, int or RandomState
When shufﬂe=True, pseudo-random number generator state used for shufﬂing. If None,
use default numpy RNG for shufﬂing.
See also:
StratifiedKFoldTakes group information into account to avoid building folds with imbalanced class dis-
tributions (for binary or multiclass classiﬁcation tasks).
GroupKFoldK-fold iterator variant with non-overlapping groups.
Notes
The ﬁrst n_samples % n_splits folds have size n_samples // n_splits + 1, other folds have
size n_samples // n_splits, where n_samples is the number of samples.
Examples
>>> from sklearn.model_selection import KFold
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([1, 2, 3, 4])
>>> kf = KFold(n_splits=2)
>>> kf.get_n_splits(X)
2
>>> print(kf)
KFold(n_splits=2, random_state=None, shuffle=False)
1204
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> for train_index, test_index in kf.split(X):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [2 3] TEST: [0 1]
TRAIN: [0 1] TEST: [2 3]
Methods
get_n_splits([X, y, groups])
Returns the number of splitting iterations in the cross-
validator
split(X[, y, groups])
Generate indices to split data into training and test set.
__init__(n_splits=3, shufﬂe=False, random_state=None)
get_n_splits(X=None, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples,)
The target variable for supervised learning problems.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
29.5. sklearn.model_selection: Model Selection
1205
scikit-learn user guide, Release 0.18.2
Examples using sklearn.model_selection.KFold
• Feature agglomeration vs. univariate selection
• Gradient Boosting Out-of-Bag estimates
• Cross-validation on diabetes Dataset Exercise
• Nested versus non-nested cross-validation
sklearn.model_selection.GroupKFold
class sklearn.model_selection.GroupKFold(n_splits=3)
K-fold iterator variant with non-overlapping groups.
The same group will not appear in two different folds (the number of distinct groups has to be at least equal to
the number of folds).
The folds are approximately balanced in the sense that the number of distinct groups is approximately the same
in each fold.
Parametersn_splits : int, default=3
Number of folds. Must be at least 2.
See also:
LeaveOneGroupOutFor splitting the data according to explicit domain-speciﬁc stratiﬁcation of the dataset.
Examples
>>> from sklearn.model_selection import GroupKFold
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
>>> y = np.array([1, 2, 3, 4])
>>> groups = np.array([0, 0, 2, 2])
>>> group_kfold = GroupKFold(n_splits=2)
>>> group_kfold.get_n_splits(X, y, groups)
2
>>> print(group_kfold)
GroupKFold(n_splits=2)
>>> for train_index, test_index in group_kfold.split(X, y, groups):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
...
print(X_train, X_test, y_train, y_test)
...
TRAIN: [0 1] TEST: [2 3]
[[1 2]
[3 4]] [[5 6]
[7 8]] [1 2] [3 4]
TRAIN: [2 3] TEST: [0 1]
[[5 6]
[7 8]] [[1 2]
[3 4]] [3 4] [1 2]
1206
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Methods
get_n_splits([X, y, groups])
Returns the number of splitting iterations in the cross-
validator
split(X[, y, groups])
Generate indices to split data into training and test set.
__init__(n_splits=3)
get_n_splits(X=None, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples,)
The target variable for supervised learning problems.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
sklearn.model_selection.StratiﬁedKFold
class sklearn.model_selection.StratifiedKFold(n_splits=3,
shufﬂe=False,
ran-
dom_state=None)
Stratiﬁed K-Folds cross-validator
Provides train/test indices to split data in train/test sets.
This cross-validation object is a variation of KFold that returns stratiﬁed folds. The folds are made by preserving
the percentage of samples for each class.
Read more in the User Guide.
29.5. sklearn.model_selection: Model Selection
1207
scikit-learn user guide, Release 0.18.2
Parametersn_splits : int, default=3
Number of folds. Must be at least 2.
shufﬂe : boolean, optional
Whether to shufﬂe each stratiﬁcation of the data before splitting into batches.
random_state : None, int or RandomState
When shufﬂe=True, pseudo-random number generator state used for shufﬂing. If None,
use default numpy RNG for shufﬂing.
Notes
All the folds have size trunc(n_samples / n_splits), the last one has the complementary.
Examples
>>> from sklearn.model_selection import StratifiedKFold
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
>>> skf = StratifiedKFold(n_splits=2)
>>> skf.get_n_splits(X, y)
2
>>> print(skf)
StratifiedKFold(n_splits=2, random_state=None, shuffle=False)
>>> for train_index, test_index in skf.split(X, y):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [1 3] TEST: [0 2]
TRAIN: [0 2] TEST: [1 3]
Methods
get_n_splits([X, y, groups])
Returns the number of splitting iterations in the cross-
validator
split(X, y[, groups])
Generate indices to split data into training and test set.
__init__(n_splits=3, shufﬂe=False, random_state=None)
get_n_splits(X=None, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
1208
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X, y, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
Note
that
providing
y
is
sufﬁcient
to
generate
the
splits
and
hence
np.zeros(n_samples) may be used as a placeholder for X instead of actual
training data.
y : array-like, shape (n_samples,)
The target variable for supervised learning problems. Stratiﬁcation is done based on the
y labels.
groups : object
Always ignored, exists for compatibility.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
Examples using sklearn.model_selection.StratifiedKFold
• Test with permutations the signiﬁcance of a classiﬁcation score
• Recursive feature elimination with cross-validation
• GMM covariances
• Receiver Operating Characteristic (ROC) with cross validation
sklearn.model_selection.LeaveOneGroupOut
class sklearn.model_selection.LeaveOneGroupOut
Leave One Group Out cross-validator
Provides train/test indices to split data according to a third-party provided group. This group information can be
used to encode arbitrary domain speciﬁc stratiﬁcations of the samples as integers.
For instance the groups could be the year of collection of the samples and thus allow for cross-validation against
time-based splits.
Read more in the User Guide.
Examples
29.5. sklearn.model_selection: Model Selection
1209
scikit-learn user guide, Release 0.18.2
>>> from sklearn.model_selection import LeaveOneGroupOut
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
>>> y = np.array([1, 2, 1, 2])
>>> groups = np.array([1, 1, 2, 2])
>>> logo = LeaveOneGroupOut()
>>> logo.get_n_splits(X, y, groups)
2
>>> print(logo)
LeaveOneGroupOut()
>>> for train_index, test_index in logo.split(X, y, groups):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
...
print(X_train, X_test, y_train, y_test)
TRAIN: [2 3] TEST: [0 1]
[[5 6]
[7 8]] [[1 2]
[3 4]] [1 2] [1 2]
TRAIN: [0 1] TEST: [2 3]
[[1 2]
[3 4]] [[5 6]
[7 8]] [1 2] [1 2]
Methods
get_n_splits(X, y, groups)
Returns the number of splitting iterations in the cross-
validator
split(X[, y, groups])
Generate indices to split data into training and test set.
__init__()
get_n_splits(X, y, groups)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, of length n_samples
1210
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The target variable for supervised learning problems.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
sklearn.model_selection.LeavePGroupsOut
class sklearn.model_selection.LeavePGroupsOut(n_groups)
Leave P Group(s) Out cross-validator
Provides train/test indices to split data according to a third-party provided group. This group information can be
used to encode arbitrary domain speciﬁc stratiﬁcations of the samples as integers.
For instance the groups could be the year of collection of the samples and thus allow for cross-validation against
time-based splits.
The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with
all the samples assigned to p different values of the groups while the latter uses samples all assigned the same
groups.
Read more in the User Guide.
Parametersn_groups : int
Number of groups (p) to leave out in the test split.
See also:
GroupKFoldK-fold iterator variant with non-overlapping groups.
Examples
>>> from sklearn.model_selection import LeavePGroupsOut
>>> X = np.array([[1, 2], [3, 4], [5, 6]])
>>> y = np.array([1, 2, 1])
>>> groups = np.array([1, 2, 3])
>>> lpgo = LeavePGroupsOut(n_groups=2)
>>> lpgo.get_n_splits(X, y, groups)
3
>>> print(lpgo)
LeavePGroupsOut(n_groups=2)
>>> for train_index, test_index in lpgo.split(X, y, groups):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
...
print(X_train, X_test, y_train, y_test)
TRAIN: [2] TEST: [0 1]
[[5 6]] [[1 2]
[3 4]] [1] [1 2]
TRAIN: [1] TEST: [0 2]
[[3 4]] [[1 2]
29.5. sklearn.model_selection: Model Selection
1211
scikit-learn user guide, Release 0.18.2
[5 6]] [2] [1 1]
TRAIN: [0] TEST: [1 2]
[[1 2]] [[3 4]
[5 6]] [1] [2 1]
Methods
get_n_splits(X, y, groups)
Returns the number of splitting iterations in the cross-
validator
split(X[, y, groups])
Generate indices to split data into training and test set.
__init__(n_groups)
get_n_splits(X, y, groups)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
Always ignored, exists for compatibility. np.zeros(n_samples) may be used as
a placeholder.
y : object
Always ignored, exists for compatibility. np.zeros(n_samples) may be used as
a placeholder.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, of length n_samples
The target variable for supervised learning problems.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
1212
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.model_selection.LeaveOneOut
class sklearn.model_selection.LeaveOneOut
Leave-One-Out cross-validator
Provides train/test indices to split data in train/test sets. Each sample is used once as a test set (singleton) while
the remaining samples form the training set.
Note: LeaveOneOut() is equivalent to KFold(n_splits=n) and LeavePOut(p=1) where n is the
number of samples.
Due to the high number of test sets (which is the same as the number of samples) this cross-validation method
can be very costly. For large datasets one should favor KFold, ShuffleSplit or StratifiedKFold.
Read more in the User Guide.
See also:
LeaveOneGroupOutFor splitting the data according to explicit, domain-speciﬁc stratiﬁcation of the dataset.
GroupKFoldK-fold iterator variant with non-overlapping groups.
Examples
>>> from sklearn.model_selection import LeaveOneOut
>>> X = np.array([[1, 2], [3, 4]])
>>> y = np.array([1, 2])
>>> loo = LeaveOneOut()
>>> loo.get_n_splits(X)
2
>>> print(loo)
LeaveOneOut()
>>> for train_index, test_index in loo.split(X):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
...
print(X_train, X_test, y_train, y_test)
TRAIN: [1] TEST: [0]
[[3 4]] [[1 2]] [2] [1]
TRAIN: [0] TEST: [1]
[[1 2]] [[3 4]] [1] [2]
Methods
get_n_splits(X[, y, groups])
Returns the number of splitting iterations in the cross-
validator
split(X[, y, groups])
Generate indices to split data into training and test set.
__init__()
get_n_splits(X, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
29.5. sklearn.model_selection: Model Selection
1213
scikit-learn user guide, Release 0.18.2
of features.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, of length n_samples
The target variable for supervised learning problems.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
sklearn.model_selection.LeavePOut
class sklearn.model_selection.LeavePOut(p)
Leave-P-Out cross-validator
Provides train/test indices to split data in train/test sets. This results in testing on all distinct samples of size p,
while the remaining n - p samples form the training set in each iteration.
Note: LeavePOut(p) is NOT equivalent to KFold(n_splits=n_samples // p) which creates non-
overlapping test sets.
Due to the high number of iterations which grows combinatorically with the number of samples this cross-
validation method can be very costly. For large datasets one should favor KFold, StratifiedKFold or
ShuffleSplit.
Read more in the User Guide.
Parametersp : int
Size of the test sets.
Examples
1214
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> from sklearn.model_selection import LeavePOut
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
>>> y = np.array([1, 2, 3, 4])
>>> lpo = LeavePOut(2)
>>> lpo.get_n_splits(X)
6
>>> print(lpo)
LeavePOut(p=2)
>>> for train_index, test_index in lpo.split(X):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [2 3] TEST: [0 1]
TRAIN: [1 3] TEST: [0 2]
TRAIN: [1 2] TEST: [0 3]
TRAIN: [0 3] TEST: [1 2]
TRAIN: [0 2] TEST: [1 3]
TRAIN: [0 1] TEST: [2 3]
Methods
get_n_splits(X[, y, groups])
Returns the number of splitting iterations in the cross-
validator
split(X[, y, groups])
Generate indices to split data into training and test set.
__init__(p)
get_n_splits(X, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
split(X, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, of length n_samples
The target variable for supervised learning problems.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
29.5. sklearn.model_selection: Model Selection
1215
scikit-learn user guide, Release 0.18.2
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
sklearn.model_selection.ShufﬂeSplit
class sklearn.model_selection.ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, ran-
dom_state=None)
Random permutation cross-validator
Yields indices to split data into training and test sets.
Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different,
although this is still very likely for sizeable datasets.
Read more in the User Guide.
Parametersn_splits : int (default 10)
Number of re-shufﬂing & splitting iterations.
test_size : ﬂoat, int, or None, default 0.1
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the test split. If int, represents the absolute number of test samples. If None,
the value is automatically set to the complement of the train size.
train_size : ﬂoat, int, or None (default is None)
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the train split. If int, represents the absolute number of train samples. If
None, the value is automatically set to the complement of the test size.
random_state : int or RandomState
Pseudo-random number generator state used for random sampling.
Examples
>>> from sklearn.model_selection import ShuffleSplit
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
>>> y = np.array([1, 2, 1, 2])
>>> rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)
>>> rs.get_n_splits(X)
3
>>> print(rs)
ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=None)
>>> for train_index, test_index in rs.split(X):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
TRAIN: [3 1 0] TEST: [2]
TRAIN: [2 1 3] TEST: [0]
TRAIN: [0 2 1] TEST: [3]
>>> rs = ShuffleSplit(n_splits=3, train_size=0.5, test_size=.25,
...
random_state=0)
>>> for train_index, test_index in rs.split(X):
1216
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
...
print("TRAIN:", train_index, "TEST:", test_index)
...
TRAIN: [3 1] TEST: [2]
TRAIN: [2 1] TEST: [0]
TRAIN: [0 2] TEST: [3]
Methods
get_n_splits([X, y, groups])
Returns the number of splitting iterations in the cross-
validator
split(X[, y, groups])
Generate indices to split data into training and test set.
__init__(n_splits=10, test_size=0.1, train_size=None, random_state=None)
get_n_splits(X=None, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples,)
The target variable for supervised learning problems.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
29.5. sklearn.model_selection: Model Selection
1217
scikit-learn user guide, Release 0.18.2
Examples using sklearn.model_selection.ShuffleSplit
• Plotting Learning Curves
• Scaling the regularization parameter for SVCs
sklearn.model_selection.GroupShufﬂeSplit
class sklearn.model_selection.GroupShuffleSplit(n_splits=5,
test_size=0.2,
train_size=None, random_state=None)
Shufﬂe-Group(s)-Out cross-validation iterator
Provides randomized train/test indices to split data according to a third-party provided group. This group infor-
mation can be used to encode arbitrary domain speciﬁc stratiﬁcations of the samples as integers.
For instance the groups could be the year of collection of the samples and thus allow for cross-validation against
time-based splits.
The difference between LeavePGroupsOut and GroupShufﬂeSplit is that the former generates splits using all
subsets of size p unique groups, whereas GroupShufﬂeSplit generates a user-determined number of random test
splits, each with a user-determined fraction of unique groups.
For example, a less computationally intensive alternative to LeavePGroupsOut(p=10) would be
GroupShuffleSplit(test_size=10,n_splits=100).
Note: The parameters test_size and train_size refer to groups, and not to samples, as in ShufﬂeSplit.
Parametersn_splits : int (default 5)
Number of re-shufﬂing & splitting iterations.
test_size : ﬂoat (default 0.2), int, or None
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the groups to
include in the test split. If int, represents the absolute number of test groups. If None,
the value is automatically set to the complement of the train size.
train_size : ﬂoat, int, or None (default is None)
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the groups to
include in the train split. If int, represents the absolute number of train groups. If None,
the value is automatically set to the complement of the test size.
random_state : int or RandomState
Pseudo-random number generator state used for random sampling.
Methods
get_n_splits([X, y, groups])
Returns the number of splitting iterations in the cross-
validator
split(X[, y, groups])
Generate indices to split data into training and test set.
__init__(n_splits=5, test_size=0.2, train_size=None, random_state=None)
get_n_splits(X=None, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
1218
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples,)
The target variable for supervised learning problems.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
sklearn.model_selection.StratiﬁedShufﬂeSplit
class sklearn.model_selection.StratifiedShuffleSplit(n_splits=10,
test_size=0.1,
train_size=None,
ran-
dom_state=None)
Stratiﬁed ShufﬂeSplit cross-validator
Provides train/test indices to split data in train/test sets.
This cross-validation object is a merge of StratiﬁedKFold and ShufﬂeSplit, which returns stratiﬁed randomized
folds. The folds are made by preserving the percentage of samples for each class.
Note: like the ShufﬂeSplit strategy, stratiﬁed random splits do not guarantee that all folds will be different,
although this is still very likely for sizeable datasets.
Read more in the User Guide.
Parametersn_splits : int (default 10)
Number of re-shufﬂing & splitting iterations.
test_size : ﬂoat (default 0.1), int, or None
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the test split. If int, represents the absolute number of test samples. If None,
the value is automatically set to the complement of the train size.
train_size : ﬂoat, int, or None (default is None)
29.5. sklearn.model_selection: Model Selection
1219
scikit-learn user guide, Release 0.18.2
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the train split. If int, represents the absolute number of train samples. If
None, the value is automatically set to the complement of the test size.
random_state : int or RandomState
Pseudo-random number generator state used for random sampling.
Examples
>>> from sklearn.model_selection import StratifiedShuffleSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
>>> sss = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=0)
>>> sss.get_n_splits(X, y)
3
>>> print(sss)
StratifiedShuffleSplit(n_splits=3, random_state=0, ...)
>>> for train_index, test_index in sss.split(X, y):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [1 2] TEST: [3 0]
TRAIN: [0 2] TEST: [1 3]
TRAIN: [0 2] TEST: [3 1]
Methods
get_n_splits([X, y, groups])
Returns the number of splitting iterations in the cross-
validator
split(X, y[, groups])
Generate indices to split data into training and test set.
__init__(n_splits=10, test_size=0.1, train_size=None, random_state=None)
get_n_splits(X=None, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X, y, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
1220
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Training data, where n_samples is the number of samples and n_features is the number
of features.
Note
that
providing
y
is
sufﬁcient
to
generate
the
splits
and
hence
np.zeros(n_samples) may be used as a placeholder for X instead of actual
training data.
y : array-like, shape (n_samples,)
The target variable for supervised learning problems. Stratiﬁcation is done based on the
y labels.
groups : object
Always ignored, exists for compatibility.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
Examples using sklearn.model_selection.StratifiedShuffleSplit
• RBF SVM parameters
sklearn.model_selection.PredeﬁnedSplit
class sklearn.model_selection.PredefinedSplit(test_fold)
Predeﬁned split cross-validator
Splits the data into training/test set folds according to a predeﬁned scheme. Each sample can be assigned to at
most one test set fold, as speciﬁed by the user through the test_fold parameter.
Read more in the User Guide.
Examples
>>> from sklearn.model_selection import PredefinedSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
>>> test_fold = [0, 1, -1, 1]
>>> ps = PredefinedSplit(test_fold)
>>> ps.get_n_splits()
2
>>> print(ps)
PredefinedSplit(test_fold=array([ 0,
1, -1,
1]))
>>> for train_index, test_index in ps.split():
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [1 2 3] TEST: [0]
TRAIN: [0 2] TEST: [1 3]
29.5. sklearn.model_selection: Model Selection
1221
scikit-learn user guide, Release 0.18.2
Methods
get_n_splits([X, y, groups])
Returns the number of splitting iterations in the cross-
validator
split([X, y, groups])
Generate indices to split data into training and test set.
__init__(test_fold)
get_n_splits(X=None, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
split(X=None, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : object
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
sklearn.model_selection.TimeSeriesSplit
class sklearn.model_selection.TimeSeriesSplit(n_splits=3)
Time Series cross-validator
Provides train/test indices to split time series data samples that are observed at ﬁxed time intervals, in train/test
sets. In each split, test indices must be higher than before, and thus shufﬂing in cross validator is inappropriate.
This cross-validation object is a variation of KFold. In the kth split, it returns ﬁrst k folds as train set and the
(k+1)th fold as test set.
Note that unlike standard cross-validation methods, successive training sets are supersets of those that come
before them.
1222
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Read more in the User Guide.
Parametersn_splits : int, default=3
Number of splits. Must be at least 1.
Notes
The training set has size i * n_samples // (n_splits + 1) + n_samples % (n_splits +
1) in the i``th split,with a test set of size ``n_samples//(n_splits + 1), where
n_samples is the number of samples.
Examples
>>> from sklearn.model_selection import TimeSeriesSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([1, 2, 3, 4])
>>> tscv = TimeSeriesSplit(n_splits=3)
>>> print(tscv)
TimeSeriesSplit(n_splits=3)
>>> for train_index, test_index in tscv.split(X):
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [0] TEST: [1]
TRAIN: [0 1] TEST: [2]
TRAIN: [0 1 2] TEST: [3]
Methods
get_n_splits([X, y, groups])
Returns the number of splitting iterations in the cross-
validator
split(X[, y, groups])
Generate indices to split data into training and test set.
__init__(n_splits=3)
get_n_splits(X=None, y=None, groups=None)
Returns the number of splitting iterations in the cross-validator
ParametersX : object
Always ignored, exists for compatibility.
y : object
Always ignored, exists for compatibility.
groups : object
Always ignored, exists for compatibility.
Returnsn_splits : int
Returns the number of splitting iterations in the cross-validator.
29.5. sklearn.model_selection: Model Selection
1223
scikit-learn user guide, Release 0.18.2
split(X, y=None, groups=None)
Generate indices to split data into training and test set.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples,)
Always ignored, exists for compatibility.
groups : array-like, with shape (n_samples,), optional
Always ignored, exists for compatibility.
Returnstrain : ndarray
The training set indices for that split.
test : ndarray
The testing set indices for that split.
29.5.2 Splitter Functions
model_selection.train_test_split(\*arrays,
...)
Split arrays or matrices into random train and test subsets
model_selection.check_cv([cv, y, classiﬁer])
Input checker utility for building a cross-validator
sklearn.model_selection.train_test_split
sklearn.model_selection.train_test_split(*arrays, **options)
Split arrays or matrices into random train and test subsets
Quick utility that wraps input validation and next(ShuffleSplit().split(X,y)) and application to
input data into a single call for splitting (and optionally subsampling) data in a oneliner.
Read more in the User Guide.
Parameters*arrays : sequence of indexables with same length / shape[0]
Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.
test_size : ﬂoat, int, or None (default is None)
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the test split. If int, represents the absolute number of test samples. If None,
the value is automatically set to the complement of the train size. If train size is also
None, test size is set to 0.25.
train_size : ﬂoat, int, or None (default is None)
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the train split. If int, represents the absolute number of train samples. If
None, the value is automatically set to the complement of the test size.
random_state : int or RandomState
Pseudo-random number generator state used for random sampling.
stratify : array-like or None (default is None)
1224
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
If not None, data is split in a stratiﬁed fashion, using this as the class labels.
Returnssplitting : list, length=2 * len(arrays)
List containing train-test split of inputs.
New
in
version
0.16:
If
the
input
is
sparse,
the
output
will
be
a
scipy.sparse.csr_matrix.
Else, output type is the same as the input
type.
Examples
>>> import numpy as np
>>> from sklearn.model_selection import train_test_split
>>> X, y = np.arange(10).reshape((5, 2)), range(5)
>>> X
array([[0, 1],
[2, 3],
[4, 5],
[6, 7],
[8, 9]])
>>> list(y)
[0, 1, 2, 3, 4]
>>> X_train, X_test, y_train, y_test = train_test_split(
...
X, y, test_size=0.33, random_state=42)
...
>>> X_train
array([[4, 5],
[0, 1],
[6, 7]])
>>> y_train
[2, 0, 3]
>>> X_test
array([[2, 3],
[8, 9]])
>>> y_test
[1, 4]
Examples using sklearn.model_selection.train_test_split
• Faces recognition example using eigenfaces and SVMs
• Prediction Latency
• Probability calibration of classiﬁers
• Probability Calibration curves
• Classiﬁer comparison
• Feature transformations with ensembles of trees
• Gradient Boosting Out-of-Bag estimates
• Partial Dependence Plots
• Comparing random forests and the multi-output meta estimator
29.5. sklearn.model_selection: Model Selection
1225
scikit-learn user guide, Release 0.18.2
• Comparing various online solvers
• Parameter estimation using grid search with cross-validation
• Confusion matrix
• Precision-Recall
• Receiver Operating Characteristic (ROC)
• Varying regularization in Multi-layer Perceptron
• Restricted Boltzmann Machine features for digit classiﬁcation
• Using FunctionTransformer to select columns
• Understanding the decision tree structure
sklearn.model_selection.check_cv
sklearn.model_selection.check_cv(cv=3, y=None, classiﬁer=False)
Input checker utility for building a cross-validator
Parameterscv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, if classiﬁer is True and y is either binary or multiclass,
StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
y : array-like, optional
The target variable for supervised learning problems.
classiﬁer : boolean, optional, default False
Whether the task is a classiﬁcation task, in which case stratiﬁed KFold will be used.
Returnschecked_cv : a cross-validator instance.
The return value is a cross-validator which generates the train/test splits via the split
method.
29.5.3 Hyper-parameter optimizers
model_selection.GridSearchCV(estimator, ...)
Exhaustive search over speciﬁed parameter values for an
estimator.
model_selection.RandomizedSearchCV(...[,
...])
Randomized search on hyper parameters.
model_selection.ParameterGrid(param_grid)
Grid of parameters with a discrete number of values for
each.
model_selection.ParameterSampler(...[, ...])
Generator on parameters sampled from given distributions.
1226
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.model_selection.GridSearchCV
class sklearn.model_selection.GridSearchCV(estimator,
param_grid,
scoring=None,
ﬁt_params=None,
n_jobs=1,
iid=True,
reﬁt=True,
cv=None,
verbose=0,
pre_dispatch=‘2*n_jobs’,
error_score=’raise’,
return_train_score=True)
Exhaustive search over speciﬁed parameter values for an estimator.
Important members are ﬁt, predict.
GridSearchCV implements a “ﬁt” and a “score” method. It also implements “predict”, “predict_proba”, “deci-
sion_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.
The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over
a parameter grid.
Read more in the User Guide.
Parametersestimator : estimator object.
This is assumed to implement the scikit-learn estimator interface. Either estimator needs
to provide a score function, or scoring must be passed.
param_grid : dict or list of dictionaries
Dictionary with parameters names (string) as keys and lists of parameter settings to try
as values, or a list of such dictionaries, in which case the grids spanned by each dic-
tionary in the list are explored. This enables searching over any sequence of parameter
settings.
scoring : string, callable or None, default=None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y). If None, the score method of the estimator
is used.
ﬁt_params : dict, optional
Parameters to pass to the ﬁt method.
n_jobs : int, default=1
Number of jobs to run in parallel.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
iid : boolean, default=True
If True, the data is assumed to be identically distributed across the folds, and the loss
minimized is the total loss per sample, and not the mean loss across the folds.
cv : int, cross-validation generator or an iterable, optional
29.5. sklearn.model_selection: Model Selection
1227
scikit-learn user guide, Release 0.18.2
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross validation,
•integer, to specify the number of folds in a (Stratiﬁed)KFold,
•An object to be used as a cross-validation generator.
•An iterable yielding train, test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
reﬁt : boolean, default=True
Reﬁt the best estimator with the entire dataset. If “False”, it is impossible to make
predictions using this GridSearchCV instance after ﬁtting.
verbose : integer
Controls the verbosity: the higher, the more messages.
error_score : ‘raise’ (default) or numeric
Value to assign to the score if an error occurs in estimator ﬁtting. If set to ‘raise’, the
error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter
does not affect the reﬁt step, which will always raise the error.
return_train_score : boolean, default=True
If 'False', the cv_results_ attribute will not include training scores.
Attributescv_results_ : dict of numpy (masked) ndarrays
A dict with keys as column headers and values as columns, that can be imported into a
pandas DataFrame.
For instance the below given table
param_kernel param_gamma param_degree split0_test_score ...
rank_....
‘poly’
–
2
0.8
...
2
‘poly’
–
3
0.7
...
4
‘rbf’
0.1
–
0.8
...
3
‘rbf’
0.2
–
0.9
...
1
will be represented by a cv_results_ dict of:
{
'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf
˓→'],
mask = [False False False False]...
˓→)
'param_gamma': masked_array(data = [-- -- 0.1 0.2],
mask = [ True
True False False]...
˓→),
'param_degree': masked_array(data = [2.0 3.0 -- --],
mask = [False False
True
True]...
˓→),
'split0_test_score'
: [0.8, 0.7, 0.8, 0.9],
'split1_test_score'
: [0.82, 0.5, 0.7, 0.78],
'mean_test_score'
: [0.81, 0.60, 0.75, 0.82],
'std_test_score'
: [0.02, 0.01, 0.03, 0.03],
'rank_test_score'
: [2, 4, 3, 1],
1228
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
'split0_train_score' : [0.8, 0.9, 0.7],
'split1_train_score' : [0.82, 0.5, 0.7],
'mean_train_score'
: [0.81, 0.7, 0.7],
'std_train_score'
: [0.03, 0.03, 0.04],
'mean_fit_time'
: [0.73, 0.63, 0.43, 0.49],
'std_fit_time'
: [0.01, 0.02, 0.01, 0.01],
'mean_score_time'
: [0.007, 0.06, 0.04, 0.04],
'std_score_time'
: [0.001, 0.002, 0.003, 0.005],
'params'
: [{'kernel': 'poly', 'degree': 2}, ...],
}
NOTE that the key 'params' is used to store a list of parameter settings dict for all
the parameter candidates.
The
mean_fit_time,
std_fit_time,
mean_score_time
and
std_score_time are all in seconds.
best_estimator_ : estimator
Estimator that was chosen by the search, i.e. estimator which gave highest score (or
smallest loss if speciﬁed) on the left out data. Not available if reﬁt=False.
best_score_ : ﬂoat
Score of best_estimator on the left out data.
best_params_ : dict
Parameter setting that gave the best results on the hold out data.
best_index_ : int
The index (of the cv_results_ arrays) which corresponds to the best candidate pa-
rameter setting.
The dict at search.cv_results_['params'][search.best_index_]
gives the parameter setting for the best model, that gives the highest mean score
(search.best_score_).
scorer_ : function
Scorer function used on the held out data to choose the best parameters for the model.
n_splits_ : int
The number of cross-validation splits (folds/iterations).
See also:
ParameterGridgenerates all the combinations of a hyperparameter grid.
sklearn.model_selection.train_test_splitutility function to split the data into a development
set usable for ﬁtting a GridSearchCV instance and an evaluation set for its ﬁnal evaluation.
sklearn.metrics.make_scorerMake a scorer from a performance metric or loss function.
Notes
The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed
in which case it is used instead.
29.5. sklearn.model_selection: Model Selection
1229
scikit-learn user guide, Release 0.18.2
If n_jobs was set to a value higher than one, the data is copied for each point in the grid (and not n_jobs times).
This is done for efﬁciency reasons if individual jobs take very little time, but may raise errors if the dataset is
large and not enough memory is available. A workaround in this case is to set pre_dispatch. Then, the memory
is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs.
Examples
>>> from sklearn import svm, datasets
>>> from sklearn.model_selection import GridSearchCV
>>> iris = datasets.load_iris()
>>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
>>> svr = svm.SVC()
>>> clf = GridSearchCV(svr, parameters)
>>> clf.fit(iris.data, iris.target)
...
GridSearchCV(cv=None, error_score=...,
estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
decision_function_shape=None, degree=..., gamma=...,
kernel='rbf', max_iter=-1, probability=False,
random_state=None, shrinking=True, tol=...,
verbose=False),
fit_params={}, iid=..., n_jobs=1,
param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,
scoring=..., verbose=...)
>>> sorted(clf.cv_results_.keys())
...
['mean_fit_time', 'mean_score_time', 'mean_test_score',...
'mean_train_score', 'param_C', 'param_kernel', 'params',...
'rank_test_score', 'split0_test_score',...
'split0_train_score', 'split1_test_score', 'split1_train_score',...
'split2_test_score', 'split2_train_score',...
'std_fit_time', 'std_score_time', 'std_test_score', 'std_train_score'...]
Methods
decision_function(\*args, \*\*kwargs)
Call decision_function on the estimator with the best
found parameters.
fit(X[, y, groups])
Run ﬁt with all sets of parameters.
get_params([deep])
Get parameters for this estimator.
inverse_transform(\*args, \*\*kwargs)
Call inverse_transform on the estimator with the best
found params.
predict(\*args, \*\*kwargs)
Call predict on the estimator with the best found param-
eters.
predict_log_proba(\*args, \*\*kwargs)
Call predict_log_proba on the estimator with the best
found parameters.
predict_proba(\*args, \*\*kwargs)
Call predict_proba on the estimator with the best found
parameters.
score(X[, y])
Returns the score on the given data, if the estimator has
been reﬁt.
set_params(\*\*params)
Set the parameters of this estimator.
Continued on next page
1230
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.47 – continued from previous page
transform(\*args, \*\*kwargs)
Call transform on the estimator with the best found pa-
rameters.
__init__(estimator,
param_grid,
scoring=None,
ﬁt_params=None,
n_jobs=1,
iid=True,
re-
ﬁt=True,
cv=None,
verbose=0,
pre_dispatch=‘2*n_jobs’,
error_score=’raise’,
re-
turn_train_score=True)
decision_function(*args, **kwargs)
Call decision_function on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports decision_function.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
fit(X, y=None, groups=None)
Run ﬁt with all sets of parameters.
ParametersX : array-like, shape = [n_samples, n_features]
Training vector, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape = [n_samples] or [n_samples, n_output], optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(*args, **kwargs)
Call inverse_transform on the estimator with the best found params.
Only available if the underlying estimator implements inverse_transform and refit=True.
ParametersXt : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict(*args, **kwargs)
Call predict on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict_log_proba(*args, **kwargs)
Call predict_log_proba on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict_log_proba.
29.5. sklearn.model_selection: Model Selection
1231
scikit-learn user guide, Release 0.18.2
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict_proba(*args, **kwargs)
Call predict_proba on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict_proba.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
score(X, y=None)
Returns the score on the given data, if the estimator has been reﬁt.
This uses the score deﬁned by scoring where provided, and the best_estimator_.score method
otherwise.
ParametersX : array-like, shape = [n_samples, n_features]
Input data, where n_samples is the number of samples and n_features is the number of
features.
y : array-like, shape = [n_samples] or [n_samples, n_output], optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
Returnsscore : ﬂoat
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
Call transform on the estimator with the best found parameters.
Only available if the underlying estimator supports transform and refit=True.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
Examples using sklearn.model_selection.GridSearchCV
• Concatenating multiple feature extraction methods
• Selecting dimensionality reduction with Pipeline and GridSearchCV
• Pipelining: chaining a PCA and a logistic regression
• Comparison of kernel ridge regression and SVR
• Faces recognition example using eigenfaces and SVMs
• Feature agglomeration vs. univariate selection
• Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood
• Model selection with Probabilistic PCA and Factor Analysis (FA)
1232
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Comparison of kernel ridge and Gaussian process regression
• Parameter estimation using grid search with cross-validation
• Sample pipeline for text feature extraction and evaluation
• Nested versus non-nested cross-validation
• Comparing randomized search and grid search for hyperparameter estimation
• Kernel Density Estimation
• RBF SVM parameters
• Scaling the regularization parameter for SVCs
sklearn.model_selection.RandomizedSearchCV
class sklearn.model_selection.RandomizedSearchCV(estimator,
param_distributions,
n_iter=10,
scoring=None,
ﬁt_params=None, n_jobs=1, iid=True,
reﬁt=True,
cv=None,
verbose=0,
pre_dispatch=‘2*n_jobs’,
ran-
dom_state=None,
error_score=’raise’,
return_train_score=True)
Randomized search on hyper parameters.
RandomizedSearchCV implements a “ﬁt” and a “score” method. It also implements “predict”, “predict_proba”,
“decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.
The parameters of the estimator used to apply these methods are optimized by cross-validated search over
parameter settings.
In contrast to GridSearchCV, not all parameter values are tried out, but rather a ﬁxed number of parameter
settings is sampled from the speciﬁed distributions. The number of parameter settings that are tried is given by
n_iter.
If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter
is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous
distributions for continuous parameters.
Read more in the User Guide.
Parametersestimator : estimator object.
A object of that type is instantiated for each grid point. This is assumed to implement
the scikit-learn estimator interface. Either estimator needs to provide a score function,
or scoring must be passed.
param_distributions : dict
Dictionary with parameters names (string) as keys and distributions or lists of parame-
ters to try. Distributions must provide a rvs method for sampling (such as those from
scipy.stats.distributions). If a list is given, it is sampled uniformly.
n_iter : int, default=10
Number of parameter settings that are sampled. n_iter trades off runtime vs quality of
the solution.
scoring : string, callable or None, default=None
29.5. sklearn.model_selection: Model Selection
1233
scikit-learn user guide, Release 0.18.2
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y). If None, the score method of the estimator
is used.
ﬁt_params : dict, optional
Parameters to pass to the ﬁt method.
n_jobs : int, default=1
Number of jobs to run in parallel.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
iid : boolean, default=True
If True, the data is assumed to be identically distributed across the folds, and the loss
minimized is the total loss per sample, and not the mean loss across the folds.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross validation,
•integer, to specify the number of folds in a (Stratiﬁed)KFold,
•An object to be used as a cross-validation generator.
•An iterable yielding train, test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
reﬁt : boolean, default=True
Reﬁt the best estimator with the entire dataset. If “False”, it is impossible to make
predictions using this RandomizedSearchCV instance after ﬁtting.
verbose : integer
Controls the verbosity: the higher, the more messages.
random_state : int or RandomState
Pseudo random number generator state used for random uniform sampling from lists of
possible values instead of scipy.stats distributions.
error_score : ‘raise’ (default) or numeric
Value to assign to the score if an error occurs in estimator ﬁtting. If set to ‘raise’, the
error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter
does not affect the reﬁt step, which will always raise the error.
1234
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
return_train_score : boolean, default=True
If 'False', the cv_results_ attribute will not include training scores.
Attributescv_results_ : dict of numpy (masked) ndarrays
A dict with keys as column headers and values as columns, that can be imported into a
pandas DataFrame.
For instance the below given table
param_kernel
param_gamma
split0_test_score
...
rank_test_score
‘rbf’
0.1
0.8
...
2
‘rbf’
0.2
0.9
...
1
‘rbf’
0.3
0.7
...
1
will be represented by a cv_results_ dict of:
{
'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],
mask = False),
'param_gamma'
: masked_array(data = [0.1 0.2 0.3], mask =
˓→False),
'split0_test_score'
: [0.8, 0.9, 0.7],
'split1_test_score'
: [0.82, 0.5, 0.7],
'mean_test_score'
: [0.81, 0.7, 0.7],
'std_test_score'
: [0.02, 0.2, 0.],
'rank_test_score'
: [3, 1, 1],
'split0_train_score' : [0.8, 0.9, 0.7],
'split1_train_score' : [0.82, 0.5, 0.7],
'mean_train_score'
: [0.81, 0.7, 0.7],
'std_train_score'
: [0.03, 0.03, 0.04],
'mean_fit_time'
: [0.73, 0.63, 0.43, 0.49],
'std_fit_time'
: [0.01, 0.02, 0.01, 0.01],
'mean_score_time'
: [0.007, 0.06, 0.04, 0.04],
'std_score_time'
: [0.001, 0.002, 0.003, 0.005],
'params' : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],
}
NOTE that the key 'params' is used to store a list of parameter settings dict for all
the parameter candidates.
The
mean_fit_time,
std_fit_time,
mean_score_time
and
std_score_time are all in seconds.
best_estimator_ : estimator
Estimator that was chosen by the search, i.e. estimator which gave highest score (or
smallest loss if speciﬁed) on the left out data. Not available if reﬁt=False.
best_score_ : ﬂoat
Score of best_estimator on the left out data.
best_params_ : dict
Parameter setting that gave the best results on the hold out data.
best_index_ : int
The index (of the cv_results_ arrays) which corresponds to the best candidate pa-
rameter setting.
29.5. sklearn.model_selection: Model Selection
1235
scikit-learn user guide, Release 0.18.2
The dict at search.cv_results_['params'][search.best_index_]
gives the parameter setting for the best model, that gives the highest mean score
(search.best_score_).
scorer_ : function
Scorer function used on the held out data to choose the best parameters for the model.
n_splits_ : int
The number of cross-validation splits (folds/iterations).
See also:
GridSearchCVDoes exhaustive search over a grid of parameters.
ParameterSamplerA generator over parameter settins, constructed from param_distributions.
Notes
The parameters selected are those that maximize the score of the held-out data, according to the scoring param-
eter.
If n_jobs was set to a value higher than one, the data is copied for each parameter setting(and not n_jobs times).
This is done for efﬁciency reasons if individual jobs take very little time, but may raise errors if the dataset is
large and not enough memory is available. A workaround in this case is to set pre_dispatch. Then, the memory
is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs.
Methods
decision_function(\*args, \*\*kwargs)
Call decision_function on the estimator with the best
found parameters.
fit(X[, y, groups])
Run ﬁt on the estimator with randomly drawn parame-
ters.
get_params([deep])
Get parameters for this estimator.
inverse_transform(\*args, \*\*kwargs)
Call inverse_transform on the estimator with the best
found params.
predict(\*args, \*\*kwargs)
Call predict on the estimator with the best found param-
eters.
predict_log_proba(\*args, \*\*kwargs)
Call predict_log_proba on the estimator with the best
found parameters.
predict_proba(\*args, \*\*kwargs)
Call predict_proba on the estimator with the best found
parameters.
score(X[, y])
Returns the score on the given data, if the estimator has
been reﬁt.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
Call transform on the estimator with the best found pa-
rameters.
__init__(estimator, param_distributions, n_iter=10, scoring=None, ﬁt_params=None, n_jobs=1,
iid=True, reﬁt=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, random_state=None,
error_score=’raise’, return_train_score=True)
decision_function(*args, **kwargs)
Call decision_function on the estimator with the best found parameters.
1236
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Only available if refit=True and the underlying estimator supports decision_function.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
fit(X, y=None, groups=None)
Run ﬁt on the estimator with randomly drawn parameters.
ParametersX : array-like, shape = [n_samples, n_features]
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples] or [n_samples, n_output], optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(*args, **kwargs)
Call inverse_transform on the estimator with the best found params.
Only available if the underlying estimator implements inverse_transform and refit=True.
ParametersXt : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict(*args, **kwargs)
Call predict on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict_log_proba(*args, **kwargs)
Call predict_log_proba on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict_log_proba.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict_proba(*args, **kwargs)
Call predict_proba on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict_proba.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
29.5. sklearn.model_selection: Model Selection
1237
scikit-learn user guide, Release 0.18.2
score(X, y=None)
Returns the score on the given data, if the estimator has been reﬁt.
This uses the score deﬁned by scoring where provided, and the best_estimator_.score method
otherwise.
ParametersX : array-like, shape = [n_samples, n_features]
Input data, where n_samples is the number of samples and n_features is the number of
features.
y : array-like, shape = [n_samples] or [n_samples, n_output], optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
Returnsscore : ﬂoat
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
Call transform on the estimator with the best found parameters.
Only available if the underlying estimator supports transform and refit=True.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
Examples using sklearn.model_selection.RandomizedSearchCV
• Comparing randomized search and grid search for hyperparameter estimation
sklearn.model_selection.ParameterGrid
class sklearn.model_selection.ParameterGrid(param_grid)
Grid of parameters with a discrete number of values for each.
Can be used to iterate over parameter value combinations with the Python built-in function iter.
Read more in the User Guide.
Parametersparam_grid : dict of string to sequence, or sequence of such
The parameter grid to explore, as a dictionary mapping estimator parameters to se-
quences of allowed values.
An empty dict signiﬁes default parameters.
A sequence of dicts signiﬁes a sequence of grids to search, and is useful to avoid ex-
ploring parameter combinations that make no sense or have no effect. See the examples
below.
See also:
GridSearchCV
1238
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Usesclass:ParameterGrid to perform a full parallelized parameter search.
Examples
>>> from sklearn.model_selection import ParameterGrid
>>> param_grid = {'a': [1, 2], 'b': [True, False]}
>>> list(ParameterGrid(param_grid)) == (
...
[{'a': 1, 'b': True}, {'a': 1, 'b': False},
...
{'a': 2, 'b': True}, {'a': 2, 'b': False}])
True
>>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]
>>> list(ParameterGrid(grid)) == [{'kernel': 'linear'},
...
{'kernel': 'rbf', 'gamma': 1},
...
{'kernel': 'rbf', 'gamma': 10}]
True
>>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}
True
.. automethod:: __init__
sklearn.model_selection.ParameterSampler
class sklearn.model_selection.ParameterSampler(param_distributions,
n_iter,
ran-
dom_state=None)
Generator on parameters sampled from given distributions.
Non-deterministic iterable over random candidate combinations for hyper- parameter search. If all parameters
are presented as a list, sampling without replacement is performed. If at least one parameter is given as a
distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for
continuous parameters.
Note that before SciPy 0.16, the scipy.stats.distributions do not accept a custom RNG instance
and always use the singleton RNG from numpy.random. Hence setting random_state will not guarantee
a deterministic iteration whenever scipy.stats distributions are used to deﬁne the parameter search space.
Deterministic behavior is however guaranteed from SciPy 0.16 onwards.
Read more in the User Guide.
Parametersparam_distributions : dict
Dictionary where the keys are parameters and values are distributions from which a
parameter is to be sampled. Distributions either have to provide a rvs function to
sample from them, or can be given as a list of values, where a uniform distribution is
assumed.
n_iter : integer
Number of parameter settings that are produced.
random_state : int or RandomState
Pseudo random number generator state used for random uniform sampling from lists of
possible values instead of scipy.stats distributions.
Returnsparams : dict of string to any
Yields dictionaries mapping each estimator parameter to as sampled value.
29.5. sklearn.model_selection: Model Selection
1239
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn.model_selection import ParameterSampler
>>> from scipy.stats.distributions import expon
>>> import numpy as np
>>> np.random.seed(0)
>>> param_grid = {'a':[1, 2], 'b': expon()}
>>> param_list = list(ParameterSampler(param_grid, n_iter=4))
>>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())
...
for d in param_list]
>>> rounded_list == [{'b': 0.89856, 'a': 1},
...
{'b': 0.923223, 'a': 1},
...
{'b': 1.878964, 'a': 2},
...
{'b': 1.038159, 'a': 2}]
True
.. automethod:: __init__
model_selection.fit_grid_point(X, y, ...[, ...])
Run ﬁt on one set of parameters.
sklearn.model_selection.ﬁt_grid_point
sklearn.model_selection.fit_grid_point(X, y, estimator, parameters, train, test, scorer, ver-
bose, error_score=’raise’, **ﬁt_params)
Run ﬁt on one set of parameters.
ParametersX : array-like, sparse matrix or list
Input data.
y : array-like or None
Targets for input data.
estimator : estimator object
A object of that type is instantiated for each grid point. This is assumed to implement
the scikit-learn estimator interface. Either estimator needs to provide a score function,
or scoring must be passed.
parameters : dict
Parameters to be set on estimator for this grid point.
train : ndarray, dtype int or bool
Boolean mask or indices for training set.
test : ndarray, dtype int or bool
Boolean mask or indices for test set.
scorer : callable or None.
If
provided
must
be
a
scorer
callable
object
/
function
with
signature
scorer(estimator,X,y).
verbose : int
Verbosity level.
**ﬁt_params : kwargs
1240
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Additional parameter passed to the ﬁt function of the estimator.
error_score : ‘raise’ (default) or numeric
Value to assign to the score if an error occurs in estimator ﬁtting. If set to ‘raise’, the
error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter
does not affect the reﬁt step, which will always raise the error.
Returnsscore : ﬂoat
Score of this parameter setting on given training / test split.
parameters : dict
The parameters that have been evaluated.
n_samples_test : int
Number of test samples in this split.
29.5.4 Model validation
model_selection.cross_val_score(estimator,
X)
Evaluate a score by cross-validation
model_selection.cross_val_predict(estimator,
X)
Generate cross-validated estimates for each input data point
model_selection.permutation_test_score(...) Evaluate the signiﬁcance of a cross-validated score with
permutations
model_selection.learning_curve(estimator, X,
y)
Learning curve.
model_selection.validation_curve(estimator,
...)
Validation curve.
sklearn.model_selection.cross_val_score
sklearn.model_selection.cross_val_score(estimator,
X,
y=None,
groups=None,
scor-
ing=None,
cv=None,
n_jobs=1,
verbose=0,
ﬁt_params=None, pre_dispatch=‘2*n_jobs’)
Evaluate a score by cross-validation
Read more in the User Guide.
Parametersestimator : estimator object implementing ‘ﬁt’
The object to use to ﬁt the data.
X : array-like
The data to ﬁt. Can be, for example a list, or an array at least 2d.
y : array-like, optional, default: None
The target variable to try to predict in the case of supervised learning.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
scoring : string, callable or None, optional, default: None
29.5. sklearn.model_selection: Model Selection
1241
scikit-learn user guide, Release 0.18.2
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross validation,
•integer, to specify the number of folds in a (Stratiﬁed)KFold,
•An object to be used as a cross-validation generator.
•An iterable yielding train, test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
n_jobs : integer, optional
The number of CPUs to use to do the computation. -1 means ‘all CPUs’.
verbose : integer, optional
The verbosity level.
ﬁt_params : dict, optional
Parameters to pass to the ﬁt method of the estimator.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
Returnsscores : array of ﬂoat, shape=(len(list(cv)),)
Array of scores of the estimator for each run of the cross validation.
See also:
sklearn.metrics.make_scorerMake a scorer from a performance metric or loss function.
Examples
>>> from sklearn import datasets, linear_model
>>> from sklearn.model_selection import cross_val_score
>>> diabetes = datasets.load_diabetes()
>>> X = diabetes.data[:150]
>>> y = diabetes.target[:150]
>>> lasso = linear_model.Lasso()
>>> print(cross_val_score(lasso, X, y))
[ 0.33150734
0.08022311
0.03531764]
1242
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.model_selection.cross_val_score
• Imputing missing values before building an estimator
• Model selection with Probabilistic PCA and Factor Analysis (FA)
• Cross-validation on diabetes Dataset Exercise
• Cross-validation on Digits Dataset Exercise
• Nested versus non-nested cross-validation
• Underﬁtting vs. Overﬁtting
• SVM-Anova: SVM with univariate feature selection
sklearn.model_selection.cross_val_predict
sklearn.model_selection.cross_val_predict(estimator, X, y=None, groups=None, cv=None,
n_jobs=1,
verbose=0,
ﬁt_params=None,
pre_dispatch=‘2*n_jobs’, method=’predict’)
Generate cross-validated estimates for each input data point
Read more in the User Guide.
Parametersestimator : estimator object implementing ‘ﬁt’ and ‘predict’
The object to use to ﬁt the data.
X : array-like
The data to ﬁt. Can be, for example a list, or an array at least 2d.
y : array-like, optional, default: None
The target variable to try to predict in the case of supervised learning.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross validation,
•integer, to specify the number of folds in a (Stratiﬁed)KFold,
•An object to be used as a cross-validation generator.
•An iterable yielding train, test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
n_jobs : integer, optional
The number of CPUs to use to do the computation. -1 means ‘all CPUs’.
verbose : integer, optional
The verbosity level.
ﬁt_params : dict, optional
29.5. sklearn.model_selection: Model Selection
1243
scikit-learn user guide, Release 0.18.2
Parameters to pass to the ﬁt method of the estimator.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
method : string, optional, default: ‘predict’
Invokes the passed method name of the passed estimator.
Returnspredictions : ndarray
This is the result of calling method
Examples
>>> from sklearn import datasets, linear_model
>>> from sklearn.model_selection import cross_val_predict
>>> diabetes = datasets.load_diabetes()
>>> X = diabetes.data[:150]
>>> y = diabetes.target[:150]
>>> lasso = linear_model.Lasso()
>>> y_pred = cross_val_predict(lasso, X, y)
Examples using sklearn.model_selection.cross_val_predict
• Plotting Cross-Validated Predictions
sklearn.model_selection.permutation_test_score
sklearn.model_selection.permutation_test_score(estimator, X, y, groups=None, cv=None,
n_permutations=100,
n_jobs=1,
random_state=0,
verbose=0,
scor-
ing=None)
Evaluate the signiﬁcance of a cross-validated score with permutations
Read more in the User Guide.
Parametersestimator : estimator object implementing ‘ﬁt’
The object to use to ﬁt the data.
X : array-like of shape at least 2D
The data to ﬁt.
y : array-like
The target variable to try to predict in the case of supervised learning.
1244
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
groups : array-like, with shape (n_samples,), optional
Labels to constrain permutation within groups, i.e. y values are permuted among sam-
ples with the same group identiﬁer. When not speciﬁed, y values are permuted among
all samples.
When a grouped cross-validator is used, the group labels are also passed on to the
split method of the cross-validator. The cross-validator uses them for grouping the
samples while splitting the dataset into train/test set.
scoring : string, callable or None, optional, default: None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross validation,
•integer, to specify the number of folds in a (Stratiﬁed)KFold,
•An object to be used as a cross-validation generator.
•An iterable yielding train, test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
n_permutations : integer, optional
Number of times to permute y.
n_jobs : integer, optional
The number of CPUs to use to do the computation. -1 means ‘all CPUs’.
random_state : RandomState or an int seed (0 by default)
A random number generator instance to deﬁne the state of the random permutations
generator.
verbose : integer, optional
The verbosity level.
Returnsscore : ﬂoat
The true score without permuting targets.
permutation_scores : array, shape (n_permutations,)
The scores obtained for each permutations.
pvalue : ﬂoat
The returned value equals p-value if scoring returns bigger numbers for better scores
(e.g., accuracy_score). If scoring is rather a loss function (i.e. when lower is better such
as with mean_squared_error) then this is actually the complement of the p-value: 1 -
p-value.
29.5. sklearn.model_selection: Model Selection
1245
scikit-learn user guide, Release 0.18.2
Notes
This function implements Test 1 in:
Ojala and Garriga. Permutation Tests for Studying Classiﬁer Performance. The Journal of Machine
Learning Research (2010) vol. 11
Examples using sklearn.model_selection.permutation_test_score
• Test with permutations the signiﬁcance of a classiﬁcation score
sklearn.model_selection.learning_curve
sklearn.model_selection.learning_curve(estimator, X, y, groups=None, train_sizes=array([
0.1, 0.33, 0.55, 0.78, 1. ]), cv=None, scoring=None,
exploit_incremental_learning=False,
n_jobs=1,
pre_dispatch=’all’, verbose=0)
Learning curve.
Determines cross-validated training and test scores for different training set sizes.
A cross-validation generator splits the whole dataset k times in training and test data. Subsets of the training set
with varying sizes will be used to train the estimator and a score for each training subset size and the test set
will be computed. Afterwards, the scores will be averaged over all k runs for each training subset size.
Read more in the User Guide.
Parametersestimator : object type that implements the “ﬁt” and “predict” methods
An object of that type which is cloned for each validation.
X : array-like, shape (n_samples, n_features)
Training vector, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples) or (n_samples, n_features), optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
train_sizes : array-like, shape (n_ticks,), dtype ﬂoat or int
Relative or absolute numbers of training examples that will be used to generate the
learning curve. If the dtype is ﬂoat, it is regarded as a fraction of the maximum size
of the training set (that is determined by the selected validation method), i.e. it has to
be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note
that for classiﬁcation the number of samples usually have to be big enough to contain at
least one sample from each class. (default: np.linspace(0.1, 1.0, 5))
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross validation,
•integer, to specify the number of folds in a (Stratiﬁed)KFold,
•An object to be used as a cross-validation generator.
1246
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
•An iterable yielding train, test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
scoring : string, callable or None, optional, default: None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
exploit_incremental_learning : boolean, optional, default: False
If the estimator supports incremental learning, this will be used to speed up ﬁtting for
different training set sizes.
n_jobs : integer, optional
Number of jobs to run in parallel (default 1).
pre_dispatch : integer or string, optional
Number of predispatched jobs for parallel execution (default is all). The option can
reduce the allocated memory. The string can be an expression like ‘2*n_jobs’.
verbose : integer, optional
Controls the verbosity: the higher, the more messages.
Returnstrain_sizes_abs : array, shape = (n_unique_ticks,), dtype int
Numbers of training examples that has been used to generate the learning curve. Note
that the number of ticks might be less than n_ticks because duplicate entries will be
removed.
train_scores : array, shape (n_ticks, n_cv_folds)
Scores on training sets.
test_scores : array, shape (n_ticks, n_cv_folds)
Scores on test set.
Notes
See examples/model_selection/plot_learning_curve.py
sklearn.model_selection.validation_curve
sklearn.model_selection.validation_curve(estimator,
X, y,
param_name,
param_range,
groups=None,
cv=None,
scoring=None,
n_jobs=1, pre_dispatch=’all’, verbose=0)
Validation curve.
Determine training and test scores for varying parameter values.
Compute scores for an estimator with different values of a speciﬁed parameter. This is similar to grid search
with one parameter. However, this will also compute training scores and is merely a utility for plotting the
results.
Read more in the User Guide.
29.5. sklearn.model_selection: Model Selection
1247
scikit-learn user guide, Release 0.18.2
Parametersestimator : object type that implements the “ﬁt” and “predict” methods
An object of that type which is cloned for each validation.
X : array-like, shape (n_samples, n_features)
Training vector, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples) or (n_samples, n_features), optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
param_name : string
Name of the parameter that will be varied.
param_range : array-like, shape (n_values,)
The values of the parameter that will be evaluated.
groups : array-like, with shape (n_samples,), optional
Group labels for the samples used while splitting the dataset into train/test set.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross validation,
•integer, to specify the number of folds in a (Stratiﬁed)KFold,
•An object to be used as a cross-validation generator.
•An iterable yielding train, test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
scoring : string, callable or None, optional, default: None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
n_jobs : integer, optional
Number of jobs to run in parallel (default 1).
pre_dispatch : integer or string, optional
Number of predispatched jobs for parallel execution (default is all). The option can
reduce the allocated memory. The string can be an expression like ‘2*n_jobs’.
verbose : integer, optional
Controls the verbosity: the higher, the more messages.
Returnstrain_scores : array, shape (n_ticks, n_cv_folds)
Scores on training sets.
test_scores : array, shape (n_ticks, n_cv_folds)
Scores on test set.
1248
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
See Plotting Validation Curves
Examples using sklearn.model_selection.validation_curve
• Plotting Validation Curves
29.6 sklearn.datasets: Datasets
The sklearn.datasets module includes utilities to load datasets, including methods to load and fetch popular
reference datasets. It also features some artiﬁcial data generators.
User guide: See the Dataset loading utilities section for further details.
29.6.1 Loaders
datasets.clear_data_home([data_home])
Delete all the content of the data home cache.
datasets.get_data_home([data_home])
Return the path of the scikit-learn data dir.
datasets.fetch_20newsgroups([data_home, ...])
Load the ﬁlenames and data from the 20 newsgroups
dataset.
datasets.fetch_20newsgroups_vectorized([...])Load the 20 newsgroups dataset and transform it into tf-idf
vectors.
datasets.load_boston([return_X_y])
Load and return the boston house-prices dataset (regres-
sion).
datasets.load_breast_cancer([return_X_y])
Load and return the breast cancer wisconsin dataset (clas-
siﬁcation).
datasets.load_diabetes([return_X_y])
Load and return the diabetes dataset (regression).
datasets.load_digits([n_class, return_X_y])
Load and return the digits dataset (classiﬁcation).
datasets.load_files(container_path[, ...])
Load text ﬁles with categories as subfolder names.
datasets.load_iris([return_X_y])
Load and return the iris dataset (classiﬁcation).
datasets.fetch_lfw_pairs([subset, ...])
Loader for the Labeled Faces in the Wild (LFW) pairs
dataset
datasets.fetch_lfw_people([data_home, ...])
Loader for the Labeled Faces in the Wild (LFW) people
dataset
datasets.load_linnerud([return_X_y])
Load and return the linnerud dataset (multivariate regres-
sion).
datasets.mldata_filename(dataname)
Convert a raw name for a data set in a mldata.org ﬁlename.
datasets.fetch_mldata(dataname[, ...])
Fetch an mldata.org data set
datasets.fetch_olivetti_faces([data_home,
...])
Loader for the Olivetti faces data-set from AT&T.
datasets.fetch_california_housing([...])
Loader for the California housing dataset from StatLib.
datasets.fetch_covtype([data_home, ...])
Load the covertype dataset, downloading it if necessary.
datasets.fetch_kddcup99([subset, shufﬂe, ...])
Load and return the kddcup 99 dataset (classiﬁcation).
datasets.fetch_rcv1([data_home, subset, ...])
Load the RCV1 multilabel dataset, downloading it if nec-
essary.
datasets.load_mlcomp(name_or_id[, set_, ...])
Load a datasets as downloaded from http://mlcomp.org
datasets.load_sample_image(image_name)
Load the numpy array of a single sample image
Continued on next page
29.6. sklearn.datasets: Datasets
1249
scikit-learn user guide, Release 0.18.2
Table 29.51 – continued from previous page
datasets.load_sample_images()
Load sample images for image manipulation.
datasets.fetch_species_distributions([...])
Loader for species distribution dataset from Phillips et.
datasets.load_svmlight_file(f[,
n_features,
...])
Load datasets in the svmlight / libsvm format into sparse
CSR matrix
datasets.load_svmlight_files(ﬁles[, ...])
Load dataset from multiple ﬁles in SVMlight format
datasets.dump_svmlight_file(X, y, f[, ...])
Dump the dataset in svmlight / libsvm ﬁle format.
sklearn.datasets.clear_data_home
sklearn.datasets.clear_data_home(data_home=None)
Delete all the content of the data home cache.
sklearn.datasets.get_data_home
sklearn.datasets.get_data_home(data_home=None)
Return the path of the scikit-learn data dir.
This folder is used by some large dataset loaders to avoid downloading the data several times.
By default the data dir is set to a folder named ‘scikit_learn_data’ in the user home folder.
Alternatively, it can be set by the ‘SCIKIT_LEARN_DATA’ environment variable or programmatically by giving
an explicit folder path. The ‘~’ symbol is expanded to the user home folder.
If the folder does not already exist, it is automatically created.
Examples using sklearn.datasets.get_data_home
• Out-of-core classiﬁcation of text documents
sklearn.datasets.fetch_20newsgroups
sklearn.datasets.fetch_20newsgroups(data_home=None,
subset=’train’,
categories=None,
shufﬂe=True,
random_state=42,
remove=(),
down-
load_if_missing=True)
Load the ﬁlenames and data from the 20 newsgroups dataset.
Read more in the User Guide.
Parameterssubset : ‘train’ or ‘test’, ‘all’, optional
Select the dataset to load: ‘train’ for the training set, ‘test’ for the test set, ‘all’ for both,
with shufﬂed ordering.
data_home : optional, default: None
Specify a download and cache folder for the datasets. If None, all scikit-learn data is
stored in ‘~/scikit_learn_data’ subfolders.
categories : None or collection of string or unicode
If None (default), load all the categories. If not None, list of category names to load
(other categories ignored).
shufﬂe : bool, optional
1250
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Whether or not to shufﬂe the data: might be important for models that make the as-
sumption that the samples are independent and identically distributed (i.i.d.), such as
stochastic gradient descent.
random_state : numpy random number generator or seed integer
Used to shufﬂe the dataset.
download_if_missing : optional, True by default
If False, raise an IOError if the data is not locally available instead of trying to download
the data from the source site.
remove : tuple
May contain any subset of (‘headers’, ‘footers’, ‘quotes’). Each of these are kinds of
text that will be detected and removed from the newsgroup posts, preventing classiﬁers
from overﬁtting on metadata.
‘headers’ removes newsgroup headers, ‘footers’ removes blocks at the ends of posts that
look like signatures, and ‘quotes’ removes lines that appear to be quoting another post.
‘headers’ follows an exact standard; the other ﬁlters are not always correct.
Examples using sklearn.datasets.fetch_20newsgroups
• Feature Union with Heterogeneous Data Sources
• Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
• Biclustering documents with the Spectral Co-clustering algorithm
• Sample pipeline for text feature extraction and evaluation
• Classiﬁcation of text documents using sparse features
• Clustering text documents using k-means
• FeatureHasher and DictVectorizer Comparison
sklearn.datasets.fetch_20newsgroups_vectorized
sklearn.datasets.fetch_20newsgroups_vectorized(subset=’train’,
remove=(),
data_home=None)
Load the 20 newsgroups dataset and transform it into tf-idf vectors.
This is a convenience function;
the tf-idf transformation is done using the default settings for
sklearn.feature_extraction.text.Vectorizer. For more advanced usage (stopword ﬁltering, n-gram extraction,
etc.), combine fetch_20newsgroups with a custom Vectorizer or CountVectorizer.
Read more in the User Guide.
Parameterssubset : ‘train’ or ‘test’, ‘all’, optional
Select the dataset to load: ‘train’ for the training set, ‘test’ for the test set, ‘all’ for both,
with shufﬂed ordering.
data_home : optional, default: None
Specify an download and cache folder for the datasets. If None, all scikit-learn data is
stored in ‘~/scikit_learn_data’ subfolders.
remove : tuple
29.6. sklearn.datasets: Datasets
1251
scikit-learn user guide, Release 0.18.2
May contain any subset of (‘headers’, ‘footers’, ‘quotes’). Each of these are kinds of
text that will be detected and removed from the newsgroup posts, preventing classiﬁers
from overﬁtting on metadata.
‘headers’ removes newsgroup headers, ‘footers’ removes blocks at the ends of posts that
look like signatures, and ‘quotes’ removes lines that appear to be quoting another post.
Returnsbunch : Bunch object
bunch.data: sparse matrix, shape [n_samples, n_features] bunch.target: array, shape
[n_samples] bunch.target_names: list, length [n_classes]
Examples using sklearn.datasets.fetch_20newsgroups_vectorized
• The Johnson-Lindenstrauss bound for embedding with random projections
• Model Complexity Inﬂuence
sklearn.datasets.load_boston
sklearn.datasets.load_boston(return_X_y=False)
Load and return the boston house-prices dataset (regression).
Samples total
506
Dimensionality
13
Features
real, positive
Targets
real 5. - 50.
Parametersreturn_X_y : boolean, default=False.
If True, returns (data,target) instead of a Bunch object. See below for more
information about the data and target object.
New in version 0.18.
Returnsdata : Bunch
Dictionary-like object, the interesting attributes are: ‘data’, the data to learn, ‘target’,
the regression targets, and ‘DESCR’, the full description of the dataset.
(data, target) : tuple if return_X_y is True
New in version 0.18.
Examples
>>> from sklearn.datasets import load_boston
>>> boston = load_boston()
>>> print(boston.data.shape)
(506, 13)
Examples using sklearn.datasets.load_boston
• Imputing missing values before building an estimator
• Plotting Cross-Validated Predictions
1252
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Model Complexity Inﬂuence
• Outlier detection on a real data set
• Gradient Boosting regression
• Feature selection using SelectFromModel and LassoCV
sklearn.datasets.load_breast_cancer
sklearn.datasets.load_breast_cancer(return_X_y=False)
Load and return the breast cancer wisconsin dataset (classiﬁcation).
The breast cancer dataset is a classic and very easy binary classiﬁcation dataset.
Classes
2
Samples per class
212(M),357(B)
Samples total
569
Dimensionality
30
Features
real, positive
Parametersreturn_X_y : boolean, default=False
If True, returns (data,target) instead of a Bunch object. See below for more
information about the data and target object.
New in version 0.18.
Returnsdata : Bunch
Dictionary-like object, the interesting attributes are: ‘data’, the data to learn, ‘target’,
the classiﬁcation labels, ‘target_names’, the meaning of the labels, ‘feature_names’, the
meaning of the features, and ‘DESCR’, the full description of the dataset.
(data, target) : tuple if return_X_y is True
New in version 0.18.
The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is :
downloaded from: :
https://goo.gl/U2Uwz2 :
Examples
Let’s say you are interested in the samples 10, 50, and 85, and want to know their class name.
>>> from sklearn.datasets import load_breast_cancer
>>> data = load_breast_cancer()
>>> data.target[[10, 50, 85]]
array([0, 1, 0])
>>> list(data.target_names)
['malignant', 'benign']
29.6. sklearn.datasets: Datasets
1253
scikit-learn user guide, Release 0.18.2
sklearn.datasets.load_diabetes
sklearn.datasets.load_diabetes(return_X_y=False)
Load and return the diabetes dataset (regression).
Samples total
442
Dimensionality
10
Features
real, -.2 < x < .2
Targets
integer 25 - 346
Read more in the User Guide.
Parametersreturn_X_y : boolean, default=False.
If True, returns (data,target) instead of a Bunch object. See below for more
information about the data and target object.
New in version 0.18.
Returnsdata : Bunch
Dictionary-like object, the interesting attributes are: ‘data’, the data to learn and ‘target’,
the regression target for each sample.
(data, target) : tuple if return_X_y is True
New in version 0.18.
Examples using sklearn.datasets.load_diabetes
• Cross-validation on diabetes Dataset Exercise
• Lasso and Elastic Net
• Lasso path using LARS
• Lasso model selection: Cross-Validation / AIC / BIC
• Linear Regression Example
• Sparsity Example: Fitting only features 1 and 2
sklearn.datasets.load_digits
sklearn.datasets.load_digits(n_class=10, return_X_y=False)
Load and return the digits dataset (classiﬁcation).
Each datapoint is a 8x8 image of a digit.
Classes
10
Samples per class
~180
Samples total
1797
Dimensionality
64
Features
integers 0-16
Read more in the User Guide.
Parametersn_class : integer, between 0 and 10, optional (default=10)
The number of classes to return.
return_X_y : boolean, default=False.
1254
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
If True, returns (data,target) instead of a Bunch object. See below for more
information about the data and target object.
New in version 0.18.
Returnsdata : Bunch
Dictionary-like object, the interesting attributes are: ‘data’, the data to learn, ‘images’,
the images corresponding to each sample, ‘target’, the classiﬁcation labels for each
sample, ‘target_names’, the meaning of the labels, and ‘DESCR’, the full description of
the dataset.
(data, target) : tuple if return_X_y is True
New in version 0.18.
Examples
To load the data and visualize the images:
>>> from sklearn.datasets import load_digits
>>> digits = load_digits()
>>> print(digits.data.shape)
(1797, 64)
>>> import matplotlib.pyplot as plt
>>> plt.gray()
>>> plt.matshow(digits.images[0])
>>> plt.show()
Examples using sklearn.datasets.load_digits
• Selecting dimensionality reduction with Pipeline and GridSearchCV
• Pipelining: chaining a PCA and a logistic regression
• The Johnson-Lindenstrauss bound for embedding with random projections
• Explicit feature map approximation for RBF kernels
• Recognizing hand-written digits
• Feature agglomeration
• Various Agglomerative Clustering on a 2D embedding of digits
• A demo of K-Means clustering on the handwritten digits data
• The Digit Dataset
• Digits Classiﬁcation Exercise
• Cross-validation on Digits Dataset Exercise
• Recursive feature elimination
• L1 Penalty and Sparsity in Logistic Regression
• Comparing various online solvers
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
• Parameter estimation using grid search with cross-validation
29.6. sklearn.datasets: Datasets
1255
scikit-learn user guide, Release 0.18.2
• Plotting Learning Curves
• Plotting Validation Curves
• Comparing randomized search and grid search for hyperparameter estimation
• Kernel Density Estimation
• Compare Stochastic learning strategies for MLPClassiﬁer
• Restricted Boltzmann Machine features for digit classiﬁcation
• Label Propagation digits: Demonstrating performance
• Label Propagation digits active learning
• SVM-Anova: SVM with univariate feature selection
sklearn.datasets.load_ﬁles
sklearn.datasets.load_files(container_path,
description=None,
categories=None,
load_content=True,
shufﬂe=True,
encoding=None,
de-
code_error=’strict’, random_state=0)
Load text ﬁles with categories as subfolder names.
Individual samples are assumed to be ﬁles stored a two levels folder structure such as the following:
container_folder/
category_1_folder/ﬁle_1.txt ﬁle_2.txt ... ﬁle_42.txt
category_2_folder/ﬁle_43.txt ﬁle_44.txt ...
The folder names are used as supervised signal label names. The individual ﬁle names are not important.
This function does not try to extract features into a numpy array or scipy sparse matrix.
In addition, if
load_content is false it does not try to load the ﬁles in memory.
To use text ﬁles in a scikit-learn classiﬁcation or clustering algorithm, you will need to use the
sklearn.feature_extraction.text module to build a feature extraction transformer that suits your problem.
If you set load_content=True, you should also specify the encoding of the text using the ‘encoding’ parame-
ter. For many modern text ﬁles, ‘utf-8’ will be the correct encoding. If you leave encoding equal to None,
then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in
sklearn.feature_extraction.text.
Similar feature extractors should be built for other kind of unstructured data input such as images, audio, video,
...
Read more in the User Guide.
Parameterscontainer_path : string or unicode
Path to the main folder holding one subfolder per category
description: string or unicode, optional (default=None) :
A paragraph describing the characteristic of the dataset: its source, reference, etc.
categories : A collection of strings or None, optional (default=None)
If None (default), load all the categories. If not None, list of category names to load
(other categories ignored).
load_content : boolean, optional (default=True)
1256
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Whether to load or not the content of the different ﬁles. If true a ‘data’ attribute con-
taining the text information is present in the data structure returned. If not, a ﬁlenames
attribute gives the path to the ﬁles.
encoding : string or None (default is None)
If None, do not try to decode the content of the ﬁles (e.g. for images or other non-text
content). If not None, encoding to use to decode text ﬁles to Unicode if load_content is
True.
decode_error: {‘strict’, ‘ignore’, ‘replace’}, optional :
Instruction on what to do if a byte sequence is given to analyze that contains characters
not of the given encoding. Passed as keyword argument ‘errors’ to bytes.decode.
shufﬂe : bool, optional (default=True)
Whether or not to shufﬂe the data: might be important for models that make the as-
sumption that the samples are independent and identically distributed (i.i.d.), such as
stochastic gradient descent.
random_state : int, RandomState instance or None, optional (default=0)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
Returnsdata : Bunch
Dictionary-like object, the interesting attributes are: either data, the raw text data to
learn, or ‘ﬁlenames’, the ﬁles holding it, ‘target’, the classiﬁcation labels (integer index),
‘target_names’, the meaning of the labels, and ‘DESCR’, the full description of the
dataset.
sklearn.datasets.load_iris
sklearn.datasets.load_iris(return_X_y=False)
Load and return the iris dataset (classiﬁcation).
The iris dataset is a classic and very easy multi-class classiﬁcation dataset.
Classes
3
Samples per class
50
Samples total
150
Dimensionality
4
Features
real, positive
Read more in the User Guide.
Parametersreturn_X_y : boolean, default=False.
If True, returns (data,target) instead of a Bunch object. See below for more
information about the data and target object.
New in version 0.18.
Returnsdata : Bunch
Dictionary-like object, the interesting attributes are: ‘data’, the data to learn, ‘target’,
the classiﬁcation labels, ‘target_names’, the meaning of the labels, ‘feature_names’, the
meaning of the features, and ‘DESCR’, the full description of the dataset.
(data, target) : tuple if return_X_y is True
29.6. sklearn.datasets: Datasets
1257
scikit-learn user guide, Release 0.18.2
New in version 0.18.
Examples
Let’s say you are interested in the samples 10, 25, and 50, and want to know their class name.
>>> from sklearn.datasets import load_iris
>>> data = load_iris()
>>> data.target[[10, 25, 50]]
array([0, 0, 1])
>>> list(data.target_names)
['setosa', 'versicolor', 'virginica']
Examples using sklearn.datasets.load_iris
• Concatenating multiple feature extraction methods
• Plot classiﬁcation probability
• K-means Clustering
• The Iris Dataset
• Incremental PCA
• PCA example with Iris Data-set
• Comparison of LDA and PCA 2D projection of Iris dataset
• Plot the decision surfaces of ensembles of trees on the iris dataset
• Plot the decision boundaries of a VotingClassiﬁer
• SVM Exercise
• Univariate Feature Selection
• Test with permutations the signiﬁcance of a classiﬁcation score
• Gaussian process classiﬁcation (GPC) on iris dataset
• Logistic Regression 3-class Classiﬁer
• Path with L1- Logistic Regression
• Plot multi-class SGD on the iris dataset
• GMM covariances
• Confusion matrix
• Nested versus non-nested cross-validation
• Precision-Recall
• Receiver Operating Characteristic (ROC)
• Receiver Operating Characteristic (ROC) with cross validation
• Nearest Neighbors Classiﬁcation
• Nearest Centroid Classiﬁcation
• Compare Stochastic learning strategies for MLPClassiﬁer
1258
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Decision boundary of label propagation versus SVM on the Iris dataset
• SVM with custom kernel
• Plot different SVM classiﬁers in the iris dataset
• RBF SVM parameters
• Plot the decision surface of a decision tree on the iris dataset
• Understanding the decision tree structure
sklearn.datasets.fetch_lfw_pairs
sklearn.datasets.fetch_lfw_pairs(subset=’train’,
data_home=None,
funneled=True,
re-
size=0.5, color=False, slice_=(slice(70, 195, None), slice(78,
172, None)), download_if_missing=True)
Loader for the Labeled Faces in the Wild (LFW) pairs dataset
This dataset is a collection of JPEG pictures of famous people collected on the internet, all details are available
on the ofﬁcial website:
http://vis-www.cs.umass.edu/lfw/
Each picture is centered on a single face. Each pixel of each channel (color in RGB) is encoded by a ﬂoat in
range 0.0 - 1.0.
The task is called Face Veriﬁcation: given a pair of two pictures, a binary classiﬁer must predict whether the
two images are from the same person.
In the ofﬁcial README.txt this task is described as the “Restricted” task. As I am not sure as to implement the
“Unrestricted” variant correctly, I left it as unsupported for now.
The original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 74.
Read more in the User Guide.
Parameterssubset : optional, default: ‘train’
Select the dataset to load: ‘train’ for the development training set, ‘test’ for the develop-
ment test set, and ‘10_folds’ for the ofﬁcial evaluation set that is meant to be used with
a 10-folds cross validation.
data_home : optional, default: None
Specify another download and cache folder for the datasets. By default all scikit learn
data is stored in ‘~/scikit_learn_data’ subfolders.
funneled : boolean, optional, default: True
Download and use the funneled variant of the dataset.
resize : ﬂoat, optional, default 0.5
Ratio used to resize the each face picture.
color : boolean, optional, default False
Keep the 3 RGB channels instead of averaging them to a single gray level channel. If
color is True the shape of the data has one more dimension than the shape with color =
False.
slice_ : optional
29.6. sklearn.datasets: Datasets
1259
scikit-learn user guide, Release 0.18.2
Provide a custom 2D slice (height, width) to extract the ‘interesting’ part of the jpeg
ﬁles and avoid use statistical correlation from the background
download_if_missing : optional, True by default
If False, raise a IOError if the data is not locally available instead of trying to download
the data from the source site.
ReturnsThe data is returned as a Bunch object with the following attributes: :
data : numpy array of shape (2200, 5828). Shape depends on subset.
Each row corresponds to 2 ravel’d face images of original size 62 x 47 pixels. Changing
the slice_, resize or subset parameters will change the shape of the output.
pairs : numpy array of shape (2200, 2, 62, 47). Shape depends on
subset.
Each row has 2 face images corresponding to same or different person from the dataset
containing 5749 people. Changing the slice_, resize or subset parameters will
change the shape of the output.
target : numpy array of shape (2200,). Shape depends on subset.
Labels associated to each pair of images. The two label values being different persons
or the same person.
DESCR : string
Description of the Labeled Faces in the Wild (LFW) dataset.
sklearn.datasets.fetch_lfw_people
sklearn.datasets.fetch_lfw_people(data_home=None,
funneled=True,
resize=0.5,
min_faces_per_person=0,
color=False,
slice_=(slice(70,
195,
None),
slice(78,
172,
None)),
down-
load_if_missing=True)
Loader for the Labeled Faces in the Wild (LFW) people dataset
This dataset is a collection of JPEG pictures of famous people collected on the internet, all details are available
on the ofﬁcial website:
http://vis-www.cs.umass.edu/lfw/
Each picture is centered on a single face. Each pixel of each channel (color in RGB) is encoded by a ﬂoat in
range 0.0 - 1.0.
The task is called Face Recognition (or Identiﬁcation): given the picture of a face, ﬁnd the name of the person
given a training set (gallery).
The original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 74.
Parametersdata_home : optional, default: None
Specify another download and cache folder for the datasets. By default all scikit learn
data is stored in ‘~/scikit_learn_data’ subfolders.
funneled : boolean, optional, default: True
Download and use the funneled variant of the dataset.
resize : ﬂoat, optional, default 0.5
Ratio used to resize the each face picture.
1260
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
min_faces_per_person : int, optional, default None
The extracted dataset will only retain pictures of people that have at least
min_faces_per_person different pictures.
color : boolean, optional, default False
Keep the 3 RGB channels instead of averaging them to a single gray level channel. If
color is True the shape of the data has one more dimension than the shape with color =
False.
slice_ : optional
Provide a custom 2D slice (height, width) to extract the ‘interesting’ part of the jpeg
ﬁles and avoid use statistical correlation from the background
download_if_missing : optional, True by default
If False, raise a IOError if the data is not locally available instead of trying to download
the data from the source site.
Returnsdataset : dict-like object with the following attributes:
dataset.data : numpy array of shape (13233, 2914)
Each row corresponds to a ravelled face image of original size 62 x 47 pixels. Changing
the slice_ or resize parameters will change the shape of the output.
dataset.images : numpy array of shape (13233, 62, 47)
Each row is a face image corresponding to one of the 5749 people in the dataset. Chang-
ing the slice_ or resize parameters will change the shape of the output.
dataset.target : numpy array of shape (13233,)
Labels associated to each face image. Those labels range from 0-5748 and correspond
to the person IDs.
dataset.DESCR : string
Description of the Labeled Faces in the Wild (LFW) dataset.
Examples using sklearn.datasets.fetch_lfw_people
• Faces recognition example using eigenfaces and SVMs
sklearn.datasets.load_linnerud
sklearn.datasets.load_linnerud(return_X_y=False)
Load and return the linnerud dataset (multivariate regression).
Samples total: 20 Dimensionality: 3 for both data and targets Features: integer Targets: integer
Parametersreturn_X_y : boolean, default=False.
If True, returns (data,target) instead of a Bunch object. See below for more
information about the data and target object.
New in version 0.18.
Returnsdata : Bunch
29.6. sklearn.datasets: Datasets
1261
scikit-learn user guide, Release 0.18.2
Dictionary-like object, the interesting attributes are: ‘data’ and ‘targets’, the two mul-
tivariate datasets, with ‘data’ corresponding to the exercise and ‘targets’ corresponding
to the physiological measurements, as well as ‘feature_names’ and ‘target_names’.
(data, target) : tuple if return_X_y is True
New in version 0.18.
sklearn.datasets.mldata_ﬁlename
sklearn.datasets.mldata_filename(dataname)
Convert a raw name for a data set in a mldata.org ﬁlename.
sklearn.datasets.fetch_mldata
sklearn.datasets.fetch_mldata(dataname,
target_name=’label’,
data_name=’data’,
trans-
pose_data=True, data_home=None)
Fetch an mldata.org data set
If the ﬁle does not exist yet, it is downloaded from mldata.org .
mldata.org does not have an enforced convention for storing data or naming the columns in a data set. The
default behavior of this function works well with the most common cases:
1.data values are stored in the column ‘data’, and target values in the column ‘label’
2.alternatively, the ﬁrst column stores target values, and the second data values
3.the data array is stored as n_features x n_samples , and thus needs to be transposed to match the sklearn
standard
Keyword arguments allow to adapt these defaults to speciﬁc data sets (see parameters target_name, data_name,
transpose_data, and the examples below).
mldata.org data sets may have multiple columns, which are stored in the Bunch object with their original name.
Parametersdataname : :
Name of the data set on mldata.org, e.g.: “leukemia”, “Whistler Daily Snowfall”, etc.
The raw name is automatically converted to a mldata.org URL .
target_name : optional, default: ‘label’
Name or index of the column containing the target values.
data_name : optional, default: ‘data’
Name or index of the column containing the data.
transpose_data : optional, default: True
If True, transpose the downloaded data array.
data_home : optional, default: None
Specify another download and cache folder for the data sets. By default all scikit learn
data is stored in ‘~/scikit_learn_data’ subfolders.
Returnsdata : Bunch
Dictionary-like object, the interesting attributes are: ‘data’, the data to learn, ‘target’, the
classiﬁcation labels, ‘DESCR’, the full description of the dataset, and ‘COL_NAMES’,
the original names of the dataset columns.
1262
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
Load the ‘iris’ dataset from mldata.org:
>>> from sklearn.datasets.mldata import fetch_mldata
>>> import tempfile
>>> test_data_home = tempfile.mkdtemp()
>>> iris = fetch_mldata('iris', data_home=test_data_home)
>>> iris.target.shape
(150,)
>>> iris.data.shape
(150, 4)
Load the ‘leukemia’ dataset from mldata.org, which needs to be transposed to respects the scikit-learn axes
convention:
>>> leuk = fetch_mldata('leukemia', transpose_data=True,
...
data_home=test_data_home)
>>> leuk.data.shape
(72, 7129)
Load an alternative ‘iris’ dataset, which has different names for the columns:
>>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1,
...
data_name=0, data_home=test_data_home)
>>> iris3 = fetch_mldata('datasets-UCI iris',
...
target_name='class', data_name='double0',
...
data_home=test_data_home)
>>> import shutil
>>> shutil.rmtree(test_data_home)
Examples using sklearn.datasets.fetch_mldata
• Gaussian process regression (GPR) on Mauna Loa CO2 data.
• Visualization of MLP weights on MNIST
sklearn.datasets.fetch_olivetti_faces
sklearn.datasets.fetch_olivetti_faces(data_home=None,
shufﬂe=False,
random_state=0,
download_if_missing=True)
Loader for the Olivetti faces data-set from AT&T.
Read more in the User Guide.
Parametersdata_home : optional, default: None
Specify another download and cache folder for the datasets. By default all scikit learn
data is stored in ‘~/scikit_learn_data’ subfolders.
shufﬂe : boolean, optional
If True the order of the dataset is shufﬂed to avoid having images of the same person
grouped.
29.6. sklearn.datasets: Datasets
1263
scikit-learn user guide, Release 0.18.2
download_if_missing : optional, True by default
If False, raise a IOError if the data is not locally available instead of trying to download
the data from the source site.
random_state : optional, integer or RandomState object
The seed or the random number generator used to shufﬂe the data.
ReturnsAn object with the following attributes: :
data : numpy array of shape (400, 4096)
Each row corresponds to a ravelled face image of original size 64 x 64 pixels.
images : numpy array of shape (400, 64, 64)
Each row is a face image corresponding to one of the 40 subjects of the dataset.
target : numpy array of shape (400, )
Labels associated to each face image. Those labels are ranging from 0-39 and corre-
spond to the Subject IDs.
DESCR : string
Description of the modiﬁed Olivetti Faces Dataset.
Notes
This dataset consists of 10 pictures each of 40 individuals. The original database was available from (now
defunct)
http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
The version retrieved here comes in MATLAB format from the personal web page of Sam Roweis:
http://www.cs.nyu.edu/~roweis/
Examples using sklearn.datasets.fetch_olivetti_faces
• Face completion with a multi-output estimators
• Online learning of a dictionary of parts of faces
• Faces dataset decompositions
• Pixel importances with a parallel forest of trees
sklearn.datasets.fetch_california_housing
sklearn.datasets.fetch_california_housing(data_home=None,
down-
load_if_missing=True)
Loader for the California housing dataset from StatLib.
Read more in the User Guide.
Parametersdata_home : optional, default: None
Specify another download and cache folder for the datasets. By default all scikit learn
data is stored in ‘~/scikit_learn_data’ subfolders.
1264
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
download_if_missing: optional, True by default :
If False, raise a IOError if the data is not locally available instead of trying to download
the data from the source site.
Returnsdataset : dict-like object with the following attributes:
dataset.data : ndarray, shape [20640, 8]
Each row corresponding to the 8 feature values in order.
dataset.target : numpy array of shape (20640,)
Each value corresponds to the average house value in units of 100,000.
dataset.feature_names : array of length 8
Array of ordered feature names used in the dataset.
dataset.DESCR : string
Description of the California housing dataset.
Notes
This dataset consists of 20,640 samples and 9 features.
Examples using sklearn.datasets.fetch_california_housing
• Partial Dependence Plots
sklearn.datasets.fetch_covtype
sklearn.datasets.fetch_covtype(data_home=None,
download_if_missing=True,
ran-
dom_state=None, shufﬂe=False)
Load the covertype dataset, downloading it if necessary.
Read more in the User Guide.
Parametersdata_home : string, optional
Specify another download and cache folder for the datasets. By default all scikit learn
data is stored in ‘~/scikit_learn_data’ subfolders.
download_if_missing : boolean, default=True
If False, raise a IOError if the data is not locally available instead of trying to download
the data from the source site.
random_state : int, RandomState instance or None, optional (default=None)
Random state for shufﬂing the dataset. If int, random_state is the seed used by the ran-
dom number generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState instance used by
np.random.
shufﬂe : bool, default=False
Whether to shufﬂe dataset.
29.6. sklearn.datasets: Datasets
1265
scikit-learn user guide, Release 0.18.2
Returnsdataset : dict-like object with the following attributes:
dataset.data : numpy array of shape (581012, 54)
Each row corresponds to the 54 features in the dataset.
dataset.target : numpy array of shape (581012,)
Each value corresponds to one of the 7 forest covertypes with values ranging between 1
to 7.
dataset.DESCR : string
Description of the forest covertype dataset.
sklearn.datasets.fetch_kddcup99
sklearn.datasets.fetch_kddcup99(subset=None,
shufﬂe=False,
random_state=None,
per-
cent10=True, download_if_missing=True)
Load and return the kddcup 99 dataset (classiﬁcation).
The KDD Cup ‘99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detec-
tion System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1] . The artiﬁcial data was generated using
a closed network and hand-injected attacks to produce a large number of different types of attack with normal
activity in the background. As the initial goal was to produce a large training set for supervised learning algo-
rithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate
for unsupervised anomaly detection which aims at detecting ‘abnormal’ data, ie
1.qualitatively different from normal data.
2.in large minority among the observations.
We thus transform the KDD Data set into two different data sets: SA and SF.
•SA is obtained by simply selecting all the normal data, and a small proportion of abnormal data to gives
an anomaly proportion of 1%.
•SF is obtained as in [2] by simply picking up the data whose attribute logged_in is positive, thus focusing
on the intrusion attack, which gives a proportion of 0.3% of attack.
•http and smtp are two subsets of SF corresponding with third feature equal to ‘http’ (resp. to ‘smtp’)
General KDD structure :
Samples total
4898431
Dimensionality
41
Features
discrete (int) or continuous (ﬂoat)
Targets
str, ‘normal.’ or name of the anomaly type
SA structure :
Samples total
976158
Dimensionality
41
Features
discrete (int) or continuous (ﬂoat)
Targets
str, ‘normal.’ or name of the anomaly type
SF structure :
Samples total
699691
Dimensionality
4
Features
discrete (int) or continuous (ﬂoat)
Targets
str, ‘normal.’ or name of the anomaly type
http structure :
1266
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Samples total
619052
Dimensionality
3
Features
discrete (int) or continuous (ﬂoat)
Targets
str, ‘normal.’ or name of the anomaly type
smtp structure :
Samples total
95373
Dimensionality
3
Features
discrete (int) or continuous (ﬂoat)
Targets
str, ‘normal.’ or name of the anomaly type
New in version 0.18.
Parameterssubset : None, ‘SA’, ‘SF’, ‘http’, ‘smtp’
To return the corresponding classical subsets of kddcup 99. If None, return the entire
kddcup 99 dataset.
random_state : int, RandomState instance or None, optional (default=None)
Random state for shufﬂing the dataset. If int, random_state is the seed used by the ran-
dom number generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState instance used by
np.random.
shufﬂe : bool, default=False
Whether to shufﬂe dataset.
percent10 : bool, default=False
Whether to load only 10 percent of the data.
download_if_missing : bool, default=True
If False, raise a IOError if the data is not locally available instead of trying to download
the data from the source site.
Returnsdata : Bunch
Dictionary-like object, the interesting attributes are: ‘data’, the data to learn and ‘target’,
the regression target for each sample.
References
[R132], [R133]
sklearn.datasets.fetch_rcv1
sklearn.datasets.fetch_rcv1(data_home=None, subset=’all’, download_if_missing=True, ran-
dom_state=None, shufﬂe=False)
Load the RCV1 multilabel dataset, downloading it if necessary.
Version: RCV1-v2, vectors, full sets, topics multilabels.
Classes
103
Samples total
804414
Dimensionality
47236
Features
real, between 0 and 1
29.6. sklearn.datasets: Datasets
1267
scikit-learn user guide, Release 0.18.2
Read more in the User Guide.
New in version 0.17.
Parametersdata_home : string, optional
Specify another download and cache folder for the datasets. By default all scikit learn
data is stored in ‘~/scikit_learn_data’ subfolders.
subset: string, ‘train’, ‘test’, or ‘all’, default=’all’ :
Select the dataset to load: ‘train’ for the training set (23149 samples), ‘test’ for the test
set (781265 samples), ‘all’ for both, with the training samples ﬁrst if shufﬂe is False.
This follows the ofﬁcial LYRL2004 chronological split.
download_if_missing : boolean, default=True
If False, raise a IOError if the data is not locally available instead of trying to download
the data from the source site.
random_state : int, RandomState instance or None, optional (default=None)
Random state for shufﬂing the dataset. If int, random_state is the seed used by the ran-
dom number generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState instance used by
np.random.
shufﬂe : bool, default=False
Whether to shufﬂe dataset.
Returnsdataset : dict-like object with the following attributes:
dataset.data : scipy csr array, dtype np.ﬂoat64, shape (804414, 47236)
The array has 0.16% of non zero values.
dataset.target : scipy csr array, dtype np.uint8, shape (804414, 103)
Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of
non zero values.
dataset.sample_id : numpy array, dtype np.uint32, shape (804414,)
Identiﬁcation number of each sample, as ordered in dataset.data.
dataset.target_names : numpy array, dtype object, length (103)
Names of each target (RCV1 topics), as ordered in dataset.target.
dataset.DESCR : string
Description of the RCV1 dataset.
References
Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization
research. The Journal of Machine Learning Research, 5, 361-397.
sklearn.datasets.load_mlcomp
sklearn.datasets.load_mlcomp(name_or_id, set_=’raw’, mlcomp_root=None, **kwargs)
Load a datasets as downloaded from http://mlcomp.org
1268
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Parametersname_or_id : the integer id or the string name metadata of the MLComp
dataset to load
set_ : select the portion to load: ‘train’, ‘test’ or ‘raw’
mlcomp_root : the ﬁlesystem path to the root folder where MLComp datasets
are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment
variable is looked up instead.
**kwargs : domain speciﬁc kwargs to be passed to the dataset loader.
Read more in the :ref:‘User Guide <datasets>‘. :
Returnsdata : Bunch
Dictionary-like object, the interesting attributes are: ‘ﬁlenames’, the ﬁles holding the
raw to learn, ‘target’, the classiﬁcation labels (integer index), ‘target_names’, the mean-
ing of the labels, and ‘DESCR’, the full description of the dataset.
Note on the lookup process: depending on the type of name_or_id, :
will choose between integer id lookup or metadata name lookup by :
looking at the unzipped archives and metadata ﬁle. :
TODO: implement zip dataset loading too :
Examples using sklearn.datasets.load_mlcomp
• Classiﬁcation of text documents: using a MLComp dataset
sklearn.datasets.load_sample_image
sklearn.datasets.load_sample_image(image_name)
Load the numpy array of a single sample image
Parametersimage_name: {‘china.jpg‘, ‘ﬂower.jpg‘} :
The name of the sample image loaded
Returnsimg: 3D array :
The image as a numpy array: height x width x color
Examples
>>> from sklearn.datasets import load_sample_image
>>> china = load_sample_image('china.jpg')
>>> china.dtype
dtype('uint8')
>>> china.shape
(427, 640, 3)
>>> flower = load_sample_image('flower.jpg')
>>> flower.dtype
dtype('uint8')
>>> flower.shape
(427, 640, 3)
29.6. sklearn.datasets: Datasets
1269
scikit-learn user guide, Release 0.18.2
Examples using sklearn.datasets.load_sample_image
• Color Quantization using K-Means
sklearn.datasets.load_sample_images
sklearn.datasets.load_sample_images()
Load sample images for image manipulation. Loads both, china and flower.
Returnsdata : Bunch
Dictionary-like object with the following attributes : ‘images’, the two sample images,
‘ﬁlenames’, the ﬁle names for the images, and ‘DESCR’ the full description of the
dataset.
Examples
To load the data and visualize the images:
>>> from sklearn.datasets import load_sample_images
>>> dataset = load_sample_images()
>>> len(dataset.images)
2
>>> first_img_data = dataset.images[0]
>>> first_img_data.shape
(427, 640, 3)
>>> first_img_data.dtype
dtype('uint8')
sklearn.datasets.fetch_species_distributions
sklearn.datasets.fetch_species_distributions(data_home=None,
down-
load_if_missing=True)
Loader for species distribution dataset from Phillips et. al. (2006)
Read more in the User Guide.
Parametersdata_home : optional, default: None
Specify another download and cache folder for the datasets. By default all scikit learn
data is stored in ‘~/scikit_learn_data’ subfolders.
download_if_missing : optional, True by default
If False, raise a IOError if the data is not locally available instead of trying to download
the data from the source site.
ReturnsThe data is returned as a Bunch object with the following attributes: :
coverages : array, shape = [14, 1592, 1212]
These represent the 14 features measured at each point of the map grid. The lati-
tude/longitude values for the grid are discussed below. Missing data is represented
by the value -9999.
train : record array, shape = (1623,)
The training points for the data. Each point has three ﬁelds:
1270
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
•train[’species’] is the species name
•train[’dd long’] is the longitude, in degrees
•train[’dd lat’] is the latitude, in degrees
test : record array, shape = (619,)
The test points for the data. Same format as the training data.
Nx, Ny : integers
The number of longitudes (x) and latitudes (y) in the grid
x_left_lower_corner, y_left_lower_corner : ﬂoats
The (x,y) position of the lower-left corner, in degrees
grid_size : ﬂoat
The spacing between points of the grid, in degrees
Notes
•See examples/applications/plot_species_distribution_modeling.py for an example of using this dataset
with scikit-learn
References
•“Maximum entropy modeling of species geographic distributions” S. J. Phillips, R. P. Anderson, R. E.
Schapire - Ecological Modelling, 190:231-259, 2006.
Examples using sklearn.datasets.fetch_species_distributions
• Species distribution modeling
• Kernel Density Estimate of Species Distributions
sklearn.datasets.load_svmlight_ﬁle
sklearn.datasets.load_svmlight_file(f, n_features=None, dtype=<type ‘numpy.ﬂoat64’>, mul-
tilabel=False, zero_based=’auto’, query_id=False)
Load datasets in the svmlight / libsvm format into sparse CSR matrix
This format is a text-based format, with one sample per line. It does not store zero valued features hence is
suitable for sparse dataset.
The ﬁrst element of each line can be used to store a target variable to predict.
This format is used as the default format for both svmlight and the libsvm command line programs.
Parsing a text based source can be expensive. When working on repeatedly on the same dataset, it is recom-
mended to wrap this loader with joblib.Memory.cache to store a memmapped backup of the CSR results of the
ﬁrst call and beneﬁt from the near instantaneous loading of memmapped structures for the subsequent calls.
In case the ﬁle contains a pairwise preference constraint (known as “qid” in the svmlight format) these are
ignored unless the query_id parameter is set to True. These pairwise preference constraints can be used to
29.6. sklearn.datasets: Datasets
1271
scikit-learn user guide, Release 0.18.2
constraint the combination of samples when using pairwise loss functions (as is the case in some learning to
rank problems) so that only pairs with the same query_id value are considered.
This implementation is written in Cython and is reasonably fast. However, a faster API-compatible loader is
also available at:
https://github.com/mblondel/svmlight-loader
Parametersf : {str, ﬁle-like, int}
(Path to) a ﬁle to load. If a path ends in ”.gz” or ”.bz2”, it will be uncompressed on
the ﬂy. If an integer is passed, it is assumed to be a ﬁle descriptor. A ﬁle-like or ﬁle
descriptor will not be closed by this function. A ﬁle-like object must be opened in binary
mode.
n_features : int or None
The number of features to use. If None, it will be inferred. This argument is useful
to load several ﬁles that are subsets of a bigger sliced dataset: each subset might not
have examples of every feature, hence the inferred shape might vary from one slice to
another.
multilabel : boolean, optional, default False
Samples
may
have
several
labels
each
(see
http://www.csie.ntu.edu.tw/~cjlin/
libsvmtools/datasets/multilabel.html)
zero_based : boolean or “auto”, optional, default “auto”
Whether column indices in f are zero-based (True) or one-based (False). If column in-
dices are one-based, they are transformed to zero-based to match Python/NumPy con-
ventions. If set to “auto”, a heuristic check is applied to determine this from the ﬁle
contents. Both kinds of ﬁles occur “in the wild”, but they are unfortunately not self-
identifying. Using “auto” or True should always be safe.
query_id : boolean, default False
If True, will return the query_id array for each ﬁle.
dtype : numpy data type, default np.ﬂoat64
Data type of dataset to be loaded. This will be the data type of the output numpy arrays
X and y.
ReturnsX : scipy.sparse matrix of shape (n_samples, n_features)
y : ndarray of shape (n_samples,), or, in the multilabel a list of
tuples of length n_samples.
query_id : array of shape (n_samples,)
query_id for each sample. Only returned when query_id is set to True.
See also:
load_svmlight_filessimilar function for loading multiple ﬁles in this
format, enforcing
1272
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
To use joblib.Memory to cache the svmlight ﬁle:
from sklearn.externals.joblib import Memory
from sklearn.datasets import load_svmlight_file
mem = Memory("./mycache")
@mem.cache
def get_data():
data = load_svmlight_file("mysvmlightfile")
return data[0], data[1]
X, y = get_data()
sklearn.datasets.load_svmlight_ﬁles
sklearn.datasets.load_svmlight_files(ﬁles, n_features=None, dtype=<type ‘numpy.ﬂoat64’>,
multilabel=False,
zero_based=’auto’,
query_id=False)
Load dataset from multiple ﬁles in SVMlight format
This function is equivalent to mapping load_svmlight_ﬁle over a list of ﬁles, except that the results are concate-
nated into a single, ﬂat list and the samples vectors are constrained to all have the same number of features.
In case the ﬁle contains a pairwise preference constraint (known as “qid” in the svmlight format) these are
ignored unless the query_id parameter is set to True. These pairwise preference constraints can be used to
constraint the combination of samples when using pairwise loss functions (as is the case in some learning to
rank problems) so that only pairs with the same query_id value are considered.
Parametersﬁles : iterable over {str, ﬁle-like, int}
(Paths of) ﬁles to load. If a path ends in ”.gz” or ”.bz2”, it will be uncompressed on
the ﬂy. If an integer is passed, it is assumed to be a ﬁle descriptor. File-likes and ﬁle
descriptors will not be closed by this function. File-like objects must be opened in
binary mode.
n_features : int or None
The number of features to use. If None, it will be inferred from the maximum column
index occurring in any of the ﬁles.
This can be set to a higher value than the actual number of features in any of the input
ﬁles, but setting it to a lower value will cause an exception to be raised.
multilabel : boolean, optional
Samples
may
have
several
labels
each
(see
http://www.csie.ntu.edu.tw/~cjlin/
libsvmtools/datasets/multilabel.html)
zero_based : boolean or “auto”, optional
Whether column indices in f are zero-based (True) or one-based (False). If column in-
dices are one-based, they are transformed to zero-based to match Python/NumPy con-
ventions. If set to “auto”, a heuristic check is applied to determine this from the ﬁle
contents. Both kinds of ﬁles occur “in the wild”, but they are unfortunately not self-
identifying. Using “auto” or True should always be safe.
query_id : boolean, defaults to False
29.6. sklearn.datasets: Datasets
1273
scikit-learn user guide, Release 0.18.2
If True, will return the query_id array for each ﬁle.
dtype : numpy data type, default np.ﬂoat64
Data type of dataset to be loaded. This will be the data type of the output numpy arrays
X and y.
Returns[X1, y1, ..., Xn, yn] :
where each (Xi, yi) pair is the result from load_svmlight_ﬁle(ﬁles[i]). :
If query_id is set to True, this will return instead [X1, y1, q1, :
..., Xn, yn, qn] where (Xi, yi, qi) is the result from :
load_svmlight_ﬁle(ﬁles[i]) :
See also:
load_svmlight_file
Notes
When ﬁtting a model to a matrix X_train and evaluating it against a matrix X_test, it is essential that X_train
and X_test have the same number of features (X_train.shape[1] == X_test.shape[1]). This may not be the case
if you load the ﬁles individually with load_svmlight_ﬁle.
sklearn.datasets.dump_svmlight_ﬁle
sklearn.datasets.dump_svmlight_file(X,
y,
f,
zero_based=True,
comment=None,
query_id=None, multilabel=False)
Dump the dataset in svmlight / libsvm ﬁle format.
This format is a text-based format, with one sample per line. It does not store zero valued features hence is
suitable for sparse dataset.
The ﬁrst element of each line can be used to store a target variable to predict.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : {array-like, sparse matrix}, shape = [n_samples (, n_labels)]
Target values. Class labels must be an integer or ﬂoat, or array-like objects of integer or
ﬂoat for multilabel classiﬁcations.
f : string or ﬁle-like in binary mode
If string, speciﬁes the path that will contain the data. If ﬁle-like, data will be written to
f. f should be opened in binary mode.
zero_based : boolean, optional
Whether column indices should be written zero-based (True) or one-based (False).
comment : string, optional
Comment to insert at the top of the ﬁle. This should be either a Unicode string, which
will be encoded as UTF-8, or an ASCII byte string. If a comment is given, then it will
be preceded by one that identiﬁes the ﬁle as having been dumped by scikit-learn. Note
that not all tools grok comments in SVMlight ﬁles.
1274
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
query_id : array-like, shape = [n_samples]
Array containing pairwise preference constraints (qid in svmlight format).
multilabel : boolean, optional
Samples
may
have
several
labels
each
(see
http://www.csie.ntu.edu.tw/~cjlin/
libsvmtools/datasets/multilabel.html)
New in version 0.17: parameter multilabel to support multilabel datasets.
Examples using sklearn.datasets.dump_svmlight_file
• Libsvm GUI
29.6.2 Samples generator
datasets.make_blobs([n_samples, n_features, ...])
Generate isotropic Gaussian blobs for clustering.
datasets.make_classification([n_samples,
...])
Generate a random n-class classiﬁcation problem.
datasets.make_circles([n_samples, shufﬂe, ...])
Make a large circle containing a smaller circle in 2d.
datasets.make_friedman1([n_samples, ...])
Generate the “Friedman #1” regression problem
datasets.make_friedman2([n_samples, noise, ...])
Generate the “Friedman #2” regression problem
datasets.make_friedman3([n_samples, noise, ...])
Generate the “Friedman #3” regression problem
datasets.make_gaussian_quantiles([mean,
...])
Generate isotropic Gaussian and label samples by quantile
datasets.make_hastie_10_2([n_samples, ...])
Generates data for binary classiﬁcation used in Hastie et al.
datasets.make_low_rank_matrix([n_samples,
...])
Generate a mostly low rank matrix with bell-shaped singu-
lar values
datasets.make_moons([n_samples, shufﬂe, ...])
Make two interleaving half circles
datasets.make_multilabel_classification([...])Generate a random multilabel classiﬁcation problem.
datasets.make_regression([n_samples, ...])
Generate a random regression problem.
datasets.make_s_curve([n_samples, noise, ...])
Generate an S curve dataset.
datasets.make_sparse_coded_signal(n_samples,
...)
Generate a signal as a sparse combination of dictionary el-
ements.
datasets.make_sparse_spd_matrix([dim, ...])
Generate a sparse symmetric deﬁnite positive matrix.
datasets.make_sparse_uncorrelated([...])
Generate a random regression problem with sparse uncor-
related design
datasets.make_spd_matrix(n_dim[,
ran-
dom_state])
Generate a random symmetric, positive-deﬁnite matrix.
datasets.make_swiss_roll([n_samples,
noise,
...])
Generate a swiss roll dataset.
datasets.make_biclusters(shape, n_clusters)
Generate an array with constant block diagonal structure
for biclustering.
datasets.make_checkerboard(shape, n_clusters)
Generate an array with block checkerboard structure for bi-
clustering.
sklearn.datasets.make_blobs
sklearn.datasets.make_blobs(n_samples=100,
n_features=2,
centers=3,
cluster_std=1.0,
center_box=(-10.0, 10.0), shufﬂe=True, random_state=None)
Generate isotropic Gaussian blobs for clustering.
29.6. sklearn.datasets: Datasets
1275
scikit-learn user guide, Release 0.18.2
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The total number of points equally divided among clusters.
n_features : int, optional (default=2)
The number of features for each sample.
centers : int or array of shape [n_centers, n_features], optional
(default=3) The number of centers to generate, or the ﬁxed center locations.
cluster_std : ﬂoat or sequence of ﬂoats, optional (default=1.0)
The standard deviation of the clusters.
center_box : pair of ﬂoats (min, max), optional (default=(-10.0, 10.0))
The bounding box for each cluster center when centers are generated at random.
shufﬂe : boolean, optional (default=True)
Shufﬂe the samples.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, n_features]
The generated samples.
y : array of shape [n_samples]
The integer labels for cluster membership of each sample.
See also:
make_classificationa more intricate variant
Examples
>>> from sklearn.datasets.samples_generator import make_blobs
>>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,
...
random_state=0)
>>> print(X.shape)
(10, 2)
>>> y
array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])
Examples using sklearn.datasets.make_blobs
• Probability calibration of classiﬁers
• Probability Calibration for 3-class classiﬁcation
• Normal and Shrinkage Linear Discriminant Analysis for classiﬁcation
• Demo of afﬁnity propagation clustering algorithm
1276
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Compare BIRCH and MiniBatchKMeans
• Comparing different clustering algorithms on toy datasets
• Demo of DBSCAN clustering algorithm
• Demonstration of k-means assumptions
• Selecting the number of clusters with silhouette analysis on KMeans clustering
• A demo of the mean-shift clustering algorithm
• Comparison of the K-Means and MiniBatchKMeans clustering algorithms
• Plot randomly generated classiﬁcation dataset
• Plot multinomial and One-vs-Rest Logistic Regression
• SGD: Maximum margin separating hyperplane
• Hyper-parameters of Approximate Nearest Neighbors
• Scalability of Approximate Nearest Neighbors
sklearn.datasets.make_classiﬁcation
sklearn.datasets.make_classification(n_samples=100,
n_features=20,
n_informative=2,
n_redundant=2,
n_repeated=0,
n_classes=2,
n_clusters_per_class=2, weights=None, ﬂip_y=0.01,
class_sep=1.0, hypercube=True, shift=0.0, scale=1.0,
shufﬂe=True, random_state=None)
Generate a random n-class classiﬁcation problem.
This initially creates clusters of points normally distributed (std=1) about vertices of a 2 * class_sep-sided
hypercube, and assigns an equal number of clusters to each class. It introduces interdependence between these
features and adds various types of further noise to the data.
Prior to shufﬂing, X stacks a number of these primary “informative” features, “redundant” linear combinations
of these, “repeated” duplicates of sampled features, and arbitrary noise for and remaining features.
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of samples.
n_features : int, optional (default=20)
The total number of features.
These comprise n_informative informative fea-
tures, n_redundant redundant features, n_repeated duplicated features and n_features-
n_informative-n_redundant- n_repeated useless features drawn at random.
n_informative : int, optional (default=2)
The number of informative features. Each class is composed of a number of gaussian
clusters each located around the vertices of a hypercube in a subspace of dimension
n_informative. For each cluster, informative features are drawn independently from
N(0, 1) and then randomly linearly combined within each cluster in order to add covari-
ance. The clusters are then placed on the vertices of the hypercube.
n_redundant : int, optional (default=2)
The number of redundant features. These features are generated as random linear com-
binations of the informative features.
29.6. sklearn.datasets: Datasets
1277
scikit-learn user guide, Release 0.18.2
n_repeated : int, optional (default=0)
The number of duplicated features, drawn randomly from the informative and the re-
dundant features.
n_classes : int, optional (default=2)
The number of classes (or labels) of the classiﬁcation problem.
n_clusters_per_class : int, optional (default=2)
The number of clusters per class.
weights : list of ﬂoats or None (default=None)
The proportions of samples assigned to each class. If None, then classes are balanced.
Note that if len(weights) == n_classes - 1, then the last class weight is automatically
inferred. More than n_samples samples may be returned if the sum of weights exceeds
1.
ﬂip_y : ﬂoat, optional (default=0.01)
The fraction of samples whose class are randomly exchanged.
class_sep : ﬂoat, optional (default=1.0)
The factor multiplying the hypercube dimension.
hypercube : boolean, optional (default=True)
If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put
on the vertices of a random polytope.
shift : ﬂoat, array of shape [n_features] or None, optional (default=0.0)
Shift features by the speciﬁed value. If None, then features are shifted by a random
value drawn in [-class_sep, class_sep].
scale : ﬂoat, array of shape [n_features] or None, optional (default=1.0)
Multiply features by the speciﬁed value. If None, then features are scaled by a random
value drawn in [1, 100]. Note that scaling happens after shifting.
shufﬂe : boolean, optional (default=True)
Shufﬂe the samples and the features.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, n_features]
The generated samples.
y : array of shape [n_samples]
The integer labels for class membership of each sample.
See also:
make_blobssimpliﬁed variant
make_multilabel_classificationunrelated generator for multilabel tasks
1278
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
The algorithm is adapted from Guyon [1] and was designed to generate the “Madelon” dataset.
References
[R7]
Examples using sklearn.datasets.make_classification
• Probability Calibration curves
• Comparison of Calibration of Classiﬁers
• Classiﬁer comparison
• Plot randomly generated classiﬁcation dataset
• OOB Errors for Random Forests
• Feature transformations with ensembles of trees
• Feature importances with forests of trees
• Pipeline Anova SVM
• Recursive feature elimination with cross-validation
• Varying regularization in Multi-layer Perceptron
• Scaling the regularization parameter for SVCs
sklearn.datasets.make_circles
sklearn.datasets.make_circles(n_samples=100, shufﬂe=True, noise=None, random_state=None,
factor=0.8)
Make a large circle containing a smaller circle in 2d.
A simple toy dataset to visualize clustering and classiﬁcation algorithms.
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The total number of points generated.
shufﬂe: bool, optional (default=True) :
Whether to shufﬂe the samples.
noise : double or None (default=None)
Standard deviation of Gaussian noise added to the data.
factor : double < 1 (default=.8)
Scale factor between inner and outer circle.
ReturnsX : array of shape [n_samples, 2]
The generated samples.
y : array of shape [n_samples]
29.6. sklearn.datasets: Datasets
1279
scikit-learn user guide, Release 0.18.2
The integer labels (0 or 1) for class membership of each sample.
Examples using sklearn.datasets.make_circles
• Classiﬁer comparison
• Comparing different clustering algorithms on toy datasets
• Kernel PCA
• Hashing feature transformation using Totally Random Trees
• Varying regularization in Multi-layer Perceptron
• Compare Stochastic learning strategies for MLPClassiﬁer
• Label Propagation learning a complex structure
sklearn.datasets.make_friedman1
sklearn.datasets.make_friedman1(n_samples=100,
n_features=10,
noise=0.0,
ran-
dom_state=None)
Generate the “Friedman #1” regression problem
This dataset is described in Friedman [1] and Breiman [2].
Inputs X are independent features uniformly distributed on the interval [0, 1]. The output y is created according
to the formula:
y(X) = 10 * sin(pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:,
˓→3] + 5 * X[:, 4] + noise * N(0, 1).
Out of the n_features features, only 5 are actually used to compute y. The remaining features are independent
of y.
The number of features has to be >= 5.
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of samples.
n_features : int, optional (default=10)
The number of features. Should be at least 5.
noise : ﬂoat, optional (default=0.0)
The standard deviation of the gaussian noise applied to the output.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, n_features]
The input samples.
y : array of shape [n_samples]
The output values.
1280
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
[R137], [R138]
sklearn.datasets.make_friedman2
sklearn.datasets.make_friedman2(n_samples=100, noise=0.0, random_state=None)
Generate the “Friedman #2” regression problem
This dataset is described in Friedman [1] and Breiman [2].
Inputs X are 4 independent features uniformly distributed on the intervals:
0 <= X[:, 0] <= 100,
40 * pi <= X[:, 1] <= 560 * pi,
0 <= X[:, 2] <= 1,
1 <= X[:, 3] <= 11.
The output y is created according to the formula:
y(X) = (X[:, 0] ** 2 + (X[:, 1] * X[:, 2]
- 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.
˓→5 + noise * N(0, 1).
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of samples.
noise : ﬂoat, optional (default=0.0)
The standard deviation of the gaussian noise applied to the output.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, 4]
The input samples.
y : array of shape [n_samples]
The output values.
References
[R139], [R140]
sklearn.datasets.make_friedman3
sklearn.datasets.make_friedman3(n_samples=100, noise=0.0, random_state=None)
Generate the “Friedman #3” regression problem
This dataset is described in Friedman [1] and Breiman [2].
Inputs X are 4 independent features uniformly distributed on the intervals:
29.6. sklearn.datasets: Datasets
1281
scikit-learn user guide, Release 0.18.2
0 <= X[:, 0] <= 100,
40 * pi <= X[:, 1] <= 560 * pi,
0 <= X[:, 2] <= 1,
1 <= X[:, 3] <= 11.
The output y is created according to the formula:
y(X) = arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) / X[:, 0]) + noise *
˓→N(0, 1).
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of samples.
noise : ﬂoat, optional (default=0.0)
The standard deviation of the gaussian noise applied to the output.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, 4]
The input samples.
y : array of shape [n_samples]
The output values.
References
[R141], [R142]
sklearn.datasets.make_gaussian_quantiles
sklearn.datasets.make_gaussian_quantiles(mean=None,
cov=1.0,
n_samples=100,
n_features=2,
n_classes=3,
shufﬂe=True,
random_state=None)
Generate isotropic Gaussian and label samples by quantile
This classiﬁcation dataset is constructed by taking a multi-dimensional standard normal distribution and deﬁning
classes separated by nested concentric multi-dimensional spheres such that roughly equal numbers of samples
are in each class (quantiles of the 𝜒2 distribution).
Read more in the User Guide.
Parametersmean : array of shape [n_features], optional (default=None)
The mean of the multi-dimensional normal distribution. If None then use the origin (0,
0, ...).
cov : ﬂoat, optional (default=1.)
The covariance matrix will be this value times the unit matrix. This dataset only pro-
duces symmetric normal distributions.
1282
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
n_samples : int, optional (default=100)
The total number of points equally divided among classes.
n_features : int, optional (default=2)
The number of features for each sample.
n_classes : int, optional (default=3)
The number of classes
shufﬂe : boolean, optional (default=True)
Shufﬂe the samples.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, n_features]
The generated samples.
y : array of shape [n_samples]
The integer labels for quantile membership of each sample.
Notes
The dataset is from Zhu et al [1].
References
[R8]
Examples using sklearn.datasets.make_gaussian_quantiles
• Plot randomly generated classiﬁcation dataset
• Multi-class AdaBoosted Decision Trees
• Two-class AdaBoost
sklearn.datasets.make_hastie_10_2
sklearn.datasets.make_hastie_10_2(n_samples=12000, random_state=None)
Generates data for binary classiﬁcation used in Hastie et al. 2009, Example 10.2.
The ten features are standard independent Gaussian and the target y is deﬁned by:
y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1
Read more in the User Guide.
Parametersn_samples : int, optional (default=12000)
The number of samples.
29.6. sklearn.datasets: Datasets
1283
scikit-learn user guide, Release 0.18.2
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, 10]
The input samples.
y : array of shape [n_samples]
The output values.
See also:
make_gaussian_quantilesa generalization of this dataset approach
References
[R9]
Examples using sklearn.datasets.make_hastie_10_2
• Discrete versus Real AdaBoost
• Gradient Boosting regularization
sklearn.datasets.make_low_rank_matrix
sklearn.datasets.make_low_rank_matrix(n_samples=100, n_features=100, effective_rank=10,
tail_strength=0.5, random_state=None)
Generate a mostly low rank matrix with bell-shaped singular values
Most of the variance can be explained by a bell-shaped curve of width effective_rank: the low rank part of the
singular values proﬁle is:
(1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)
The remaining singular values’ tail is fat, decreasing as:
tail_strength * exp(-0.1 * i / effective_rank).
The low rank part of the proﬁle can be considered the structured signal part of the data while the tail can be
considered the noisy part of the data that cannot be summarized by a low number of linear components (singular
vectors).
This kind of singular proﬁles is often seen in practice, for instance:
•gray level pictures of faces
•TF-IDF vectors of text documents crawled from the web
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of samples.
1284
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
n_features : int, optional (default=100)
The number of features.
effective_rank : int, optional (default=10)
The approximate number of singular vectors required to explain most of the data by
linear combinations.
tail_strength : ﬂoat between 0.0 and 1.0, optional (default=0.5)
The relative importance of the fat noisy tail of the singular values proﬁle.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, n_features]
The matrix.
sklearn.datasets.make_moons
sklearn.datasets.make_moons(n_samples=100, shufﬂe=True, noise=None, random_state=None)
Make two interleaving half circles
A simple toy dataset to visualize clustering and classiﬁcation algorithms. Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The total number of points generated.
shufﬂe : bool, optional (default=True)
Whether to shufﬂe the samples.
noise : double or None (default=None)
Standard deviation of Gaussian noise added to the data.
ReturnsX : array of shape [n_samples, 2]
The generated samples.
y : array of shape [n_samples]
The integer labels (0 or 1) for class membership of each sample.
Examples using sklearn.datasets.make_moons
• Classiﬁer comparison
• Comparing different clustering algorithms on toy datasets
• Varying regularization in Multi-layer Perceptron
• Compare Stochastic learning strategies for MLPClassiﬁer
29.6. sklearn.datasets: Datasets
1285
scikit-learn user guide, Release 0.18.2
sklearn.datasets.make_multilabel_classiﬁcation
sklearn.datasets.make_multilabel_classification(n_samples=100,
n_features=20,
n_classes=5, n_labels=2, length=50,
allow_unlabeled=True, sparse=False,
return_indicator=’dense’,
re-
turn_distributions=False,
ran-
dom_state=None)
Generate a random multilabel classiﬁcation problem.
For each sample, the generative process is:
•pick the number of labels: n ~ Poisson(n_labels)
•n times, choose a class c: c ~ Multinomial(theta)
•pick the document length: k ~ Poisson(length)
•k times, choose a word: w ~ Multinomial(theta_c)
In the above process, rejection sampling is used to make sure that n is never zero or more than n_classes, and
that the document length is never zero. Likewise, we reject classes which have already been chosen.
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of samples.
n_features : int, optional (default=20)
The total number of features.
n_classes : int, optional (default=5)
The number of classes of the classiﬁcation problem.
n_labels : int, optional (default=2)
The average number of labels per instance. More precisely, the number of labels per
sample is drawn from a Poisson distribution with n_labels as its expected value, but
samples are bounded (using rejection sampling) by n_classes, and must be nonzero
if allow_unlabeled is False.
length : int, optional (default=50)
The sum of the features (number of words if documents) is drawn from a Poisson dis-
tribution with this expected value.
allow_unlabeled : bool, optional (default=True)
If True, some instances might not belong to any class.
sparse : bool, optional (default=False)
If True, return a sparse feature matrix
New in version 0.17: parameter to allow sparse output.
return_indicator : ‘dense’ (default) | ‘sparse’ | False
If dense return Y in the dense binary indicator format. If 'sparse' return Y in the
sparse binary indicator format. False returns a list of lists of labels.
return_distributions : bool, optional (default=False)
1286
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
If True, return the prior class probability and conditional probabilities of features given
classes, from which the data was drawn.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, n_features]
The generated samples.
Y : array or sparse CSR matrix of shape [n_samples, n_classes]
The label sets.
p_c : array, shape [n_classes]
The
probability
of
each
class
being
drawn.
Only
returned
if
return_distributions=True.
p_w_c : array, shape [n_features, n_classes]
The probability of each feature being drawn given each class.
Only returned if
return_distributions=True.
Examples using sklearn.datasets.make_multilabel_classification
• Multilabel classiﬁcation
• Plot randomly generated multilabel dataset
sklearn.datasets.make_regression
sklearn.datasets.make_regression(n_samples=100,
n_features=100,
n_informative=10,
n_targets=1,
bias=0.0,
effective_rank=None,
tail_strength=0.5,
noise=0.0,
shufﬂe=True,
coef=False,
random_state=None)
Generate a random regression problem.
The input set can either be well conditioned (by default) or have a low rank-fat tail singular proﬁle. See
make_low_rank_matrix for more details.
The output is generated by applying a (potentially biased) random linear regression model with n_informative
nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable
scale.
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of samples.
n_features : int, optional (default=100)
The number of features.
n_informative : int, optional (default=10)
The number of informative features, i.e., the number of features used to build the linear
model used to generate the output.
29.6. sklearn.datasets: Datasets
1287
scikit-learn user guide, Release 0.18.2
n_targets : int, optional (default=1)
The number of regression targets, i.e., the dimension of the y output vector associated
with a sample. By default, the output is a scalar.
bias : ﬂoat, optional (default=0.0)
The bias term in the underlying linear model.
effective_rank : int or None, optional (default=None)
if not None:The approximate number of singular vectors required to explain most of
the input data by linear combinations. Using this kind of singular spectrum in the
input allows the generator to reproduce the correlations often observed in practice.
if None:The input set is well conditioned, centered and gaussian with unit variance.
tail_strength : ﬂoat between 0.0 and 1.0, optional (default=0.5)
The relative importance of the fat noisy tail of the singular values proﬁle if effec-
tive_rank is not None.
noise : ﬂoat, optional (default=0.0)
The standard deviation of the gaussian noise applied to the output.
shufﬂe : boolean, optional (default=True)
Shufﬂe the samples and the features.
coef : boolean, optional (default=False)
If True, the coefﬁcients of the underlying linear model are returned.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, n_features]
The input samples.
y : array of shape [n_samples] or [n_samples, n_targets]
The output values.
coef : array of shape [n_features] or [n_features, n_targets], optional
The coefﬁcient of the underlying linear model. It is returned only if coef is True.
Examples using sklearn.datasets.make_regression
• Prediction Latency
• Lasso on dense and sparse data
• HuberRegressor vs Ridge on dataset with strong outliers
• Robust linear model estimation using RANSAC
• Plot Ridge coefﬁcients as a function of the L2 regularization
1288
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.datasets.make_s_curve
sklearn.datasets.make_s_curve(n_samples=100, noise=0.0, random_state=None)
Generate an S curve dataset.
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of sample points on the S curve.
noise : ﬂoat, optional (default=0.0)
The standard deviation of the gaussian noise.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, 3]
The points.
t : array of shape [n_samples]
The univariate position of the sample according to the main dimension of the points in
the manifold.
Examples using sklearn.datasets.make_s_curve
• Comparison of Manifold Learning methods
sklearn.datasets.make_sparse_coded_signal
sklearn.datasets.make_sparse_coded_signal(n_samples,
n_components,
n_features,
n_nonzero_coefs, random_state=None)
Generate a signal as a sparse combination of dictionary elements.
Returns a matrix Y = DX, such as D is (n_features, n_components), X is (n_components, n_samples) and each
column of X has exactly n_nonzero_coefs non-zero elements.
Read more in the User Guide.
Parametersn_samples : int
number of samples to generate
n_components: int, :
number of components in the dictionary
n_features : int
number of features of the dataset to generate
n_nonzero_coefs : int
number of active (non-zero) coefﬁcients in each sample
random_state : int or RandomState instance, optional (default=None)
seed used by the pseudo random number generator
29.6. sklearn.datasets: Datasets
1289
scikit-learn user guide, Release 0.18.2
Returnsdata : array of shape [n_features, n_samples]
The encoded signal (Y).
dictionary : array of shape [n_features, n_components]
The dictionary with normalized components (D).
code : array of shape [n_components, n_samples]
The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-
zero items (X).
Examples using sklearn.datasets.make_sparse_coded_signal
• Orthogonal Matching Pursuit
sklearn.datasets.make_sparse_spd_matrix
sklearn.datasets.make_sparse_spd_matrix(dim=1,
alpha=0.95,
norm_diag=False,
smallest_coef=0.1,
largest_coef=0.9,
ran-
dom_state=None)
Generate a sparse symmetric deﬁnite positive matrix.
Read more in the User Guide.
Parametersdim : integer, optional (default=1)
The size of the random matrix to generate.
alpha : ﬂoat between 0 and 1, optional (default=0.95)
The probability that a coefﬁcient is zero (see notes). Larger values enforce more spar-
sity.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
largest_coef : ﬂoat between 0 and 1, optional (default=0.9)
The value of the largest coefﬁcient.
smallest_coef : ﬂoat between 0 and 1, optional (default=0.1)
The value of the smallest coefﬁcient.
norm_diag : boolean, optional (default=False)
Whether to normalize the output matrix to make the leading diagonal elements all 1
Returnsprec : sparse matrix of shape (dim, dim)
The generated matrix.
See also:
make_spd_matrix
1290
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into
the ﬁlling fraction of the matrix itself.
Examples using sklearn.datasets.make_sparse_spd_matrix
• Sparse inverse covariance estimation
sklearn.datasets.make_sparse_uncorrelated
sklearn.datasets.make_sparse_uncorrelated(n_samples=100,
n_features=10,
ran-
dom_state=None)
Generate a random regression problem with sparse uncorrelated design
This dataset is described in Celeux et al [1]. as:
X ~ N(0, 1)
y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]
Only the ﬁrst 4 features are informative. The remaining features are useless.
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of samples.
n_features : int, optional (default=10)
The number of features.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, n_features]
The input samples.
y : array of shape [n_samples]
The output values.
References
[R145]
sklearn.datasets.make_spd_matrix
sklearn.datasets.make_spd_matrix(n_dim, random_state=None)
Generate a random symmetric, positive-deﬁnite matrix.
Read more in the User Guide.
Parametersn_dim : int
29.6. sklearn.datasets: Datasets
1291
scikit-learn user guide, Release 0.18.2
The matrix dimension.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_dim, n_dim]
The random symmetric, positive-deﬁnite matrix.
See also:
make_sparse_spd_matrix
sklearn.datasets.make_swiss_roll
sklearn.datasets.make_swiss_roll(n_samples=100, noise=0.0, random_state=None)
Generate a swiss roll dataset.
Read more in the User Guide.
Parametersn_samples : int, optional (default=100)
The number of sample points on the S curve.
noise : ﬂoat, optional (default=0.0)
The standard deviation of the gaussian noise.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape [n_samples, 3]
The points.
t : array of shape [n_samples]
The univariate position of the sample according to the main dimension of the points in
the manifold.
Notes
The algorithm is from Marsland [1].
References
[R10]
Examples using sklearn.datasets.make_swiss_roll
• Hierarchical clustering: structured vs unstructured ward
• Swiss Roll reduction with LLE
1292
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.datasets.make_biclusters
sklearn.datasets.make_biclusters(shape, n_clusters, noise=0.0, minval=10, maxval=100, shuf-
ﬂe=True, random_state=None)
Generate an array with constant block diagonal structure for biclustering.
Read more in the User Guide.
Parametersshape : iterable (n_rows, n_cols)
The shape of the result.
n_clusters : integer
The number of biclusters.
noise : ﬂoat, optional (default=0.0)
The standard deviation of the gaussian noise.
minval : int, optional (default=10)
Minimum value of a bicluster.
maxval : int, optional (default=100)
Maximum value of a bicluster.
shufﬂe : boolean, optional (default=True)
Shufﬂe the samples.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape shape
The generated array.
rows : array of shape (n_clusters, X.shape[0],)
The indicators for cluster membership of each row.
cols : array of shape (n_clusters, X.shape[1],)
The indicators for cluster membership of each column.
See also:
make_checkerboard
References
[R5]
Examples using sklearn.datasets.make_biclusters
• A demo of the Spectral Co-Clustering algorithm
29.6. sklearn.datasets: Datasets
1293
scikit-learn user guide, Release 0.18.2
sklearn.datasets.make_checkerboard
sklearn.datasets.make_checkerboard(shape, n_clusters, noise=0.0, minval=10, maxval=100,
shufﬂe=True, random_state=None)
Generate an array with block checkerboard structure for biclustering.
Read more in the User Guide.
Parametersshape : iterable (n_rows, n_cols)
The shape of the result.
n_clusters : integer or iterable (n_row_clusters, n_column_clusters)
The number of row and column clusters.
noise : ﬂoat, optional (default=0.0)
The standard deviation of the gaussian noise.
minval : int, optional (default=10)
Minimum value of a bicluster.
maxval : int, optional (default=100)
Maximum value of a bicluster.
shufﬂe : boolean, optional (default=True)
Shufﬂe the samples.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
ReturnsX : array of shape shape
The generated array.
rows : array of shape (n_clusters, X.shape[0],)
The indicators for cluster membership of each row.
cols : array of shape (n_clusters, X.shape[1],)
The indicators for cluster membership of each column.
See also:
make_biclusters
References
[R6]
Examples using sklearn.datasets.make_checkerboard
• A demo of the Spectral Biclustering algorithm
1294
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.7 sklearn.decomposition: Matrix Decomposition
The sklearn.decomposition module includes matrix decomposition algorithms, including among others PCA,
NMF or ICA. Most of the algorithms of this module can be regarded as dimensionality reduction techniques.
User guide: See the Decomposing signals in components (matrix factorization problems) section for further details.
decomposition.PCA([n_components, copy, ...])
Principal component analysis (PCA)
decomposition.IncrementalPCA([n_components,
...])
Incremental principal components analysis (IPCA).
decomposition.ProjectedGradientNMF(\*args,
...)
Non-Negative Matrix Factorization (NMF)
decomposition.KernelPCA([n_components, ...])
Kernel Principal component analysis (KPCA)
decomposition.FactorAnalysis([n_components,
...])
Factor Analysis (FA)
decomposition.FastICA([n_components, ...])
FastICA: a fast algorithm for Independent Component
Analysis.
decomposition.TruncatedSVD([n_components,
...])
Dimensionality reduction using truncated SVD (aka LSA).
decomposition.NMF([n_components, init, ...])
Non-Negative Matrix Factorization (NMF)
decomposition.SparsePCA([n_components, ...])
Sparse Principal Components Analysis (SparsePCA)
decomposition.MiniBatchSparsePCA([...])
Mini-batch Sparse Principal Components Analysis
decomposition.SparseCoder(dictionary[, ...])
Sparse coding
decomposition.DictionaryLearning([...])
Dictionary learning
decomposition.MiniBatchDictionaryLearning([...])
Mini-batch dictionary learning
decomposition.LatentDirichletAllocation([...])Latent Dirichlet Allocation with online variational Bayes
algorithm
29.7.1 sklearn.decomposition.PCA
class sklearn.decomposition.PCA(n_components=None,
copy=True,
whiten=False,
svd_solver=’auto’,
tol=0.0,
iterated_power=’auto’,
ran-
dom_state=None)
Principal component analysis (PCA)
Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimen-
sional space.
It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko
et al. 2009, depending on the shape of the input data and the number of components to extract.
It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.
Notice that this class does not support sparse input. See TruncatedSVD for an alternative with sparse data.
Read more in the User Guide.
Parametersn_components : int, ﬂoat, None or string
Number of components to keep. if n_components is not set all components are kept:
n_components == min(n_samples, n_features)
if n_components == ‘mle’ and svd_solver == ‘full’, Minka’s MLE is used to guess the
dimension if 0 < n_components < 1 and svd_solver == ‘full’, select the number
of components such that the amount of variance that needs to be explained is greater than
29.7. sklearn.decomposition: Matrix Decomposition
1295
scikit-learn user guide, Release 0.18.2
the percentage speciﬁed by n_components n_components cannot be equal to n_features
for svd_solver == ‘arpack’.
copy : bool (default True)
If False, data passed to ﬁt are overwritten and running ﬁt(X).transform(X) will not yield
the expected results, use ﬁt_transform(X) instead.
whiten : bool, optional (default False)
When True (False by default) the components_ vectors are multiplied by the square root
of n_samples and then divided by the singular values to ensure uncorrelated outputs
with unit component-wise variances.
Whitening will remove some information from the transformed signal (the relative vari-
ance scales of the components) but can sometime improve the predictive accuracy of
the downstream estimators by making their data respect some hard-wired assumptions.
svd_solver : string {‘auto’, ‘full’, ‘arpack’, ‘randomized’}
auto :the solver is selected by a default policy based on X.shape and n_components:
if the input data is larger than 500x500 and the number of components to extract
is lower than 80% of the smallest dimension of the data, then the more efﬁcient
‘randomized’ method is enabled. Otherwise the exact full SVD is computed and
optionally truncated afterwards.
full :run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and
select the components by postprocessing
arpack :run
SVD
truncated
to
n_components
calling
ARPACK
solver
via
scipy.sparse.linalg.svds. It requires strictly 0 < n_components < X.shape[1]
randomized :run randomized SVD by the method of Halko et al.
New in version 0.18.0.
tol : ﬂoat >= 0, optional (default .0)
Tolerance for singular values computed by svd_solver == ‘arpack’.
New in version 0.18.0.
iterated_power : int >= 0, or ‘auto’, (default ‘auto’)
Number of iterations for the power method computed by svd_solver == ‘randomized’.
New in version 0.18.0.
random_state : int or RandomState instance or None (default None)
Pseudo Random Number generator seed control. If None, use the numpy.random sin-
gleton. Used by svd_solver == ‘arpack’ or ‘randomized’.
New in version 0.18.0.
Attributescomponents_ : array, [n_components, n_features]
Principal axes in feature space, representing the directions of maximum variance in the
data. The components are sorted by explained_variance_.
explained_variance_ : array, [n_components]
The amount of variance explained by each of the selected components.
New in version 0.18.
explained_variance_ratio_ : array, [n_components]
1296
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Percentage of variance explained by each of the selected components.
If n_components is not set then all components are stored and the sum of explained
variances is equal to 1.0.
mean_ : array, [n_features]
Per-feature empirical mean, estimated from the training set.
Equal to X.mean(axis=1).
n_components_ : int
The estimated number of components. When n_components is set to ‘mle’ or a num-
ber between 0 and 1 (with svd_solver == ‘full’) this number is estimated from input
data. Otherwise it equals the parameter n_components, or n_features if n_components
is None.
noise_variance_ : ﬂoat
The estimated noise covariance following the Probabilistic PCA model from Tipping
and Bishop 1999. See “Pattern Recognition and Machine Learning” by C. Bishop,
12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to
computed the estimated data covariance and score samples.
See also:
KernelPCA, SparsePCA, TruncatedSVD, IncrementalPCA
References
For n_components == ‘mle’, this class uses the method of Thomas P. Minka: Automatic Choice of Dimension-
ality for PCA. NIPS 2000: 598-604
Implements the probabilistic PCA model from: M. Tipping and C. Bishop, Probabilistic Principal Compo-
nent Analysis, Journal of the Royal Statistical Society, Series B, 61, Part 3, pp. 611-622 via the score and
score_samples methods. See http://www.miketipping.com/papers/met-mppca.pdf
For svd_solver == ‘arpack’, refer to scipy.sparse.linalg.svds.
For svd_solver == ‘randomized’, see: Finding structure with randomness: Stochastic algorithms for construct-
ing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) A randomized algorithm for the decom-
position of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert
Examples
>>> import numpy as np
>>> from sklearn.decomposition import PCA
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> pca = PCA(n_components=2)
>>> pca.fit(X)
PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
svd_solver='auto', tol=0.0, whiten=False)
>>> print(pca.explained_variance_ratio_)
[ 0.99244...
0.00755...]
>>> pca = PCA(n_components=2, svd_solver='full')
>>> pca.fit(X)
PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
29.7. sklearn.decomposition: Matrix Decomposition
1297
scikit-learn user guide, Release 0.18.2
svd_solver='full', tol=0.0, whiten=False)
>>> print(pca.explained_variance_ratio_)
[ 0.99244...
0.00755...]
>>> pca = PCA(n_components=1, svd_solver='arpack')
>>> pca.fit(X)
PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,
svd_solver='arpack', tol=0.0, whiten=False)
>>> print(pca.explained_variance_ratio_)
[ 0.99244...]
Methods
fit(X[, y])
Fit the model with X.
fit_transform(X[, y])
Fit the model with X and apply the dimensionality re-
duction on X.
get_covariance()
Compute data covariance with the generative model.
get_params([deep])
Get parameters for this estimator.
get_precision()
Compute data precision matrix with the generative
model.
inverse_transform(X[, y])
Transform data back to its original space.
score(X[, y])
Return the average log-likelihood of all samples.
score_samples(X)
Return the log-likelihood of each sample.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Apply dimensionality reduction to X.
__init__(n_components=None,
copy=True,
whiten=False,
svd_solver=’auto’,
tol=0.0,
iter-
ated_power=’auto’, random_state=None)
fit(X, y=None)
Fit the model with X.
ParametersX: array-like, shape (n_samples, n_features) :
Training data, where n_samples in the number of samples and n_features is the number
of features.
Returnsself : object
Returns the instance itself.
fit_transform(X, y=None)
Fit the model with X and apply the dimensionality reduction on X.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
ReturnsX_new : array-like, shape (n_samples, n_components)
get_covariance()
Compute data covariance with the generative model.
cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)
where S**2 contains the explained variances, and sigma2 contains the noise variances.
1298
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnscov : array, shape=(n_features, n_features)
Estimated covariance of data.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Compute data precision matrix with the generative model.
Equals the inverse of the covariance but computed with the matrix inversion lemma for efﬁciency.
Returnsprecision : array, shape=(n_features, n_features)
Estimated precision of data.
inverse_transform(X, y=None)
Transform data back to its original space.
In other words, return an input X_original whose transform would be X.
ParametersX : array-like, shape (n_samples, n_components)
New data, where n_samples is the number of samples and n_components is the number
of components.
ReturnsX_original array-like, shape (n_samples, n_features) :
Notes
If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes re-
versing whitening.
score(X, y=None)
Return the average log-likelihood of all samples.
See. “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.
com/papers/met-mppca.pdf
ParametersX: array, shape(n_samples, n_features) :
The data.
Returnsll: ﬂoat :
Average log-likelihood of the samples under the current model
score_samples(X)
Return the log-likelihood of each sample.
See. “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.
com/papers/met-mppca.pdf
ParametersX: array, shape(n_samples, n_features) :
The data.
29.7. sklearn.decomposition: Matrix Decomposition
1299
scikit-learn user guide, Release 0.18.2
Returnsll: array, shape (n_samples,) :
Log-likelihood of each sample under the current model
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Apply dimensionality reduction to X.
X is projected on the ﬁrst principal components previously extracted from a training set.
ParametersX : array-like, shape (n_samples, n_features)
New data, where n_samples is the number of samples and n_features is the number of
features.
ReturnsX_new : array-like, shape (n_samples, n_components)
Examples
>>> import numpy as np
>>> from sklearn.decomposition import IncrementalPCA
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> ipca = IncrementalPCA(n_components=2, batch_size=3)
>>> ipca.fit(X)
IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False)
>>> ipca.transform(X)
Examples using sklearn.decomposition.PCA
• Concatenating multiple feature extraction methods
• Selecting dimensionality reduction with Pipeline and GridSearchCV
• Pipelining: chaining a PCA and a logistic regression
• Explicit feature map approximation for RBF kernels
• Multilabel classiﬁcation
• Faces recognition example using eigenfaces and SVMs
• A demo of K-Means clustering on the handwritten digits data
• The Iris Dataset
• Faces dataset decompositions
• Blind source separation using FastICA
• FastICA on 2D point clouds
• Incremental PCA
• Kernel PCA
1300
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Principal components analysis (PCA)
• PCA example with Iris Data-set
• Model selection with Probabilistic PCA and Factor Analysis (FA)
• Comparison of LDA and PCA 2D projection of Iris dataset
• Multi-dimensional scaling
• Kernel Density Estimation
• Using FunctionTransformer to select columns
29.7.2 sklearn.decomposition.IncrementalPCA
class sklearn.decomposition.IncrementalPCA(n_components=None, whiten=False, copy=True,
batch_size=None)
Incremental principal components analysis (IPCA).
Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most
signiﬁcant singular vectors to project the data to a lower dimensional space.
Depending on the size of the input data, this algorithm can be much more memory efﬁcient than a PCA.
This algorithm has constant memory complexity, on the order of batch_size, enabling use of np.memmap
ﬁles without loading the entire ﬁle into memory.
The computational overhead of each SVD is O(batch_size * n_features ** 2), but only 2 *
batch_size samples remain in memory at a time. There will be n_samples / batch_size SVD compu-
tations to get the principal components, versus 1 large SVD of complexity O(n_samples * n_features
** 2) for PCA.
Read more in the User Guide.
Parametersn_components : int or None, (default=None)
Number of components to keep.
If n_components `` is ``None, then
n_components is set to min(n_samples,n_features).
batch_size : int or None, (default=None)
The number of samples to use for each batch.
Only used when calling fit.
If
batch_size is None, then batch_size is inferred from the data and set to 5
* n_features, to provide a balance between approximation accuracy and memory
consumption.
copy : bool, (default=True)
If False, X will be overwritten. copy=False can be used to save memory but is unsafe
for general use.
whiten : bool, optional
When True (False by default) the components_ vectors are divided by n_samples
times components_ to ensure uncorrelated outputs with unit component-wise vari-
ances.
Whitening will remove some information from the transformed signal (the relative vari-
ance scales of the components) but can sometimes improve the predictive accuracy of
the downstream estimators by making data respect some hard-wired assumptions.
Attributescomponents_ : array, shape (n_components, n_features)
29.7. sklearn.decomposition: Matrix Decomposition
1301
scikit-learn user guide, Release 0.18.2
Components with maximum variance.
explained_variance_ : array, shape (n_components,)
Variance explained by each of the selected components.
explained_variance_ratio_ : array, shape (n_components,)
Percentage of variance explained by each of the selected components. If all components
are stored, the sum of explained variances is equal to 1.0
mean_ : array, shape (n_features,)
Per-feature empirical mean, aggregate over calls to partial_fit.
var_ : array, shape (n_features,)
Per-feature empirical variance, aggregate over calls to partial_fit.
noise_variance_ : ﬂoat
The estimated noise covariance following the Probabilistic PCA model from Tipping
and Bishop 1999. See “Pattern Recognition and Machine Learning” by C. Bishop,
12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf.
n_components_ : int
The estimated number of components. Relevant when n_components=None.
n_samples_seen_ : int
The number of samples processed by the estimator. Will be reset on new calls to ﬁt, but
increments across partial_fit calls.
See also:
PCA, RandomizedPCA, KernelPCA, SparsePCA, TruncatedSVD
Notes
Implements the incremental PCA model from: D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for
Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May
2008. See http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf
This model is an extension of the Sequential Karhunen-Loeve Transform from: A. Levy and M. Lindenbaum, Se-
quential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Process-
ing, Volume 9, Number 8, pp. 1371-1374, August 2000. See http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf
We have speciﬁcally abstained from an optimization used by authors of both papers, a QR decomposition used
in speciﬁc situations to reduce the algorithmic complexity of the SVD. The source for this technique is Matrix
Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253.. This technique
has been omitted because it is advantageous only when decomposing a matrix with n_samples (rows) >=
5/3 * n_features (columns), and hurts the readability of the implemented algorithm. This would be a good
opportunity for future optimization, if it is deemed necessary.
References
4.Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust VisualTracking, International Jour-
nal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.
7.Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,Section 5.4.4, pp. 252-253.
1302
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Methods
fit(X[, y])
Fit the model with X, using minibatches of size
batch_size.
fit_transform(X[, y])
Fit to data, then transform it.
get_covariance()
Compute data covariance with the generative model.
get_params([deep])
Get parameters for this estimator.
get_precision()
Compute data precision matrix with the generative
model.
inverse_transform(X[, y])
Transform data back to its original space.
partial_fit(X[, y, check_input])
Incremental ﬁt with X.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Apply dimensionality reduction to X.
__init__(n_components=None, whiten=False, copy=True, batch_size=None)
fit(X, y=None)
Fit the model with X, using minibatches of size batch_size.
ParametersX: array-like, shape (n_samples, n_features) :
Training data, where n_samples is the number of samples and n_features is the number
of features.
y: Passthrough for ‘‘Pipeline‘‘ compatibility. :
Returnsself: object :
Returns the instance itself.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_covariance()
Compute data covariance with the generative model.
cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)
where S**2 contains the explained variances, and sigma2 contains the noise variances.
Returnscov : array, shape=(n_features, n_features)
Estimated covariance of data.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
29.7. sklearn.decomposition: Matrix Decomposition
1303
scikit-learn user guide, Release 0.18.2
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Compute data precision matrix with the generative model.
Equals the inverse of the covariance but computed with the matrix inversion lemma for efﬁciency.
Returnsprecision : array, shape=(n_features, n_features)
Estimated precision of data.
inverse_transform(X, y=None)
Transform data back to its original space.
In other words, return an input X_original whose transform would be X.
ParametersX : array-like, shape (n_samples, n_components)
New data, where n_samples is the number of samples and n_components is the number
of components.
ReturnsX_original array-like, shape (n_samples, n_features) :
Notes
If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes re-
versing whitening.
partial_fit(X, y=None, check_input=True)
Incremental ﬁt with X. All of X is processed as a single batch.
ParametersX: array-like, shape (n_samples, n_features) :
Training data, where n_samples is the number of samples and n_features is the number
of features.
Returnsself: object :
Returns the instance itself.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Apply dimensionality reduction to X.
X is projected on the ﬁrst principal components previously extracted from a training set.
ParametersX : array-like, shape (n_samples, n_features)
New data, where n_samples is the number of samples and n_features is the number of
features.
ReturnsX_new : array-like, shape (n_samples, n_components)
1304
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
>>> import numpy as np
>>> from sklearn.decomposition import IncrementalPCA
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> ipca = IncrementalPCA(n_components=2, batch_size=3)
>>> ipca.fit(X)
IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False)
>>> ipca.transform(X)
Examples using sklearn.decomposition.IncrementalPCA
• Incremental PCA
29.7.3 sklearn.decomposition.ProjectedGradientNMF
class sklearn.decomposition.ProjectedGradientNMF(*args, **kwargs)
Non-Negative Matrix Factorization (NMF)
Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factoriza-
tion can be used for example for dimensionality reduction, source separation or topic extraction.
The objective function is:
0.5 * ||X - WH||_Fro^2
+ alpha * l1_ratio * ||vec(W)||_1
+ alpha * l1_ratio * ||vec(H)||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
+ 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2
Where:
||A||_Fro^2 = \sum_{i,j} A_{ij}^2 (Frobenius norm)
||vec(A)||_1 = \sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)
The objective function is minimized with an alternating minimization of W and H.
Read more in the User Guide.
Parametersn_components : int or None
Number of components, if n_components is not set all features are kept.
init : ‘random’ | ‘nndsvd’ | ‘nndsvda’ | ‘nndsvdar’ | ‘custom’
Method used to initialize the procedure.
Default: ‘nndsvdar’ if n_components <
n_features, otherwise random. Valid options:
•‘random’: non-negative random matrices, scaled with:sqrt(X.mean()
/
n_components)
•‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD)
initialization (better for sparseness)
•‘nndsvda’: NNDSVD with zeros ﬁlled with the average of X(better when sparsity
is not desired)
29.7. sklearn.decomposition: Matrix Decomposition
1305
scikit-learn user guide, Release 0.18.2
•‘nndsvdar’: NNDSVD with zeros ﬁlled with small random values(generally
faster, less accurate alternative to NNDSVDa for when sparsity is not desired)
•‘custom’: use custom matrices W and H
solver : ‘pg’ | ‘cd’
Numerical solver to use: ‘pg’ is a Projected Gradient solver (deprecated). ‘cd’ is a
Coordinate Descent solver (recommended).
New in version 0.17: Coordinate Descent solver.
Changed in version 0.17: Deprecated Projected Gradient solver.
tol : double, default: 1e-4
Tolerance value used in stopping conditions.
max_iter : integer, default: 200
Number of iterations to compute.
random_state : integer seed, RandomState instance, or None (default)
Random number generator seed control.
alpha : double, default: 0.
Constant that multiplies the regularization terms. Set it to zero to have no regularization.
New in version 0.17: alpha used in the Coordinate Descent solver.
l1_ratio : double, default: 0.
The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the
penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an
elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and
L2.
New in version 0.17: Regularization parameter l1_ratio used in the Coordinate Descent
solver.
shufﬂe : boolean, default: False
If true, randomize the order of coordinates in the CD solver.
New in version 0.17: shufﬂe parameter used in the Coordinate Descent solver.
nls_max_iter : integer, default: 2000
Number of iterations in NLS subproblem. Used only in the deprecated ‘pg’ solver.
Changed in version 0.17: Deprecated Projected Gradient solver. Use Coordinate De-
scent solver instead.
sparseness : ‘data’ | ‘components’ | None, default: None
Where to enforce sparsity in the model. Used only in the deprecated ‘pg’ solver.
Changed in version 0.17: Deprecated Projected Gradient solver. Use Coordinate De-
scent solver instead.
beta : double, default: 1
Degree of sparseness, if sparseness is not None. Larger values mean more sparseness.
Used only in the deprecated ‘pg’ solver.
1306
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Changed in version 0.17: Deprecated Projected Gradient solver. Use Coordinate De-
scent solver instead.
eta : double, default: 0.1
Degree of correctness to maintain, if sparsity is not None. Smaller values mean larger
error. Used only in the deprecated ‘pg’ solver.
Changed in version 0.17: Deprecated Projected Gradient solver. Use Coordinate De-
scent solver instead.
Attributescomponents_ : array, [n_components, n_features]
Non-negative components of the data.
reconstruction_err_ : number
Frobenius norm of the matrix difference between the training data and the reconstructed
data from the ﬁt produced by the model. || X -WH ||_2
n_iter_ : int
Actual number of iterations.
References
C.-J. Lin. Projected gradient methods for non-negative matrix factorization. Neural Computation, 19(2007),
2756-2779. http://www.csie.ntu.edu.tw/~cjlin/nmf/
Cichocki, Andrzej, and P. H. A. N. Anh-Huy. “Fast local algorithms for large scale nonnegative matrix and ten-
sor factorizations.” IEICE transactions on fundamentals of electronics, communications and computer sciences
92.3: 708-721, 2009.
Examples
>>> import numpy as np
>>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
>>> from sklearn.decomposition import NMF
>>> model = NMF(n_components=2, init='random', random_state=0)
>>> model.fit(X)
NMF(alpha=0.0, beta=1, eta=0.1, init='random', l1_ratio=0.0, max_iter=200,
n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,
solver='cd', sparseness=None, tol=0.0001, verbose=0)
>>> model.components_
array([[ 2.09783018,
0.30560234],
[ 2.13443044,
2.13171694]])
>>> model.reconstruction_err_
0.00115993...
Methods
fit(X[, y])
Learn a NMF model for the data X.
Continued on next page
29.7. sklearn.decomposition: Matrix Decomposition
1307
scikit-learn user guide, Release 0.18.2
Table 29.56 – continued from previous page
fit_transform(X[, y, W, H])
Learn a NMF model for the data X and returns the trans-
formed data.
get_params([deep])
Get parameters for this estimator.
inverse_transform(W)
Transform data back to its original space.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform the data X according to the ﬁtted NMF model
__init__(*args, **kwargs)
DEPRECATED: It will be removed in release 0.19. Use NMF instead.’pg’ solver is still available until
release 0.19.
fit(X, y=None, **params)
Learn a NMF model for the data X.
ParametersX: {array-like, sparse matrix}, shape (n_samples, n_features) :
Data matrix to be decomposed
Returnsself :
fit_transform(X, y=None, W=None, H=None)
Learn a NMF model for the data X and returns the transformed data.
This is more efﬁcient than calling ﬁt followed by transform.
ParametersX: {array-like, sparse matrix}, shape (n_samples, n_features) :
Data matrix to be decomposed
W : array-like, shape (n_samples, n_components)
If init=’custom’, it is used as initial guess for the solution.
H : array-like, shape (n_components, n_features)
If init=’custom’, it is used as initial guess for the solution.
ReturnsW: array, shape (n_samples, n_components) :
Transformed data.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(W)
Transform data back to its original space.
ParametersW: {array-like, sparse matrix}, shape (n_samples, n_components) :
Transformed data matrix
ReturnsX: {array-like, sparse matrix}, shape (n_samples, n_features) :
Data matrix of original shape
.. versionadded:: 0.18 :
1308
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transform the data X according to the ﬁtted NMF model
ParametersX: {array-like, sparse matrix}, shape (n_samples, n_features) :
Data matrix to be transformed by the model
ReturnsW: array, shape (n_samples, n_components) :
Transformed data
29.7.4 sklearn.decomposition.KernelPCA
class sklearn.decomposition.KernelPCA(n_components=None,
kernel=’linear’,
gamma=None,
degree=3, coef0=1, kernel_params=None, alpha=1.0,
ﬁt_inverse_transform=False,
eigen_solver=’auto’,
tol=0, max_iter=None, remove_zero_eig=False, ran-
dom_state=None, copy_X=True, n_jobs=1)
Kernel Principal component analysis (KPCA)
Non-linear dimensionality reduction through the use of kernels (see Pairwise metrics, Afﬁnities and Kernels).
Read more in the User Guide.
Parametersn_components : int, default=None
Number of components. If None, all non-zero components are kept.
kernel : “linear” | “poly” | “rbf” | “sigmoid” | “cosine” | “precomputed”
Kernel. Default=”linear”.
degree : int, default=3
Degree for poly kernels. Ignored by other kernels.
gamma : ﬂoat, default=1/n_features
Kernel coefﬁcient for rbf and poly kernels. Ignored by other kernels.
coef0 : ﬂoat, default=1
Independent term in poly and sigmoid kernels. Ignored by other kernels.
kernel_params : mapping of string to any, default=None
Parameters (keyword arguments) and values for kernel passed as callable object. Ig-
nored by other kernels.
alpha : int, default=1.0
Hyperparameter of the ridge regression that learns the inverse transform (when
ﬁt_inverse_transform=True).
ﬁt_inverse_transform : bool, default=False
29.7. sklearn.decomposition: Matrix Decomposition
1309
scikit-learn user guide, Release 0.18.2
Learn the inverse transform for non-precomputed kernels. (i.e. learn to ﬁnd the pre-
image of a point)
eigen_solver : string [’auto’|’dense’|’arpack’], default=’auto’
Select eigensolver to use. If n_components is much less than the number of training
samples, arpack may be more efﬁcient than the dense eigensolver.
tol : ﬂoat, default=0
Convergence tolerance for arpack. If 0, optimal value will be chosen by arpack.
max_iter : int, default=None
Maximum number of iterations for arpack. If None, optimal value will be chosen by
arpack.
remove_zero_eig : boolean, default=False
If True, then all components with zero eigenvalues are removed, so that the number
of components in the output may be < n_components (and sometimes even zero due
to numerical instability). When n_components is None, this parameter is ignored and
components with zero eigenvalues are removed regardless.
random_state : int seed, RandomState instance, or None, default=None
A pseudo random number generator used for the initialization of the residuals when
eigen_solver == ‘arpack’.
New in version 0.18.
n_jobs : int, default=1
The number of parallel jobs to run. If -1, then the number of jobs is set to the number
of CPU cores.
New in version 0.18.
copy_X : boolean, default=True
If True, input X is copied and stored by the model in the X_ﬁt_ attribute. If no further
changes will be done to X, setting copy_X=False saves memory by storing a reference.
New in version 0.18.
Attributeslambdas_ : array, (n_components,)
Eigenvalues of the centered kernel matrix in decreasing order. If n_components and
remove_zero_eig are not set, then all values are stored.
alphas_ : array, (n_samples, n_components)
Eigenvectors of the centered kernel matrix. If n_components and remove_zero_eig are
not set, then all components are stored.
dual_coef_ : array, (n_samples, n_features)
Inverse transform matrix. Set if ﬁt_inverse_transform is True.
X_transformed_ﬁt_ : array, (n_samples, n_components)
Projection of the ﬁtted data on the kernel principal components.
X_ﬁt_ : (n_samples, n_features)
The data used to ﬁt the model. If copy_X=False, then X_ﬁt_ is a reference. This attribute
is used for the calls to transform.
1310
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
Kernel PCA was introduced in:Bernhard Schoelkopf, Alexander J. Smola, and Klaus-Robert Mueller. 1999.
Kernel principal component analysis. In Advances in kernel methods, MIT Press, Cambridge, MA, USA
327-352.
Methods
fit(X[, y])
Fit the model from data in X.
fit_transform(X[, y])
Fit the model from data in X and transform X.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X)
Transform X back to original space.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform X.
__init__(n_components=None,
kernel=’linear’,
gamma=None,
degree=3,
coef0=1,
ker-
nel_params=None, alpha=1.0, ﬁt_inverse_transform=False, eigen_solver=’auto’, tol=0,
max_iter=None, remove_zero_eig=False, random_state=None, copy_X=True, n_jobs=1)
fit(X, y=None)
Fit the model from data in X.
ParametersX: array-like, shape (n_samples, n_features) :
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
Returnsself : object
Returns the instance itself.
fit_transform(X, y=None, **params)
Fit the model from data in X and transform X.
ParametersX: array-like, shape (n_samples, n_features) :
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
ReturnsX_new: array-like, shape (n_samples, n_components) :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(X)
Transform X back to original space.
ParametersX: array-like, shape (n_samples, n_components) :
ReturnsX_new: array-like, shape (n_samples, n_features) :
29.7. sklearn.decomposition: Matrix Decomposition
1311
scikit-learn user guide, Release 0.18.2
References
“Learning to Find Pre-Images”, G BakIr et al, 2004.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transform X.
ParametersX: array-like, shape (n_samples, n_features) :
ReturnsX_new: array-like, shape (n_samples, n_components) :
Examples using sklearn.decomposition.KernelPCA
• Kernel PCA
29.7.5 sklearn.decomposition.FactorAnalysis
class sklearn.decomposition.FactorAnalysis(n_components=None,
tol=0.01,
copy=True,
max_iter=1000,
noise_variance_init=None,
svd_method=’randomized’,
iterated_power=3,
random_state=0)
Factor Analysis (FA)
A simple linear generative model with Gaussian latent variables.
The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and
added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero
mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.
If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries
are the same) we would obtain PPCA.
FactorAnalysis performs a maximum likelihood estimate of the so-called loading matrix, the transformation of
the latent variables to the observed ones, using expectation-maximization (EM).
Read more in the User Guide.
Parametersn_components : int | None
Dimensionality of latent space, the number of components of X that are obtained after
transform. If None, n_components is set to the number of features.
tol : ﬂoat
Stopping tolerance for EM algorithm.
copy : bool
Whether to make a copy of X. If False, the input X gets overwritten during ﬁtting.
max_iter : int
Maximum number of iterations.
1312
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
noise_variance_init : None | array, shape=(n_features,)
The initial guess of the noise variance for each feature.
If None, it defaults to
np.ones(n_features)
svd_method : {‘lapack’, ‘randomized’}
Which SVD method to use. If ‘lapack’ use standard SVD from scipy.linalg, if ‘ran-
domized’ use fast randomized_svd function. Defaults to ‘randomized’. For most
applications ‘randomized’ will be sufﬁciently precise while providing signiﬁcant speed
gains. Accuracy can also be improved by setting higher values for iterated_power. If
this is not sufﬁcient, for maximum precision you should choose ‘lapack’.
iterated_power : int, optional
Number of iterations for the power method. 3 by default. Only used if svd_method
equals ‘randomized’
random_state : int or RandomState
Pseudo number generator state used for random sampling. Only used if svd_method
equals ‘randomized’
Attributescomponents_ : array, [n_components, n_features]
Components with maximum variance.
loglike_ : list, [n_iterations]
The log likelihood at each iteration.
noise_variance_ : array, shape=(n_features,)
The estimated noise variance for each feature.
n_iter_ : int
Number of iterations run.
See also:
PCAPrincipal component analysis is also a latent linear variable model which however assumes equal noise
variance for each feature. This extra assumption makes probabilistic PCA faster as it can be computed in
closed form.
FastICAIndependent component analysis, a latent variable model with non-Gaussian latent variables.
References
Methods
fit(X[, y])
Fit the FactorAnalysis model to X using EM
fit_transform(X[, y])
Fit to data, then transform it.
get_covariance()
Compute data covariance with the FactorAnalysis
model.
get_params([deep])
Get parameters for this estimator.
get_precision()
Compute data precision matrix with the FactorAnalysis
model.
score(X[, y])
Compute the average log-likelihood of the samples
Continued on next page
29.7. sklearn.decomposition: Matrix Decomposition
1313
scikit-learn user guide, Release 0.18.2
Table 29.58 – continued from previous page
score_samples(X)
Compute the log-likelihood of each sample
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Apply dimensionality reduction to X using the model.
__init__(n_components=None, tol=0.01, copy=True, max_iter=1000, noise_variance_init=None,
svd_method=’randomized’, iterated_power=3, random_state=0)
fit(X, y=None)
Fit the FactorAnalysis model to X using EM
ParametersX : array-like, shape (n_samples, n_features)
Training data.
Returnsself :
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_covariance()
Compute data covariance with the FactorAnalysis model.
cov = components_.T * components_ + diag(noise_variance)
Returnscov : array, shape (n_features, n_features)
Estimated covariance of data.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_precision()
Compute data precision matrix with the FactorAnalysis model.
Returnsprecision : array, shape (n_features, n_features)
Estimated precision of data.
score(X, y=None)
Compute the average log-likelihood of the samples
ParametersX: array, shape (n_samples, n_features) :
1314
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The data
Returnsll: ﬂoat :
Average log-likelihood of the samples under the current model
score_samples(X)
Compute the log-likelihood of each sample
ParametersX: array, shape (n_samples, n_features) :
The data
Returnsll: array, shape (n_samples,) :
Log-likelihood of each sample under the current model
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Apply dimensionality reduction to X using the model.
Compute the expected mean of the latent variables. See Barber, 21.2.33 (or Bishop, 12.66).
ParametersX : array-like, shape (n_samples, n_features)
Training data.
ReturnsX_new : array-like, shape (n_samples, n_components)
The latent variables of X.
Examples using sklearn.decomposition.FactorAnalysis
• Faces dataset decompositions
• Model selection with Probabilistic PCA and Factor Analysis (FA)
29.7.6 sklearn.decomposition.FastICA
class sklearn.decomposition.FastICA(n_components=None, algorithm=’parallel’, whiten=True,
fun=’logcosh’, fun_args=None, max_iter=200, tol=0.0001,
w_init=None, random_state=None)
FastICA: a fast algorithm for Independent Component Analysis.
Read more in the User Guide.
Parametersn_components : int, optional
Number of components to use. If none is passed, all are used.
algorithm : {‘parallel’, ‘deﬂation’}
Apply parallel or deﬂational algorithm for FastICA.
whiten : boolean, optional
29.7. sklearn.decomposition: Matrix Decomposition
1315
scikit-learn user guide, Release 0.18.2
If whiten is false, the data is already considered to be whitened, and no whitening is
performed.
fun : string or function, optional. Default: ‘logcosh’
The functional form of the G function used in the approximation to neg-entropy. Could
be either ‘logcosh’, ‘exp’, or ‘cube’. You can also provide your own function. It should
return a tuple containing the value of the function, and of its derivative, in the point.
Example:
def my_g(x):return x ** 3, 3 * x ** 2
fun_args : dictionary, optional
Arguments to send to the functional form. If empty and if fun=’logcosh’, fun_args will
take value {‘alpha’ : 1.0}.
max_iter : int, optional
Maximum number of iterations during ﬁt.
tol : ﬂoat, optional
Tolerance on update at each iteration.
w_init : None of an (n_components, n_components) ndarray
The mixing matrix to be used to initialize the algorithm.
random_state : int or RandomState
Pseudo number generator state used for random sampling.
Attributescomponents_ : 2D array, shape (n_components, n_features)
The unmixing matrix.
mixing_ : array, shape (n_features, n_components)
The mixing matrix.
n_iter_ : int
If the algorithm is “deﬂation”, n_iter is the maximum number of iterations run across
all components. Else they are just the number of iterations taken to converge.
Notes
Implementation based on A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Appli-
cations, Neural Networks, 13(4-5), 2000, pp. 411-430
Methods
fit(X[, y])
Fit the model to X.
fit_transform(X[, y])
Fit the model and recover the sources from X.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X[, copy])
Transform the sources back to the mixed data (apply
mixing matrix).
set_params(\*\*params)
Set the parameters of this estimator.
Continued on next page
1316
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.59 – continued from previous page
transform(X[, y, copy])
Recover the sources from X (apply the unmixing ma-
trix).
__init__(n_components=None, algorithm=’parallel’, whiten=True, fun=’logcosh’, fun_args=None,
max_iter=200, tol=0.0001, w_init=None, random_state=None)
fit(X, y=None)
Fit the model to X.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
Returnsself :
fit_transform(X, y=None)
Fit the model and recover the sources from X.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples is the number of samples and n_features is the number
of features.
ReturnsX_new : array-like, shape (n_samples, n_components)
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(X, copy=True)
Transform the sources back to the mixed data (apply mixing matrix).
ParametersX : array-like, shape (n_samples, n_components)
Sources, where n_samples is the number of samples and n_components is the number
of components.
copy : bool (optional)
If False, data passed to ﬁt are overwritten. Defaults to True.
ReturnsX_new : array-like, shape (n_samples, n_features)
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None, copy=True)
Recover the sources from X (apply the unmixing matrix).
29.7. sklearn.decomposition: Matrix Decomposition
1317
scikit-learn user guide, Release 0.18.2
ParametersX : array-like, shape (n_samples, n_features)
Data to transform, where n_samples is the number of samples and n_features is the
number of features.
copy : bool (optional)
If False, data passed to ﬁt are overwritten. Defaults to True.
ReturnsX_new : array-like, shape (n_samples, n_components)
Examples using sklearn.decomposition.FastICA
• Faces dataset decompositions
• Blind source separation using FastICA
• FastICA on 2D point clouds
29.7.7 sklearn.decomposition.TruncatedSVD
class sklearn.decomposition.TruncatedSVD(n_components=2,
algorithm=’randomized’,
n_iter=5, random_state=None, tol=0.0)
Dimensionality reduction using truncated SVD (aka LSA).
This transformer performs linear dimensionality reduction by means of truncated singular value decomposition
(SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decompo-
sition. This means it can work with scipy.sparse matrices efﬁciently.
In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in
sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).
This estimator supports two algorithms: a fast randomized SVD solver, and a “naive” algorithm that uses
ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efﬁcient.
Read more in the User Guide.
Parametersn_components : int, default = 2
Desired dimensionality of output data. Must be strictly less than the number of features.
The default value is useful for visualisation. For LSA, a value of 100 is recommended.
algorithm : string, default = “randomized”
SVD solver to use.
Either “arpack” for the ARPACK wrapper in SciPy
(scipy.sparse.linalg.svds), or “randomized” for the randomized algorithm due to Halko
(2009).
n_iter : int, optional (default 5)
Number of iterations for randomized SVD solver. Not used by ARPACK. The default is
larger than the default in randomized_svd to handle sparse matrices that may have large
slowly decaying spectrum.
random_state : int or RandomState, optional
(Seed for) pseudo-random number generator. If not given, the numpy.random singleton
is used.
tol : ﬂoat, optional
Tolerance for ARPACK. 0 means machine precision. Ignored by randomized SVD
solver.
1318
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Attributescomponents_ : array, shape (n_components, n_features)
explained_variance_ratio_ : array, [n_components]
Percentage of variance explained by each of the selected components.
explained_variance_ : array, [n_components]
The variance of the training samples transformed by a projection to each component.
See also:
PCA, RandomizedPCA
Notes
SVD suffers from a problem called “sign indeterminancy”, which means the sign of the components_ and
the output from transform depend on the algorithm and random state. To work around this, ﬁt instances of this
class to data once, then keep the instance around to do transformations.
References
Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions
Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061
Examples
>>> from sklearn.decomposition import TruncatedSVD
>>> from sklearn.random_projection import sparse_random_matrix
>>> X = sparse_random_matrix(100, 100, density=0.01, random_state=42)
>>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)
>>> svd.fit(X)
TruncatedSVD(algorithm='randomized', n_components=5, n_iter=7,
random_state=42, tol=0.0)
>>> print(svd.explained_variance_ratio_)
[ 0.0606... 0.0584... 0.0497... 0.0434... 0.0372...]
>>> print(svd.explained_variance_ratio_.sum())
0.249...
Methods
fit(X[, y])
Fit LSI model on training data X.
fit_transform(X[, y])
Fit LSI model to X and perform dimensionality reduc-
tion on X.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X)
Transform X back to its original space.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Perform dimensionality reduction on X.
__init__(n_components=2, algorithm=’randomized’, n_iter=5, random_state=None, tol=0.0)
fit(X, y=None)
29.7. sklearn.decomposition: Matrix Decomposition
1319
scikit-learn user guide, Release 0.18.2
Fit LSI model on training data X.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training data.
Returnsself : object
Returns the transformer object.
fit_transform(X, y=None)
Fit LSI model to X and perform dimensionality reduction on X.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training data.
ReturnsX_new : array, shape (n_samples, n_components)
Reduced version of X. This will always be a dense array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(X)
Transform X back to its original space.
Returns an array X_original whose transform would be X.
ParametersX : array-like, shape (n_samples, n_components)
New data.
ReturnsX_original : array, shape (n_samples, n_features)
Note that this is always a dense array.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Perform dimensionality reduction on X.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
New data.
ReturnsX_new : array, shape (n_samples, n_components)
Reduced version of X. This will always be a dense array.
1320
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.decomposition.TruncatedSVD
• Feature Union with Heterogeneous Data Sources
• Hashing feature transformation using Totally Random Trees
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
• Clustering text documents using k-means
29.7.8 sklearn.decomposition.NMF
class sklearn.decomposition.NMF(n_components=None,
init=None,
solver=’cd’,
tol=0.0001,
max_iter=200, random_state=None, alpha=0.0, l1_ratio=0.0,
verbose=0, shufﬂe=False, nls_max_iter=2000, sparseness=None,
beta=1, eta=0.1)
Non-Negative Matrix Factorization (NMF)
Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factoriza-
tion can be used for example for dimensionality reduction, source separation or topic extraction.
The objective function is:
0.5 * ||X - WH||_Fro^2
+ alpha * l1_ratio * ||vec(W)||_1
+ alpha * l1_ratio * ||vec(H)||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
+ 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2
Where:
||A||_Fro^2 = \sum_{i,j} A_{ij}^2 (Frobenius norm)
||vec(A)||_1 = \sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)
The objective function is minimized with an alternating minimization of W and H.
Read more in the User Guide.
Parametersn_components : int or None
Number of components, if n_components is not set all features are kept.
init : ‘random’ | ‘nndsvd’ | ‘nndsvda’ | ‘nndsvdar’ | ‘custom’
Method used to initialize the procedure.
Default: ‘nndsvdar’ if n_components <
n_features, otherwise random. Valid options:
•‘random’: non-negative random matrices, scaled with:sqrt(X.mean()
/
n_components)
•‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD)
initialization (better for sparseness)
•‘nndsvda’: NNDSVD with zeros ﬁlled with the average of X(better when sparsity
is not desired)
•‘nndsvdar’: NNDSVD with zeros ﬁlled with small random values(generally
faster, less accurate alternative to NNDSVDa for when sparsity is not desired)
•‘custom’: use custom matrices W and H
solver : ‘pg’ | ‘cd’
29.7. sklearn.decomposition: Matrix Decomposition
1321
scikit-learn user guide, Release 0.18.2
Numerical solver to use: ‘pg’ is a Projected Gradient solver (deprecated). ‘cd’ is a
Coordinate Descent solver (recommended).
New in version 0.17: Coordinate Descent solver.
Changed in version 0.17: Deprecated Projected Gradient solver.
tol : double, default: 1e-4
Tolerance value used in stopping conditions.
max_iter : integer, default: 200
Number of iterations to compute.
random_state : integer seed, RandomState instance, or None (default)
Random number generator seed control.
alpha : double, default: 0.
Constant that multiplies the regularization terms. Set it to zero to have no regularization.
New in version 0.17: alpha used in the Coordinate Descent solver.
l1_ratio : double, default: 0.
The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the
penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an
elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and
L2.
New in version 0.17: Regularization parameter l1_ratio used in the Coordinate Descent
solver.
shufﬂe : boolean, default: False
If true, randomize the order of coordinates in the CD solver.
New in version 0.17: shufﬂe parameter used in the Coordinate Descent solver.
nls_max_iter : integer, default: 2000
Number of iterations in NLS subproblem. Used only in the deprecated ‘pg’ solver.
Changed in version 0.17: Deprecated Projected Gradient solver. Use Coordinate De-
scent solver instead.
sparseness : ‘data’ | ‘components’ | None, default: None
Where to enforce sparsity in the model. Used only in the deprecated ‘pg’ solver.
Changed in version 0.17: Deprecated Projected Gradient solver. Use Coordinate De-
scent solver instead.
beta : double, default: 1
Degree of sparseness, if sparseness is not None. Larger values mean more sparseness.
Used only in the deprecated ‘pg’ solver.
Changed in version 0.17: Deprecated Projected Gradient solver. Use Coordinate De-
scent solver instead.
eta : double, default: 0.1
Degree of correctness to maintain, if sparsity is not None. Smaller values mean larger
error. Used only in the deprecated ‘pg’ solver.
1322
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Changed in version 0.17: Deprecated Projected Gradient solver. Use Coordinate De-
scent solver instead.
Attributescomponents_ : array, [n_components, n_features]
Non-negative components of the data.
reconstruction_err_ : number
Frobenius norm of the matrix difference between the training data and the reconstructed
data from the ﬁt produced by the model. || X -WH ||_2
n_iter_ : int
Actual number of iterations.
References
C.-J. Lin. Projected gradient methods for non-negative matrix factorization. Neural Computation, 19(2007),
2756-2779. http://www.csie.ntu.edu.tw/~cjlin/nmf/
Cichocki, Andrzej, and P. H. A. N. Anh-Huy. “Fast local algorithms for large scale nonnegative matrix and ten-
sor factorizations.” IEICE transactions on fundamentals of electronics, communications and computer sciences
92.3: 708-721, 2009.
Examples
>>> import numpy as np
>>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
>>> from sklearn.decomposition import NMF
>>> model = NMF(n_components=2, init='random', random_state=0)
>>> model.fit(X)
NMF(alpha=0.0, beta=1, eta=0.1, init='random', l1_ratio=0.0, max_iter=200,
n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,
solver='cd', sparseness=None, tol=0.0001, verbose=0)
>>> model.components_
array([[ 2.09783018,
0.30560234],
[ 2.13443044,
2.13171694]])
>>> model.reconstruction_err_
0.00115993...
Methods
fit(X[, y])
Learn a NMF model for the data X.
fit_transform(X[, y, W, H])
Learn a NMF model for the data X and returns the trans-
formed data.
get_params([deep])
Get parameters for this estimator.
inverse_transform(W)
Transform data back to its original space.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform the data X according to the ﬁtted NMF model
29.7. sklearn.decomposition: Matrix Decomposition
1323
scikit-learn user guide, Release 0.18.2
__init__(n_components=None,
init=None,
solver=’cd’,
tol=0.0001,
max_iter=200,
ran-
dom_state=None, alpha=0.0, l1_ratio=0.0, verbose=0, shufﬂe=False, nls_max_iter=2000,
sparseness=None, beta=1, eta=0.1)
fit(X, y=None, **params)
Learn a NMF model for the data X.
ParametersX: {array-like, sparse matrix}, shape (n_samples, n_features) :
Data matrix to be decomposed
Returnsself :
fit_transform(X, y=None, W=None, H=None)
Learn a NMF model for the data X and returns the transformed data.
This is more efﬁcient than calling ﬁt followed by transform.
ParametersX: {array-like, sparse matrix}, shape (n_samples, n_features) :
Data matrix to be decomposed
W : array-like, shape (n_samples, n_components)
If init=’custom’, it is used as initial guess for the solution.
H : array-like, shape (n_components, n_features)
If init=’custom’, it is used as initial guess for the solution.
ReturnsW: array, shape (n_samples, n_components) :
Transformed data.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(W)
Transform data back to its original space.
ParametersW: {array-like, sparse matrix}, shape (n_samples, n_components) :
Transformed data matrix
ReturnsX: {array-like, sparse matrix}, shape (n_samples, n_features) :
Data matrix of original shape
.. versionadded:: 0.18 :
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
1324
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
transform(X)
Transform the data X according to the ﬁtted NMF model
ParametersX: {array-like, sparse matrix}, shape (n_samples, n_features) :
Data matrix to be transformed by the model
ReturnsW: array, shape (n_samples, n_components) :
Transformed data
Examples using sklearn.decomposition.NMF
• Selecting dimensionality reduction with Pipeline and GridSearchCV
• Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
• Faces dataset decompositions
29.7.9 sklearn.decomposition.SparsePCA
class sklearn.decomposition.SparsePCA(n_components=None,
alpha=1,
ridge_alpha=0.01,
max_iter=1000, tol=1e-08, method=’lars’, n_jobs=1,
U_init=None,
V_init=None,
verbose=False,
ran-
dom_state=None)
Sparse Principal Components Analysis (SparsePCA)
Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is control-
lable by the coefﬁcient of the L1 penalty, given by the parameter alpha.
Read more in the User Guide.
Parametersn_components : int,
Number of sparse atoms to extract.
alpha : ﬂoat,
Sparsity controlling parameter. Higher values lead to sparser components.
ridge_alpha : ﬂoat,
Amount of ridge shrinkage to apply in order to improve conditioning when calling the
transform method.
max_iter : int,
Maximum number of iterations to perform.
tol : ﬂoat,
Tolerance for the stopping condition.
method : {‘lars’, ‘cd’}
lars:
uses the least angle regression method to solve the lasso problem (lin-
ear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso so-
lution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.
n_jobs : int,
Number of parallel jobs to run.
U_init : array of shape (n_samples, n_components),
29.7. sklearn.decomposition: Matrix Decomposition
1325
scikit-learn user guide, Release 0.18.2
Initial values for the loadings for warm restart scenarios.
V_init : array of shape (n_components, n_features),
Initial values for the components for warm restart scenarios.
verbose : :
Degree of verbosity of the printed output.
random_state : int or RandomState
Pseudo number generator state used for random sampling.
Attributescomponents_ : array, [n_components, n_features]
Sparse components extracted from the data.
error_ : array
Vector of errors at each iteration.
n_iter_ : int
Number of iterations run.
See also:
PCA, MiniBatchSparsePCA, DictionaryLearning
Methods
fit(X[, y])
Fit the model from data in X.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, ridge_alpha])
Least Squares projection of the data onto the sparse
components.
__init__(n_components=None,
alpha=1,
ridge_alpha=0.01,
max_iter=1000,
tol=1e-08,
method=’lars’,
n_jobs=1,
U_init=None,
V_init=None,
verbose=False,
ran-
dom_state=None)
fit(X, y=None)
Fit the model from data in X.
ParametersX: array-like, shape (n_samples, n_features) :
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
Returnsself : object
Returns the instance itself.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
1326
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, ridge_alpha=None)
Least Squares projection of the data onto the sparse components.
To avoid instability issues in case the system is under-determined, regularization can be applied (Ridge
regression) via the ridge_alpha parameter.
Note that Sparse PCA components orthogonality is not enforced as in PCA hence one cannot use a simple
linear projection.
ParametersX: array of shape (n_samples, n_features) :
Test data to be transformed, must have the same number of features as the data used to
train the model.
ridge_alpha: ﬂoat, default: 0.01 :
Amount of ridge shrinkage to apply in order to improve conditioning.
ReturnsX_new array, shape (n_samples, n_components) :
Transformed data.
29.7.10 sklearn.decomposition.MiniBatchSparsePCA
class sklearn.decomposition.MiniBatchSparsePCA(n_components=None,
alpha=1,
ridge_alpha=0.01,
n_iter=100,
call-
back=None, batch_size=3, verbose=False,
shufﬂe=True,
n_jobs=1,
method=’lars’,
random_state=None)
Mini-batch Sparse Principal Components Analysis
Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is control-
lable by the coefﬁcient of the L1 penalty, given by the parameter alpha.
Read more in the User Guide.
29.7. sklearn.decomposition: Matrix Decomposition
1327
scikit-learn user guide, Release 0.18.2
Parametersn_components : int,
number of sparse atoms to extract
alpha : int,
Sparsity controlling parameter. Higher values lead to sparser components.
ridge_alpha : ﬂoat,
Amount of ridge shrinkage to apply in order to improve conditioning when calling the
transform method.
n_iter : int,
number of iterations to perform for each mini batch
callback : callable,
callable that gets invoked every ﬁve iterations
batch_size : int,
the number of features to take in each mini batch
verbose : :
degree of output the procedure will print
shufﬂe : boolean,
whether to shufﬂe the data before splitting it in batches
n_jobs : int,
number of parallel jobs to run, or -1 to autodetect.
method : {‘lars’, ‘cd’}
lars:
uses the least angle regression method to solve the lasso problem (lin-
ear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso so-
lution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.
random_state : int or RandomState
Pseudo number generator state used for random sampling.
Attributescomponents_ : array, [n_components, n_features]
Sparse components extracted from the data.
error_ : array
Vector of errors at each iteration.
n_iter_ : int
Number of iterations run.
See also:
PCA, SparsePCA, DictionaryLearning
Methods
fit(X[, y])
Fit the model from data in X.
Continued on next page
1328
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.63 – continued from previous page
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, ridge_alpha])
Least Squares projection of the data onto the sparse
components.
__init__(n_components=None,
alpha=1,
ridge_alpha=0.01,
n_iter=100,
callback=None,
batch_size=3,
verbose=False,
shufﬂe=True,
n_jobs=1,
method=’lars’,
ran-
dom_state=None)
fit(X, y=None)
Fit the model from data in X.
ParametersX: array-like, shape (n_samples, n_features) :
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
Returnsself : object
Returns the instance itself.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, ridge_alpha=None)
Least Squares projection of the data onto the sparse components.
29.7. sklearn.decomposition: Matrix Decomposition
1329
scikit-learn user guide, Release 0.18.2
To avoid instability issues in case the system is under-determined, regularization can be applied (Ridge
regression) via the ridge_alpha parameter.
Note that Sparse PCA components orthogonality is not enforced as in PCA hence one cannot use a simple
linear projection.
ParametersX: array of shape (n_samples, n_features) :
Test data to be transformed, must have the same number of features as the data used to
train the model.
ridge_alpha: ﬂoat, default: 0.01 :
Amount of ridge shrinkage to apply in order to improve conditioning.
ReturnsX_new array, shape (n_samples, n_components) :
Transformed data.
Examples using sklearn.decomposition.MiniBatchSparsePCA
• Faces dataset decompositions
29.7.11 sklearn.decomposition.SparseCoder
class sklearn.decomposition.SparseCoder(dictionary,
transform_algorithm=’omp’,
transform_n_nonzero_coefs=None,
trans-
form_alpha=None, split_sign=False, n_jobs=1)
Sparse coding
Finds a sparse representation of data against a ﬁxed, precomputed dictionary.
Each row of the result is the solution to a sparse coding problem. The goal is to ﬁnd a sparse array code such
that:
X ~= code * dictionary
Read more in the User Guide.
Parametersdictionary : array, [n_components, n_features]
The dictionary atoms used for sparse coding. Lines are assumed to be normalized to
unit norm.
transform_algorithm : {‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’, ‘threshold’}
Algorithm used to transform the data: lars: uses the least angle regression method (lin-
ear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses
the coordinate descent method to compute the Lasso solution (linear_model.Lasso).
lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal
matching pursuit to estimate the sparse solution threshold: squashes to zero all coefﬁ-
cients less than alpha from the projection dictionary * X'
transform_n_nonzero_coefs : int, 0.1 * n_features by default
Number of nonzero coefﬁcients to target in each column of the solution. This is only
used by algorithm=’lars’ and algorithm=’omp’ and is overridden by alpha in the omp
case.
transform_alpha : ﬂoat, 1. by default
1330
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
If algorithm=’lasso_lars’ or algorithm=’lasso_cd’, alpha is the penalty applied to the
L1 norm. If algorithm=’threshold’, alpha is the absolute value of the threshold below
which coefﬁcients will be squashed to zero. If algorithm=’omp’, alpha is the toler-
ance parameter: the value of the reconstruction error targeted. In this case, it overrides
n_nonzero_coefs.
split_sign : bool, False by default
Whether to split the sparse feature vector into the concatenation of its negative part and
its positive part. This can improve the performance of downstream classiﬁers.
n_jobs : int,
number of parallel jobs to run
Attributescomponents_ : array, [n_components, n_features]
The unchanged dictionary atoms
See also:
DictionaryLearning,
MiniBatchDictionaryLearning,
SparsePCA,
MiniBatchSparsePCA, sparse_encode
Methods
fit(X[, y])
Do nothing and return the estimator unchanged
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Encode the data as a sparse combination of the dictio-
nary atoms.
__init__(dictionary,
transform_algorithm=’omp’,
transform_n_nonzero_coefs=None,
trans-
form_alpha=None, split_sign=False, n_jobs=1)
fit(X, y=None)
Do nothing and return the estimator unchanged
This method is just there to implement the usual API and hence work in pipelines.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
29.7. sklearn.decomposition: Matrix Decomposition
1331
scikit-learn user guide, Release 0.18.2
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Encode the data as a sparse combination of the dictionary atoms.
Coding method is determined by the object parameter transform_algorithm.
ParametersX : array of shape (n_samples, n_features)
Test data to be transformed, must have the same number of features as the data used to
train the model.
ReturnsX_new : array, shape (n_samples, n_components)
Transformed data
Examples using sklearn.decomposition.SparseCoder
• Sparse coding with a precomputed dictionary
29.7.12 sklearn.decomposition.DictionaryLearning
class sklearn.decomposition.DictionaryLearning(n_components=None,
alpha=1,
max_iter=1000,
tol=1e-08,
ﬁt_algorithm=’lars’,
trans-
form_algorithm=’omp’,
trans-
form_n_nonzero_coefs=None,
trans-
form_alpha=None,
n_jobs=1,
code_init=None,
dict_init=None,
ver-
bose=False,
split_sign=False,
ran-
dom_state=None)
Dictionary learning
Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.
Solves the optimization problem:
(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1
(U,V)
with || V_k ||_2 = 1 for all
0 <= k < n_components
Read more in the User Guide.
Parametersn_components : int,
number of dictionary elements to extract
1332
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
alpha : ﬂoat,
sparsity controlling parameter
max_iter : int,
maximum number of iterations to perform
tol : ﬂoat,
tolerance for numerical error
ﬁt_algorithm : {‘lars’, ‘cd’}
lars:
uses the least angle regression method to solve the lasso problem (lin-
ear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso so-
lution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.
New in version 0.17: cd coordinate descent method to improve speed.
transform_algorithm : {‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’, ‘threshold’}
Algorithm used to transform the data lars: uses the least angle regression method (lin-
ear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses
the coordinate descent method to compute the Lasso solution (linear_model.Lasso).
lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal
matching pursuit to estimate the sparse solution threshold: squashes to zero all coefﬁ-
cients less than alpha from the projection dictionary * X'
New in version 0.17: lasso_cd coordinate descent method to improve speed.
transform_n_nonzero_coefs : int, 0.1 * n_features by default
Number of nonzero coefﬁcients to target in each column of the solution. This is only
used by algorithm=’lars’ and algorithm=’omp’ and is overridden by alpha in the omp
case.
transform_alpha : ﬂoat, 1. by default
If algorithm=’lasso_lars’ or algorithm=’lasso_cd’, alpha is the penalty applied to the
L1 norm. If algorithm=’threshold’, alpha is the absolute value of the threshold below
which coefﬁcients will be squashed to zero. If algorithm=’omp’, alpha is the toler-
ance parameter: the value of the reconstruction error targeted. In this case, it overrides
n_nonzero_coefs.
split_sign : bool, False by default
Whether to split the sparse feature vector into the concatenation of its negative part and
its positive part. This can improve the performance of downstream classiﬁers.
n_jobs : int,
number of parallel jobs to run
code_init : array of shape (n_samples, n_components),
initial value for the code, for warm restart
dict_init : array of shape (n_components, n_features),
initial values for the dictionary, for warm restart
verbose : :
degree of verbosity of the printed output
random_state : int or RandomState
29.7. sklearn.decomposition: Matrix Decomposition
1333
scikit-learn user guide, Release 0.18.2
Pseudo number generator state used for random sampling.
Attributescomponents_ : array, [n_components, n_features]
dictionary atoms extracted from the data
error_ : array
vector of errors at each iteration
n_iter_ : int
Number of iterations run.
See also:
SparseCoder, MiniBatchDictionaryLearning, SparsePCA, MiniBatchSparsePCA
Notes
References:
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (http://www.di.ens.
fr/sierra/pdfs/icml09.pdf)
Methods
fit(X[, y])
Fit the model from data in X.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Encode the data as a sparse combination of the dictio-
nary atoms.
__init__(n_components=None, alpha=1, max_iter=1000, tol=1e-08, ﬁt_algorithm=’lars’, trans-
form_algorithm=’omp’,
transform_n_nonzero_coefs=None,
transform_alpha=None,
n_jobs=1,
code_init=None,
dict_init=None,
verbose=False,
split_sign=False,
ran-
dom_state=None)
fit(X, y=None)
Fit the model from data in X.
ParametersX: array-like, shape (n_samples, n_features) :
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
Returnsself: object :
Returns the object itself
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
1334
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Encode the data as a sparse combination of the dictionary atoms.
Coding method is determined by the object parameter transform_algorithm.
ParametersX : array of shape (n_samples, n_features)
Test data to be transformed, must have the same number of features as the data used to
train the model.
ReturnsX_new : array, shape (n_samples, n_components)
Transformed data
29.7.13 sklearn.decomposition.MiniBatchDictionaryLearning
class sklearn.decomposition.MiniBatchDictionaryLearning(n_components=None,
al-
pha=1,
n_iter=1000,
ﬁt_algorithm=’lars’,
n_jobs=1,
batch_size=3,
shufﬂe=True,
dict_init=None,
trans-
form_algorithm=’omp’, trans-
form_n_nonzero_coefs=None,
transform_alpha=None,
ver-
bose=False, split_sign=False,
random_state=None)
Mini-batch dictionary learning
Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.
Solves the optimization problem:
29.7. sklearn.decomposition: Matrix Decomposition
1335
scikit-learn user guide, Release 0.18.2
(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1
(U,V)
with || V_k ||_2 = 1 for all
0 <= k < n_components
Read more in the User Guide.
Parametersn_components : int,
number of dictionary elements to extract
alpha : ﬂoat,
sparsity controlling parameter
n_iter : int,
total number of iterations to perform
ﬁt_algorithm : {‘lars’, ‘cd’}
lars:
uses the least angle regression method to solve the lasso problem (lin-
ear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso so-
lution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.
transform_algorithm : {‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’, ‘threshold’}
Algorithm used to transform the data. lars: uses the least angle regression method (lin-
ear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses
the coordinate descent method to compute the Lasso solution (linear_model.Lasso).
lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal
matching pursuit to estimate the sparse solution threshold: squashes to zero all coefﬁ-
cients less than alpha from the projection dictionary * X’
transform_n_nonzero_coefs : int, 0.1 * n_features by default
Number of nonzero coefﬁcients to target in each column of the solution. This is only
used by algorithm=’lars’ and algorithm=’omp’ and is overridden by alpha in the omp
case.
transform_alpha : ﬂoat, 1. by default
If algorithm=’lasso_lars’ or algorithm=’lasso_cd’, alpha is the penalty applied to the
L1 norm. If algorithm=’threshold’, alpha is the absolute value of the threshold below
which coefﬁcients will be squashed to zero. If algorithm=’omp’, alpha is the toler-
ance parameter: the value of the reconstruction error targeted. In this case, it overrides
n_nonzero_coefs.
split_sign : bool, False by default
Whether to split the sparse feature vector into the concatenation of its negative part and
its positive part. This can improve the performance of downstream classiﬁers.
n_jobs : int,
number of parallel jobs to run
dict_init : array of shape (n_components, n_features),
initial value of the dictionary for warm restart scenarios
verbose : :
degree of verbosity of the printed output
batch_size : int,
1336
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
number of samples in each mini-batch
shufﬂe : bool,
whether to shufﬂe the samples before forming batches
random_state : int or RandomState
Pseudo number generator state used for random sampling.
Attributescomponents_ : array, [n_components, n_features]
components extracted from the data
inner_stats_ : tuple of (A, B) ndarrays
Internal sufﬁcient statistics that are kept by the algorithm. Keeping them is useful in
online settings, to avoid loosing the history of the evolution, but they shouldn’t have
any use for the end user. A (n_components, n_components) is the dictionary covariance
matrix. B (n_features, n_components) is the data approximation matrix
n_iter_ : int
Number of iterations run.
See also:
SparseCoder, DictionaryLearning, SparsePCA, MiniBatchSparsePCA
Notes
References:
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (http://www.di.ens.
fr/sierra/pdfs/icml09.pdf)
Methods
fit(X[, y])
Fit the model from data in X.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
partial_fit(X[, y, iter_offset])
Updates the model using the data in X as a mini-batch.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Encode the data as a sparse combination of the dictio-
nary atoms.
__init__(n_components=None,
alpha=1,
n_iter=1000,
ﬁt_algorithm=’lars’,
n_jobs=1,
batch_size=3,
shufﬂe=True,
dict_init=None,
transform_algorithm=’omp’,
trans-
form_n_nonzero_coefs=None, transform_alpha=None, verbose=False, split_sign=False,
random_state=None)
fit(X, y=None)
Fit the model from data in X.
ParametersX: array-like, shape (n_samples, n_features) :
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
29.7. sklearn.decomposition: Matrix Decomposition
1337
scikit-learn user guide, Release 0.18.2
Returnsself : object
Returns the instance itself.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y=None, iter_offset=None)
Updates the model using the data in X as a mini-batch.
ParametersX: array-like, shape (n_samples, n_features) :
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
iter_offset: integer, optional :
The number of iteration on data batches that has been performed before this call to
partial_ﬁt. This is optional: if no number is passed, the memory of the object is used.
Returnsself : object
Returns the instance itself.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Encode the data as a sparse combination of the dictionary atoms.
Coding method is determined by the object parameter transform_algorithm.
ParametersX : array of shape (n_samples, n_features)
Test data to be transformed, must have the same number of features as the data used to
train the model.
1338
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ReturnsX_new : array, shape (n_samples, n_components)
Transformed data
Examples using sklearn.decomposition.MiniBatchDictionaryLearning
• Faces dataset decompositions
• Image denoising using dictionary learning
29.7.14 sklearn.decomposition.LatentDirichletAllocation
class sklearn.decomposition.LatentDirichletAllocation(n_topics=10,
doc_topic_prior=None,
topic_word_prior=None,
learning_method=None,
learning_decay=0.7,
learning_offset=10.0,
max_iter=10,
batch_size=128,
evaluate_every=-1,
to-
tal_samples=1000000.0,
perp_tol=0.1,
mean_change_tol=0.001,
max_doc_update_iter=100,
n_jobs=1,
verbose=0,
ran-
dom_state=None)
Latent Dirichlet Allocation with online variational Bayes algorithm
New in version 0.17.
Read more in the User Guide.
Parametersn_topics : int, optional (default=10)
Number of topics.
doc_topic_prior : ﬂoat, optional (default=None)
Prior of document topic distribution theta. If the value is None, defaults to 1 / n_topics.
In the literature, this is called alpha.
topic_word_prior : ﬂoat, optional (default=None)
Prior of topic word distribution beta. If the value is None, defaults to 1 / n_topics. In
the literature, this is called eta.
learning_method : ‘batch’ | ‘online’, default=’online’
Method used to update _component. Only used in ﬁt method. In general, if the data
size is large, the online update will be much faster than the batch update. The default
learning method is going to be changed to ‘batch’ in the 0.20 release. Valid options:
'batch': Batch variational Bayes method. Use all training data
˓→in
each EM update.
Old `components_` will be overwritten in each iteration.
'online': Online variational Bayes method. In each EM update,
˓→use
mini-batch of training data to update the ``components_``
29.7. sklearn.decomposition: Matrix Decomposition
1339
scikit-learn user guide, Release 0.18.2
variable incrementally. The learning rate is controlled by
˓→the
``learning_decay`` and the ``learning_offset`` parameters.
learning_decay : ﬂoat, optional (default=0.7)
It is a parameter that control learning rate in the online learning method. The value
should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value
is 0.0 and batch_size is n_samples, the update method is same as batch learning. In
the literature, this is called kappa.
learning_offset : ﬂoat, optional (default=10.)
A (positive) parameter that downweights early iterations in online learning. It should
be greater than 1.0. In the literature, this is called tau_0.
max_iter : integer, optional (default=10)
The maximum number of iterations.
total_samples : int, optional (default=1e6)
Total number of documents. Only used in the partial_ﬁt method.
batch_size : int, optional (default=128)
Number of documents to use in each EM iteration. Only used in online learning.
evaluate_every : int optional (default=0)
How often to evaluate perplexity. Only used in ﬁt method. set it to 0 or negative number
to not evalute perplexity in training at all. Evaluating perplexity can help you check
convergence in training process, but it will also increase total training time. Evaluating
perplexity in every iteration might increase training time up to two-fold.
perp_tol : ﬂoat, optional (default=1e-1)
Perplexity tolerance in batch learning. Only used when evaluate_every is greater
than 0.
mean_change_tol : ﬂoat, optional (default=1e-3)
Stopping tolerance for updating document topic distribution in E-step.
max_doc_update_iter : int (default=100)
Max number of iterations for updating document topic distribution in the E-step.
n_jobs : int, optional (default=1)
The number of jobs to use in the E-step. If -1, all CPUs are used. For n_jobs below
-1, (n_cpus + 1 + n_jobs) are used.
verbose : int, optional (default=0)
Verbosity level.
random_state : int or RandomState instance or None, optional (default=None)
Pseudo-random number generator seed control.
Attributescomponents_ : array, [n_topics, n_features]
Topic word distribution. components_[i,j] represents word j in topic i.
n_batch_iter_ : int
1340
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Number of iterations of the EM step.
n_iter_ : int
Number of passes over the dataset.
References
[1] “Online Learning for Latent Dirichlet Allocation”, Matthew D. Hoffman,David M. Blei, Francis Bach,
2010
[2] “Stochastic Variational Inference”, Matthew D. Hoffman, David M. Blei,Chong Wang, John Paisley,
2013
[3] Matthew D. Hoffman’s onlineldavb code. Link:http://matthewdhoffman.com//code/onlineldavb.tar
Methods
fit(X[, y])
Learn model for the data X with variational Bayes
method.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
partial_fit(X[, y])
Online VB with Mini-Batch update.
perplexity(X[, doc_topic_distr, sub_sampling])
Calculate approximate perplexity for data X.
score(X[, y])
Calculate approximate log-likelihood as score.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform data X according to the ﬁtted model.
__init__(n_topics=10, doc_topic_prior=None, topic_word_prior=None, learning_method=None,
learning_decay=0.7,
learning_offset=10.0,
max_iter=10,
batch_size=128,
evaluate_every=-1,
total_samples=1000000.0,
perp_tol=0.1,
mean_change_tol=0.001,
max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None)
fit(X, y=None)
Learn model for the data X with variational Bayes method.
When learning_method is ‘online’, use mini-batch update. Otherwise, use batch update.
ParametersX : array-like or sparse matrix, shape=(n_samples, n_features)
Document word matrix.
Returnsself :
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
29.7. sklearn.decomposition: Matrix Decomposition
1341
scikit-learn user guide, Release 0.18.2
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y=None)
Online VB with Mini-Batch update.
ParametersX : array-like or sparse matrix, shape=(n_samples, n_features)
Document word matrix.
Returnsself :
perplexity(X, doc_topic_distr=None, sub_sampling=False)
Calculate approximate perplexity for data X.
Perplexity is deﬁned as exp(-1. * log-likelihood per word)
ParametersX : array-like or sparse matrix, [n_samples, n_features]
Document word matrix.
doc_topic_distr : None or array, shape=(n_samples, n_topics)
Document topic distribution. If it is None, it will be generated by applying transform
on X.
Returnsscore : ﬂoat
Perplexity score.
score(X, y=None)
Calculate approximate log-likelihood as score.
ParametersX : array-like or sparse matrix, shape=(n_samples, n_features)
Document word matrix.
Returnsscore : ﬂoat
Use approximate bound as score.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transform data X according to the ﬁtted model.
ParametersX : array-like or sparse matrix, shape=(n_samples, n_features)
Document word matrix.
1342
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsdoc_topic_distr : shape=(n_samples, n_topics)
Document topic distribution for X.
Examples using sklearn.decomposition.LatentDirichletAllocation
• Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
decomposition.fastica(X[, n_components, ...])
Perform Fast Independent Component Analysis.
decomposition.dict_learning(X,
n_components, ...)
Solves a dictionary learning matrix factorization problem.
decomposition.dict_learning_online(X[,
...])
Solves a dictionary learning matrix factorization problem
online.
decomposition.sparse_encode(X,
dictionary[,
...])
Sparse coding
29.7.15 sklearn.decomposition.fastica
sklearn.decomposition.fastica(X,
n_components=None,
algorithm=’parallel’,
whiten=True,
fun=’logcosh’,
fun_args=None,
max_iter=200,
tol=0.0001,
w_init=None,
random_state=None,
return_X_mean=False,
compute_sources=True, return_n_iter=False)
Perform Fast Independent Component Analysis.
Read more in the User Guide.
ParametersX : array-like, shape (n_samples, n_features)
Training vector, where n_samples is the number of samples and n_features is the number
of features.
n_components : int, optional
Number of components to extract. If None no dimension reduction is performed.
algorithm : {‘parallel’, ‘deﬂation’}, optional
Apply a parallel or deﬂational FASTICA algorithm.
whiten : boolean, optional
If True perform an initial whitening of the data. If False, the data is assumed to have
already been preprocessed: it should be centered, normed and white. Otherwise you
will get incorrect results. In this case the parameter n_components will be ignored.
fun : string or function, optional. Default: ‘logcosh’
The functional form of the G function used in the approximation to neg-entropy. Could
be either ‘logcosh’, ‘exp’, or ‘cube’. You can also provide your own function. It should
return a tuple containing the value of the function, and of its derivative, in the point.
Example:
def my_g(x):return x ** 3, 3 * x ** 2
fun_args : dictionary, optional
Arguments to send to the functional form. If empty or None and if fun=’logcosh’,
fun_args will take value {‘alpha’ : 1.0}
max_iter : int, optional
29.7. sklearn.decomposition: Matrix Decomposition
1343
scikit-learn user guide, Release 0.18.2
Maximum number of iterations to perform.
tol: ﬂoat, optional :
A positive scalar giving the tolerance at which the un-mixing matrix is considered to
have converged.
w_init : (n_components, n_components) array, optional
Initial un-mixing array of dimension (n.comp,n.comp). If None (default) then an array
of normal r.v.’s is used.
random_state : int or RandomState
Pseudo number generator state used for random sampling.
return_X_mean : bool, optional
If True, X_mean is returned too.
compute_sources : bool, optional
If False, sources are not computed, but only the rotation matrix. This can save memory
when working with big data. Defaults to True.
return_n_iter : bool, optional
Whether or not to return the number of iterations.
ReturnsK : array, shape (n_components, n_features) | None.
If whiten is ‘True’, K is the pre-whitening matrix that projects data onto the ﬁrst
n_components principal components. If whiten is ‘False’, K is ‘None’.
W : array, shape (n_components, n_components)
Estimated un-mixing matrix. The mixing matrix can be obtained by:
w = np.dot(W, K.T)
A = w.T * (w * w.T).I
S : array, shape (n_samples, n_components) | None
Estimated source matrix
X_mean : array, shape (n_features, )
The mean over features. Returned only if return_X_mean is True.
n_iter : int
If the algorithm is “deﬂation”, n_iter is the maximum number of iterations run across
all components. Else they are just the number of iterations taken to converge. This is
returned only when return_n_iter is set to True.
Notes
The data matrix X is considered to be a linear combination of non-Gaussian (independent) components i.e. X
= AS where columns of S contain the independent components and A is a linear mixing matrix. In short ICA
attempts to un-mix’ the data by estimating an un-mixing matrix W where ‘‘S = W K X.‘
This implementation was originally made for data of shape [n_features, n_samples]. Now the input is transposed
before the algorithm is applied. This makes it slightly faster for Fortran-ordered input.
1344
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Implemented using FastICA: A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and
Applications, Neural Networks, 13(4-5), 2000, pp. 411-430
29.7.16 sklearn.decomposition.dict_learning
sklearn.decomposition.dict_learning(X,
n_components,
alpha,
max_iter=100,
tol=1e-
08,
method=’lars’,
n_jobs=1,
dict_init=None,
code_init=None,
callback=None,
verbose=False,
random_state=None, return_n_iter=False)
Solves a dictionary learning matrix factorization problem.
Finds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving:
(U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1
(U,V)
with || V_k ||_2 = 1 for all
0 <= k < n_components
where V is the dictionary and U is the sparse code.
Read more in the User Guide.
ParametersX : array of shape (n_samples, n_features)
Data matrix.
n_components : int,
Number of dictionary atoms to extract.
alpha : int,
Sparsity controlling parameter.
max_iter : int,
Maximum number of iterations to perform.
tol : ﬂoat,
Tolerance for the stopping condition.
method : {‘lars’, ‘cd’}
lars:
uses the least angle regression method to solve the lasso problem (lin-
ear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso so-
lution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.
n_jobs : int,
Number of parallel jobs to run, or -1 to autodetect.
dict_init : array of shape (n_components, n_features),
Initial value for the dictionary for warm restart scenarios.
code_init : array of shape (n_samples, n_components),
Initial value for the sparse code for warm restart scenarios.
callback : :
Callable that gets invoked every ﬁve iterations.
verbose : :
Degree of output the procedure will print.
29.7. sklearn.decomposition: Matrix Decomposition
1345
scikit-learn user guide, Release 0.18.2
random_state : int or RandomState
Pseudo number generator state used for random sampling.
return_n_iter : bool
Whether or not to return the number of iterations.
Returnscode : array of shape (n_samples, n_components)
The sparse code factor in the matrix factorization.
dictionary : array of shape (n_components, n_features),
The dictionary factor in the matrix factorization.
errors : array
Vector of errors at each iteration.
n_iter : int
Number of iterations run. Returned only if return_n_iter is set to True.
See also:
dict_learning_online,
DictionaryLearning,
MiniBatchDictionaryLearning,
SparsePCA, MiniBatchSparsePCA
29.7.17 sklearn.decomposition.dict_learning_online
sklearn.decomposition.dict_learning_online(X, n_components=2, alpha=1, n_iter=100,
return_code=True,
dict_init=None,
call-
back=None,
batch_size=3,
verbose=False,
shufﬂe=True,
n_jobs=1,
method=’lars’,
iter_offset=0,
random_state=None,
re-
turn_inner_stats=False,
inner_stats=None,
return_n_iter=False)
Solves a dictionary learning matrix factorization problem online.
Finds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving:
(U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1
(U,V)
with || V_k ||_2 = 1 for all
0 <= k < n_components
where V is the dictionary and U is the sparse code. This is accomplished by repeatedly iterating over mini-
batches by slicing the input data.
Read more in the User Guide.
ParametersX: array of shape (n_samples, n_features) :
Data matrix.
n_components : int,
Number of dictionary atoms to extract.
alpha : ﬂoat,
Sparsity controlling parameter.
n_iter : int,
1346
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Number of iterations to perform.
return_code : boolean,
Whether to also return the code U or just the dictionary V.
dict_init : array of shape (n_components, n_features),
Initial value for the dictionary for warm restart scenarios.
callback : :
Callable that gets invoked every ﬁve iterations.
batch_size : int,
The number of samples to take in each batch.
verbose : :
Degree of output the procedure will print.
shufﬂe : boolean,
Whether to shufﬂe the data before splitting it in batches.
n_jobs : int,
Number of parallel jobs to run, or -1 to autodetect.
method : {‘lars’, ‘cd’}
lars:
uses the least angle regression method to solve the lasso problem (lin-
ear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso so-
lution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.
iter_offset : int, default 0
Number of previous iterations completed on the dictionary used for initialization.
random_state : int or RandomState
Pseudo number generator state used for random sampling.
return_inner_stats : boolean, optional
Return the inner statistics A (dictionary covariance) and B (data approximation). Useful
to restart the algorithm in an online setting. If return_inner_stats is True, return_code is
ignored
inner_stats : tuple of (A, B) ndarrays
Inner sufﬁcient statistics that are kept by the algorithm. Passing them at initialization is
useful in online settings, to avoid loosing the history of the evolution. A (n_components,
n_components) is the dictionary covariance matrix. B (n_features, n_components) is the
data approximation matrix
return_n_iter : bool
Whether or not to return the number of iterations.
Returnscode : array of shape (n_samples, n_components),
the sparse code (only returned if return_code=True)
dictionary : array of shape (n_components, n_features),
the solutions to the dictionary learning problem
29.7. sklearn.decomposition: Matrix Decomposition
1347
scikit-learn user guide, Release 0.18.2
n_iter : int
Number of iterations run. Returned only if return_n_iter is set to True.
See also:
dict_learning,
DictionaryLearning,
MiniBatchDictionaryLearning,
SparsePCA,
MiniBatchSparsePCA
29.7.18 sklearn.decomposition.sparse_encode
sklearn.decomposition.sparse_encode(X,
dictionary,
gram=None,
cov=None,
algo-
rithm=’lasso_lars’,
n_nonzero_coefs=None,
al-
pha=None, copy_cov=True, init=None, max_iter=1000,
n_jobs=1, check_input=True, verbose=0)
Sparse coding
Each row of the result is the solution to a sparse coding problem. The goal is to ﬁnd a sparse array code such
that:
X ~= code * dictionary
Read more in the User Guide.
ParametersX: array of shape (n_samples, n_features) :
Data matrix
dictionary: array of shape (n_components, n_features) :
The dictionary matrix against which to solve the sparse coding of the data. Some of the
algorithms assume normalized rows for meaningful output.
gram: array, shape=(n_components, n_components) :
Precomputed Gram matrix, dictionary * dictionary’
cov: array, shape=(n_components, n_samples) :
Precomputed covariance, dictionary’ * X
algorithm: {‘lasso_lars’, ‘lasso_cd’, ‘lars’, ‘omp’, ‘threshold’} :
lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses
Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to
compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the es-
timated components are sparse. omp: uses orthogonal matching pursuit to estimate
the sparse solution threshold: squashes to zero all coefﬁcients less than alpha from the
projection dictionary * X’
n_nonzero_coefs: int, 0.1 * n_features by default :
Number of nonzero coefﬁcients to target in each column of the solution. This is only
used by algorithm=’lars’ and algorithm=’omp’ and is overridden by alpha in the omp
case.
alpha: ﬂoat, 1. by default :
If algorithm=’lasso_lars’ or algorithm=’lasso_cd’, alpha is the penalty applied to the
L1 norm. If algorithm=’threshold’, alpha is the absolute value of the threshold below
which coefﬁcients will be squashed to zero. If algorithm=’omp’, alpha is the toler-
ance parameter: the value of the reconstruction error targeted. In this case, it overrides
n_nonzero_coefs.
1348
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
init: array of shape (n_samples, n_components) :
Initialization value of the sparse codes. Only used if algorithm=’lasso_cd’.
max_iter: int, 1000 by default :
Maximum number of iterations to perform if algorithm=’lasso_cd’.
copy_cov: boolean, optional :
Whether to copy the precomputed covariance matrix; if False, it may be overwritten.
n_jobs: int, optional :
Number of parallel jobs to run.
check_input: boolean, optional :
If False, the input arrays X and dictionary will not be checked.
verbose : int, optional
Controls the verbosity; the higher, the more messages. Defaults to 0.
Returnscode: array of shape (n_samples, n_components) :
The sparse codes
See also:
sklearn.linear_model.lars_path,
sklearn.linear_model.orthogonal_mp,
sklearn.linear_model.Lasso, SparseCoder
29.8 sklearn.dummy: Dummy estimators
User guide: See the Model evaluation: quantifying the quality of predictions section for further details.
dummy.DummyClassifier([strategy, ...])
DummyClassiﬁer is a classiﬁer that makes predictions us-
ing simple rules.
dummy.DummyRegressor([strategy, constant, ...])
DummyRegressor is a regressor that makes predictions us-
ing simple rules.
29.8.1 sklearn.dummy.DummyClassiﬁer
class sklearn.dummy.DummyClassifier(strategy=’stratiﬁed’, random_state=None, constant=None)
DummyClassiﬁer is a classiﬁer that makes predictions using simple rules.
This classiﬁer is useful as a simple baseline to compare with other (real) classiﬁers. Do not use it for real
problems.
Read more in the User Guide.
Parametersstrategy : str, default=”stratiﬁed”
Strategy to use to generate predictions.
•“stratiﬁed”: generates predictions by respecting the training set’s class distribution.
•“most_frequent”: always predicts the most frequent label in the training set.
•“prior”:
always
predicts
the
class
that
maximizes
the
class
prior
(like
“most_frequent”) and predict_proba returns the class prior.
29.8. sklearn.dummy: Dummy estimators
1349
scikit-learn user guide, Release 0.18.2
•“uniform”: generates predictions uniformly at random.
•“constant”: always predicts a constant label that is provided by the user. This is useful
for metrics that evaluate a non-majority class
New in version 0.17: Dummy Classiﬁer now supports prior ﬁtting strategy using
parameter prior.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use.
constant : int or str or array of shape = [n_outputs]
The explicit constant as predicted by the “constant” strategy. This parameter is useful
only for the “constant” strategy.
Attributesclasses_ : array or list of array of shape = [n_classes]
Class labels for each output.
n_classes_ : array or list of array of shape = [n_classes]
Number of label for each output.
class_prior_ : array or list of array of shape = [n_classes]
Probability of each class for each output.
n_outputs_ : int,
Number of outputs.
outputs_2d_ : bool,
True if the output at ﬁt is 2d, else false.
sparse_output_ : bool,
True if the array returned from predict is to be in sparse CSC format. Is automatically
set to True if the input y is passed in sparse format.
Methods
fit(X, y[, sample_weight])
Fit the random classiﬁer.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform classiﬁcation on test vectors X.
predict_log_proba(X)
Return log probability estimates for the test vectors X.
predict_proba(X)
Return probability estimates for the test vectors X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(strategy=’stratiﬁed’, random_state=None, constant=None)
fit(X, y, sample_weight=None)
Fit the random classiﬁer.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
1350
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
Target values.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform classiﬁcation on test vectors X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Input vectors, where n_samples is the number of samples and n_features is the number
of features.
Returnsy : array, shape = [n_samples] or [n_samples, n_outputs]
Predicted target values for X.
predict_log_proba(X)
Return log probability estimates for the test vectors X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Input vectors, where n_samples is the number of samples and n_features is the number
of features.
ReturnsP : array-like or list of array-like of shape = [n_samples, n_classes]
Returns the log probability of the sample for each class in the model, where classes are
ordered arithmetically for each output.
predict_proba(X)
Return probability estimates for the test vectors X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Input vectors, where n_samples is the number of samples and n_features is the number
of features.
ReturnsP : array-like or list of array-lke of shape = [n_samples, n_classes]
Returns the probability of the sample for each class in the model, where classes are
ordered arithmetically, for each output.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
29.8. sklearn.dummy: Dummy estimators
1351
scikit-learn user guide, Release 0.18.2
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.8.2 sklearn.dummy.DummyRegressor
class sklearn.dummy.DummyRegressor(strategy=’mean’, constant=None, quantile=None)
DummyRegressor is a regressor that makes predictions using simple rules.
This regressor is useful as a simple baseline to compare with other (real) regressors. Do not use it for real
problems.
Read more in the User Guide.
Parametersstrategy : str
Strategy to use to generate predictions.
•“mean”: always predicts the mean of the training set
•“median”: always predicts the median of the training set
•“quantile”: always predicts a speciﬁed quantile of the training set, provided with the
quantile parameter.
•“constant”: always predicts a constant value that is provided by the user.
constant : int or ﬂoat or array of shape = [n_outputs]
The explicit constant as predicted by the “constant” strategy. This parameter is useful
only for the “constant” strategy.
quantile : ﬂoat in [0.0, 1.0]
The quantile to predict using the “quantile” strategy. A quantile of 0.5 corresponds to
the median, while 0.0 to the minimum and 1.0 to the maximum.
Attributesconstant_ : ﬂoat or array of shape [n_outputs]
Mean or median or quantile of the training targets or constant value given by the user.
n_outputs_ : int,
Number of outputs.
outputs_2d_ : bool,
1352
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
True if the output at ﬁt is 2d, else false.
Methods
fit(X, y[, sample_weight])
Fit the random regressor.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform classiﬁcation on test vectors X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(strategy=’mean’, constant=None, quantile=None)
fit(X, y, sample_weight=None)
Fit the random regressor.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
Target values.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform classiﬁcation on test vectors X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Input vectors, where n_samples is the number of samples and n_features is the number
of features.
Returnsy : array, shape = [n_samples] or [n_samples, n_outputs]
Predicted target values for X.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
29.8. sklearn.dummy: Dummy estimators
1353
scikit-learn user guide, Release 0.18.2
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.9 sklearn.ensemble: Ensemble Methods
The sklearn.ensemble module includes ensemble-based methods for classiﬁcation, regression and anomaly de-
tection.
User guide: See the Ensemble methods section for further details.
ensemble.AdaBoostClassifier([...])
An AdaBoost classiﬁer.
ensemble.AdaBoostRegressor([base_estimator,
...])
An AdaBoost regressor.
ensemble.BaggingClassifier([base_estimator,
...])
A Bagging classiﬁer.
ensemble.BaggingRegressor([base_estimator,
...])
A Bagging regressor.
ensemble.ExtraTreesClassifier([...])
An extra-trees classiﬁer.
ensemble.ExtraTreesRegressor([n_estimators,
...])
An extra-trees regressor.
ensemble.GradientBoostingClassifier([loss,
...])
Gradient Boosting for classiﬁcation.
ensemble.GradientBoostingRegressor([loss,
...])
Gradient Boosting for regression.
ensemble.IsolationForest([n_estimators, ...])
Isolation Forest Algorithm
ensemble.RandomForestClassifier([...])
A random forest classiﬁer.
ensemble.RandomTreesEmbedding([...])
An ensemble of totally random trees.
ensemble.RandomForestRegressor([...])
A random forest regressor.
ensemble.VotingClassifier(estimators[, ...])
Soft Voting/Majority Rule classiﬁer for unﬁtted estimators.
1354
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.9.1 sklearn.ensemble.AdaBoostClassiﬁer
class sklearn.ensemble.AdaBoostClassifier(base_estimator=None,
n_estimators=50,
learn-
ing_rate=1.0,
algorithm=’SAMME.R’,
ran-
dom_state=None)
An AdaBoost classiﬁer.
An AdaBoost [1] classiﬁer is a meta-estimator that begins by ﬁtting a classiﬁer on the original dataset and then
ﬁts additional copies of the classiﬁer on the same dataset but where the weights of incorrectly classiﬁed instances
are adjusted such that subsequent classiﬁers focus more on difﬁcult cases.
This class implements the algorithm known as AdaBoost-SAMME [2].
Read more in the User Guide.
Parametersbase_estimator : object, optional (default=DecisionTreeClassiﬁer)
The base estimator from which the boosted ensemble is built.
Support for sample
weighting is required, as well as proper classes_ and n_classes_ attributes.
n_estimators : integer, optional (default=50)
The maximum number of estimators at which boosting is terminated. In case of perfect
ﬁt, the learning procedure is stopped early.
learning_rate : ﬂoat, optional (default=1.)
Learning rate shrinks the contribution of each classiﬁer by learning_rate. There
is a trade-off between learning_rate and n_estimators.
algorithm : {‘SAMME’, ‘SAMME.R’}, optional (default=’SAMME.R’)
If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. base_estimator
must support calculation of class probabilities. If ‘SAMME’ then use the SAMME
discrete boosting algorithm. The SAMME.R algorithm typically converges faster than
SAMME, achieving a lower test error with fewer boosting iterations.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
Attributesestimators_ : list of classiﬁers
The collection of ﬁtted sub-estimators.
classes_ : array of shape = [n_classes]
The classes labels.
n_classes_ : int
The number of classes.
estimator_weights_ : array of ﬂoats
Weights for each estimator in the boosted ensemble.
estimator_errors_ : array of ﬂoats
Classiﬁcation error for each estimator in the boosted ensemble.
feature_importances_ : array of shape = [n_features]
The feature importances if supported by the base_estimator.
29.9. sklearn.ensemble: Ensemble Methods
1355
scikit-learn user guide, Release 0.18.2
See also:
AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier
References
[R11], [R12]
Methods
decision_function(X)
Compute the decision function of X.
fit(X, y[, sample_weight])
Build a boosted classiﬁer from the training set (X, y).
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict classes for X.
predict_log_proba(X)
Predict class log-probabilities for X.
predict_proba(X)
Predict class probabilities for X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
staged_decision_function(X)
Compute decision function of X for each boosting itera-
tion.
staged_predict(X)
Return staged predictions for X.
staged_predict_proba(X)
Predict class probabilities for X.
staged_score(X, y[, sample_weight])
Return staged scores for X, y.
__init__(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, ran-
dom_state=None)
decision_function(X)
Compute the decision function of X.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
Returnsscore : array, shape = [n_samples, k]
The decision function of the input samples. The order of outputs is the same of that of
the classes_ attribute. Binary classiﬁcation is a special cases with k == 1, otherwise
k==n_classes. For binary classiﬁcation, values closer to -1 or 1 mean more like the
ﬁrst or second class in classes_, respectively.
feature_importances_
Return the feature importances (the higher, the more important thefeature).
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None)
Build a boosted classiﬁer from the training set (X, y).
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
1356
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
y : array-like of shape = [n_samples]
The target values (class labels).
sample_weight : array-like of shape = [n_samples], optional
Sample weights. If None, the sample weights are initialized to 1 / n_samples.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict classes for X.
The predicted class of an input sample is computed as the weighted mean prediction of the classiﬁers in
the ensemble.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
Returnsy : array of shape = [n_samples]
The predicted classes.
predict_log_proba(X)
Predict class log-probabilities for X.
The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class
log-probabilities of the classiﬁers in the ensemble.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
Returnsp : array of shape = [n_samples]
The class probabilities of the input samples. The order of outputs is the same of that of
the classes_ attribute.
predict_proba(X)
Predict class probabilities for X.
The predicted class probabilities of an input sample is computed as the weighted mean predicted class
probabilities of the classiﬁers in the ensemble.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
Returnsp : array of shape = [n_samples]
29.9. sklearn.ensemble: Ensemble Methods
1357
scikit-learn user guide, Release 0.18.2
The class probabilities of the input samples. The order of outputs is the same of that of
the classes_ attribute.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
staged_decision_function(X)
Compute decision function of X for each boosting iteration.
This method allows monitoring (i.e. determine error on testing set) after each boosting iteration.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
Returnsscore : generator of array, shape = [n_samples, k]
The decision function of the input samples. The order of outputs is the same of that of
the classes_ attribute. Binary classiﬁcation is a special cases with k == 1, otherwise
k==n_classes. For binary classiﬁcation, values closer to -1 or 1 mean more like the
ﬁrst or second class in classes_, respectively.
staged_predict(X)
Return staged predictions for X.
The predicted class of an input sample is computed as the weighted mean prediction of the classiﬁers in
the ensemble.
This generator method yields the ensemble prediction after each iteration of boosting and therefore allows
monitoring, such as to determine the prediction on a test set after each boost.
ParametersX : array-like of shape = [n_samples, n_features]
The input samples.
Returnsy : generator of array, shape = [n_samples]
The predicted classes.
1358
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
staged_predict_proba(X)
Predict class probabilities for X.
The predicted class probabilities of an input sample is computed as the weighted mean predicted class
probabilities of the classiﬁers in the ensemble.
This generator method yields the ensemble predicted class probabilities after each iteration of boosting
and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after
each boost.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
Returnsp : generator of array, shape = [n_samples]
The class probabilities of the input samples. The order of outputs is the same of that of
the classes_ attribute.
staged_score(X, y, sample_weight=None)
Return staged scores for X, y.
This generator method yields the ensemble score after each iteration of boosting and therefore allows
monitoring, such as to determine the score on a test set after each boost.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
y : array-like, shape = [n_samples]
Labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsz : ﬂoat
Examples using sklearn.ensemble.AdaBoostClassifier
• Classiﬁer comparison
• Discrete versus Real AdaBoost
• Multi-class AdaBoosted Decision Trees
• Two-class AdaBoost
• Plot the decision surfaces of ensembles of trees on the iris dataset
29.9.2 sklearn.ensemble.AdaBoostRegressor
class sklearn.ensemble.AdaBoostRegressor(base_estimator=None,
n_estimators=50,
learn-
ing_rate=1.0, loss=’linear’, random_state=None)
An AdaBoost regressor.
An AdaBoost [1] regressor is a meta-estimator that begins by ﬁtting a regressor on the original dataset and
then ﬁts additional copies of the regressor on the same dataset but where the weights of instances are adjusted
according to the error of the current prediction. As such, subsequent regressors focus more on difﬁcult cases.
29.9. sklearn.ensemble: Ensemble Methods
1359
scikit-learn user guide, Release 0.18.2
This class implements the algorithm known as AdaBoost.R2 [2].
Read more in the User Guide.
Parametersbase_estimator : object, optional (default=DecisionTreeRegressor)
The base estimator from which the boosted ensemble is built.
Support for sample
weighting is required.
n_estimators : integer, optional (default=50)
The maximum number of estimators at which boosting is terminated. In case of perfect
ﬁt, the learning procedure is stopped early.
learning_rate : ﬂoat, optional (default=1.)
Learning rate shrinks the contribution of each regressor by learning_rate. There
is a trade-off between learning_rate and n_estimators.
loss : {‘linear’, ‘square’, ‘exponential’}, optional (default=’linear’)
The loss function to use when updating the weights after each boosting iteration.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
Attributesestimators_ : list of classiﬁers
The collection of ﬁtted sub-estimators.
estimator_weights_ : array of ﬂoats
Weights for each estimator in the boosted ensemble.
estimator_errors_ : array of ﬂoats
Regression error for each estimator in the boosted ensemble.
feature_importances_ : array of shape = [n_features]
The feature importances if supported by the base_estimator.
See also:
AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor
References
[R13], [R14]
Methods
fit(X, y[, sample_weight])
Build a boosted regressor from the training set (X, y).
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict regression value for X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
Continued on next page
1360
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.75 – continued from previous page
set_params(\*\*params)
Set the parameters of this estimator.
staged_predict(X)
Return staged predictions for X.
staged_score(X, y[, sample_weight])
Return staged scores for X, y.
__init__(base_estimator=None,
n_estimators=50,
learning_rate=1.0,
loss=’linear’,
ran-
dom_state=None)
feature_importances_
Return the feature importances (the higher, the more important thefeature).
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None)
Build a boosted regressor from the training set (X, y).
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
y : array-like of shape = [n_samples]
The target values (real numbers).
sample_weight : array-like of shape = [n_samples], optional
Sample weights. If None, the sample weights are initialized to 1 / n_samples.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict regression value for X.
The predicted regression value of an input sample is computed as the weighted median prediction of the
classiﬁers in the ensemble.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
Returnsy : array of shape = [n_samples]
The predicted regression values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
29.9. sklearn.ensemble: Ensemble Methods
1361
scikit-learn user guide, Release 0.18.2
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
staged_predict(X)
Return staged predictions for X.
The predicted regression value of an input sample is computed as the weighted median prediction of the
classiﬁers in the ensemble.
This generator method yields the ensemble prediction after each iteration of boosting and therefore allows
monitoring, such as to determine the prediction on a test set after each boost.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
Returnsy : generator of array, shape = [n_samples]
The predicted regression values.
staged_score(X, y, sample_weight=None)
Return staged scores for X, y.
This generator method yields the ensemble score after each iteration of boosting and therefore allows
monitoring, such as to determine the score on a test set after each boost.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK
and LIL are converted to CSR.
y : array-like, shape = [n_samples]
Labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
1362
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsz : ﬂoat
Examples using sklearn.ensemble.AdaBoostRegressor
• Decision Tree Regression with AdaBoost
29.9.3 sklearn.ensemble.BaggingClassiﬁer
class sklearn.ensemble.BaggingClassifier(base_estimator=None,
n_estimators=10,
max_samples=1.0,
max_features=1.0,
boot-
strap=True,
bootstrap_features=False,
oob_score=False,
warm_start=False,
n_jobs=1,
random_state=None, verbose=0)
A Bagging classiﬁer.
A Bagging classiﬁer is an ensemble meta-estimator that ﬁts base classiﬁers each on random subsets of the
original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a
ﬁnal prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box
estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making
an ensemble out of it.
This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn
as random subsets of the samples, then this algorithm is known as Pasting [R151]. If samples are drawn with
replacement, then the method is known as Bagging [R152]. When random subsets of the dataset are drawn as
random subsets of the features, then the method is known as Random Subspaces [R153]. Finally, when base
estimators are built on subsets of both samples and features, then the method is known as Random Patches
[R154].
Read more in the User Guide.
Parametersbase_estimator : object or None, optional (default=None)
The base estimator to ﬁt on random subsets of the dataset. If None, then the base
estimator is a decision tree.
n_estimators : int, optional (default=10)
The number of base estimators in the ensemble.
max_samples : int or ﬂoat, optional (default=1.0)
The number of samples to draw from X to train each base estimator.
•If int, then draw max_samples samples.
•If ﬂoat, then draw max_samples * X.shape[0] samples.
max_features : int or ﬂoat, optional (default=1.0)
The number of features to draw from X to train each base estimator.
•If int, then draw max_features features.
•If ﬂoat, then draw max_features * X.shape[1] features.
bootstrap : boolean, optional (default=True)
Whether samples are drawn with replacement.
bootstrap_features : boolean, optional (default=False)
Whether features are drawn with replacement.
29.9. sklearn.ensemble: Ensemble Methods
1363
scikit-learn user guide, Release 0.18.2
oob_score : bool
Whether to use out-of-bag samples to estimate the generalization error.
warm_start : bool, optional (default=False)
When set to True, reuse the solution of the previous call to ﬁt and add more estimators
to the ensemble, otherwise, just ﬁt a whole new ensemble.
New in version 0.17: warm_start constructor parameter.
n_jobs : int, optional (default=1)
The number of jobs to run in parallel for both ﬁt and predict. If -1, then the number of
jobs is set to the number of cores.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
verbose : int, optional (default=0)
Controls the verbosity of the building process.
Attributesbase_estimator_ : estimator
The base estimator from which the ensemble is grown.
estimators_ : list of estimators
The collection of ﬁtted base estimators.
estimators_samples_ : list of arrays
The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each
subset is deﬁned by a boolean mask.
estimators_features_ : list of arrays
The subset of drawn features for each base estimator.
classes_ : array of shape = [n_classes]
The classes labels.
n_classes_ : int or list
The number of classes.
oob_score_ : ﬂoat
Score of the training dataset obtained using an out-of-bag estimate.
oob_decision_function_ : array of shape = [n_samples, n_classes]
Decision function computed with out-of-bag estimate on the training set.
If
n_estimators is small it might be possible that a data point was never left out during
the bootstrap. In this case, oob_decision_function_ might contain NaN.
References
[R151], [R152], [R153], [R154]
1364
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Methods
decision_function(\*args, \*\*kwargs)
Average of the decision functions of the base classiﬁers.
fit(X, y[, sample_weight])
Build a Bagging ensemble of estimators from the train-
ing set (X, y).
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class for X.
predict_log_proba(X)
Predict class log-probabilities for X.
predict_proba(X)
Predict class probabilities for X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(base_estimator=None,
n_estimators=10,
max_samples=1.0,
max_features=1.0,
boot-
strap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1,
random_state=None, verbose=0)
decision_function(*args, **kwargs)
Average of the decision functions of the base classiﬁers.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrices are accepted only if they are supported by
the base estimator.
Returnsscore : array, shape = [n_samples, k]
The decision function of the input samples. The columns correspond to the classes in
sorted order, as they appear in the attribute classes_. Regression and binary classiﬁ-
cation are special cases with k == 1, otherwise k==n_classes.
estimators_samples_
The subset of drawn samples for each base estimator.
Returns a dynamically generated list of boolean masks identifying the samples used for for ﬁtting each
member of the ensemble, i.e., the in-bag samples.
Note: the list is re-created at each call to the property in order to reduce the object memory footprint by
not storing the sampling data. Thus fetching the property may be slower than expected.
fit(X, y, sample_weight=None)
Build a Bagging ensemble of estimators from the trainingset (X, y).
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrices are accepted only if they are supported by
the base estimator.
y : array-like, shape = [n_samples]
The target values (class labels in classiﬁcation, real numbers in regression).
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Note that this is supported
only if the base estimator supports sample weighting.
Returnsself : object
Returns self.
29.9. sklearn.ensemble: Ensemble Methods
1365
scikit-learn user guide, Release 0.18.2
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class for X.
The predicted class of an input sample is computed as the class with the highest mean predicted probability.
If base estimators do not implement a predict_proba method, then it resorts to voting.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrices are accepted only if they are supported by
the base estimator.
Returnsy : array of shape = [n_samples]
The predicted classes.
predict_log_proba(X)
Predict class log-probabilities for X.
The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class
probabilities of the base estimators in the ensemble.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrices are accepted only if they are supported by
the base estimator.
Returnsp : array of shape = [n_samples, n_classes]
The class log-probabilities of the input samples. The order of the classes corresponds to
that in the attribute classes_.
predict_proba(X)
Predict class probabilities for X.
The predicted class probabilities of an input sample is computed as the mean predicted class probabilities
of the base estimators in the ensemble. If base estimators do not implement a predict_proba method,
then it resorts to voting and the predicted class probabilities of an input sample represents the proportion
of estimators predicting each class.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrices are accepted only if they are supported by
the base estimator.
Returnsp : array of shape = [n_samples, n_classes]
The class probabilities of the input samples. The order of the classes corresponds to that
in the attribute classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
1366
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.9.4 sklearn.ensemble.BaggingRegressor
class sklearn.ensemble.BaggingRegressor(base_estimator=None,
n_estimators=10,
max_samples=1.0,
max_features=1.0,
boot-
strap=True,
bootstrap_features=False,
oob_score=False,
warm_start=False,
n_jobs=1,
random_state=None, verbose=0)
A Bagging regressor.
A Bagging regressor is an ensemble meta-estimator that ﬁts base regressors each on random subsets of the
original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a
ﬁnal prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box
estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making
an ensemble out of it.
This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn
as random subsets of the samples, then this algorithm is known as Pasting [R15]. If samples are drawn with
replacement, then the method is known as Bagging [R16]. When random subsets of the dataset are drawn as
random subsets of the features, then the method is known as Random Subspaces [R17]. Finally, when base
estimators are built on subsets of both samples and features, then the method is known as Random Patches
[R18].
Read more in the User Guide.
Parametersbase_estimator : object or None, optional (default=None)
The base estimator to ﬁt on random subsets of the dataset. If None, then the base
estimator is a decision tree.
n_estimators : int, optional (default=10)
The number of base estimators in the ensemble.
max_samples : int or ﬂoat, optional (default=1.0)
The number of samples to draw from X to train each base estimator.
•If int, then draw max_samples samples.
29.9. sklearn.ensemble: Ensemble Methods
1367
scikit-learn user guide, Release 0.18.2
•If ﬂoat, then draw max_samples * X.shape[0] samples.
max_features : int or ﬂoat, optional (default=1.0)
The number of features to draw from X to train each base estimator.
•If int, then draw max_features features.
•If ﬂoat, then draw max_features * X.shape[1] features.
bootstrap : boolean, optional (default=True)
Whether samples are drawn with replacement.
bootstrap_features : boolean, optional (default=False)
Whether features are drawn with replacement.
oob_score : bool
Whether to use out-of-bag samples to estimate the generalization error.
warm_start : bool, optional (default=False)
When set to True, reuse the solution of the previous call to ﬁt and add more estimators
to the ensemble, otherwise, just ﬁt a whole new ensemble.
n_jobs : int, optional (default=1)
The number of jobs to run in parallel for both ﬁt and predict. If -1, then the number of
jobs is set to the number of cores.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
verbose : int, optional (default=0)
Controls the verbosity of the building process.
Attributesestimators_ : list of estimators
The collection of ﬁtted sub-estimators.
estimators_samples_ : list of arrays
The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each
subset is deﬁned by a boolean mask.
estimators_features_ : list of arrays
The subset of drawn features for each base estimator.
oob_score_ : ﬂoat
Score of the training dataset obtained using an out-of-bag estimate.
oob_prediction_ : array of shape = [n_samples]
Prediction computed with out-of-bag estimate on the training set. If n_estimators is
small it might be possible that a data point was never left out during the bootstrap. In
this case, oob_prediction_ might contain NaN.
1368
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
[R15], [R16], [R17], [R18]
Methods
fit(X, y[, sample_weight])
Build a Bagging ensemble of estimators from the train-
ing set (X, y).
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict regression target for X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(base_estimator=None,
n_estimators=10,
max_samples=1.0,
max_features=1.0,
boot-
strap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1,
random_state=None, verbose=0)
estimators_samples_
The subset of drawn samples for each base estimator.
Returns a dynamically generated list of boolean masks identifying the samples used for for ﬁtting each
member of the ensemble, i.e., the in-bag samples.
Note: the list is re-created at each call to the property in order to reduce the object memory footprint by
not storing the sampling data. Thus fetching the property may be slower than expected.
fit(X, y, sample_weight=None)
Build a Bagging ensemble of estimators from the trainingset (X, y).
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrices are accepted only if they are supported by
the base estimator.
y : array-like, shape = [n_samples]
The target values (class labels in classiﬁcation, real numbers in regression).
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Note that this is supported
only if the base estimator supports sample weighting.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
29.9. sklearn.ensemble: Ensemble Methods
1369
scikit-learn user guide, Release 0.18.2
Parameter names mapped to their values.
predict(X)
Predict regression target for X.
The predicted regression target of an input sample is computed as the mean predicted regression targets of
the estimators in the ensemble.
ParametersX : {array-like, sparse matrix} of shape = [n_samples, n_features]
The training input samples. Sparse matrices are accepted only if they are supported by
the base estimator.
Returnsy : array of shape = [n_samples]
The predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.ensemble.BaggingRegressor
• Single estimator versus bagging: bias-variance decomposition
29.9.5 sklearn.ensemble.IsolationForest
class sklearn.ensemble.IsolationForest(n_estimators=100,
max_samples=’auto’,
contam-
ination=0.1,
max_features=1.0,
bootstrap=False,
n_jobs=1, random_state=None, verbose=0)
Isolation Forest Algorithm
Return the anomaly score of each sample using the IsolationForest algorithm
1370
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split
value between the maximum and minimum values of the selected feature.
Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a
sample is equivalent to the path length from the root node to the terminating node.
This path length, averaged over a forest of such random trees, is a measure of normality and our decision
function.
Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees
collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.
Read more in the User Guide.
New in version 0.18.
Parametersn_estimators : int, optional (default=100)
The number of base estimators in the ensemble.
max_samples : int or ﬂoat, optional (default=”auto”)
The number of samples to draw from X to train each base estimator.
•If int, then draw max_samples samples.
•If ﬂoat, then draw max_samples * X.shape[0] samples.
•If “auto”, then max_samples=min(256, n_samples).
If max_samples is larger than the number of samples provided, all samples will be used
for all trees (no sampling).
contamination : ﬂoat in (0., 0.5), optional (default=0.1)
The amount of contamination of the data set, i.e. the proportion of outliers in the data
set. Used when ﬁtting to deﬁne the threshold on the decision function.
max_features : int or ﬂoat, optional (default=1.0)
The number of features to draw from X to train each base estimator.
•If int, then draw max_features features.
•If ﬂoat, then draw max_features * X.shape[1] features.
bootstrap : boolean, optional (default=False)
If True, individual trees are ﬁt on random subsets of the training data sampled with
replacement. If False, sampling without replacement is performed.
n_jobs : integer, optional (default=1)
The number of jobs to run in parallel for both ﬁt and predict. If -1, then the number of
jobs is set to the number of cores.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
verbose : int, optional (default=0)
Controls the verbosity of the tree building process.
Attributesestimators_ : list of DecisionTreeClassiﬁer
29.9. sklearn.ensemble: Ensemble Methods
1371
scikit-learn user guide, Release 0.18.2
The collection of ﬁtted sub-estimators.
estimators_samples_ : list of arrays
The subset of drawn samples (i.e., the in-bag samples) for each base estimator.
max_samples_ : integer
The actual number of samples
References
[R21], [R22]
Methods
decision_function(X)
Average anomaly score of X of the base classiﬁers.
fit(X[, y, sample_weight])
Fit estimator.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict if a particular sample is an outlier or not.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_estimators=100, max_samples=’auto’, contamination=0.1, max_features=1.0, boot-
strap=False, n_jobs=1, random_state=None, verbose=0)
decision_function(X)
Average anomaly score of X of the base classiﬁers.
The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.
The measure of normality of an observation given a tree is the depth of the leaf containing this observation,
which is equivalent to the number of splittings required to isolate this point. In case of several observations
n_left in the leaf, the average path length of a n_left samples isolation tree is added.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
The training input samples. Sparse matrices are accepted only if they are supported by
the base estimator.
Returnsscores : array of shape (n_samples,)
The anomaly score of the input samples. The lower, the more abnormal.
estimators_samples_
The subset of drawn samples for each base estimator.
Returns a dynamically generated list of boolean masks identifying the samples used for for ﬁtting each
member of the ensemble, i.e., the in-bag samples.
Note: the list is re-created at each call to the property in order to reduce the object memory footprint by
not storing the sampling data. Thus fetching the property may be slower than expected.
fit(X, y=None, sample_weight=None)
Fit estimator.
ParametersX : array-like or sparse matrix, shape (n_samples, n_features)
The input samples. Use dtype=np.float32 for maximum efﬁciency. Sparse ma-
trices are also supported, use sparse csc_matrix for maximum efﬁciency.
1372
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict if a particular sample is an outlier or not.
ParametersX : array-like or sparse matrix, shape (n_samples, n_features)
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
Returnsis_inlier : array, shape (n_samples,)
For each observations, tells whether or not (+1 or -1) it should be considered as an inlier
according to the ﬁtted model.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.ensemble.IsolationForest
• Outlier detection with several methods.
• IsolationForest example
29.9.6 sklearn.ensemble.RandomTreesEmbedding
class sklearn.ensemble.RandomTreesEmbedding(n_estimators=10,
max_depth=5,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_leaf_nodes=None, min_impurity_split=1e-
07,
sparse_output=True,
n_jobs=1,
random_state=None,
verbose=0,
warm_start=False)
An ensemble of totally random trees.
An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded
according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a
binary coding with as many ones as there are trees in the forest.
29.9. sklearn.ensemble: Ensemble Methods
1373
scikit-learn user guide, Release 0.18.2
The dimensionality of the resulting representation is n_out <= n_estimators * max_leaf_nodes.
If max_leaf_nodes == None, the number of leaf nodes is at most n_estimators * 2 **
max_depth.
Read more in the User Guide.
Parametersn_estimators : integer, optional (default=10)
Number of trees in the forest.
max_depth : integer, optional (default=5)
The maximum depth of each tree. If None, then nodes are expanded until all leaves are
pure or until all leaves contain less than min_samples_split samples.
min_samples_split : int, ﬂoat, optional (default=2)
The minimum number of samples required to split an internal node:
•If int, then consider min_samples_split as the minimum number.
•If ﬂoat, then min_samples_split is a percentage and ceil(min_samples_split *
n_samples) is the minimum number of samples for each split.
Changed in version 0.18: Added ﬂoat values for percentages.
min_samples_leaf : int, ﬂoat, optional (default=1)
The minimum number of samples required to be at a leaf node:
•If int, then consider min_samples_leaf as the minimum number.
•If ﬂoat, then min_samples_leaf is a percentage and ceil(min_samples_leaf *
n_samples) is the minimum number of samples for each node.
Changed in version 0.18: Added ﬂoat values for percentages.
min_weight_fraction_leaf : ﬂoat, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples)
required to be at a leaf node. Samples have equal weight when sample_weight is not
provided.
max_leaf_nodes : int or None, optional (default=None)
Grow trees with max_leaf_nodes in best-ﬁrst fashion. Best nodes are deﬁned as
relative reduction in impurity. If None then unlimited number of leaf nodes.
min_impurity_split : ﬂoat, optional (default=1e-7)
Threshold for early stopping in tree growth. A node will split if its impurity is above
the threshold, otherwise it is a leaf.
New in version 0.18.
sparse_output : bool, optional (default=True)
Whether or not to return a sparse CSR matrix, as default behavior, or to return a dense
array compatible with dense pipeline operators.
n_jobs : integer, optional (default=1)
The number of jobs to run in parallel for both ﬁt and predict. If -1, then the number of
jobs is set to the number of cores.
random_state : int, RandomState instance or None, optional (default=None)
1374
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
verbose : int, optional (default=0)
Controls the verbosity of the tree building process.
warm_start : bool, optional (default=False)
When set to True, reuse the solution of the previous call to ﬁt and add more estimators
to the ensemble, otherwise, just ﬁt a whole new forest.
Attributesestimators_ : list of DecisionTreeClassiﬁer
The collection of ﬁtted sub-estimators.
References
[R25], [R26]
Methods
apply(X)
Apply trees in the forest to X, return leaf indices.
decision_path(X)
Return the decision path in the forest
fit(X[, y, sample_weight])
Fit estimator.
fit_transform(X[, y, sample_weight])
Fit estimator and transform dataset.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform dataset.
__init__(n_estimators=10,
max_depth=5,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_leaf_nodes=None,
min_impurity_split=1e-07,
sparse_output=True, n_jobs=1, random_state=None, verbose=0, warm_start=False)
apply(X)
Apply trees in the forest to X, return leaf indices.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
ReturnsX_leaves : array_like, shape = [n_samples, n_estimators]
For each datapoint x in X and for each tree in the forest, return the index of the leaf x
ends up in.
decision_path(X)
Return the decision path in the forest
New in version 0.18.
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, its dtype will be converted to dtype=np.float32. If
a sparse matrix is provided, it will be converted into a sparse csr_matrix.
Returnsindicator : sparse csr array, shape = [n_samples, n_nodes]
29.9. sklearn.ensemble: Ensemble Methods
1375
scikit-learn user guide, Release 0.18.2
Return a node indicator matrix where non zero elements indicates that the samples goes
through the nodes.
n_nodes_ptr : array of size (n_estimators + 1, )
The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value
for the i-th estimator.
feature_importances_
Return the feature importances (the higher, the more important thefeature).
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y=None, sample_weight=None)
Fit estimator.
ParametersX : array-like or sparse matrix, shape=(n_samples, n_features)
The input samples. Use dtype=np.float32 for maximum efﬁciency. Sparse ma-
trices are also supported, use sparse csc_matrix for maximum efﬁciency.
Returnsself : object
Returns self.
fit_transform(X, y=None, sample_weight=None)
Fit estimator and transform dataset.
ParametersX : array-like or sparse matrix, shape=(n_samples, n_features)
Input data used to build forests. Use dtype=np.float32 for maximum efﬁciency.
ReturnsX_transformed : sparse matrix, shape=(n_samples, n_out)
Transformed dataset.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transform dataset.
ParametersX : array-like or sparse matrix, shape=(n_samples, n_features)
Input data to be transformed. Use dtype=np.float32 for maximum efﬁciency.
Sparse matrices are also supported, use sparse csr_matrix for maximum efﬁciency.
1376
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ReturnsX_transformed : sparse matrix, shape=(n_samples, n_out)
Transformed dataset.
Examples using sklearn.ensemble.RandomTreesEmbedding
• Feature transformations with ensembles of trees
• Hashing feature transformation using Totally Random Trees
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
29.9.7 sklearn.ensemble.VotingClassiﬁer
class sklearn.ensemble.VotingClassifier(estimators, voting=’hard’, weights=None, n_jobs=1)
Soft Voting/Majority Rule classiﬁer for unﬁtted estimators.
New in version 0.17.
Read more in the User Guide.
Parametersestimators : list of (string, estimator) tuples
Invoking the fit method on the VotingClassifier will ﬁt clones of those original
estimators that will be stored in the class attribute self.estimators_.
voting : str, {‘hard’, ‘soft’} (default=’hard’)
If ‘hard’, uses predicted class labels for majority rule voting. Else if ‘soft’, predicts
the class label based on the argmax of the sums of the predicted probabilities, which is
recommended for an ensemble of well-calibrated classiﬁers.
weights : array-like, shape = [n_classiﬁers], optional (default=‘None‘)
Sequence of weights (ﬂoat or int) to weight the occurrences of predicted class labels
(hard voting) or class probabilities before averaging (soft voting). Uses uniform weights
if None.
n_jobs : int, optional (default=1)
The number of jobs to run in parallel for fit. If -1, then the number of jobs is set to
the number of cores.
Attributesestimators_ : list of classiﬁers
The collection of ﬁtted sub-estimators.
classes_ : array-like, shape = [n_predictions]
The classes labels.
Examples
>>> import numpy as np
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier
>>> clf1 = LogisticRegression(random_state=1)
>>> clf2 = RandomForestClassifier(random_state=1)
>>> clf3 = GaussianNB()
29.9. sklearn.ensemble: Ensemble Methods
1377
scikit-learn user guide, Release 0.18.2
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> eclf1 = VotingClassifier(estimators=[
...
('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
>>> eclf1 = eclf1.fit(X, y)
>>> print(eclf1.predict(X))
[1 1 1 2 2 2]
>>> eclf2 = VotingClassifier(estimators=[
...
('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...
voting='soft')
>>> eclf2 = eclf2.fit(X, y)
>>> print(eclf2.predict(X))
[1 1 1 2 2 2]
>>> eclf3 = VotingClassifier(estimators=[
...
('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...
voting='soft', weights=[2,1,1])
>>> eclf3 = eclf3.fit(X, y)
>>> print(eclf3.predict(X))
[1 1 1 2 2 2]
>>>
Methods
fit(X, y[, sample_weight])
Fit the estimators.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Return estimator parameter names for GridSearch sup-
port
predict(X)
Predict class labels for X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Return class labels or probabilities for X for each esti-
mator.
__init__(estimators, voting=’hard’, weights=None, n_jobs=1)
fit(X, y, sample_weight=None)
Fit the estimators.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples]
Target values.
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Note that this is supported
only if all underlying estimators support sample weights.
Returnsself : object
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
1378
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Return estimator parameter names for GridSearch support
predict(X)
Predict class labels for X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
Returnsmaj : array-like, shape = [n_samples]
Predicted class labels.
predict_proba
Compute probabilities of possible outcomes for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
Returnsavg : array-like, shape = [n_samples, n_classes]
Weighted average probability for each class per sample.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
29.9. sklearn.ensemble: Ensemble Methods
1379
scikit-learn user guide, Release 0.18.2
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Return class labels or probabilities for X for each estimator.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
ReturnsIf ‘voting=’soft’‘: :
array-like = [n_classiﬁers, n_samples, n_classes]Class probabilities calculated by
each classiﬁer.
If ‘voting=’hard’‘: :
array-like = [n_samples, n_classiﬁers]Class labels predicted by each classiﬁer.
Examples using sklearn.ensemble.VotingClassifier
• Plot the decision boundaries of a VotingClassiﬁer
• Plot class probabilities calculated by the VotingClassiﬁer
29.9.8 partial dependence
Partial dependence plots for tree ensembles.
ensemble.partial_dependence.partial_dependence(...)
Partial dependence of target_variables.
ensemble.partial_dependence.plot_partial_dependence(...)
Partial dependence plots for features.
sklearn.ensemble.partial_dependence.partial_dependence
sklearn.ensemble.partial_dependence.partial_dependence(gbrt,
target_variables,
grid=None,
X=None,
percentiles=(0.05,
0.95),
grid_resolution=100)
Partial dependence of target_variables.
Partial dependence plots show the dependence between the joint values of the target_variables and the
function represented by the gbrt.
Read more in the User Guide.
Parametersgbrt : BaseGradientBoosting
A ﬁtted gradient boosting model.
target_variables : array-like, dtype=int
The target features for which the partial dependecy should be computed (size should be
smaller than 3 for visual renderings).
1380
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
grid : array-like, shape=(n_points, len(target_variables))
The grid of target_variables values for which the partial dependecy should be
evaluated (either grid or X must be speciﬁed).
X : array-like, shape=(n_samples, n_features)
The data on which gbrt was trained.
It is used to generate a grid for the
target_variables. The grid comprises grid_resolution equally spaced
points between the two percentiles.
percentiles : (low, high), default=(0.05, 0.95)
The lower and upper percentile used create the extreme values for the grid. Only if X
is not None.
grid_resolution : int, default=100
The number of equally spaced points on the grid.
Returnspdp : array, shape=(n_classes, n_points)
The partial dependence function evaluated on the grid. For regression and binary
classiﬁcation n_classes==1.
axes : seq of ndarray or None
The axes with which the grid has been created or None if the grid has been given.
Examples
>>> samples = [[0, 0, 2], [1, 0, 0]]
>>> labels = [0, 1]
>>> from sklearn.ensemble import GradientBoostingClassifier
>>> gb = GradientBoostingClassifier(random_state=0).fit(samples, labels)
>>> kwargs = dict(X=samples, percentiles=(0, 1), grid_resolution=2)
>>> partial_dependence(gb, [0], **kwargs)
(array([[-4.52...,
4.52...]]), [array([ 0.,
1.])])
sklearn.ensemble.partial_dependence.plot_partial_dependence
sklearn.ensemble.partial_dependence.plot_partial_dependence(gbrt, X, features, fea-
ture_names=None,
label=None,
n_cols=3,
grid_resolution=100,
percentiles=(0.05,
0.95), n_jobs=1, ver-
bose=0,
ax=None,
line_kw=None,
contour_kw=None,
**ﬁg_kw)
Partial dependence plots for features.
The len(features) plots are arranged in a grid with n_cols columns. Two-way partial dependence plots
are plotted as contour plots.
Read more in the User Guide.
29.9. sklearn.ensemble: Ensemble Methods
1381
scikit-learn user guide, Release 0.18.2
Parametersgbrt : BaseGradientBoosting
A ﬁtted gradient boosting model.
X : array-like, shape=(n_samples, n_features)
The data on which gbrt was trained.
features : seq of ints, strings, or tuples of ints or strings
If seq[i] is an int or a tuple with one int value, a one-way PDP is created; if seq[i] is a
tuple of two ints, a two-way PDP is created. If feature_names is speciﬁed and seq[i] is
an int, seq[i] must be < len(feature_names). If seq[i] is a string, feature_names must be
speciﬁed, and seq[i] must be in feature_names.
feature_names : seq of str
Name of each feature; feature_names[i] holds the name of the feature with index i.
label : object
The class label for which the PDPs should be computed. Only if gbrt is a multi-class
model. Must be in gbrt.classes_.
n_cols : int
The number of columns in the grid plot (default: 3).
percentiles : (low, high), default=(0.05, 0.95)
The lower and upper percentile used to create the extreme values for the PDP axes.
grid_resolution : int, default=100
The number of equally spaced points on the axes.
n_jobs : int
The number of CPUs to use to compute the PDs. -1 means ‘all CPUs’. Defaults to 1.
verbose : int
Verbose output during PD computations. Defaults to 0.
ax : Matplotlib axis object, default None
An axis object onto which the plots will be drawn.
line_kw : dict
Dict with keywords passed to the matplotlib.pyplot.plot call. For one-way
partial dependence plots.
contour_kw : dict
Dict with keywords passed to the matplotlib.pyplot.plot call. For two-way
partial dependence plots.
ﬁg_kw : dict
Dict with keywords passed to the ﬁgure() call. Note that all keywords not recognized
above will be automatically included here.
Returnsﬁg : ﬁgure
The Matplotlib Figure object.
axs : seq of Axis objects
1382
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
A seq of Axis objects, one for each subplot.
Examples
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> X, y = make_friedman1()
>>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)
>>> fig, axs = plot_partial_dependence(clf, X, [0, (0, 1)])
...
Examples using sklearn.ensemble.partial_dependence.plot_partial_dependence
• Partial Dependence Plots
29.10 sklearn.exceptions: Exceptions and warnings
The sklearn.exceptions module includes all custom warnings and error classes used across scikit-learn.
exceptions.NotFittedError
Exception class to raise if estimator is used before ﬁtting.
exceptions.ChangedBehaviorWarning
Warning class used to notify the user of any change in the
behavior.
exceptions.ConvergenceWarning
Custom warning to capture convergence problems
exceptions.DataConversionWarning
Warning used to notify implicit data conversions happening
in the code.
exceptions.DataDimensionalityWarning
Custom warning to notify potential issues with data dimen-
sionality.
exceptions.EfficiencyWarning
Warning used to notify the user of inefﬁcient computation.
exceptions.FitFailedWarning
Warning class used if there is an error while ﬁtting the es-
timator.
exceptions.NonBLASDotWarning
Warning used when the dot operation does not use BLAS.
exceptions.UndefinedMetricWarning
Warning used when the metric is invalid
29.10.1 sklearn.exceptions.NotFittedError
class sklearn.exceptions.NotFittedError
Exception class to raise if estimator is used before ﬁtting.
This class inherits from both ValueError and AttributeError to help with exception handling and backward
compatibility.
Examples
>>> from sklearn.svm import LinearSVC
>>> from sklearn.exceptions import NotFittedError
>>> try:
...
LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
... except NotFittedError as e:
29.10. sklearn.exceptions: Exceptions and warnings
1383
scikit-learn user guide, Release 0.18.2
...
print(repr(e))
...
NotFittedError('This LinearSVC instance is not fitted yet',)
Changed in version 0.18: Moved from sklearn.utils.validation.
29.10.2 sklearn.exceptions.ChangedBehaviorWarning
class sklearn.exceptions.ChangedBehaviorWarning
Warning class used to notify the user of any change in the behavior.
Changed in version 0.18: Moved from sklearn.base.
29.10.3 sklearn.exceptions.ConvergenceWarning
class sklearn.exceptions.ConvergenceWarning
Custom warning to capture convergence problems
Changed in version 0.18: Moved from sklearn.utils.
Examples using sklearn.exceptions.ConvergenceWarning
• Sparse recovery: feature selection for sparse linear models
29.10.4 sklearn.exceptions.DataConversionWarning
class sklearn.exceptions.DataConversionWarning
Warning used to notify implicit data conversions happening in the code.
This warning occurs when some input data needs to be converted or interpreted in a way that may not match the
user’s expectations.
For example, this warning may occur when the user
•passes an integer array to a function which expects ﬂoat input and will convert the input
•requests a non-copying operation, but a copy is required to meet the implementation’s data-type ex-
pectations;
•passes an input whose shape can be interpreted ambiguously.
Changed in version 0.18: Moved from sklearn.utils.validation.
29.10.5 sklearn.exceptions.DataDimensionalityWarning
class sklearn.exceptions.DataDimensionalityWarning
Custom warning to notify potential issues with data dimensionality.
For example, in random projection, this warning is raised when the number of components, which quantiﬁes
the dimensionality of the target projection space, is higher than the number of features, which quantiﬁes the
dimensionality of the original source space, to imply that the dimensionality of the problem will not be reduced.
Changed in version 0.18: Moved from sklearn.utils.
1384
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.10.6 sklearn.exceptions.EfﬁciencyWarning
class sklearn.exceptions.EfficiencyWarning
Warning used to notify the user of inefﬁcient computation.
This warning notiﬁes the user that the efﬁciency may not be optimal due to some reason which may be included
as a part of the warning message. This may be subclassed into a more speciﬁc Warning class.
New in version 0.18.
29.10.7 sklearn.exceptions.FitFailedWarning
class sklearn.exceptions.FitFailedWarning
Warning class used if there is an error while ﬁtting the estimator.
This Warning is used in meta estimators GridSearchCV and RandomizedSearchCV and the cross-validation
helper function cross_val_score to warn when there is an error while ﬁtting the estimator.
Examples
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.svm import LinearSVC
>>> from sklearn.exceptions import FitFailedWarning
>>> import warnings
>>> warnings.simplefilter('always', FitFailedWarning)
>>> gs = GridSearchCV(LinearSVC(), {'C': [-1, -2]}, error_score=0)
>>> X, y = [[1, 2], [3, 4], [5, 6], [7, 8], [8, 9]], [0, 0, 0, 1, 1]
>>> with warnings.catch_warnings(record=True) as w:
...
try:
...
gs.fit(X, y)
# This will raise a ValueError since C is < 0
...
except ValueError:
...
pass
...
print(repr(w[-1].message))
...
FitFailedWarning("Classifier fit failed. The score on this train-test
partition for these parameters will be set to 0.000000. Details:
\nValueError('Penalty term must be positive; got (C=-2)',)",)
Changed in version 0.18: Moved from sklearn.cross_validation.
29.10.8 sklearn.exceptions.NonBLASDotWarning
class sklearn.exceptions.NonBLASDotWarning
Warning used when the dot operation does not use BLAS.
This warning is used to notify the user that BLAS was not used for dot operation and hence the efﬁciency may
be affected.
Changed in version 0.18: Moved from sklearn.utils.validation, extends EfﬁciencyWarning.
29.10.9 sklearn.exceptions.UndeﬁnedMetricWarning
class sklearn.exceptions.UndefinedMetricWarning
Warning used when the metric is invalid
29.10. sklearn.exceptions: Exceptions and warnings
1385
scikit-learn user guide, Release 0.18.2
Changed in version 0.18: Moved from sklearn.base.
29.11 sklearn.feature_extraction: Feature Extraction
The sklearn.feature_extraction module deals with feature extraction from raw data. It currently includes
methods to extract features from text and images.
User guide: See the Feature extraction section for further details.
feature_extraction.DictVectorizer([dtype,
...])
Transforms lists of feature-value mappings to vectors.
feature_extraction.FeatureHasher([...])
Implements feature hashing, aka the hashing trick.
29.11.1 sklearn.feature_extraction.DictVectorizer
class sklearn.feature_extraction.DictVectorizer(dtype=<type ‘numpy.ﬂoat64’>,
separa-
tor=’=’, sparse=True, sort=True)
Transforms lists of feature-value mappings to vectors.
This transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays
or scipy.sparse matrices for use with scikit-learn estimators.
When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-
valued feature is constructed for each of the possible string values that the feature can take on. For instance, a
feature “f” that can take on the values “ham” and “spam” will become two features in the output, one signifying
“f=ham”, the other “f=spam”.
However, note that this transformer will only do a binary one-hot encoding when feature values are of type
string. If categorical features are represented as numeric values such as int, the DictVectorizer can be followed
by OneHotEncoder to complete binary one-hot encoding.
Features that do not occur in a sample (mapping) will have a zero value in the resulting array/matrix.
Read more in the User Guide.
Parametersdtype : callable, optional
The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as
the dtype argument.
separator : string, optional
Separator string used when constructing new features for one-hot coding.
sparse : boolean, optional.
Whether transform should produce scipy.sparse matrices. True by default.
sort : boolean, optional.
Whether feature_names_ and vocabulary_ should be sorted when ﬁtting. True
by default.
Attributesvocabulary_ : dict
A dictionary mapping feature names to feature indices.
feature_names_ : list
A list of length n_features containing the feature names (e.g., “f=ham” and “f=spam”).
1386
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
See also:
FeatureHasherperforms vectorization using only a hash function.
sklearn.preprocessing.OneHotEncoderhandles nominal/categorical features encoded as columns
of integers.
Examples
>>> from sklearn.feature_extraction import DictVectorizer
>>> v = DictVectorizer(sparse=False)
>>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]
>>> X = v.fit_transform(D)
>>> X
array([[ 2.,
0.,
1.],
[ 0.,
1.,
3.]])
>>> v.inverse_transform(X) ==
[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo
˓→': 3.0}]
True
>>> v.transform({'foo': 4, 'unseen_feature': 3})
array([[ 0.,
0.,
4.]])
Methods
fit(X[, y])
Learn a list of feature name -> indices mappings.
fit_transform(X[, y])
Learn a list of feature name -> indices mappings and
transform X.
get_feature_names()
Returns a list of feature names, ordered by their indices.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X[, dict_type])
Transform array or sparse matrix X back to feature map-
pings.
restrict(support[, indices])
Restrict the features to those in support using feature
selection.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Transform feature->value dicts to array or sparse ma-
trix.
__init__(dtype=<type ‘numpy.ﬂoat64’>, separator=’=’, sparse=True, sort=True)
fit(X, y=None)
Learn a list of feature name -> indices mappings.
ParametersX : Mapping or iterable over Mappings
Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values
(strings or convertible to dtype).
y : (ignored)
Returnsself :
fit_transform(X, y=None)
Learn a list of feature name -> indices mappings and transform X.
Like ﬁt(X) followed by transform(X), but does not require materializing X in memory.
29.11. sklearn.feature_extraction: Feature Extraction
1387
scikit-learn user guide, Release 0.18.2
ParametersX : Mapping or iterable over Mappings
Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values
(strings or convertible to dtype).
y : (ignored)
ReturnsXa : {array, sparse matrix}
Feature vectors; always 2-d.
get_feature_names()
Returns a list of feature names, ordered by their indices.
If one-of-K coding is applied to categorical features, this will include the constructed feature names but
not the original ones.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(X, dict_type=<type ‘dict’>)
Transform array or sparse matrix X back to feature mappings.
X must have been produced by this DictVectorizer’s transform or ﬁt_transform method; it may only have
passed through transformers that preserve the number of features and their order.
In the case of one-hot/one-of-K coding, the constructed feature names and values are returned rather than
the original ones.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Sample matrix.
dict_type : callable, optional
Constructor for feature mappings. Must conform to the collections.Mapping API.
ReturnsD : list of dict_type objects, length = n_samples
Feature mappings for the samples in X.
restrict(support, indices=False)
Restrict the features to those in support using feature selection.
This function modiﬁes the estimator in-place.
Parameterssupport : array-like
Boolean mask or list of indices (as returned by the get_support member of feature se-
lectors).
indices : boolean, optional
Whether support is a list of indices.
Returnsself :
1388
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn.feature_extraction import DictVectorizer
>>> from sklearn.feature_selection import SelectKBest, chi2
>>> v = DictVectorizer()
>>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]
>>> X = v.fit_transform(D)
>>> support = SelectKBest(chi2, k=2).fit(X, [0, 1])
>>> v.get_feature_names()
['bar', 'baz', 'foo']
>>> v.restrict(support.get_support())
DictVectorizer(dtype=..., separator='=', sort=True,
sparse=True)
>>> v.get_feature_names()
['bar', 'foo']
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Transform feature->value dicts to array or sparse matrix.
Named features not encountered during ﬁt or ﬁt_transform will be silently ignored.
ParametersX : Mapping or iterable over Mappings, length = n_samples
Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values
(strings or convertible to dtype).
y : (ignored)
ReturnsXa : {array, sparse matrix}
Feature vectors; always 2-d.
Examples using sklearn.feature_extraction.DictVectorizer
• Feature Union with Heterogeneous Data Sources
• FeatureHasher and DictVectorizer Comparison
29.11.2 sklearn.feature_extraction.FeatureHasher
class sklearn.feature_extraction.FeatureHasher(n_features=1048576,
input_type=’dict’,
dtype=<type
‘numpy.ﬂoat64’>,
non_negative=False)
Implements feature hashing, aka the hashing trick.
This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function
to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version
of Murmurhash3.
29.11. sklearn.feature_extraction: Feature Extraction
1389
scikit-learn user guide, Release 0.18.2
Feature names of type byte string are used as-is. Unicode strings are converted to UTF-8 ﬁrst, but no Unicode
normalization is done. Feature values must be (ﬁnite) numbers.
This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online)
learning and situations where memory is tight, e.g. when running prediction code on embedded devices.
Read more in the User Guide.
Parametersn_features : integer, optional
The number of features (columns) in the output matrices. Small numbers of features are
likely to cause hash collisions, but large numbers will cause larger coefﬁcient dimen-
sions in linear learners.
dtype : numpy type, optional, default np.ﬂoat64
The type of feature values. Passed to scipy.sparse matrix constructors as the dtype
argument. Do not set this to bool, np.boolean or any unsigned integer type.
input_type : string, optional, default “dict”
Either “dict” (the default) to accept dictionaries over (feature_name, value); “pair” to
accept pairs of (feature_name, value); or “string” to accept single strings. feature_name
should be a string, while value should be a number. In the case of “string”, a value of 1
is implied. The feature_name is hashed to ﬁnd the appropriate column for the feature.
The value’s sign might be ﬂipped in the output (but see non_negative, below).
non_negative : boolean, optional, default False
Whether output matrices should contain non-negative values only; effectively calls abs
on the matrix prior to returning it. When True, output values can be interpreted as
frequencies. When False, output values will have expected value zero.
See also:
DictVectorizervectorizes string-valued features using a hash table.
sklearn.preprocessing.OneHotEncoderhandles nominal/categorical features encoded as columns
of integers.
Examples
>>> from sklearn.feature_extraction import FeatureHasher
>>> h = FeatureHasher(n_features=10)
>>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]
>>> f = h.transform(D)
>>> f.toarray()
array([[ 0.,
0., -4., -1.,
0.,
0.,
0.,
0.,
0.,
2.],
[ 0.,
0.,
0., -2., -5.,
0.,
0.,
0.,
0.,
0.]])
Methods
fit([X, y])
No-op.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
Continued on next page
1390
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.86 – continued from previous page
transform(raw_X[, y])
Transform a sequence of instances to a scipy.sparse ma-
trix.
__init__(n_features=1048576,
input_type=’dict’,
dtype=<type
‘numpy.ﬂoat64’>,
non_negative=False)
fit(X=None, y=None)
No-op.
This method doesn’t do anything. It exists purely for compatibility with the scikit-learn transformer API.
Returnsself : FeatureHasher
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(raw_X, y=None)
Transform a sequence of instances to a scipy.sparse matrix.
Parametersraw_X : iterable over iterable over raw features, length = n_samples
Samples. Each sample must be iterable an (e.g., a list or tuple) containing/generating
feature names (and optionally values, see the input_type constructor argument) which
will be hashed. raw_X need not support the len function, so it can be the result of a
generator; n_samples is determined on the ﬂy.
y : (ignored)
ReturnsX : scipy.sparse matrix, shape = (n_samples, self.n_features)
29.11. sklearn.feature_extraction: Feature Extraction
1391
scikit-learn user guide, Release 0.18.2
Feature matrix, for use with estimators or further transformers.
Examples using sklearn.feature_extraction.FeatureHasher
• FeatureHasher and DictVectorizer Comparison
29.11.3 From images
The sklearn.feature_extraction.image submodule gathers utilities to extract features from images.
feature_extraction.image.img_to_graph(img[,
...])
Graph of the pixel-to-pixel gradient connections
feature_extraction.image.grid_to_graph(n_x,
n_y)
Graph of the pixel-to-pixel connections
feature_extraction.image.extract_patches_2d(...)
Reshape a 2D image into a collection of patches
feature_extraction.image.reconstruct_from_patches_2d(...)
Reconstruct the image from all of its patches.
feature_extraction.image.PatchExtractor([...])
Extracts patches from a collection of images
sklearn.feature_extraction.image.img_to_graph
sklearn.feature_extraction.image.img_to_graph(img,
mask=None,
return_as=<class
‘scipy.sparse.coo.coo_matrix’>,
dtype=None)
Graph of the pixel-to-pixel gradient connections
Edges are weighted with the gradient values.
Read more in the User Guide.
Parametersimg : ndarray, 2D or 3D
2D or 3D image
mask : ndarray of booleans, optional
An optional mask of the image, to consider only part of the pixels.
return_as : np.ndarray or a sparse matrix class, optional
The class to use to build the returned adjacency matrix.
dtype : None or dtype, optional
The data of the returned sparse matrix. By default it is the dtype of img
Notes
For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix
instance. Going forward, np.ndarray returns an np.ndarray, as expected.
For compatibility, user code relying on this method should wrap its calls in np.asarray to avoid type issues.
1392
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.feature_extraction.image.grid_to_graph
sklearn.feature_extraction.image.grid_to_graph(n_x,
n_y,
n_z=1,
mask=None,
return_as=<class
‘scipy.sparse.coo.coo_matrix’>,
dtype=<type ‘int’>)
Graph of the pixel-to-pixel connections
Edges exist if 2 voxels are connected.
Parametersn_x : int
Dimension in x axis
n_y : int
Dimension in y axis
n_z : int, optional, default 1
Dimension in z axis
mask : ndarray of booleans, optional
An optional mask of the image, to consider only part of the pixels.
return_as : np.ndarray or a sparse matrix class, optional
The class to use to build the returned adjacency matrix.
dtype : dtype, optional, default int
The data of the returned sparse matrix. By default it is int
Notes
For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix
instance. Going forward, np.ndarray returns an np.ndarray, as expected.
For compatibility, user code relying on this method should wrap its calls in np.asarray to avoid type issues.
sklearn.feature_extraction.image.extract_patches_2d
sklearn.feature_extraction.image.extract_patches_2d(image,
patch_size,
max_patches=None,
ran-
dom_state=None)
Reshape a 2D image into a collection of patches
The resulting patches are allocated in a dedicated array.
Read more in the User Guide.
Parametersimage : array, shape = (image_height, image_width) or
(image_height, image_width, n_channels) The original image data. For color images,
the last dimension speciﬁes the channel: a RGB image would have n_channels=3.
patch_size : tuple of ints (patch_height, patch_width)
the dimensions of one patch
max_patches : integer or ﬂoat, optional default is None
29.11. sklearn.feature_extraction: Feature Extraction
1393
scikit-learn user guide, Release 0.18.2
The maximum number of patches to extract. If max_patches is a ﬂoat between 0 and 1,
it is taken to be a proportion of the total number of patches.
random_state : int or RandomState
Pseudo number generator state used for random sampling to use if max_patches is not
None.
Returnspatches : array, shape = (n_patches, patch_height, patch_width) or
(n_patches, patch_height, patch_width, n_channels) The collection of patches extracted
from the image, where n_patches is either max_patches or the total number of patches
that can be extracted.
Examples
>>> from sklearn.feature_extraction import image
>>> one_image = np.arange(16).reshape((4, 4))
>>> one_image
array([[ 0,
1,
2,
3],
[ 4,
5,
6,
7],
[ 8,
9, 10, 11],
[12, 13, 14, 15]])
>>> patches = image.extract_patches_2d(one_image, (2, 2))
>>> print(patches.shape)
(9, 2, 2)
>>> patches[0]
array([[0, 1],
[4, 5]])
>>> patches[1]
array([[1, 2],
[5, 6]])
>>> patches[8]
array([[10, 11],
[14, 15]])
Examples using sklearn.feature_extraction.image.extract_patches_2d
• Online learning of a dictionary of parts of faces
• Image denoising using dictionary learning
sklearn.feature_extraction.image.reconstruct_from_patches_2d
sklearn.feature_extraction.image.reconstruct_from_patches_2d(patches,
im-
age_size)
Reconstruct the image from all of its patches.
Patches are assumed to overlap and the image is constructed by ﬁlling in the patches from left to right, top to
bottom, averaging the overlapping regions.
Read more in the User Guide.
Parameterspatches : array, shape = (n_patches, patch_height, patch_width) or
1394
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
(n_patches, patch_height, patch_width, n_channels) The complete set of patches. If
the patches contain colour information, channels are indexed along the last dimension:
RGB patches would have n_channels=3.
image_size : tuple of ints (image_height, image_width) or
(image_height, image_width, n_channels) the size of the image that will be recon-
structed
Returnsimage : array, shape = image_size
the reconstructed image
Examples using sklearn.feature_extraction.image.reconstruct_from_patches_2d
• Image denoising using dictionary learning
sklearn.feature_extraction.image.PatchExtractor
class sklearn.feature_extraction.image.PatchExtractor(patch_size=None,
max_patches=None,
ran-
dom_state=None)
Extracts patches from a collection of images
Read more in the User Guide.
Parameterspatch_size : tuple of ints (patch_height, patch_width)
the dimensions of one patch
max_patches : integer or ﬂoat, optional default is None
The maximum number of patches per image to extract. If max_patches is a ﬂoat in (0,
1), it is taken to mean a proportion of the total number of patches.
random_state : int or RandomState
Pseudo number generator state used for random sampling.
Methods
fit(X[, y])
Do nothing and return the estimator unchanged
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transforms the image samples in X into a matrix of
patch data.
__init__(patch_size=None, max_patches=None, random_state=None)
fit(X, y=None)
Do nothing and return the estimator unchanged
This method is just there to implement the usual API and hence work in pipelines.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
29.11. sklearn.feature_extraction: Feature Extraction
1395
scikit-learn user guide, Release 0.18.2
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transforms the image samples in X into a matrix of patch data.
ParametersX : array, shape = (n_samples, image_height, image_width) or
(n_samples, image_height, image_width, n_channels) Array of images from which to
extract patches. For color images, the last dimension speciﬁes the channel: a RGB
image would have n_channels=3.
Returnspatches: array, shape = (n_patches, patch_height, patch_width) or :
(n_patches, patch_height, patch_width, n_channels) The collection of patches extracted
from the images, where n_patches is either n_samples * max_patches or the total num-
ber of patches that can be extracted.
29.11.4 From text
The sklearn.feature_extraction.text submodule gathers utilities to build feature vectors from text doc-
uments.
feature_extraction.text.CountVectorizer([...])
Convert a collection of text documents to a matrix of token
counts
feature_extraction.text.HashingVectorizer([...])
Convert a collection of text documents to a matrix of token
occurrences
feature_extraction.text.TfidfTransformer([...])
Transform a count matrix to a normalized tf or tf-idf repre-
sentation
feature_extraction.text.TfidfVectorizer([...])
Convert a collection of raw documents to a matrix of TF-
IDF features.
1396
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.feature_extraction.text.CountVectorizer
class sklearn.feature_extraction.text.CountVectorizer(input=u’content’,
encoding=u’utf-8’,
de-
code_error=u’strict’,
strip_accents=None,
low-
ercase=True,
preproces-
sor=None,
tokenizer=None,
stop_words=None,
to-
ken_pattern=u’(?u)\b\w\w+\b’,
ngram_range=(1,
1),
ana-
lyzer=u’word’,
max_df=1.0,
min_df=1,
max_features=None,
vocabulary=None, binary=False,
dtype=<type ‘numpy.int64’>)
Convert a collection of text documents to a matrix of token counts
This implementation produces a sparse representation of the counts using scipy.sparse.coo_matrix.
If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature
selection then the number of features will be equal to the vocabulary size found by analyzing the data.
Read more in the User Guide.
Parametersinput : string {‘ﬁlename’, ‘ﬁle’, ‘content’}
If ‘ﬁlename’, the sequence passed as an argument to ﬁt is expected to be a list of ﬁle-
names that need reading to fetch the raw content to analyze.
If ‘ﬁle’, the sequence items must have a ‘read’ method (ﬁle-like object) that is called to
fetch the bytes in memory.
Otherwise the input is expected to be the sequence strings or bytes items are expected
to be analyzed directly.
encoding : string, ‘utf-8’ by default.
If bytes or ﬁles are given to analyze, this encoding is used to decode.
decode_error : {‘strict’, ‘ignore’, ‘replace’}
Instruction on what to do if a byte sequence is given to analyze that contains characters
not of the given encoding. By default, it is ‘strict’, meaning that a UnicodeDecodeError
will be raised. Other values are ‘ignore’ and ‘replace’.
strip_accents : {‘ascii’, ‘unicode’, None}
Remove accents during the preprocessing step. ‘ascii’ is a fast method that only works
on characters that have an direct ASCII mapping. ‘unicode’ is a slightly slower method
that works on any characters. None (default) does nothing.
analyzer : string, {‘word’, ‘char’, ‘char_wb’} or callable
Whether the feature should be made of word or character n-grams. Option ‘char_wb’
creates character n-grams only from text inside word boundaries.
If a callable is passed it is used to extract the sequence of features out of the raw, unpro-
cessed input.
preprocessor : callable or None (default)
Override the preprocessing (string transformation) stage while preserving the tokenizing
and n-grams generation steps.
29.11. sklearn.feature_extraction: Feature Extraction
1397
scikit-learn user guide, Release 0.18.2
tokenizer : callable or None (default)
Override the string tokenization step while preserving the preprocessing and n-grams
generation steps. Only applies if analyzer == 'word'.
ngram_range : tuple (min_n, max_n)
The lower and upper boundary of the range of n-values for different n-grams to be
extracted. All values of n such that min_n <= n <= max_n will be used.
stop_words : string {‘english’}, list, or None (default)
If ‘english’, a built-in stop word list for English is used.
If a list, that list is assumed to contain stop words, all of which will be removed from
the resulting tokens. Only applies if analyzer == 'word'.
If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0)
to automatically detect and ﬁlter stop words based on intra corpus document frequency
of terms.
lowercase : boolean, True by default
Convert all characters to lowercase before tokenizing.
token_pattern : string
Regular expression denoting what constitutes a “token”, only used if analyzer ==
'word'. The default regexp select tokens of 2 or more alphanumeric characters (punc-
tuation is completely ignored and always treated as a token separator).
max_df : ﬂoat in range [0.0, 1.0] or int, default=1.0
When building the vocabulary ignore terms that have a document frequency strictly
higher than the given threshold (corpus-speciﬁc stop words). If ﬂoat, the parameter
represents a proportion of documents, integer absolute counts. This parameter is ignored
if vocabulary is not None.
min_df : ﬂoat in range [0.0, 1.0] or int, default=1
When building the vocabulary ignore terms that have a document frequency strictly
lower than the given threshold. This value is also called cut-off in the literature. If
ﬂoat, the parameter represents a proportion of documents, integer absolute counts. This
parameter is ignored if vocabulary is not None.
max_features : int or None, default=None
If not None, build a vocabulary that only consider the top max_features ordered by term
frequency across the corpus.
This parameter is ignored if vocabulary is not None.
vocabulary : Mapping or iterable, optional
Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature
matrix, or an iterable over terms. If not given, a vocabulary is determined from the input
documents. Indices in the mapping should not be repeated and should not have any gap
between 0 and the largest index.
binary : boolean, default=False
If True, all non zero counts are set to 1. This is useful for discrete probabilistic models
that model binary events rather than integer counts.
dtype : type, optional
1398
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Type of the matrix returned by ﬁt_transform() or transform().
Attributesvocabulary_ : dict
A mapping of terms to feature indices.
stop_words_ : set
Terms that were ignored because they either:
•occurred in too many documents (max_df)
•occurred in too few documents (min_df)
•were cut off by feature selection (max_features).
This is only available if no vocabulary was given.
See also:
HashingVectorizer, TfidfVectorizer
Notes
The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided
only for introspection and can be safely removed using delattr or set to None before pickling.
Methods
build_analyzer()
Return a callable that handles preprocessing and tok-
enization
build_preprocessor()
Return a function to preprocess the text before tokeniza-
tion
build_tokenizer()
Return a function that splits a string into a sequence of
tokens
decode(doc)
Decode the input into a string of unicode symbols
fit(raw_documents[, y])
Learn a vocabulary dictionary of all tokens in the raw
documents.
fit_transform(raw_documents[, y])
Learn the vocabulary dictionary and return term-
document matrix.
get_feature_names()
Array mapping from feature integer indices to feature
name
get_params([deep])
Get parameters for this estimator.
get_stop_words()
Build or fetch the effective stop words list
inverse_transform(X)
Return terms per document with nonzero entries in X.
set_params(\*\*params)
Set the parameters of this estimator.
transform(raw_documents)
Transform documents to document-term matrix.
__init__(input=u’content’,
encoding=u’utf-8’,
decode_error=u’strict’,
strip_accents=None,
lowercase=True,
preprocessor=None,
tokenizer=None,
stop_words=None,
to-
ken_pattern=u’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=u’word’, max_df=1.0,
min_df=1,
max_features=None,
vocabulary=None,
binary=False,
dtype=<type
‘numpy.int64’>)
build_analyzer()
Return a callable that handles preprocessing and tokenization
29.11. sklearn.feature_extraction: Feature Extraction
1399
scikit-learn user guide, Release 0.18.2
build_preprocessor()
Return a function to preprocess the text before tokenization
build_tokenizer()
Return a function that splits a string into a sequence of tokens
decode(doc)
Decode the input into a string of unicode symbols
The decoding strategy depends on the vectorizer parameters.
fit(raw_documents, y=None)
Learn a vocabulary dictionary of all tokens in the raw documents.
Parametersraw_documents : iterable
An iterable which yields either str, unicode or ﬁle objects.
Returnsself :
fit_transform(raw_documents, y=None)
Learn the vocabulary dictionary and return term-document matrix.
This is equivalent to ﬁt followed by transform, but more efﬁciently implemented.
Parametersraw_documents : iterable
An iterable which yields either str, unicode or ﬁle objects.
ReturnsX : array, [n_samples, n_features]
Document-term matrix.
get_feature_names()
Array mapping from feature integer indices to feature name
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_stop_words()
Build or fetch the effective stop words list
inverse_transform(X)
Return terms per document with nonzero entries in X.
ParametersX : {array, sparse matrix}, shape = [n_samples, n_features]
ReturnsX_inv : list of arrays, len = n_samples
List of arrays of terms.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
1400
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsself :
transform(raw_documents)
Transform documents to document-term matrix.
Extract token counts out of raw text documents using the vocabulary ﬁtted with ﬁt or the one provided to
the constructor.
Parametersraw_documents : iterable
An iterable which yields either str, unicode or ﬁle objects.
ReturnsX : sparse matrix, [n_samples, n_features]
Document-term matrix.
Examples using sklearn.feature_extraction.text.CountVectorizer
• Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
• Sample pipeline for text feature extraction and evaluation
sklearn.feature_extraction.text.HashingVectorizer
class sklearn.feature_extraction.text.HashingVectorizer(input=u’content’,
encoding=u’utf-8’,
de-
code_error=u’strict’,
strip_accents=None,
low-
ercase=True,
preproces-
sor=None,
tokenizer=None,
stop_words=None,
to-
ken_pattern=u’(?u)\b\w\w+\b’,
ngram_range=(1,
1),
analyzer=u’word’,
n_features=1048576,
bi-
nary=False,
norm=u’l2’,
non_negative=False,
dtype=<type
‘numpy.ﬂoat64’>)
Convert a collection of text documents to a matrix of token occurrences
It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary
occurrence information), possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean
unit sphere if norm=’l2’.
This text vectorizer implementation uses the hashing trick to ﬁnd the token string name to feature integer index
mapping.
This strategy has several advantages:
•it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in
memory
•it is fast to pickle and un-pickle as it holds no state besides the constructor parameters
•it can be used in a streaming (partial ﬁt) or parallel pipeline as there is no state computed during ﬁt.
There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):
29.11. sklearn.feature_extraction: Feature Extraction
1401
scikit-learn user guide, Release 0.18.2
•there is no way to compute the inverse transform (from feature indices to string feature names) which can
be a problem when trying to introspect which features are most important to a model.
•there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this
is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classiﬁcation problems).
•no IDF weighting as this would render the transformer stateful.
The hash function employed is the signed 32-bit version of Murmurhash3.
Read more in the User Guide.
Parametersinput : string {‘ﬁlename’, ‘ﬁle’, ‘content’}
If ‘ﬁlename’, the sequence passed as an argument to ﬁt is expected to be a list of ﬁle-
names that need reading to fetch the raw content to analyze.
If ‘ﬁle’, the sequence items must have a ‘read’ method (ﬁle-like object) that is called to
fetch the bytes in memory.
Otherwise the input is expected to be the sequence strings or bytes items are expected
to be analyzed directly.
encoding : string, default=’utf-8’
If bytes or ﬁles are given to analyze, this encoding is used to decode.
decode_error : {‘strict’, ‘ignore’, ‘replace’}
Instruction on what to do if a byte sequence is given to analyze that contains characters
not of the given encoding. By default, it is ‘strict’, meaning that a UnicodeDecodeError
will be raised. Other values are ‘ignore’ and ‘replace’.
strip_accents : {‘ascii’, ‘unicode’, None}
Remove accents during the preprocessing step. ‘ascii’ is a fast method that only works
on characters that have an direct ASCII mapping. ‘unicode’ is a slightly slower method
that works on any characters. None (default) does nothing.
analyzer : string, {‘word’, ‘char’, ‘char_wb’} or callable
Whether the feature should be made of word or character n-grams. Option ‘char_wb’
creates character n-grams only from text inside word boundaries.
If a callable is passed it is used to extract the sequence of features out of the raw, unpro-
cessed input.
preprocessor : callable or None (default)
Override the preprocessing (string transformation) stage while preserving the tokenizing
and n-grams generation steps.
tokenizer : callable or None (default)
Override the string tokenization step while preserving the preprocessing and n-grams
generation steps. Only applies if analyzer == 'word'.
ngram_range : tuple (min_n, max_n), default=(1, 1)
The lower and upper boundary of the range of n-values for different n-grams to be
extracted. All values of n such that min_n <= n <= max_n will be used.
stop_words : string {‘english’}, list, or None (default)
If ‘english’, a built-in stop word list for English is used.
1402
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
If a list, that list is assumed to contain stop words, all of which will be removed from
the resulting tokens. Only applies if analyzer == 'word'.
lowercase : boolean, default=True
Convert all characters to lowercase before tokenizing.
token_pattern : string
Regular expression denoting what constitutes a “token”, only used if analyzer ==
'word'.
The default regexp selects tokens of 2 or more alphanumeric characters
(punctuation is completely ignored and always treated as a token separator).
n_features : integer, default=(2 ** 20)
The number of features (columns) in the output matrices. Small numbers of features are
likely to cause hash collisions, but large numbers will cause larger coefﬁcient dimen-
sions in linear learners.
norm : ‘l1’, ‘l2’ or None, optional
Norm used to normalize term vectors. None for no normalization.
binary : boolean, default=False.
If True, all non zero counts are set to 1. This is useful for discrete probabilistic models
that model binary events rather than integer counts.
dtype : type, optional
Type of the matrix returned by ﬁt_transform() or transform().
non_negative : boolean, default=False
Whether output matrices should contain non-negative values only; effectively calls abs
on the matrix prior to returning it. When True, output values can be interpreted as
frequencies. When False, output values will have expected value zero.
See also:
CountVectorizer, TfidfVectorizer
Methods
build_analyzer()
Return a callable that handles preprocessing and tok-
enization
build_preprocessor()
Return a function to preprocess the text before tokeniza-
tion
build_tokenizer()
Return a function that splits a string into a sequence of
tokens
decode(doc)
Decode the input into a string of unicode symbols
fit(X[, y])
Does nothing: this transformer is stateless.
fit_transform(X[, y])
Transform a sequence of documents to a document-term
matrix.
get_params([deep])
Get parameters for this estimator.
get_stop_words()
Build or fetch the effective stop words list
partial_fit(X[, y])
Does nothing: this transformer is stateless.
set_params(\*\*params)
Set the parameters of this estimator.
Continued on next page
29.11. sklearn.feature_extraction: Feature Extraction
1403
scikit-learn user guide, Release 0.18.2
Table 29.91 – continued from previous page
transform(X[, y])
Transform a sequence of documents to a document-term
matrix.
__init__(input=u’content’,
encoding=u’utf-8’,
decode_error=u’strict’,
strip_accents=None,
lowercase=True,
preprocessor=None,
tokenizer=None,
stop_words=None,
to-
ken_pattern=u’(?u)\\b\\w\\w+\\b’,
ngram_range=(1,
1),
analyzer=u’word’,
n_features=1048576,
binary=False,
norm=u’l2’,
non_negative=False,
dtype=<type
‘numpy.ﬂoat64’>)
build_analyzer()
Return a callable that handles preprocessing and tokenization
build_preprocessor()
Return a function to preprocess the text before tokenization
build_tokenizer()
Return a function that splits a string into a sequence of tokens
decode(doc)
Decode the input into a string of unicode symbols
The decoding strategy depends on the vectorizer parameters.
fit(X, y=None)
Does nothing: this transformer is stateless.
fit_transform(X, y=None)
Transform a sequence of documents to a document-term matrix.
ParametersX : iterable over raw text documents, length = n_samples
Samples. Each sample must be a text document (either bytes or unicode strings, ﬁle
name or ﬁle object depending on the constructor argument) which will be tokenized and
hashed.
y : (ignored)
ReturnsX : scipy.sparse matrix, shape = (n_samples, self.n_features)
Document-term matrix.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_stop_words()
Build or fetch the effective stop words list
partial_fit(X, y=None)
Does nothing: this transformer is stateless.
This method is just there to mark the fact that this transformer can work in a streaming setup.
1404
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Transform a sequence of documents to a document-term matrix.
ParametersX : iterable over raw text documents, length = n_samples
Samples. Each sample must be a text document (either bytes or unicode strings, ﬁle
name or ﬁle object depending on the constructor argument) which will be tokenized and
hashed.
y : (ignored)
ReturnsX : scipy.sparse matrix, shape = (n_samples, self.n_features)
Document-term matrix.
Examples using sklearn.feature_extraction.text.HashingVectorizer
• Out-of-core classiﬁcation of text documents
• Classiﬁcation of text documents using sparse features
• Clustering text documents using k-means
sklearn.feature_extraction.text.TﬁdfTransformer
class sklearn.feature_extraction.text.TfidfTransformer(norm=u’l2’,
use_idf=True,
smooth_idf=True,
sublin-
ear_tf=False)
Transform a count matrix to a normalized tf or tf-idf representation
Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a com-
mon term weighting scheme in information retrieval, that has also found good use in document classiﬁcation.
The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale
down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less
informative than features that occur in a small fraction of the training corpus.
The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed
as idf(d, t) = log [ n / df(d, t) ] + 1 (if smooth_idf=False), where n is the total number of documents and
df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t.
The effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in
all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the
standard textbook notation that deﬁnes the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).
If smooth_idf=True (the default), the constant “1” is added to the numerator and denominator of the idf
as if an extra document was seen containing every term in the collection exactly once, which prevents zero
divisions: idf(d, t) = log [ (1 + n) / 1 + df(d, t) ] + 1.
Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the
SMART notation used in IR as follows:
29.11. sklearn.feature_extraction: Feature Extraction
1405
scikit-learn user guide, Release 0.18.2
Tf is “n” (natural) by default, “l” (logarithmic) when sublinear_tf=True. Idf is “t” when use_idf is given,
“n” (none) otherwise. Normalization is “c” (cosine) when norm='l2', “n” (none) when norm=None.
Read more in the User Guide.
Parametersnorm : ‘l1’, ‘l2’ or None, optional
Norm used to normalize term vectors. None for no normalization.
use_idf : boolean, default=True
Enable inverse-document-frequency reweighting.
smooth_idf : boolean, default=True
Smooth idf weights by adding one to document frequencies, as if an extra document
was seen containing every term in the collection exactly once. Prevents zero divisions.
sublinear_tf : boolean, default=False
Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).
References
[Yates2011], [MRS2008]
Methods
fit(X[, y])
Learn the idf vector (global term weights)
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, copy])
Transform a count matrix to a tf or tf-idf representation
__init__(norm=u’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False)
fit(X, y=None)
Learn the idf vector (global term weights)
ParametersX : sparse matrix, [n_samples, n_features]
a matrix of term/token counts
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
1406
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, copy=True)
Transform a count matrix to a tf or tf-idf representation
ParametersX : sparse matrix, [n_samples, n_features]
a matrix of term/token counts
copy : boolean, default True
Whether to copy X and operate on the copy or perform in-place operations.
Returnsvectors : sparse matrix, [n_samples, n_features]
Examples using sklearn.feature_extraction.text.TfidfTransformer
• Sample pipeline for text feature extraction and evaluation
• Clustering text documents using k-means
29.11. sklearn.feature_extraction: Feature Extraction
1407
scikit-learn user guide, Release 0.18.2
sklearn.feature_extraction.text.TﬁdfVectorizer
class sklearn.feature_extraction.text.TfidfVectorizer(input=u’content’,
encoding=u’utf-8’,
de-
code_error=u’strict’,
strip_accents=None,
lowercase=True,
pre-
processor=None,
tok-
enizer=None, analyzer=u’word’,
stop_words=None,
to-
ken_pattern=u’(?u)\b\w\w+\b’,
ngram_range=(1,
1),
max_df=1.0,
min_df=1,
max_features=None,
vocab-
ulary=None,
binary=False,
dtype=<type
‘numpy.int64’>,
norm=u’l2’,
use_idf=True,
smooth_idf=True,
sublin-
ear_tf=False)
Convert a collection of raw documents to a matrix of TF-IDF features.
Equivalent to CountVectorizer followed by TﬁdfTransformer.
Read more in the User Guide.
Parametersinput : string {‘ﬁlename’, ‘ﬁle’, ‘content’}
If ‘ﬁlename’, the sequence passed as an argument to ﬁt is expected to be a list of ﬁle-
names that need reading to fetch the raw content to analyze.
If ‘ﬁle’, the sequence items must have a ‘read’ method (ﬁle-like object) that is called to
fetch the bytes in memory.
Otherwise the input is expected to be the sequence strings or bytes items are expected
to be analyzed directly.
encoding : string, ‘utf-8’ by default.
If bytes or ﬁles are given to analyze, this encoding is used to decode.
decode_error : {‘strict’, ‘ignore’, ‘replace’}
Instruction on what to do if a byte sequence is given to analyze that contains characters
not of the given encoding. By default, it is ‘strict’, meaning that a UnicodeDecodeError
will be raised. Other values are ‘ignore’ and ‘replace’.
strip_accents : {‘ascii’, ‘unicode’, None}
Remove accents during the preprocessing step. ‘ascii’ is a fast method that only works
on characters that have an direct ASCII mapping. ‘unicode’ is a slightly slower method
that works on any characters. None (default) does nothing.
analyzer : string, {‘word’, ‘char’} or callable
Whether the feature should be made of word or character n-grams.
If a callable is passed it is used to extract the sequence of features out of the raw, unpro-
cessed input.
preprocessor : callable or None (default)
Override the preprocessing (string transformation) stage while preserving the tokenizing
and n-grams generation steps.
1408
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
tokenizer : callable or None (default)
Override the string tokenization step while preserving the preprocessing and n-grams
generation steps. Only applies if analyzer == 'word'.
ngram_range : tuple (min_n, max_n)
The lower and upper boundary of the range of n-values for different n-grams to be
extracted. All values of n such that min_n <= n <= max_n will be used.
stop_words : string {‘english’}, list, or None (default)
If a string, it is passed to _check_stop_list and the appropriate stop list is returned.
‘english’ is currently the only supported string value.
If a list, that list is assumed to contain stop words, all of which will be removed from
the resulting tokens. Only applies if analyzer == 'word'.
If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0)
to automatically detect and ﬁlter stop words based on intra corpus document frequency
of terms.
lowercase : boolean, default True
Convert all characters to lowercase before tokenizing.
token_pattern : string
Regular expression denoting what constitutes a “token”, only used if analyzer ==
'word'.
The default regexp selects tokens of 2 or more alphanumeric characters
(punctuation is completely ignored and always treated as a token separator).
max_df : ﬂoat in range [0.0, 1.0] or int, default=1.0
When building the vocabulary ignore terms that have a document frequency strictly
higher than the given threshold (corpus-speciﬁc stop words). If ﬂoat, the parameter
represents a proportion of documents, integer absolute counts. This parameter is ignored
if vocabulary is not None.
min_df : ﬂoat in range [0.0, 1.0] or int, default=1
When building the vocabulary ignore terms that have a document frequency strictly
lower than the given threshold. This value is also called cut-off in the literature. If
ﬂoat, the parameter represents a proportion of documents, integer absolute counts. This
parameter is ignored if vocabulary is not None.
max_features : int or None, default=None
If not None, build a vocabulary that only consider the top max_features ordered by term
frequency across the corpus.
This parameter is ignored if vocabulary is not None.
vocabulary : Mapping or iterable, optional
Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature
matrix, or an iterable over terms. If not given, a vocabulary is determined from the input
documents.
binary : boolean, default=False
If True, all non-zero term counts are set to 1. This does not mean outputs will have only
0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to
get 0/1 outputs.)
29.11. sklearn.feature_extraction: Feature Extraction
1409
scikit-learn user guide, Release 0.18.2
dtype : type, optional
Type of the matrix returned by ﬁt_transform() or transform().
norm : ‘l1’, ‘l2’ or None, optional
Norm used to normalize term vectors. None for no normalization.
use_idf : boolean, default=True
Enable inverse-document-frequency reweighting.
smooth_idf : boolean, default=True
Smooth idf weights by adding one to document frequencies, as if an extra document
was seen containing every term in the collection exactly once. Prevents zero divisions.
sublinear_tf : boolean, default=False
Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).
Attributesvocabulary_ : dict
A mapping of terms to feature indices.
idf_ : array, shape = [n_features], or None
The learned idf vector (global term weights) when use_idf is set to True, None oth-
erwise.
stop_words_ : set
Terms that were ignored because they either:
•occurred in too many documents (max_df)
•occurred in too few documents (min_df)
•were cut off by feature selection (max_features).
This is only available if no vocabulary was given.
See also:
CountVectorizerTokenize the documents and count the occurrences of token and return them as a sparse
matrix
TfidfTransformerApply Term Frequency Inverse Document Frequency normalization to a sparse matrix
of occurrence counts.
Notes
The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided
only for introspection and can be safely removed using delattr or set to None before pickling.
Methods
build_analyzer()
Return a callable that handles preprocessing and tok-
enization
build_preprocessor()
Return a function to preprocess the text before tokeniza-
tion
Continued on next page
1410
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.93 – continued from previous page
build_tokenizer()
Return a function that splits a string into a sequence of
tokens
decode(doc)
Decode the input into a string of unicode symbols
fit(raw_documents[, y])
Learn vocabulary and idf from training set.
fit_transform(raw_documents[, y])
Learn vocabulary and idf, return term-document matrix.
get_feature_names()
Array mapping from feature integer indices to feature
name
get_params([deep])
Get parameters for this estimator.
get_stop_words()
Build or fetch the effective stop words list
inverse_transform(X)
Return terms per document with nonzero entries in X.
set_params(\*\*params)
Set the parameters of this estimator.
transform(raw_documents[, copy])
Transform documents to document-term matrix.
__init__(input=u’content’, encoding=u’utf-8’, decode_error=u’strict’, strip_accents=None, lower-
case=True, preprocessor=None, tokenizer=None, analyzer=u’word’, stop_words=None,
token_pattern=u’(?u)\\b\\w\\w+\\b’,
ngram_range=(1,
1),
max_df=1.0,
min_df=1,
max_features=None,
vocabulary=None,
binary=False,
dtype=<type ‘numpy.int64’>,
norm=u’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False)
build_analyzer()
Return a callable that handles preprocessing and tokenization
build_preprocessor()
Return a function to preprocess the text before tokenization
build_tokenizer()
Return a function that splits a string into a sequence of tokens
decode(doc)
Decode the input into a string of unicode symbols
The decoding strategy depends on the vectorizer parameters.
fit(raw_documents, y=None)
Learn vocabulary and idf from training set.
Parametersraw_documents : iterable
an iterable which yields either str, unicode or ﬁle objects
Returnsself : TﬁdfVectorizer
fit_transform(raw_documents, y=None)
Learn vocabulary and idf, return term-document matrix.
This is equivalent to ﬁt followed by transform, but more efﬁciently implemented.
Parametersraw_documents : iterable
an iterable which yields either str, unicode or ﬁle objects
ReturnsX : sparse matrix, [n_samples, n_features]
Tf-idf-weighted document-term matrix.
get_feature_names()
Array mapping from feature integer indices to feature name
get_params(deep=True)
Get parameters for this estimator.
29.11. sklearn.feature_extraction: Feature Extraction
1411
scikit-learn user guide, Release 0.18.2
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_stop_words()
Build or fetch the effective stop words list
inverse_transform(X)
Return terms per document with nonzero entries in X.
ParametersX : {array, sparse matrix}, shape = [n_samples, n_features]
ReturnsX_inv : list of arrays, len = n_samples
List of arrays of terms.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(raw_documents, copy=True)
Transform documents to document-term matrix.
Uses the vocabulary and document frequencies (df) learned by ﬁt (or ﬁt_transform).
Parametersraw_documents : iterable
an iterable which yields either str, unicode or ﬁle objects
copy : boolean, default True
Whether to copy X and operate on the copy or perform in-place operations.
ReturnsX : sparse matrix, [n_samples, n_features]
Tf-idf-weighted document-term matrix.
Examples using sklearn.feature_extraction.text.TfidfVectorizer
• Feature Union with Heterogeneous Data Sources
• Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
• Biclustering documents with the Spectral Co-clustering algorithm
• Classiﬁcation of text documents using sparse features
• Clustering text documents using k-means
• Classiﬁcation of text documents: using a MLComp dataset
1412
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.12 sklearn.feature_selection: Feature Selection
The sklearn.feature_selection module implements feature selection algorithms. It currently includes uni-
variate ﬁlter selection methods and the recursive feature elimination algorithm.
User guide: See the Feature selection section for further details.
feature_selection.GenericUnivariateSelect([...])
Univariate feature selector with conﬁgurable strategy.
feature_selection.SelectPercentile([...])
Select features according to a percentile of the highest
scores.
feature_selection.SelectKBest([score_func,
k])
Select features according to the k highest scores.
feature_selection.SelectFpr([score_func,
al-
pha])
Filter: Select the pvalues below alpha based on a FPR test.
feature_selection.SelectFdr([score_func,
al-
pha])
Filter: Select the p-values for an estimated false discovery
rate
feature_selection.SelectFromModel(estimator) Meta-transformer for selecting features based on impor-
tance weights.
feature_selection.SelectFwe([score_func,
al-
pha])
Filter: Select the p-values corresponding to Family-wise
error rate
feature_selection.RFE(estimator[, ...])
Feature ranking with recursive feature elimination.
feature_selection.RFECV(estimator[, step, ...])
Feature ranking with recursive feature elimination and
cross-validated selection of the best number of features.
feature_selection.VarianceThreshold([threshold])
Feature selector that removes all low-variance features.
29.12.1 sklearn.feature_selection.GenericUnivariateSelect
class sklearn.feature_selection.GenericUnivariateSelect(score_func=<function
f_classif>, mode=’percentile’,
param=1e-05)
Univariate feature selector with conﬁgurable strategy.
Read more in the User Guide.
Parametersscore_func : callable
Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For
modes ‘percentile’ or ‘kbest’ it can return a single array scores.
mode : {‘percentile’, ‘k_best’, ‘fpr’, ‘fdr’, ‘fwe’}
Feature selection mode.
param : ﬂoat or int depending on the feature selection mode
Parameter of the corresponding mode.
Attributesscores_ : array-like, shape=(n_features,)
Scores of features.
pvalues_ : array-like, shape=(n_features,)
p-values of feature scores, None if score_func returned scores only.
See also:
f_classifANOVA F-value between label/feature for classiﬁcation tasks.
29.12. sklearn.feature_selection: Feature Selection
1413
scikit-learn user guide, Release 0.18.2
mutual_info_classifMutual information for a discrete target.
chi2Chi-squared stats of non-negative features for classiﬁcation tasks.
f_regressionF-value between label/feature for regression tasks.
mutual_info_regressionMutual information for a continuous target.
SelectPercentileSelect features based on percentile of the highest scores.
SelectKBestSelect features based on the k highest scores.
SelectFprSelect features based on a false positive rate test.
SelectFdrSelect features based on an estimated false discovery rate.
SelectFweSelect features based on family-wise error rate.
Methods
fit(X, y)
Run score function on (X, y) and get the appropriate
features.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(score_func=<function f_classif>, mode=’percentile’, param=1e-05)
fit(X, y)
Run score function on (X, y) and get the appropriate features.
ParametersX : array-like, shape = [n_samples, n_features]
The training input samples.
y : array-like, shape = [n_samples]
The target values (class labels in classiﬁcation, real numbers in regression).
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
1414
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Get a mask, or integer index, of the features selected
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
29.12.2 sklearn.feature_selection.SelectPercentile
class sklearn.feature_selection.SelectPercentile(score_func=<function f_classif>,
per-
centile=10)
Select features according to a percentile of the highest scores.
Read more in the User Guide.
29.12. sklearn.feature_selection: Feature Selection
1415
scikit-learn user guide, Release 0.18.2
Parametersscore_func : callable
Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues)
or a single array with scores. Default is f_classif (see below “See also”). The default
function only works with classiﬁcation tasks.
percentile : int, optional, default=10
Percent of features to keep.
Attributesscores_ : array-like, shape=(n_features,)
Scores of features.
pvalues_ : array-like, shape=(n_features,)
p-values of feature scores, None if score_func returned only scores.
See also:
f_classifANOVA F-value between label/feature for classiﬁcation tasks.
mutual_info_classifMutual information for a discrete target.
chi2Chi-squared stats of non-negative features for classiﬁcation tasks.
f_regressionF-value between label/feature for regression tasks.
mutual_info_regressionMutual information for a continuous target.
SelectKBestSelect features based on the k highest scores.
SelectFprSelect features based on a false positive rate test.
SelectFdrSelect features based on an estimated false discovery rate.
SelectFweSelect features based on family-wise error rate.
GenericUnivariateSelectUnivariate feature selector with conﬁgurable mode.
Notes
Ties between features with equal scores will be broken in an unspeciﬁed way.
Methods
fit(X, y)
Run score function on (X, y) and get the appropriate
features.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(score_func=<function f_classif>, percentile=10)
fit(X, y)
Run score function on (X, y) and get the appropriate features.
1416
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ParametersX : array-like, shape = [n_samples, n_features]
The training input samples.
y : array-like, shape = [n_samples]
The target values (class labels in classiﬁcation, real numbers in regression).
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Get a mask, or integer index, of the features selected
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
29.12. sklearn.feature_selection: Feature Selection
1417
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.feature_selection.SelectPercentile
• Feature agglomeration vs. univariate selection
• Univariate Feature Selection
• SVM-Anova: SVM with univariate feature selection
29.12.3 sklearn.feature_selection.SelectKBest
class sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, k=10)
Select features according to the k highest scores.
Read more in the User Guide.
Parametersscore_func : callable
Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues)
or a single array with scores. Default is f_classif (see below “See also”). The default
function only works with classiﬁcation tasks.
k : int or “all”, optional, default=10
Number of top features to select. The “all” option bypasses selection, for use in a
parameter search.
Attributesscores_ : array-like, shape=(n_features,)
Scores of features.
pvalues_ : array-like, shape=(n_features,)
p-values of feature scores, None if score_func returned only scores.
See also:
f_classifANOVA F-value between label/feature for classiﬁcation tasks.
mutual_info_classifMutual information for a discrete target.
chi2Chi-squared stats of non-negative features for classiﬁcation tasks.
f_regressionF-value between label/feature for regression tasks.
1418
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
mutual_info_regressionMutual information for a continious target.
SelectPercentileSelect features based on percentile of the highest scores.
SelectFprSelect features based on a false positive rate test.
SelectFdrSelect features based on an estimated false discovery rate.
SelectFweSelect features based on family-wise error rate.
GenericUnivariateSelectUnivariate feature selector with conﬁgurable mode.
Notes
Ties between features with equal scores will be broken in an unspeciﬁed way.
Methods
fit(X, y)
Run score function on (X, y) and get the appropriate
features.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(score_func=<function f_classif>, k=10)
fit(X, y)
Run score function on (X, y) and get the appropriate features.
ParametersX : array-like, shape = [n_samples, n_features]
The training input samples.
y : array-like, shape = [n_samples]
The target values (class labels in classiﬁcation, real numbers in regression).
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
29.12. sklearn.feature_selection: Feature Selection
1419
scikit-learn user guide, Release 0.18.2
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Get a mask, or integer index, of the features selected
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.feature_selection.SelectKBest
• Concatenating multiple feature extraction methods
• Selecting dimensionality reduction with Pipeline and GridSearchCV
• Pipeline Anova SVM
1420
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Classiﬁcation of text documents using sparse features
29.12.4 sklearn.feature_selection.SelectFpr
class sklearn.feature_selection.SelectFpr(score_func=<function f_classif>, alpha=0.05)
Filter: Select the pvalues below alpha based on a FPR test.
FPR test stands for False Positive Rate test. It controls the total amount of false detections.
Read more in the User Guide.
Parametersscore_func : callable
Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues).
Default is f_classif (see below “See also”). The default function only works with clas-
siﬁcation tasks.
alpha : ﬂoat, optional
The highest p-value for features to be kept.
Attributesscores_ : array-like, shape=(n_features,)
Scores of features.
pvalues_ : array-like, shape=(n_features,)
p-values of feature scores.
See also:
f_classifANOVA F-value between label/feature for classiﬁcation tasks.
chi2Chi-squared stats of non-negative features for classiﬁcation tasks.
mutual_info_classif
f_regressionF-value between label/feature for regression tasks.
mutual_info_regressionMutual information between features and the target.
SelectPercentileSelect features based on percentile of the highest scores.
SelectKBestSelect features based on the k highest scores.
SelectFdrSelect features based on an estimated false discovery rate.
SelectFweSelect features based on family-wise error rate.
GenericUnivariateSelectUnivariate feature selector with conﬁgurable mode.
Methods
fit(X, y)
Run score function on (X, y) and get the appropriate
features.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
Continued on next page
29.12. sklearn.feature_selection: Feature Selection
1421
scikit-learn user guide, Release 0.18.2
Table 29.98 – continued from previous page
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(score_func=<function f_classif>, alpha=0.05)
fit(X, y)
Run score function on (X, y) and get the appropriate features.
ParametersX : array-like, shape = [n_samples, n_features]
The training input samples.
y : array-like, shape = [n_samples]
The target values (class labels in classiﬁcation, real numbers in regression).
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Get a mask, or integer index, of the features selected
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
1422
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
29.12.5 sklearn.feature_selection.SelectFdr
class sklearn.feature_selection.SelectFdr(score_func=<function f_classif>, alpha=0.05)
Filter: Select the p-values for an estimated false discovery rate
This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.
Read more in the User Guide.
Parametersscore_func : callable
Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues).
Default is f_classif (see below “See also”). The default function only works with clas-
siﬁcation tasks.
alpha : ﬂoat, optional
The highest uncorrected p-value for features to keep.
Attributesscores_ : array-like, shape=(n_features,)
Scores of features.
pvalues_ : array-like, shape=(n_features,)
p-values of feature scores.
See also:
f_classifANOVA F-value between label/feature for classiﬁcation tasks.
mutual_info_classifMutual information for a discrete target.
chi2Chi-squared stats of non-negative features for classiﬁcation tasks.
f_regressionF-value between label/feature for regression tasks.
29.12. sklearn.feature_selection: Feature Selection
1423
scikit-learn user guide, Release 0.18.2
mutual_info_regressionMutual information for a contnuous target.
SelectPercentileSelect features based on percentile of the highest scores.
SelectKBestSelect features based on the k highest scores.
SelectFprSelect features based on a false positive rate test.
SelectFweSelect features based on family-wise error rate.
GenericUnivariateSelectUnivariate feature selector with conﬁgurable mode.
References
https://en.wikipedia.org/wiki/False_discovery_rate
Methods
fit(X, y)
Run score function on (X, y) and get the appropriate
features.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(score_func=<function f_classif>, alpha=0.05)
fit(X, y)
Run score function on (X, y) and get the appropriate features.
ParametersX : array-like, shape = [n_samples, n_features]
The training input samples.
y : array-like, shape = [n_samples]
The target values (class labels in classiﬁcation, real numbers in regression).
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
1424
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Get a mask, or integer index, of the features selected
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
29.12.6 sklearn.feature_selection.SelectFromModel
class sklearn.feature_selection.SelectFromModel(estimator, threshold=None, preﬁt=False)
Meta-transformer for selecting features based on importance weights.
New in version 0.17.
29.12. sklearn.feature_selection: Feature Selection
1425
scikit-learn user guide, Release 0.18.2
Parametersestimator : object
The base estimator from which the transformer is built. This can be both a ﬁtted (if
prefit is set to True) or a non-ﬁtted estimator.
threshold : string, ﬂoat, optional default None
The threshold value to use for feature selection. Features whose importance is greater
or equal are kept while the others are discarded. If “median” (resp. “mean”), then
the threshold value is the median (resp. the mean) of the feature importances. A
scaling factor (e.g., “1.25*mean”) may also be used. If None and if the estimator has
a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold
used is 1e-5. Otherwise, “mean” is used by default.
preﬁt : bool, default False
Whether a preﬁt model is expected to be passed into the constructor directly or not. If
True, transform must be called directly and SelectFromModel cannot be used with
cross_val_score, GridSearchCV and similar utilities that clone the estimator.
Otherwise train the model using fit and then transform to do feature selection.
Attributes‘estimator_‘: an estimator :
The base estimator from which the transformer is built. This is stored only when a
non-ﬁtted estimator is passed to the SelectFromModel, i.e when preﬁt is False.
‘threshold_‘: ﬂoat :
The threshold value used for feature selection.
Methods
fit(X[, y])
Fit the SelectFromModel meta-transformer.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
partial_fit(X[, y])
Fit the SelectFromModel meta-transformer only once.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(estimator, threshold=None, preﬁt=False)
fit(X, y=None, **ﬁt_params)
Fit the SelectFromModel meta-transformer.
ParametersX : array-like of shape (n_samples, n_features)
The training input samples.
y : array-like, shape (n_samples,)
The target values (integers that correspond to classes in classiﬁcation, real numbers in
regression).
**ﬁt_params : Other estimator speciﬁc parameters
Returnsself : object
Returns self.
1426
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Get a mask, or integer index, of the features selected
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
partial_fit(X, y=None, **ﬁt_params)
Fit the SelectFromModel meta-transformer only once.
ParametersX : array-like of shape (n_samples, n_features)
The training input samples.
y : array-like, shape (n_samples,)
The target values (integers that correspond to classes in classiﬁcation, real numbers in
regression).
**ﬁt_params : Other estimator speciﬁc parameters
29.12. sklearn.feature_selection: Feature Selection
1427
scikit-learn user guide, Release 0.18.2
Returnsself : object
Returns self.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.feature_selection.SelectFromModel
• Feature selection using SelectFromModel and LassoCV
29.12.7 sklearn.feature_selection.SelectFwe
class sklearn.feature_selection.SelectFwe(score_func=<function f_classif>, alpha=0.05)
Filter: Select the p-values corresponding to Family-wise error rate
Read more in the User Guide.
Parametersscore_func : callable
Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues).
Default is f_classif (see below “See also”). The default function only works with clas-
siﬁcation tasks.
alpha : ﬂoat, optional
The highest uncorrected p-value for features to keep.
Attributesscores_ : array-like, shape=(n_features,)
Scores of features.
pvalues_ : array-like, shape=(n_features,)
p-values of feature scores.
See also:
f_classifANOVA F-value between label/feature for classiﬁcation tasks.
chi2Chi-squared stats of non-negative features for classiﬁcation tasks.
f_regressionF-value between label/feature for regression tasks.
SelectPercentileSelect features based on percentile of the highest scores.
SelectKBestSelect features based on the k highest scores.
1428
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
SelectFprSelect features based on a false positive rate test.
SelectFdrSelect features based on an estimated false discovery rate.
GenericUnivariateSelectUnivariate feature selector with conﬁgurable mode.
Methods
fit(X, y)
Run score function on (X, y) and get the appropriate
features.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(score_func=<function f_classif>, alpha=0.05)
fit(X, y)
Run score function on (X, y) and get the appropriate features.
ParametersX : array-like, shape = [n_samples, n_features]
The training input samples.
y : array-like, shape = [n_samples]
The target values (class labels in classiﬁcation, real numbers in regression).
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
29.12. sklearn.feature_selection: Feature Selection
1429
scikit-learn user guide, Release 0.18.2
get_support(indices=False)
Get a mask, or integer index, of the features selected
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
29.12.8 sklearn.feature_selection.RFE
class sklearn.feature_selection.RFE(estimator, n_features_to_select=None, step=1, verbose=0)
Feature ranking with recursive feature elimination.
Given an external estimator that assigns weights to features (e.g., the coefﬁcients of a linear model), the goal of
recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of
features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them.
Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure
is recursively repeated on the pruned set until the desired number of features to select is eventually reached.
Read more in the User Guide.
Parametersestimator : object
A supervised learning estimator with a ﬁt method that updates a coef_ attribute that
holds the ﬁtted parameters. Important features must correspond to high absolute values
in the coef_ array.
1430
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
For instance, this is the case for most supervised learning algorithms such as Support
Vector Classiﬁers and Generalized Linear Models from the svm and linear_model mod-
ules.
n_features_to_select : int or None (default=None)
The number of features to select. If None, half of the features are selected.
step : int or ﬂoat, optional (default=1)
If greater than or equal to 1, then step corresponds to the (integer) number of features
to remove at each iteration. If within (0.0, 1.0), then step corresponds to the percentage
(rounded down) of features to remove at each iteration.
verbose : int, default=0
Controls verbosity of output.
Attributesn_features_ : int
The number of selected features.
support_ : array of shape [n_features]
The mask of selected features.
ranking_ : array of shape [n_features]
The feature ranking, such that ranking_[i] corresponds to the ranking position of
the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.
estimator_ : object
The external estimator ﬁt on the reduced dataset.
References
[R27]
Examples
The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.feature_selection import RFE
>>> from sklearn.svm import SVR
>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)
>>> estimator = SVR(kernel="linear")
>>> selector = RFE(estimator, 5, step=1)
>>> selector = selector.fit(X, y)
>>> selector.support_
array([ True,
True,
True,
True,
True,
False, False, False, False, False], dtype=bool)
>>> selector.ranking_
array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])
Methods
29.12. sklearn.feature_selection: Feature Selection
1431
scikit-learn user guide, Release 0.18.2
decision_function(\*args, \*\*kwargs)
fit(X, y)
Fit the RFE model and then the underlying estimator on
the selected features.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
predict(\*args, \*\*kwargs)
Reduce X to the selected features and then predict using
the underlying estimator.
predict_log_proba(\*args, \*\*kwargs)
predict_proba(\*args, \*\*kwargs)
score(\*args, \*\*kwargs)
Reduce X to the selected features and then return the
score of the underlying estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(estimator, n_features_to_select=None, step=1, verbose=0)
fit(X, y)
Fit the RFE model and then the underlying estimator on the selectedfeatures.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
The training input samples.
y : array-like, shape = [n_samples]
The target values.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Get a mask, or integer index, of the features selected
1432
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
predict(*args, **kwargs)
Reduce X to the selected features and then predict using theunderlying estimator.
ParametersX : array of shape [n_samples, n_features]
The input samples.
Returnsy : array of shape [n_samples]
The predicted target values.
score(*args, **kwargs)
Reduce X to the selected features and then return the score of theunderlying estimator.
ParametersX : array of shape [n_samples, n_features]
The input samples.
y : array of shape [n_samples]
The target values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
29.12. sklearn.feature_selection: Feature Selection
1433
scikit-learn user guide, Release 0.18.2
Examples using sklearn.feature_selection.RFE
• Recursive feature elimination
29.12.9 sklearn.feature_selection.RFECV
class sklearn.feature_selection.RFECV(estimator, step=1, cv=None, scoring=None, verbose=0,
n_jobs=1)
Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.
Read more in the User Guide.
Parametersestimator : object
A supervised learning estimator with a ﬁt method that updates a coef_ attribute that
holds the ﬁtted parameters. Important features must correspond to high absolute values
in the coef_ array.
For instance, this is the case for most supervised learning algorithms such as Support
Vector Classiﬁers and Generalized Linear Models from the svm and linear_model mod-
ules.
step : int or ﬂoat, optional (default=1)
If greater than or equal to 1, then step corresponds to the (integer) number of features
to remove at each iteration. If within (0.0, 1.0), then step corresponds to the percentage
(rounded down) of features to remove at each iteration.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For
integer/None
inputs,
if
y
is
binary
or
multiclass,
sklearn.model_selection.StratifiedKFold
is
used.
If
the
estimator
is
a
classiﬁer
or
if
y
is
neither
binary
nor
multiclass,
sklearn.model_selection.KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
scoring : string, callable or None, optional, default: None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
verbose : int, default=0
Controls verbosity of output.
n_jobs : int, default 1
Number of cores to run in parallel while ﬁtting across folds. Defaults to 1 core. If
n_jobs=-1, then number of jobs is set to number of cores.
Attributesn_features_ : int
The number of selected features with cross-validation.
1434
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
support_ : array of shape [n_features]
The mask of selected features.
ranking_ : array of shape [n_features]
The feature ranking, such that ranking_[i] corresponds to the ranking position of the
i-th feature. Selected (i.e., estimated best) features are assigned rank 1.
grid_scores_ : array of shape [n_subsets_of_features]
The cross-validation scores such that grid_scores_[i] corresponds to the CV
score of the i-th subset of features.
estimator_ : object
The external estimator ﬁt on the reduced dataset.
Notes
The size of grid_scores_ is equal to ceil((n_features - 1) / step) + 1, where step is the number of features
removed at each iteration.
References
[R28]
Examples
The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1
dataset.
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.feature_selection import RFECV
>>> from sklearn.svm import SVR
>>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)
>>> estimator = SVR(kernel="linear")
>>> selector = RFECV(estimator, step=1, cv=5)
>>> selector = selector.fit(X, y)
>>> selector.support_
array([ True,
True,
True,
True,
True,
False, False, False, False, False], dtype=bool)
>>> selector.ranking_
array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])
Methods
decision_function(\*args, \*\*kwargs)
fit(X, y)
Fit the RFE model and automatically tune the number
of selected features.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
Continued on next page
29.12. sklearn.feature_selection: Feature Selection
1435
scikit-learn user guide, Release 0.18.2
Table 29.103 – continued from previous page
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
predict(\*args, \*\*kwargs)
Reduce X to the selected features and then predict using
the underlying estimator.
predict_log_proba(\*args, \*\*kwargs)
predict_proba(\*args, \*\*kwargs)
score(\*args, \*\*kwargs)
Reduce X to the selected features and then return the
score of the underlying estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(estimator, step=1, cv=None, scoring=None, verbose=0, n_jobs=1)
fit(X, y)
Fit the RFE model and automatically tune the number of selectedfeatures.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vector, where n_samples is the number of samples and n_features is the total
number of features.
y : array-like, shape = [n_samples]
Target values (integers for classiﬁcation, real numbers for regression).
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Get a mask, or integer index, of the features selected
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
1436
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
predict(*args, **kwargs)
Reduce X to the selected features and then predict using theunderlying estimator.
ParametersX : array of shape [n_samples, n_features]
The input samples.
Returnsy : array of shape [n_samples]
The predicted target values.
score(*args, **kwargs)
Reduce X to the selected features and then return the score of theunderlying estimator.
ParametersX : array of shape [n_samples, n_features]
The input samples.
y : array of shape [n_samples]
The target values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.feature_selection.RFECV
• Recursive feature elimination with cross-validation
29.12. sklearn.feature_selection: Feature Selection
1437
scikit-learn user guide, Release 0.18.2
29.12.10 sklearn.feature_selection.VarianceThreshold
class sklearn.feature_selection.VarianceThreshold(threshold=0.0)
Feature selector that removes all low-variance features.
This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used
for unsupervised learning.
Read more in the User Guide.
Parametersthreshold : ﬂoat, optional
Features with a training-set variance lower than this threshold will be removed. The
default is to keep all features with non-zero variance, i.e. remove the features that have
the same value in all samples.
Attributesvariances_ : array, shape (n_features,)
Variances of individual features.
Examples
The following dataset has integer features, two of which are the same in every sample. These are removed with
the default setting for threshold:
>>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]
>>> selector = VarianceThreshold()
>>> selector.fit_transform(X)
array([[2, 0],
[1, 4],
[1, 1]])
Methods
fit(X[, y])
Learn empirical variances from X.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Get a mask, or integer index, of the features selected
inverse_transform(X)
Reverse the transformation operation
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Reduce X to the selected features.
__init__(threshold=0.0)
fit(X, y=None)
Learn empirical variances from X.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Sample vectors from which to compute variances.
y : any
Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline.
Returnsself :
1438
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Get a mask, or integer index, of the features selected
Parametersindices : boolean (default False)
If True, the return value will be an array of integers, rather than a boolean mask.
Returnssupport : array
An index that selects the retained features from a feature vector. If indices is False,
this is a boolean array of shape [# input features], in which an element is True iff its
corresponding feature is selected for retention. If indices is True, this is an integer array
of shape [# output features] whose values are indices into the input feature vector.
inverse_transform(X)
Reverse the transformation operation
ParametersX : array of shape [n_samples, n_selected_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_original_features]
X with columns of zeros inserted where features would have been removed by trans-
form.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Reduce X to the selected features.
29.12. sklearn.feature_selection: Feature Selection
1439
scikit-learn user guide, Release 0.18.2
ParametersX : array of shape [n_samples, n_features]
The input samples.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
feature_selection.chi2(X, y)
Compute chi-squared stats between each non-negative fea-
ture and class.
feature_selection.f_classif(X, y)
Compute the ANOVA F-value for the provided sample.
feature_selection.f_regression(X, y[, cen-
ter])
Univariate linear regression tests.
feature_selection.mutual_info_classif(X,
y)
Estimate mutual information for a discrete target variable.
feature_selection.mutual_info_regression(X,
y)
Estimate mutual information for a continuous target vari-
able.
29.12.11 sklearn.feature_selection.chi2
sklearn.feature_selection.chi2(X, y)
Compute chi-squared stats between each non-negative feature and class.
This score can be used to select the n_features features with the highest values for the test chi-squared statistic
from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in
document classiﬁcation), relative to the classes.
Recall that the chi-square test measures dependence between stochastic variables, so using this function “weeds
out” the features that are the most likely to be independent of class and therefore irrelevant for classiﬁcation.
Read more in the User Guide.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features_in)
Sample vectors.
y : array-like, shape = (n_samples,)
Target vector (class labels).
Returnschi2 : array, shape = (n_features,)
chi2 statistics of each feature.
pval : array, shape = (n_features,)
p-values of each feature.
See also:
f_classifANOVA F-value between label/feature for classiﬁcation tasks.
f_regressionF-value between label/feature for regression tasks.
Notes
Complexity of this algorithm is O(n_classes * n_features).
1440
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.feature_selection.chi2
• Selecting dimensionality reduction with Pipeline and GridSearchCV
• Classiﬁcation of text documents using sparse features
29.12.12 sklearn.feature_selection.f_classif
sklearn.feature_selection.f_classif(X, y)
Compute the ANOVA F-value for the provided sample.
Read more in the User Guide.
ParametersX : {array-like, sparse matrix} shape = [n_samples, n_features]
The set of regressors that will be tested sequentially.
y : array of shape(n_samples)
The data matrix.
ReturnsF : array, shape = [n_features,]
The set of F values.
pval : array, shape = [n_features,]
The set of p-values.
See also:
chi2Chi-squared stats of non-negative features for classiﬁcation tasks.
f_regressionF-value between label/feature for regression tasks.
Examples using sklearn.feature_selection.f_classif
• Univariate Feature Selection
• SVM-Anova: SVM with univariate feature selection
29.12.13 sklearn.feature_selection.f_regression
sklearn.feature_selection.f_regression(X, y, center=True)
Univariate linear regression tests.
Quick linear model for testing the effect of a single regressor, sequentially for many regressors.
This is done in 2 steps:
1.The cross correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) *
(y - mean_y)) / (std(X[:, i]) * std(y)).
2.It is converted to an F score then to a p-value.
Read more in the User Guide.
ParametersX : {array-like, sparse matrix} shape = (n_samples, n_features)
The set of regressors that will be tested sequentially.
y : array of shape(n_samples).
29.12. sklearn.feature_selection: Feature Selection
1441
scikit-learn user guide, Release 0.18.2
The data matrix
center : True, bool,
If true, X and y will be centered.
ReturnsF : array, shape=(n_features,)
F values of features.
pval : array, shape=(n_features,)
p-values of F-scores.
See also:
f_classifANOVA F-value between label/feature for classiﬁcation tasks.
chi2Chi-squared stats of non-negative features for classiﬁcation tasks.
Examples using sklearn.feature_selection.f_regression
• Feature agglomeration vs. univariate selection
• Pipeline Anova SVM
• Comparison of F-test and mutual information
• Sparse recovery: feature selection for sparse linear models
29.12.14 sklearn.feature_selection.mutual_info_classif
sklearn.feature_selection.mutual_info_classif(X,
y,
discrete_features=’auto’,
n_neighbors=3,
copy=True,
ran-
dom_state=None)
Estimate mutual information for a discrete target variable.
Mutual information (MI) [R169] between two random variables is a non-negative value, which measures the
dependency between the variables. It is equal to zero if and only if two random variables are independent, and
higher values mean higher dependency.
The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances
as described in [R170] and [R171]. Both methods are based on the idea originally proposed in [R172].
It can be used for univariate features selection, read more in the User Guide.
ParametersX : array_like or sparse matrix, shape (n_samples, n_features)
Feature matrix.
y : array_like, shape (n_samples,)
Target vector.
discrete_features : {‘auto’, bool, array_like}, default ‘auto’
If bool, then determines whether to consider all features discrete or continuous. If array,
then it should be either a boolean mask with shape (n_features,) or array with indices of
discrete features. If ‘auto’, it is assigned to False for dense X and to True for sparse X.
n_neighbors : int, default 3
Number of neighbors to use for MI estimation for continuous variables, see [R170] and
[R171]. Higher values reduce variance of the estimation, but could introduce a bias.
1442
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
copy : bool, default True
Whether to make a copy of the given data. If set to False, the initial data will be over-
written.
random_state : int seed, RandomState instance or None, default None
The seed of the pseudo random number generator for adding small noise to continuous
variables in order to remove repeated values.
Returnsmi : ndarray, shape (n_features,)
Estimated mutual information between each feature and the target.
Notes
1.The term “discrete features” is used instead of naming them “categorical”, because it describes the essence
more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical)
and you will get better results if mark them as such. Also note, that treating a continuous variable as
discrete and vice versa will usually give incorrect results, so be attentive about that.
2.True mutual information can’t be negative. If its estimate turns out to be negative, it is replaced by zero.
References
[R169], [R170], [R171], [R172]
29.12.15 sklearn.feature_selection.mutual_info_regression
sklearn.feature_selection.mutual_info_regression(X,
y,
discrete_features=’auto’,
n_neighbors=3,
copy=True,
ran-
dom_state=None)
Estimate mutual information for a continuous target variable.
Mutual information (MI) [R29] between two random variables is a non-negative value, which measures the
dependency between the variables. It is equal to zero if and only if two random variables are independent, and
higher values mean higher dependency.
The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances
as described in [R30] and [R31]. Both methods are based on the idea originally proposed in [R32].
It can be used for univariate features selection, read more in the User Guide.
ParametersX : array_like or sparse matrix, shape (n_samples, n_features)
Feature matrix.
y : array_like, shape (n_samples,)
Target vector.
discrete_features : {‘auto’, bool, array_like}, default ‘auto’
If bool, then determines whether to consider all features discrete or continuous. If array,
then it should be either a boolean mask with shape (n_features,) or array with indices of
discrete features. If ‘auto’, it is assigned to False for dense X and to True for sparse X.
n_neighbors : int, default 3
29.12. sklearn.feature_selection: Feature Selection
1443
scikit-learn user guide, Release 0.18.2
Number of neighbors to use for MI estimation for continuous variables, see [R30] and
[R31]. Higher values reduce variance of the estimation, but could introduce a bias.
copy : bool, default True
Whether to make a copy of the given data. If set to False, the initial data will be over-
written.
random_state : int seed, RandomState instance or None, default None
The seed of the pseudo random number generator for adding small noise to continuous
variables in order to remove repeated values.
Returnsmi : ndarray, shape (n_features,)
Estimated mutual information between each feature and the target.
Notes
1.The term “discrete features” is used instead of naming them “categorical”, because it describes the essence
more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical)
and you will get better results if mark them as such. Also note, that treating a continuous variable as
discrete and vice versa will usually give incorrect results, so be attentive about that.
2.True mutual information can’t be negative. If its estimate turns out to be negative, it is replaced by zero.
References
[R29], [R30], [R31], [R32]
Examples using sklearn.feature_selection.mutual_info_regression
• Comparison of F-test and mutual information
29.13 sklearn.gaussian_process: Gaussian Processes
The sklearn.gaussian_process module implements Gaussian Process based regression and classiﬁcation.
User guide: See the Gaussian Processes section for further details.
gaussian_process.GaussianProcessRegressor([...])
Gaussian process regression (GPR).
gaussian_process.GaussianProcessClassifier([...])
Gaussian process classiﬁcation (GPC) based on Laplace
approximation.
1444
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.13.1 sklearn.gaussian_process.GaussianProcessRegressor
class sklearn.gaussian_process.GaussianProcessRegressor(kernel=None,
alpha=1e-10,
optimizer=’fmin_l_bfgs_b’,
n_restarts_optimizer=0,
normalize_y=False,
copy_X_train=True,
ran-
dom_state=None)
Gaussian process regression (GPR).
The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Ras-
mussen and Williams.
In addition to standard scikit-learn estimator API, GaussianProcessRegressor:
•allows prediction without prior ﬁtting (based on the GP prior)
•provides an additional method sample_y(X), which evaluates samples drawn from the GPR (prior or pos-
terior) at given inputs
•exposes a method log_marginal_likelihood(theta), which can be used externally for other ways of selecting
hyperparameters, e.g., via Markov chain Monte Carlo.
Read more in the User Guide.
New in version 0.18.
Parameterskernel : kernel object
The kernel specifying the covariance function of the GP. If None is passed, the kernel
“1.0 * RBF(1.0)” is used as default. Note that the kernel’s hyperparameters are opti-
mized during ﬁtting.
alpha : ﬂoat or array-like, optional (default: 1e-10)
Value added to the diagonal of the kernel matrix during ﬁtting. Larger values correspond
to increased noise level in the observations and reduce potential numerical issue during
ﬁtting. If an array is passed, it must have the same number of entries as the data used
for ﬁtting and is used as datapoint-dependent noise level. Note that this is equivalent to
adding a WhiteKernel with c=alpha. Allowing to specify the noise level directly as a
parameter is mainly for convenience and for consistency with Ridge.
optimizer : string or callable, optional (default: “fmin_l_bfgs_b”)
Can either be one of the internally supported optimizers for optimizing the kernel’s
parameters, speciﬁed by a string, or an externally deﬁned optimizer passed as a callable.
If a callable is passed, it must have the signature:
def optimizer(obj_func, initial_theta, bounds):
# * 'obj_func' is the objective function to be maximized,
˓→which
#
takes the hyperparameters theta as parameter and an
#
optional flag eval_gradient, which determines if the
#
gradient is returned additionally to the function value
# * 'initial_theta': the initial value for theta, which can
˓→be
#
used by local optimizers
# * 'bounds': the bounds on the values of theta
....
# Returned are the best found hyperparameters theta and
# the corresponding value of the target function.
return theta_opt, func_min
29.13. sklearn.gaussian_process: Gaussian Processes
1445
scikit-learn user guide, Release 0.18.2
Per default, the ‘fmin_l_bfgs_b’ algorithm from scipy.optimize is used. If None is
passed, the kernel’s parameters are kept ﬁxed. Available internal optimizers are:
'fmin_l_bfgs_b'
n_restarts_optimizer : int, optional (default: 0)
The number of restarts of the optimizer for ﬁnding the kernel’s parameters which max-
imize the log-marginal likelihood. The ﬁrst run of the optimizer is performed from the
kernel’s initial parameters, the remaining ones (if any) from thetas sampled log-uniform
randomly from the space of allowed theta-values. If greater than 0, all bounds must be
ﬁnite. Note that n_restarts_optimizer == 0 implies that one run is performed.
normalize_y : boolean, optional (default: False)
Whether the target values y are normalized, i.e., the mean of the observed target val-
ues become zero. This parameter should be set to True if the target values’ mean is
expected to differ considerable from zero. When enabled, the normalization effectively
modiﬁes the GP’s prior based on the data, which contradicts the likelihood principle;
normalization is thus disabled per default.
copy_X_train : bool, optional (default: True)
If True, a persistent copy of the training data is stored in the object. Otherwise, just a
reference to the training data is stored, which might cause predictions to change if the
data is modiﬁed externally.
random_state : integer or numpy.RandomState, optional
The generator used to initialize the centers. If an integer is given, it ﬁxes the seed.
Defaults to the global numpy random number generator.
AttributesX_train_ : array-like, shape = (n_samples, n_features)
Feature values in training data (also required for prediction)
y_train_ : array-like, shape = (n_samples, [n_output_dims])
Target values in training data (also required for prediction)
kernel_ : kernel object
The kernel used for prediction. The structure of the kernel is the same as the one passed
as parameter but with optimized hyperparameters
L_ : array-like, shape = (n_samples, n_samples)
Lower-triangular Cholesky decomposition of the kernel in X_train_
alpha_ : array-like, shape = (n_samples,)
Dual coefﬁcients of training data points in kernel space
log_marginal_likelihood_value_ : ﬂoat
The log-marginal-likelihood of self.kernel_.theta
Methods
fit(X, y)
Fit Gaussian process regression model
Continued on next page
1446
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.107 – continued from previous page
get_params([deep])
Get parameters for this estimator.
log_marginal_likelihood([theta,
eval_gradient])
Returns log-marginal likelihood of theta for training
data.
predict(X[, return_std, return_cov])
Predict using the Gaussian process regression model
sample_y(X[, n_samples, random_state])
Draw samples from Gaussian process and evaluate at X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(kernel=None, alpha=1e-10, optimizer=’fmin_l_bfgs_b’, n_restarts_optimizer=0, normal-
ize_y=False, copy_X_train=True, random_state=None)
fit(X, y)
Fit Gaussian process regression model
ParametersX : array-like, shape = (n_samples, n_features)
Training data
y : array-like, shape = (n_samples, [n_output_dims])
Target values
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
log_marginal_likelihood(theta=None, eval_gradient=False)
Returns log-marginal likelihood of theta for training data.
Parameterstheta : array-like, shape = (n_kernel_params,) or None
Kernel hyperparameters for which the log-marginal likelihood is evaluated. If None, the
precomputed log_marginal_likelihood of self.kernel_.theta is returned.
eval_gradient : bool, default: False
If True, the gradient of the log-marginal likelihood with respect to the kernel hyperpa-
rameters at position theta is returned additionally. If True, theta must not be None.
Returnslog_likelihood : ﬂoat
Log-marginal likelihood of theta for training data.
log_likelihood_gradient : array, shape = (n_kernel_params,), optional
Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at
position theta. Only returned when eval_gradient is True.
predict(X, return_std=False, return_cov=False)
Predict using the Gaussian process regression model
29.13. sklearn.gaussian_process: Gaussian Processes
1447
scikit-learn user guide, Release 0.18.2
We can also predict based on an unﬁtted model by using the GP prior. In addition to the mean of the pre-
dictive distribution, also its standard deviation (return_std=True) or covariance (return_cov=True). Note
that at most one of the two can be requested.
ParametersX : array-like, shape = (n_samples, n_features)
Query points where the GP is evaluated
return_std : bool, default: False
If True, the standard-deviation of the predictive distribution at the query points is re-
turned along with the mean.
return_cov : bool, default: False
If True, the covariance of the joint predictive distribution at the query points is returned
along with the mean
Returnsy_mean : array, shape = (n_samples, [n_output_dims])
Mean of predictive distribution a query points
y_std : array, shape = (n_samples,), optional
Standard deviation of predictive distribution at query points. Only returned when re-
turn_std is True.
y_cov : array, shape = (n_samples, n_samples), optional
Covariance of joint predictive distribution a query points.
Only returned when re-
turn_cov is True.
sample_y(X, n_samples=1, random_state=0)
Draw samples from Gaussian process and evaluate at X.
ParametersX : array-like, shape = (n_samples_X, n_features)
Query points where the GP samples are evaluated
n_samples : int, default: 1
The number of samples drawn from the Gaussian process
random_state: RandomState or an int seed (0 by default) :
A random number generator instance
Returnsy_samples : array, shape = (n_samples_X, [n_output_dims], n_samples)
Values of n_samples samples drawn from Gaussian process and evaluated at query
points.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
1448
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.gaussian_process.GaussianProcessRegressor
• Comparison of kernel ridge and Gaussian process regression
• Gaussian process regression (GPR) on Mauna Loa CO2 data.
• Gaussian process regression (GPR) with noise-level estimation
• Gaussian Processes regression: basic introductory example
• Illustration of prior and posterior Gaussian process for different kernels
29.13.2 sklearn.gaussian_process.GaussianProcessClassiﬁer
class sklearn.gaussian_process.GaussianProcessClassifier(kernel=None,
opti-
mizer=’fmin_l_bfgs_b’,
n_restarts_optimizer=0,
max_iter_predict=100,
warm_start=False,
copy_X_train=True,
random_state=None,
multi_class=’one_vs_rest’,
n_jobs=1)
Gaussian process classiﬁcation (GPC) based on Laplace approximation.
The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning
(GPML) by Rasmussen and Williams.
Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.
Currently, the implementation is restricted to using the logistic link function. For multi-class classiﬁcation,
several binary one-versus rest classiﬁers are ﬁtted. Note that this class thus does not implement a true multi-
class Laplace approximation.
Parameterskernel : kernel object
The kernel specifying the covariance function of the GP. If None is passed, the kernel
“1.0 * RBF(1.0)” is used as default. Note that the kernel’s hyperparameters are opti-
mized during ﬁtting.
optimizer : string or callable, optional (default: “fmin_l_bfgs_b”)
29.13. sklearn.gaussian_process: Gaussian Processes
1449
scikit-learn user guide, Release 0.18.2
Can either be one of the internally supported optimizers for optimizing the kernel’s
parameters, speciﬁed by a string, or an externally deﬁned optimizer passed as a callable.
If a callable is passed, it must have the signature:
def optimizer(obj_func, initial_theta, bounds):
# * 'obj_func' is the objective function to be maximized,
˓→which
#
takes the hyperparameters theta as parameter and an
#
optional flag eval_gradient, which determines if the
#
gradient is returned additionally to the function value
# * 'initial_theta': the initial value for theta, which can
˓→be
#
used by local optimizers
# * 'bounds': the bounds on the values of theta
....
# Returned are the best found hyperparameters theta and
# the corresponding value of the target function.
return theta_opt, func_min
Per default, the ‘fmin_l_bfgs_b’ algorithm from scipy.optimize is used. If None is
passed, the kernel’s parameters are kept ﬁxed. Available internal optimizers are:
'fmin_l_bfgs_b'
n_restarts_optimizer : int, optional (default: 0)
The number of restarts of the optimizer for ﬁnding the kernel’s parameters which max-
imize the log-marginal likelihood. The ﬁrst run of the optimizer is performed from the
kernel’s initial parameters, the remaining ones (if any) from thetas sampled log-uniform
randomly from the space of allowed theta-values. If greater than 0, all bounds must be
ﬁnite. Note that n_restarts_optimizer=0 implies that one run is performed.
max_iter_predict : int, optional (default: 100)
The maximum number of iterations in Newton’s method for approximating the posterior
during predict. Smaller values will reduce computation time at the cost of worse results.
warm_start : bool, optional (default: False)
If warm-starts are enabled, the solution of the last Newton iteration on the Laplace
approximation of the posterior mode is used as initialization for the next call of _pos-
terior_mode(). This can speed up convergence when _posterior_mode is called several
times on similar problems as in hyperparameter optimization.
copy_X_train : bool, optional (default: True)
If True, a persistent copy of the training data is stored in the object. Otherwise, just a
reference to the training data is stored, which might cause predictions to change if the
data is modiﬁed externally.
random_state : integer or numpy.RandomState, optional
The generator used to initialize the centers. If an integer is given, it ﬁxes the seed.
Defaults to the global numpy random number generator.
multi_class: string, default : “one_vs_rest”
Speciﬁes how multi-class classiﬁcation problems are handled.
Supported are
“one_vs_rest” and “one_vs_one”. In “one_vs_rest”, one binary Gaussian process clas-
siﬁer is ﬁtted for each class, which is trained to separate this class from the rest. In
“one_vs_one”, one binary Gaussian process classiﬁer is ﬁtted for each pair of classes,
1450
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
which is trained to separate these two classes. The predictions of these binary predic-
tors are combined into multi-class predictions. Note that “one_vs_one” does not support
predicting probability estimates.
n_jobs : int, optional, default: 1
The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no
parallel computing code is used at all, which is useful for debugging. For n_jobs below
-1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
Attributeskernel_ : kernel object
The kernel used for prediction. In case of binary classiﬁcation, the structure of the
kernel is the same as the one passed as parameter but with optimized hyperparameters.
In case of multi-class classiﬁcation, a CompoundKernel is returned which consists of
the different kernels used in the one-versus-rest classiﬁers.
log_marginal_likelihood_value_ : ﬂoat
The log-marginal-likelihood of self.kernel_.theta
classes_ : array-like, shape = (n_classes,)
Unique class labels.
n_classes_ : int
The number of classes in the training data
.. versionadded:: 0.18 :
Methods
fit(X, y)
Fit Gaussian process classiﬁcation model
get_params([deep])
Get parameters for this estimator.
log_marginal_likelihood([theta,
eval_gradient])
Returns log-marginal likelihood of theta for training
data.
predict(X)
Perform classiﬁcation on an array of test vectors X.
predict_proba(X)
Return probability estimates for the test vector X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(kernel=None, optimizer=’fmin_l_bfgs_b’, n_restarts_optimizer=0, max_iter_predict=100,
warm_start=False, copy_X_train=True, random_state=None, multi_class=’one_vs_rest’,
n_jobs=1)
fit(X, y)
Fit Gaussian process classiﬁcation model
ParametersX : array-like, shape = (n_samples, n_features)
Training data
y : array-like, shape = (n_samples,)
Target values, must be binary
Returnsself : returns an instance of self.
29.13. sklearn.gaussian_process: Gaussian Processes
1451
scikit-learn user guide, Release 0.18.2
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
log_marginal_likelihood(theta=None, eval_gradient=False)
Returns log-marginal likelihood of theta for training data.
In the case of multi-class classiﬁcation, the mean log-marginal likelihood of the one-versus-rest classiﬁers
are returned.
Parameterstheta : array-like, shape = (n_kernel_params,) or none
Kernel hyperparameters for which the log-marginal likelihood is evaluated.
In the
case of multi-class classiﬁcation, theta may be the hyperparameters of the compound
kernel or of an individual kernel.
In the latter case, all individual kernel get as-
signed the same theta values. If None, the precomputed log_marginal_likelihood of
self.kernel_.theta is returned.
eval_gradient : bool, default: False
If True, the gradient of the log-marginal likelihood with respect to the kernel hyperpa-
rameters at position theta is returned additionally. Note that gradient computation is not
supported for non-binary classiﬁcation. If True, theta must not be None.
Returnslog_likelihood : ﬂoat
Log-marginal likelihood of theta for training data.
log_likelihood_gradient : array, shape = (n_kernel_params,), optional
Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at
position theta. Only returned when eval_gradient is True.
predict(X)
Perform classiﬁcation on an array of test vectors X.
ParametersX : array-like, shape = (n_samples, n_features)
ReturnsC : array, shape = (n_samples,)
Predicted target values for X, values are from classes_
predict_proba(X)
Return probability estimates for the test vector X.
ParametersX : array-like, shape = (n_samples, n_features)
ReturnsC : array-like, shape = (n_samples, n_classes)
Returns the probability of the samples for each class in the model. The columns corre-
spond to the classes in sorted order, as they appear in the attribute classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
1452
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.gaussian_process.GaussianProcessClassifier
• Plot classiﬁcation probability
• Classiﬁer comparison
• Probabilistic predictions with Gaussian process classiﬁcation (GPC)
• Gaussian process classiﬁcation (GPC) on iris dataset
• Iso-probability lines for Gaussian Processes classiﬁcation (GPC)
• Illustration of Gaussian process classiﬁcation (GPC) on the XOR dataset
Kernels:
gaussian_process.kernels.Kernel
Base class for all kernels.
gaussian_process.kernels.Sum(k1, k2)
Sum-kernel k1 + k2 of two kernels k1 and k2.
gaussian_process.kernels.Product(k1, k2)
Product-kernel k1 * k2 of two kernels k1 and k2.
gaussian_process.kernels.Exponentiation(...)Exponentiate kernel by given exponent.
gaussian_process.kernels.ConstantKernel([...])Constant kernel.
gaussian_process.kernels.WhiteKernel([...])
White kernel.
gaussian_process.kernels.RBF([length_scale,
...])
Radial-basis function kernel (aka squared-exponential ker-
nel).
gaussian_process.kernels.Matern([...])
Matern kernel.
gaussian_process.kernels.RationalQuadratic([...])
Rational Quadratic kernel.
gaussian_process.kernels.ExpSineSquared([...])
Exp-Sine-Squared kernel.
gaussian_process.kernels.DotProduct([...])
Dot-Product kernel.
gaussian_process.kernels.PairwiseKernel([...])Wrapper for kernels in sklearn.metrics.pairwise.
gaussian_process.kernels.CompoundKernel(kernels)
Kernel which is composed of a set of other kernels.
gaussian_process.kernels.Hyperparameter
A kernel hyperparameter’s speciﬁcation in form of a
namedtuple.
29.13. sklearn.gaussian_process: Gaussian Processes
1453
scikit-learn user guide, Release 0.18.2
29.13.3 sklearn.gaussian_process.kernels.Kernel
class sklearn.gaussian_process.kernels.Kernel
Base class for all kernels.
New in version 0.18.
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
1454
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
29.13.4 sklearn.gaussian_process.kernels.Sum
class sklearn.gaussian_process.kernels.Sum(k1, k2)
Sum-kernel k1 + k2 of two kernels k1 and k2.
The resulting kernel is deﬁned as k_sum(X, Y) = k1(X, Y) + k2(X, Y)
New in version 0.18.
Parametersk1 : Kernel object
The ﬁrst base-kernel of the sum-kernel
k2 : Kernel object
The second base-kernel of the sum-kernel
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(k1, k2)
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
29.13. sklearn.gaussian_process: Gaussian Processes
1455
scikit-learn user guide, Release 0.18.2
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
29.13.5 sklearn.gaussian_process.kernels.Product
class sklearn.gaussian_process.kernels.Product(k1, k2)
Product-kernel k1 * k2 of two kernels k1 and k2.
The resulting kernel is deﬁned as k_prod(X, Y) = k1(X, Y) * k2(X, Y)
New in version 0.18.
Parametersk1 : Kernel object
1456
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The ﬁrst base-kernel of the product-kernel
k2 : Kernel object
The second base-kernel of the product-kernel
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(k1, k2)
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
29.13. sklearn.gaussian_process: Gaussian Processes
1457
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
29.13.6 sklearn.gaussian_process.kernels.Exponentiation
class sklearn.gaussian_process.kernels.Exponentiation(kernel, exponent)
Exponentiate kernel by given exponent.
The resulting kernel is deﬁned as k_exp(X, Y) = k(X, Y) ** exponent
New in version 0.18.
Parameterskernel : Kernel object
The base kernel
exponent : ﬂoat
The exponent for the base kernel
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(kernel, exponent)
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
1458
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
29.13.7 sklearn.gaussian_process.kernels.ConstantKernel
class sklearn.gaussian_process.kernels.ConstantKernel(constant_value=1.0,
constant_value_bounds=(1e-
05, 100000.0))
Constant kernel.
Can be used as part of a product-kernel where it scales the magnitude of the other factor (kernel) or as part of a
sum-kernel, where it modiﬁes the mean of the Gaussian process.
k(x_1, x_2) = constant_value for all x_1, x_2
New in version 0.18.
29.13. sklearn.gaussian_process: Gaussian Processes
1459
scikit-learn user guide, Release 0.18.2
Parametersconstant_value : ﬂoat, default: 1.0
The constant value which deﬁnes the covariance: k(x_1, x_2) = constant_value
constant_value_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on constant_value
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(constant_value=1.0, constant_value_bounds=(1e-05, 100000.0))
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
1460
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
Examples using sklearn.gaussian_process.kernels.ConstantKernel
• Iso-probability lines for Gaussian Processes classiﬁcation (GPC)
• Gaussian Processes regression: basic introductory example
• Illustration of prior and posterior Gaussian process for different kernels
29.13.8 sklearn.gaussian_process.kernels.WhiteKernel
class sklearn.gaussian_process.kernels.WhiteKernel(noise_level=1.0,
noise_level_bounds=(1e-05,
100000.0))
White kernel.
The main use-case of this kernel is as part of a sum-kernel where it explains the noise-component of the signal.
Tuning its parameter corresponds to estimating the noise-level.
k(x_1, x_2) = noise_level if x_1 == x_2 else 0
New in version 0.18.
Parametersnoise_level : ﬂoat, default: 1.0
Parameter controlling the noise level
noise_level_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on noise_level
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
Continued on next page
29.13. sklearn.gaussian_process: Gaussian Processes
1461
scikit-learn user guide, Release 0.18.2
Table 29.115 – continued from previous page
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(noise_level=1.0, noise_level_bounds=(1e-05, 100000.0))
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
1462
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
Examples using sklearn.gaussian_process.kernels.WhiteKernel
• Comparison of kernel ridge and Gaussian process regression
• Gaussian process regression (GPR) on Mauna Loa CO2 data.
• Gaussian process regression (GPR) with noise-level estimation
29.13.9 sklearn.gaussian_process.kernels.RBF
class sklearn.gaussian_process.kernels.RBF(length_scale=1.0,
length_scale_bounds=(1e-05,
100000.0))
Radial-basis function kernel (aka squared-exponential kernel).
The RBF kernel is a stationary kernel. It is also known as the “squared exponential” kernel. It is parameterized
by a length-scale parameter length_scale>0, which can either be a scalar (isotropic variant of the kernel) or a
vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is
given by:
k(x_i, x_j) = exp(-1 / 2 d(x_i / length_scale, x_j / length_scale)^2)
This kernel is inﬁnitely differentiable, which implies that GPs with this kernel as covariance function have mean
square derivatives of all orders, and are thus very smooth.
New in version 0.18.
Parameterslength_scale : ﬂoat or array with shape (n_features,), default: 1.0
The length scale of the kernel. If a ﬂoat, an isotropic kernel is used. If an array, an
anisotropic kernel is used where each dimension of l deﬁnes the length-scale of the
respective feature dimension.
length_scale_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on length_scale
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0))
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
29.13. sklearn.gaussian_process: Gaussian Processes
1463
scikit-learn user guide, Release 0.18.2
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
Examples using sklearn.gaussian_process.kernels.RBF
• Plot classiﬁcation probability
• Classiﬁer comparison
• Probabilistic predictions with Gaussian process classiﬁcation (GPC)
1464
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Gaussian process classiﬁcation (GPC) on iris dataset
• Illustration of Gaussian process classiﬁcation (GPC) on the XOR dataset
• Gaussian process regression (GPR) on Mauna Loa CO2 data.
• Gaussian process regression (GPR) with noise-level estimation
• Gaussian Processes regression: basic introductory example
• Illustration of prior and posterior Gaussian process for different kernels
29.13.10 sklearn.gaussian_process.kernels.Matern
class sklearn.gaussian_process.kernels.Matern(length_scale=1.0, length_scale_bounds=(1e-
05, 100000.0), nu=1.5)
Matern kernel.
The class of Matern kernels is a generalization of the RBF and the absolute exponential kernel parameterized by
an additional parameter nu. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel
becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate
values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).
See Rasmussen and Williams 2006, pp84 for details regarding the different variants of the Matern kernel.
New in version 0.18.
Parameterslength_scale : ﬂoat or array with shape (n_features,), default: 1.0
The length scale of the kernel. If a ﬂoat, an isotropic kernel is used. If an array, an
anisotropic kernel is used where each dimension of l deﬁnes the length-scale of the
respective feature dimension.
length_scale_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on length_scale
nu: ﬂoat, default: 1.5 :
The parameter nu controlling the smoothness of the learned function. The smaller nu,
the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent
to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important interme-
diate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable
functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher
computational cost (appr. 10 times higher) since they require to evaluate the modiﬁed
Bessel function. Furthermore, in contrast to l, nu is kept ﬁxed to its initial value and not
optimized.
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0), nu=1.5)
29.13. sklearn.gaussian_process: Gaussian Processes
1465
scikit-learn user guide, Release 0.18.2
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
1466
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.gaussian_process.kernels.Matern
• Illustration of prior and posterior Gaussian process for different kernels
29.13.11 sklearn.gaussian_process.kernels.RationalQuadratic
class sklearn.gaussian_process.kernels.RationalQuadratic(length_scale=1.0,
alpha=1.0,
length_scale_bounds=(1e-
05,
100000.0),
alpha_bounds=(1e-05,
100000.0))
Rational Quadratic kernel.
The RationalQuadratic kernel can be seen as a scale mixture (an inﬁnite sum) of RBF kernels with different
characteristic length-scales. It is parameterized by a length-scale parameter length_scale>0 and a scale mixture
parameter alpha>0. Only the isotropic variant where length_scale is a scalar is supported at the moment. The
kernel given by:
k(x_i, x_j) = (1 + d(x_i, x_j)^2 / (2*alpha * length_scale^2))^-alpha
New in version 0.18.
Parameterslength_scale : ﬂoat > 0, default: 1.0
The length scale of the kernel.
alpha : ﬂoat > 0, default: 1.0
Scale mixture parameter
length_scale_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on length_scale
alpha_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on alpha
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(length_scale=1.0, alpha=1.0, length_scale_bounds=(1e-05, 100000.0), alpha_bounds=(1e-
05, 100000.0))
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
29.13. sklearn.gaussian_process: Gaussian Processes
1467
scikit-learn user guide, Release 0.18.2
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
Examples using sklearn.gaussian_process.kernels.RationalQuadratic
• Gaussian process regression (GPR) on Mauna Loa CO2 data.
• Illustration of prior and posterior Gaussian process for different kernels
1468
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.13.12 sklearn.gaussian_process.kernels.ExpSineSquared
class sklearn.gaussian_process.kernels.ExpSineSquared(length_scale=1.0,
periodicity=1.0,
length_scale_bounds=(1e-
05,
100000.0),
periodicity_bounds=(1e-05,
100000.0))
Exp-Sine-Squared kernel.
The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter
length_scale>0 and a periodicity parameter periodicity>0. Only the isotropic variant where l is a scalar is
supported at the moment. The kernel given by:
k(x_i, x_j) = exp(-2 sin(pi / periodicity * d(x_i, x_j)) / length_scale)^2
New in version 0.18.
Parameterslength_scale : ﬂoat > 0, default: 1.0
The length scale of the kernel.
periodicity : ﬂoat > 0, default: 1.0
The periodicity of the kernel.
length_scale_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on length_scale
periodicity_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on periodicity
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(length_scale=1.0,
periodicity=1.0,
length_scale_bounds=(1e-05,
100000.0),
periodicity_bounds=(1e-05, 100000.0))
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
29.13. sklearn.gaussian_process: Gaussian Processes
1469
scikit-learn user guide, Release 0.18.2
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
Examples using sklearn.gaussian_process.kernels.ExpSineSquared
• Comparison of kernel ridge and Gaussian process regression
• Gaussian process regression (GPR) on Mauna Loa CO2 data.
• Illustration of prior and posterior Gaussian process for different kernels
29.13.13 sklearn.gaussian_process.kernels.DotProduct
class sklearn.gaussian_process.kernels.DotProduct(sigma_0=1.0,
sigma_0_bounds=(1e-
05, 100000.0))
Dot-Product kernel.
1470
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The DotProduct kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors
on the coefﬁcients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct
kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by
a parameter sigma_0^2. For sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise it is
inhomogeneous. The kernel is given by
k(x_i, x_j) = sigma_0 ^ 2 + x_i cdot x_j
The DotProduct kernel is commonly combined with exponentiation.
New in version 0.18.
Parameterssigma_0 : ﬂoat >= 0, default: 1.0
Parameter controlling the inhomogenity of the kernel. If sigma_0=0, the kernel is ho-
mogenous.
sigma_0_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on l
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0))
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
29.13. sklearn.gaussian_process: Gaussian Processes
1471
scikit-learn user guide, Release 0.18.2
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
Examples using sklearn.gaussian_process.kernels.DotProduct
• Iso-probability lines for Gaussian Processes classiﬁcation (GPC)
• Illustration of Gaussian process classiﬁcation (GPC) on the XOR dataset
• Illustration of prior and posterior Gaussian process for different kernels
29.13.14 sklearn.gaussian_process.kernels.PairwiseKernel
class sklearn.gaussian_process.kernels.PairwiseKernel(gamma=1.0,
gamma_bounds=(1e-05,
100000.0), metric=’linear’, pair-
wise_kernels_kwargs=None)
Wrapper for kernels in sklearn.metrics.pairwise.
A thin wrapper around the functionality of the kernels in sklearn.metrics.pairwise.
Note: Evaluation of eval_gradient is not analytic but numeric and allkernels support only isotropic dis-
tances. The parameter gamma is considered to be a hyperparameter and may be optimized. The other
kernel parameters are set directly at initialization and are kept ﬁxed.
New in version 0.18.
Parametersgamma: ﬂoat >= 0, default: 1.0 :
Parameter gamma of the pairwise kernel speciﬁed by metric
1472
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
gamma_bounds : pair of ﬂoats >= 0, default: (1e-5, 1e5)
The lower and upper bound on gamma
metric : string, or callable, default: “linear”
The metric to use when calculating kernel between instances in a feature
array.
If
metric
is
a
string,
it
must
be
one
of
the
metrics
in
pair-
wise.PAIRWISE_KERNEL_FUNCTIONS. If metric is “precomputed”, X is assumed
to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable should take two
arrays from X as input and return a value indicating the distance between them.
pairwise_kernels_kwargs : dict, default: None
All entries of this dict (if any) are passed as keyword arguments to the pairwise kernel
function.
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(gamma=1.0,
gamma_bounds=(1e-05,
100000.0),
metric=’linear’,
pair-
wise_kernels_kwargs=None)
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X,)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
29.13. sklearn.gaussian_process: Gaussian Processes
1473
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
29.13.15 sklearn.gaussian_process.kernels.CompoundKernel
class sklearn.gaussian_process.kernels.CompoundKernel(kernels)
Kernel which is composed of a set of other kernels.
New in version 0.18.
Methods
clone_with_theta(theta)
Returns a clone of self with given hyperparameters
theta.
diag(X)
Returns the diagonal of the kernel k(X, X).
get_params([deep])
Get parameters of this kernel.
is_stationary()
Returns whether the kernel is stationary.
set_params(\*\*params)
Set the parameters of this kernel.
__init__(kernels)
bounds
Returns the log-transformed bounds on the theta.
Returnsbounds : array, shape (n_dims, 2)
The log-transformed bounds on the kernel’s hyperparameters theta
clone_with_theta(theta)
Returns a clone of self with given hyperparameters theta.
1474
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
diag(X)
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efﬁciently
since only the diagonal is evaluated.
ParametersX : array, shape (n_samples_X, n_features)
Left argument of the returned kernel k(X, Y)
ReturnsK_diag : array, shape (n_samples_X, n_kernels)
Diagonal of kernel k(X, X)
get_params(deep=True)
Get parameters of this kernel.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
hyperparameters
Returns a list of all hyperparameter speciﬁcations.
is_stationary()
Returns whether the kernel is stationary.
n_dims
Returns the number of non-ﬁxed hyperparameters of the kernel.
set_params(**params)
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels. The latter have parameters of the form
<component>__<parameter> so that it’s possible to update each component of a nested object.
Returnsself :
theta
Returns the (ﬂattened, log-transformed) non-ﬁxed hyperparameters.
Note that theta are typically the log-transformed values of the kernel’s hyperparameters as this representa-
tion of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales
naturally live on a log-scale.
Returnstheta : array, shape (n_dims,)
The non-ﬁxed, log-transformed hyperparameters of the kernel
29.13.16 sklearn.gaussian_process.kernels.Hyperparameter
class sklearn.gaussian_process.kernels.Hyperparameter
A kernel hyperparameter’s speciﬁcation in form of a namedtuple.
New in version 0.18.
Attributesname : string
The name of the hyperparameter. Note that a kernel using a hyperparameter with name
“x” must have the attributes self.x and self.x_bounds
29.13. sklearn.gaussian_process: Gaussian Processes
1475
scikit-learn user guide, Release 0.18.2
value_type : string
The type of the hyperparameter. Currently, only “numeric” hyperparameters are sup-
ported.
bounds : pair of ﬂoats >= 0 or “ﬁxed”
The lower and upper bound on the parameter. If n_elements>1, a pair of 1d array with
n_elements each may be given alternatively. If the string “ﬁxed” is passed as bounds,
the hyperparameter’s value cannot be changed.
n_elements : int, default=1
The number of elements of the hyperparameter value. Defaults to 1, which corresponds
to a scalar hyperparameter. n_elements > 1 corresponds to a hyperparameter which is
vector-valued, such as, e.g., anisotropic length-scales.
ﬁxed : bool, default: None
Whether the value of this hyperparameter is ﬁxed, i.e., cannot be changed during hyper-
parameter tuning. If None is passed, the “ﬁxed” is derived based on the given bounds.
Methods
count(...)
index((value, [start, ...)
Raises ValueError if the value is not present.
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
bounds
Alias for ﬁeld number 2
count(value) →integer – return number of occurrences of value
fixed
Alias for ﬁeld number 4
index(value[, start[, stop]]) →integer – return ﬁrst index of value.
Raises ValueError if the value is not present.
n_elements
Alias for ﬁeld number 3
name
Alias for ﬁeld number 0
value_type
Alias for ﬁeld number 1
29.14 sklearn.isotonic: Isotonic regression
User guide: See the Isotonic regression section for further details.
isotonic.IsotonicRegression([y_min,
y_max,
...])
Isotonic regression model.
1476
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.14.1 sklearn.isotonic.IsotonicRegression
class sklearn.isotonic.IsotonicRegression(y_min=None,
y_max=None,
increasing=True,
out_of_bounds=’nan’)
Isotonic regression model.
The isotonic regression optimization problem is deﬁned by:
min sum w_i (y[i] - y_[i]) ** 2
subject to y_[i] <= y_[j] whenever X[i] <= X[j]
and min(y_) = y_min, max(y_) = y_max
where:
•y[i] are inputs (real numbers)
•y_[i] are ﬁtted
•X speciﬁes the order. If X is non-decreasing then y_ is non-decreasing.
•w[i] are optional strictly positive weights (default to 1.0)
Read more in the User Guide.
Parametersy_min : optional, default: None
If not None, set the lowest value of the ﬁt to y_min.
y_max : optional, default: None
If not None, set the highest value of the ﬁt to y_max.
increasing : boolean or string, optional, default: True
If boolean, whether or not to ﬁt the isotonic regression with y increasing or decreasing.
The string value “auto” determines whether y should increase or decrease based on the
Spearman correlation estimate’s sign.
out_of_bounds : string, optional, default: “nan”
The out_of_bounds parameter handles how x-values outside of the training domain
are handled. When set to “nan”, predicted y-values will be NaN. When set to “clip”,
predicted y-values will be set to the value corresponding to the nearest train interval
endpoint. When set to “raise”, allow interp1d to throw ValueError.
AttributesX_min_ : ﬂoat
Minimum value of input array X_ for left bound.
X_max_ : ﬂoat
Maximum value of input array X_ for right bound.
f_ : function
The stepwise interpolating function that covers the domain X_.
Notes
Ties are broken using the secondary method from Leeuw, 1977.
29.14. sklearn.isotonic: Isotonic regression
1477
scikit-learn user guide, Release 0.18.2
References
Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations
Research Vol. 14, No. 2 (May, 1989), pp. 303-308
Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik,
Mair Journal of Statistical Software 2009
Correctness of Kruskal’s algorithms for monotone regression with ties Leeuw, Psychometrica, 1977
Methods
fit(X, y[, sample_weight])
Fit the model using X, y as training data.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(T)
Predict new data by linear interpolation.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
transform(T)
Transform new data by linear interpolation
__init__(y_min=None, y_max=None, increasing=True, out_of_bounds=’nan’)
X_
DEPRECATED: Attribute X_ is deprecated in version 0.18 and will be removed in version 0.20.
fit(X, y, sample_weight=None)
Fit the model using X, y as training data.
ParametersX : array-like, shape=(n_samples,)
Training data.
y : array-like, shape=(n_samples,)
Training target.
sample_weight : array-like, shape=(n_samples,), optional, default: None
Weights. If set to None, all weights will be set to 1 (equal weights).
Returnsself : object
Returns an instance of self.
Notes
X is stored for future use, as transform needs X to interpolate new input data.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
1478
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(T)
Predict new data by linear interpolation.
ParametersT : array-like, shape=(n_samples,)
Data to transform.
ReturnsT_ : array, shape=(n_samples,)
Transformed data.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(T)
Transform new data by linear interpolation
ParametersT : array-like, shape=(n_samples,)
29.14. sklearn.isotonic: Isotonic regression
1479
scikit-learn user guide, Release 0.18.2
Data to transform.
ReturnsT_ : array, shape=(n_samples,)
The transformed data
y_
DEPRECATED: Attribute y_ is deprecated in version 0.18 and will be removed in version 0.20.
Examples using sklearn.isotonic.IsotonicRegression
• Isotonic Regression
isotonic.isotonic_regression(y[, ...])
Solve the isotonic regression model:
isotonic.check_increasing(x, y)
Determine whether y is monotonically correlated with x.
29.14.2 sklearn.isotonic.isotonic_regression
sklearn.isotonic.isotonic_regression(y, sample_weight=None, y_min=None, y_max=None,
increasing=True)
Solve the isotonic regression model:
min sum w[i] (y[i] - y_[i]) ** 2
subject to y_min = y_[1] <= y_[2] ... <= y_[n] = y_max
where:
•y[i] are inputs (real numbers)
•y_[i] are ﬁtted
•w[i] are optional strictly positive weights (default to 1.0)
Read more in the User Guide.
Parametersy : iterable of ﬂoating-point values
The data.
sample_weight : iterable of ﬂoating-point values, optional, default: None
Weights on each point of the regression. If None, weight is set to 1 (equal weights).
y_min : optional, default: None
If not None, set the lowest value of the ﬁt to y_min.
y_max : optional, default: None
If not None, set the highest value of the ﬁt to y_max.
increasing : boolean, optional, default: True
Whether to compute y_ is increasing (if set to True) or decreasing (if set to False)
Returnsy_ : list of ﬂoating-point values
Isotonic ﬁt of y.
1480
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
“Active set algorithms for isotonic regression; A unifying framework” by Michael J. Best and Nilotpal
Chakravarti, section 3.
29.14.3 sklearn.isotonic.check_increasing
sklearn.isotonic.check_increasing(x, y)
Determine whether y is monotonically correlated with x.
y is found increasing or decreasing with respect to x based on a Spearman correlation test.
Parametersx : array-like, shape=(n_samples,)
Training data.
y : array-like, shape=(n_samples,)
Training target.
Returns‘increasing_bool‘ : boolean
Whether the relationship is increasing or decreasing.
Notes
The Spearman correlation coefﬁcient is estimated from the data, and the sign of the resulting estimate is used as
the result.
In the event that the 95% conﬁdence interval based on Fisher transform spans zero, a warning is raised.
References
Fisher transformation. Wikipedia. https://en.wikipedia.org/wiki/Fisher_transformation
29.15 sklearn.kernel_approximation Kernel Approximation
The sklearn.kernel_approximation module implements several approximate kernel feature maps base on
Fourier transforms.
User guide: See the Kernel Approximation section for further details.
kernel_approximation.AdditiveChi2Sampler([...])
Approximate feature map for additive chi2 kernel.
kernel_approximation.Nystroem([kernel, ...])
Approximate a kernel map using a subset of the training
data.
kernel_approximation.RBFSampler([gamma,
...])
Approximates feature map of an RBF kernel by Monte
Carlo approximation of its Fourier transform.
kernel_approximation.SkewedChi2Sampler([...])Approximates feature map of the “skewed chi-squared”
kernel by Monte Carlo approximation of its Fourier trans-
form.
29.15. sklearn.kernel_approximation Kernel Approximation
1481
scikit-learn user guide, Release 0.18.2
29.15.1 sklearn.kernel_approximation.AdditiveChi2Sampler
class sklearn.kernel_approximation.AdditiveChi2Sampler(sample_steps=2,
sam-
ple_interval=None)
Approximate feature map for additive chi2 kernel.
Uses sampling the fourier transform of the kernel characteristic at regular intervals.
Since the kernel that is to be approximated is additive, the components of the input vectors can be treated
separately. Each entry in the original space is transformed into 2*sample_steps+1 features, where sample_steps
is a parameter of the method. Typical values of sample_steps include 1, 2 and 3.
Optimal choices for the sampling interval for certain data ranges can be computed (see the reference). The
default values should be reasonable.
Read more in the User Guide.
Parameterssample_steps : int, optional
Gives the number of (complex) sampling points.
sample_interval : ﬂoat, optional
Sampling interval. Must be speciﬁed when sample_steps not in {1,2,3}.
See also:
SkewedChi2SamplerA Fourier-approximation to a non-additive variant of the chi squared kernel.
sklearn.metrics.pairwise.chi2_kernelThe exact chi squared kernel.
sklearn.metrics.pairwise.additive_chi2_kernelThe exact additive chi squared kernel.
Notes
This estimator approximates a slightly different version of the additive chi squared kernel then
metric.additive_chi2 computes.
References
See “Efﬁcient additive kernels via explicit feature maps” A. Vedaldi and A. Zisserman, Pattern Analysis and
Machine Intelligence, 2011
Methods
fit(X[, y])
Set parameters.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Apply approximate feature map to X.
__init__(sample_steps=2, sample_interval=None)
fit(X, y=None)
Set parameters.
1482
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Apply approximate feature map to X.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
ReturnsX_new : {array, sparse matrix}, shape = (n_samples, n_features * (2*sample_steps +
1))
Whether the return value is an array of sparse matrix depends on the type of the input
X.
29.15.2 sklearn.kernel_approximation.Nystroem
class sklearn.kernel_approximation.Nystroem(kernel=’rbf’,
gamma=None,
coef0=1,
degree=3,
kernel_params=None,
n_components=100, random_state=None)
Approximate a kernel map using a subset of the training data.
Constructs an approximate feature map for an arbitrary kernel using a subset of the data as basis.
Read more in the User Guide.
Parameterskernel : string or callable, default=”rbf”
Kernel map to be approximated. A callable should accept two arguments and the key-
word arguments passed to this object as kernel_params, and should return a ﬂoating
point number.
29.15. sklearn.kernel_approximation Kernel Approximation
1483
scikit-learn user guide, Release 0.18.2
n_components : int
Number of features to construct. How many data points will be used to construct the
mapping.
gamma : ﬂoat, default=None
Gamma parameter for the RBF, polynomial, exponential chi2 and sigmoid kernels.
Interpretation of the default value is left to the kernel; see the documentation for
sklearn.metrics.pairwise. Ignored by other kernels.
degree : ﬂoat, default=3
Degree of the polynomial kernel. Ignored by other kernels.
coef0 : ﬂoat, default=1
Zero coefﬁcient for polynomial and sigmoid kernels. Ignored by other kernels.
kernel_params : mapping of string to any, optional
Additional parameters (keyword arguments) for kernel function passed as callable ob-
ject.
random_state : {int, RandomState}, optional
If int, random_state is the seed used by the random number generator; if RandomState
instance, random_state is the random number generator.
Attributescomponents_ : array, shape (n_components, n_features)
Subset of training points used to construct the feature map.
component_indices_ : array, shape (n_components)
Indices of components_ in the training set.
normalization_ : array, shape (n_components, n_components)
Normalization matrix needed for embedding.
Square root of the kernel matrix on
components_.
See also:
RBFSamplerAn approximation to the RBF kernel using random Fourier features.
sklearn.metrics.pairwise.kernel_metricsList of built-in kernels.
References
•Williams, C.K.I. and Seeger, M. “Using the Nystroem method to speed up kernel machines”, Advances in
neural information processing systems 2001
•T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou “Nystroem Method vs Random Fourier Features: A
Theoretical and Empirical Comparison”, Advances in Neural Information Processing Systems 2012
Methods
fit(X[, y])
Fit estimator to data.
fit_transform(X[, y])
Fit to data, then transform it.
Continued on next page
1484
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.129 – continued from previous page
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Apply feature map to X.
__init__(kernel=’rbf’,
gamma=None,
coef0=1,
degree=3,
kernel_params=None,
n_components=100, random_state=None)
fit(X, y=None)
Fit estimator to data.
Samples a subset of training points, computes kernel on these and computes normalization matrix.
ParametersX : array-like, shape=(n_samples, n_feature)
Training data.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Apply feature map to X.
Computes an approximate feature map using the kernel between some training points and X.
ParametersX : array-like, shape=(n_samples, n_features)
Data to transform.
ReturnsX_transformed : array, shape=(n_samples, n_components)
Transformed data.
29.15. sklearn.kernel_approximation Kernel Approximation
1485
scikit-learn user guide, Release 0.18.2
Examples using sklearn.kernel_approximation.Nystroem
• Explicit feature map approximation for RBF kernels
29.15.3 sklearn.kernel_approximation.RBFSampler
class sklearn.kernel_approximation.RBFSampler(gamma=1.0,
n_components=100,
ran-
dom_state=None)
Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.
It implements a variant of Random Kitchen Sinks.[1]
Read more in the User Guide.
Parametersgamma : ﬂoat
Parameter of RBF kernel: exp(-gamma * x^2)
n_components : int
Number of Monte Carlo samples per original feature. Equals the dimensionality of the
computed feature space.
random_state : {int, RandomState}, optional
If int, random_state is the seed used by the random number generator; if RandomState
instance, random_state is the random number generator.
Notes
See “Random Features for Large-Scale Kernel Machines” by A. Rahimi and Benjamin Recht.
[1] “Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning” by A.
Rahimi and Benjamin Recht. (http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)
Methods
fit(X[, y])
Fit the model with X.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Apply the approximate feature map to X.
__init__(gamma=1.0, n_components=100, random_state=None)
fit(X, y=None)
Fit the model with X.
Samples random projection according to n_features.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training data, where n_samples in the number of samples and n_features is the number
of features.
Returnsself : object
1486
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returns the transformer.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Apply the approximate feature map to X.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
New data, where n_samples in the number of samples and n_features is the number of
features.
ReturnsX_new : array-like, shape (n_samples, n_components)
Examples using sklearn.kernel_approximation.RBFSampler
• Explicit feature map approximation for RBF kernels
29.15.4 sklearn.kernel_approximation.SkewedChi2Sampler
class sklearn.kernel_approximation.SkewedChi2Sampler(skewedness=1.0,
n_components=100,
ran-
dom_state=None)
Approximates feature map of the “skewed chi-squared” kernel by Monte Carlo approximation of its Fourier
transform.
Read more in the User Guide.
29.15. sklearn.kernel_approximation Kernel Approximation
1487
scikit-learn user guide, Release 0.18.2
Parametersskewedness : ﬂoat
“skewedness” parameter of the kernel. Needs to be cross-validated.
n_components : int
number of Monte Carlo samples per original feature. Equals the dimensionality of the
computed feature space.
random_state : {int, RandomState}, optional
If int, random_state is the seed used by the random number generator; if RandomState
instance, random_state is the random number generator.
See also:
AdditiveChi2SamplerA different approach for approximating an additive variant of the chi squared ker-
nel.
sklearn.metrics.pairwise.chi2_kernelThe exact chi squared kernel.
References
See “Random Fourier Approximations for Skewed Multiplicative Histogram Kernels” by Fuxin Li, Catalin
Ionescu and Cristian Sminchisescu.
Methods
fit(X[, y])
Fit the model with X.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Apply the approximate feature map to X.
__init__(skewedness=1.0, n_components=100, random_state=None)
fit(X, y=None)
Fit the model with X.
Samples random projection according to n_features.
ParametersX : array-like, shape (n_samples, n_features)
Training data, where n_samples in the number of samples and n_features is the number
of features.
Returnsself : object
Returns the transformer.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
1488
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Apply the approximate feature map to X.
ParametersX : array-like, shape (n_samples, n_features)
New data, where n_samples in the number of samples and n_features is the number of
features.
ReturnsX_new : array-like, shape (n_samples, n_components)
29.16 sklearn.kernel_ridge Kernel Ridge Regression
Module sklearn.kernel_ridge implements kernel ridge regression.
User guide: See the Kernel ridge regression section for further details.
kernel_ridge.KernelRidge([alpha, kernel, ...])
Kernel ridge regression.
29.16.1 sklearn.kernel_ridge.KernelRidge
class sklearn.kernel_ridge.KernelRidge(alpha=1, kernel=’linear’, gamma=None, degree=3,
coef0=1, kernel_params=None)
Kernel ridge regression.
Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with
the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For
non-linear kernels, this corresponds to a non-linear function in the original space.
The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss
functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss,
both combined with l2 regularization. In contrast to SVR, ﬁtting a KRR model can be done in closed-form and
29.16. sklearn.kernel_ridge Kernel Ridge Regression
1489
scikit-learn user guide, Release 0.18.2
is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower
than SVR, which learns a sparse model for epsilon > 0, at prediction-time.
This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples,
n_targets]).
Read more in the User Guide.
Parametersalpha : {ﬂoat, array-like}, shape = [n_targets]
Small positive values of alpha improve the conditioning of the problem and reduce the
variance of the estimates. Alpha corresponds to (2*C)^-1 in other linear models such
as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be
speciﬁc to the targets. Hence they must correspond in number.
kernel : string or callable, default=”linear”
Kernel mapping used internally. A callable should accept two arguments and the key-
word arguments passed to this object as kernel_params, and should return a ﬂoating
point number.
gamma : ﬂoat, default=None
Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid
kernels. Interpretation of the default value is left to the kernel; see the documentation
for sklearn.metrics.pairwise. Ignored by other kernels.
degree : ﬂoat, default=3
Degree of the polynomial kernel. Ignored by other kernels.
coef0 : ﬂoat, default=1
Zero coefﬁcient for polynomial and sigmoid kernels. Ignored by other kernels.
kernel_params : mapping of string to any, optional
Additional parameters (keyword arguments) for kernel function passed as callable ob-
ject.
Attributesdual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]
Representation of weight vector(s) in kernel space
X_ﬁt_ : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training data, which is also required for prediction
See also:
RidgeLinear ridge regression.
SVRSupport Vector Regression implemented using libsvm.
References
•Kevin P. Murphy “Machine Learning: A Probabilistic Perspective”, The MIT Press chapter 14.4.3, pp.
492-493
1490
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn.kernel_ridge import KernelRidge
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> clf = KernelRidge(alpha=1.0)
>>> clf.fit(X, y)
KernelRidge(alpha=1.0, coef0=1, degree=3, gamma=None, kernel='linear',
kernel_params=None)
Methods
fit(X[, y, sample_weight])
Fit Kernel Ridge regression model
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the kernel ridge model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1, kernel=’linear’, gamma=None, degree=3, coef0=1, kernel_params=None)
fit(X, y=None, sample_weight=None)
Fit Kernel Ridge regression model
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training data
y : array-like, shape = [n_samples] or [n_samples, n_targets]
Target values
sample_weight : ﬂoat or numpy array of shape [n_samples]
Individual weights for each sample, ignored if None is passed.
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the kernel ridge model
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
29.16. sklearn.kernel_ridge Kernel Ridge Regression
1491
scikit-learn user guide, Release 0.18.2
ReturnsC : array, shape = [n_samples] or [n_samples, n_targets]
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.kernel_ridge.KernelRidge
• Comparison of kernel ridge regression and SVR
• Comparison of kernel ridge and Gaussian process regression
29.17 sklearn.discriminant_analysis: Discriminant Analysis
Linear Discriminant Analysis and Quadratic Discriminant Analysis
User guide: See the Linear and Quadratic Discriminant Analysis section for further details.
discriminant_analysis.LinearDiscriminantAnalysis([...])
Linear Discriminant Analysis
discriminant_analysis.QuadraticDiscriminantAnalysis([...])
Quadratic Discriminant Analysis
1492
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.17.1 sklearn.discriminant_analysis.LinearDiscriminantAnalysis
class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(solver=’svd’,
shrinkage=None,
priors=None,
n_components=None,
store_covariance=False,
tol=0.0001)
Linear Discriminant Analysis
A classiﬁer with a linear decision boundary, generated by ﬁtting class conditional densities to the data and using
Bayes’ rule.
The model ﬁts a Gaussian density to each class, assuming that all classes share the same covariance matrix.
The ﬁtted model can also be used to reduce the dimensionality of the input by projecting it to the most discrim-
inative directions.
New in version 0.17: LinearDiscriminantAnalysis.
Read more in the User Guide.
Parameterssolver : string, optional
Solver to use, possible values:
•‘svd’: Singular value decomposition (default). Does not compute the covariance
matrix, therefore this solver is recommended for data with a large number of fea-
tures.
•‘lsqr’: Least squares solution, can be combined with shrinkage.
•‘eigen’: Eigenvalue decomposition, can be combined with shrinkage.
shrinkage : string or ﬂoat, optional
Shrinkage parameter, possible values:
•None: no shrinkage (default).
•‘auto’: automatic shrinkage using the Ledoit-Wolf lemma.
•ﬂoat between 0 and 1: ﬁxed shrinkage parameter.
Note that shrinkage works only with ‘lsqr’ and ‘eigen’ solvers.
priors : array, optional, shape (n_classes,)
Class priors.
n_components : int, optional
Number of components (< n_classes - 1) for dimensionality reduction.
store_covariance : bool, optional
Additionally compute class covariance matrix (default False).
New in version 0.17.
tol : ﬂoat, optional
Threshold used for rank estimation in SVD solver.
New in version 0.17.
Attributescoef_ : array, shape (n_features,) or (n_classes, n_features)
29.17. sklearn.discriminant_analysis: Discriminant Analysis
1493
scikit-learn user guide, Release 0.18.2
Weight vector(s).
intercept_ : array, shape (n_features,)
Intercept term.
covariance_ : array-like, shape (n_features, n_features)
Covariance matrix (shared by all classes).
explained_variance_ratio_ : array, shape (n_components,)
Percentage of variance explained by each of the selected components.
If
n_components is not set then all components are stored and the sum of explained
variances is equal to 1.0. Only available when eigen or svd solver is used.
means_ : array-like, shape (n_classes, n_features)
Class means.
priors_ : array-like, shape (n_classes,)
Class priors (sum to 1).
scalings_ : array-like, shape (rank, n_classes - 1)
Scaling of the features in the space spanned by the class centroids.
xbar_ : array-like, shape (n_features,)
Overall mean.
classes_ : array-like, shape (n_classes,)
Unique class labels.
See also:
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysisQuadratic
Discrimi-
nant Analysis
Notes
The default solver is ‘svd’. It can perform both classiﬁcation and transform, and it does not rely on the calcu-
lation of the covariance matrix. This can be an advantage in situations where the number of features is large.
However, the ‘svd’ solver cannot be used with shrinkage.
The ‘lsqr’ solver is an efﬁcient algorithm that only works for classiﬁcation. It supports shrinkage.
The ‘eigen’ solver is based on the optimization of the between class scatter to within class scatter ratio. It can
be used for both classiﬁcation and transform, and it supports shrinkage. However, the ‘eigen’ solver needs to
compute the covariance matrix, so it might not be suitable for situations with a high number of features.
Examples
>>> import numpy as np
>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> clf = LinearDiscriminantAnalysis()
>>> clf.fit(X, y)
1494
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,
solver='svd', store_covariance=False, tol=0.0001)
>>> print(clf.predict([[-0.8, -1]]))
[1]
Methods
decision_function(X)
Predict conﬁdence scores for samples.
fit(X, y[, store_covariance, tol])
Fit LinearDiscriminantAnalysis model according to the
given training data and parameters.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class labels for samples in X.
predict_log_proba(X)
Estimate log probability.
predict_proba(X)
Estimate probability.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Project data to maximize class separation.
__init__(solver=’svd’,
shrinkage=None,
priors=None,
n_components=None,
store_covariance=False, tol=0.0001)
decision_function(X)
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
fit(X, y, store_covariance=None, tol=None)
Fit LinearDiscriminantAnalysis model according to the giventraining data and parameters.
Changed in version 0.17: Deprecated store_covariance have been moved to main constructor.
Changed in version 0.17: Deprecated tol have been moved to main constructor.
ParametersX : array-like, shape (n_samples, n_features)
Training data.
y : array, shape (n_samples,)
Target values.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
29.17. sklearn.discriminant_analysis: Discriminant Analysis
1495
scikit-learn user guide, Release 0.18.2
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class labels for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
predict_log_proba(X)
Estimate log probability.
ParametersX : array-like, shape (n_samples, n_features)
Input data.
ReturnsC : array, shape (n_samples, n_classes)
Estimated log probabilities.
predict_proba(X)
Estimate probability.
ParametersX : array-like, shape (n_samples, n_features)
Input data.
ReturnsC : array, shape (n_samples, n_classes)
Estimated probabilities.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
1496
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Project data to maximize class separation.
ParametersX : array-like, shape (n_samples, n_features)
Input data.
ReturnsX_new : array, shape (n_samples, n_components)
Transformed data.
Examples using sklearn.discriminant_analysis.LinearDiscriminantAnalysis
• Normal and Shrinkage Linear Discriminant Analysis for classiﬁcation
• Linear and Quadratic Discriminant Analysis with conﬁdence ellipsoid
• Comparison of LDA and PCA 2D projection of Iris dataset
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
29.17.2 sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
class sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(priors=None,
reg_param=0.0,
store_covariances=False,
tol=0.0001)
Quadratic Discriminant Analysis
A classiﬁer with a quadratic decision boundary, generated by ﬁtting class conditional densities to the data and
using Bayes’ rule.
The model ﬁts a Gaussian density to each class.
New in version 0.17: QuadraticDiscriminantAnalysis
Read more in the User Guide.
Parameterspriors : array, optional, shape = [n_classes]
Priors on classes
reg_param : ﬂoat, optional
29.17. sklearn.discriminant_analysis: Discriminant Analysis
1497
scikit-learn user guide, Release 0.18.2
Regularizes
the
covariance
estimate
as
(1-reg_param)*Sigma +
reg_param*np.eye(n_features)
Attributescovariances_ : list of array-like, shape = [n_features, n_features]
Covariance matrices of each class.
means_ : array-like, shape = [n_classes, n_features]
Class means.
priors_ : array-like, shape = [n_classes]
Class priors (sum to 1).
rotations_ : list of arrays
For
each
class
k
an
array
of
shape
[n_features,
n_k],
with
n_k =
min(n_features,number of elements in class k)
It
is
the
rota-
tion of the Gaussian distribution, i.e. its principal axis.
scalings_ : list of arrays
For each class k an array of shape [n_k]. It contains the scaling of the Gaussian distri-
butions along its principal axes, i.e. the variance in the rotated coordinate system.
store_covariances : boolean
If True the covariance matrices are computed and stored in the self.covariances_ at-
tribute.
New in version 0.17.
tol : ﬂoat, optional, default 1.0e-4
Threshold used for rank estimation.
New in version 0.17.
See also:
sklearn.discriminant_analysis.LinearDiscriminantAnalysisLinear Discriminant Anal-
ysis
Examples
>>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> clf = QuadraticDiscriminantAnalysis()
>>> clf.fit(X, y)
...
QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
store_covariances=False, tol=0.0001)
>>> print(clf.predict([[-0.8, -1]]))
[1]
Methods
1498
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
decision_function(X)
Apply decision function to an array of samples.
fit(X, y[, store_covariances, tol])
Fit the model according to the given training data and
parameters.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform classiﬁcation on an array of test vectors X.
predict_log_proba(X)
Return posterior probabilities of classiﬁcation.
predict_proba(X)
Return posterior probabilities of classiﬁcation.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(priors=None, reg_param=0.0, store_covariances=False, tol=0.0001)
decision_function(X)
Apply decision function to an array of samples.
ParametersX : array-like, shape = [n_samples, n_features]
Array of samples (test vectors).
ReturnsC : array, shape = [n_samples, n_classes] or [n_samples,]
Decision function values related to each class, per sample. In the two-class case, the
shape is [n_samples,], giving the log likelihood ratio of the positive class.
fit(X, y, store_covariances=None, tol=None)
Fit the model according to the given training data and parameters.
Changed in version 0.17: Deprecated store_covariance have been moved to main constructor.
Changed in version 0.17: Deprecated tol have been moved to main constructor.
ParametersX : array-like, shape = [n_samples, n_features]
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
y : array, shape = [n_samples]
Target values (integers)
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform classiﬁcation on an array of test vectors X.
The predicted class C for each sample in X is returned.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = [n_samples]
29.17. sklearn.discriminant_analysis: Discriminant Analysis
1499
scikit-learn user guide, Release 0.18.2
predict_log_proba(X)
Return posterior probabilities of classiﬁcation.
ParametersX : array-like, shape = [n_samples, n_features]
Array of samples/test vectors.
ReturnsC : array, shape = [n_samples, n_classes]
Posterior log-probabilities of classiﬁcation per class.
predict_proba(X)
Return posterior probabilities of classiﬁcation.
ParametersX : array-like, shape = [n_samples, n_features]
Array of samples/test vectors.
ReturnsC : array, shape = [n_samples, n_classes]
Posterior probabilities of classiﬁcation per class.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
• Classiﬁer comparison
• Linear and Quadratic Discriminant Analysis with conﬁdence ellipsoid
1500
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.18 sklearn.linear_model: Generalized Linear Models
The sklearn.linear_model module implements generalized linear models.
It includes Ridge regression,
Bayesian Regression, Lasso and Elastic Net estimators computed with Least Angle Regression and coordinate de-
scent. It also implements Stochastic Gradient Descent related algorithms.
User guide: See the Generalized Linear Models section for further details.
linear_model.ARDRegression([n_iter, tol, ...])
Bayesian ARD regression.
linear_model.BayesianRidge([n_iter, tol, ...])
Bayesian ridge regression
linear_model.ElasticNet([alpha, l1_ratio, ...])
Linear regression with combined L1 and L2 priors as regu-
larizer.
linear_model.ElasticNetCV([l1_ratio, eps, ...])
Elastic Net model with iterative ﬁtting along a regulariza-
tion path
linear_model.HuberRegressor([epsilon, ...])
Linear regression model that is robust to outliers.
linear_model.Lars([ﬁt_intercept, verbose, ...])
Least Angle Regression model a.k.a.
linear_model.LarsCV([ﬁt_intercept, ...])
Cross-validated Least Angle Regression model
linear_model.Lasso([alpha, ﬁt_intercept, ...])
Linear Model trained with L1 prior as regularizer (aka the
Lasso)
linear_model.LassoCV([eps, n_alphas, ...])
Lasso linear model with iterative ﬁtting along a regulariza-
tion path
linear_model.LassoLars([alpha, ...])
Lasso model ﬁt with Least Angle Regression a.k.a.
linear_model.LassoLarsCV([ﬁt_intercept, ...])
Cross-validated Lasso, using the LARS algorithm
linear_model.LassoLarsIC([criterion, ...])
Lasso model ﬁt with Lars using BIC or AIC for model se-
lection
linear_model.LinearRegression([...])
Ordinary least squares Linear Regression.
linear_model.LogisticRegression([penalty,
...])
Logistic Regression (aka logit, MaxEnt) classiﬁer.
linear_model.LogisticRegressionCV([Cs,
...])
Logistic Regression CV (aka logit, MaxEnt) classiﬁer.
linear_model.MultiTaskLasso([alpha, ...])
Multi-task Lasso model trained with L1/L2 mixed-norm as
regularizer
linear_model.MultiTaskElasticNet([alpha,
...])
Multi-task ElasticNet model trained with L1/L2 mixed-
norm as regularizer
linear_model.MultiTaskLassoCV([eps, ...])
Multi-task L1/L2 Lasso with built-in cross-validation.
linear_model.MultiTaskElasticNetCV([...])
Multi-task L1/L2 ElasticNet with built-in cross-validation.
linear_model.OrthogonalMatchingPursuit([...])Orthogonal Matching Pursuit model (OMP)
linear_model.OrthogonalMatchingPursuitCV([...])
Cross-validated
Orthogonal
Matching
Pursuit
model
(OMP)
linear_model.PassiveAggressiveClassifier([...])
Passive Aggressive Classiﬁer
linear_model.PassiveAggressiveRegressor([C,
...])
Passive Aggressive Regressor
linear_model.Perceptron([penalty, alpha, ...])
Read more in the User Guide.
linear_model.RandomizedLasso([alpha, ...])
Randomized Lasso.
linear_model.RandomizedLogisticRegression([...])
Randomized Logistic Regression
linear_model.RANSACRegressor([...])
RANSAC (RANdom SAmple Consensus) algorithm.
linear_model.Ridge([alpha, ﬁt_intercept, ...])
Linear least squares with l2 regularization.
linear_model.RidgeClassifier([alpha, ...])
Classiﬁer using Ridge regression.
linear_model.RidgeClassifierCV([alphas, ...])
Ridge classiﬁer with built-in cross-validation.
linear_model.RidgeCV([alphas, ...])
Ridge regression with built-in cross-validation.
linear_model.SGDClassifier([loss, penalty, ...])
Linear classiﬁers (SVM, logistic regression, a.o.)
with
SGD training.
Continued on next page
29.18. sklearn.linear_model: Generalized Linear Models
1501
scikit-learn user guide, Release 0.18.2
Table 29.137 – continued from previous page
linear_model.SGDRegressor([loss, penalty, ...])
Linear model ﬁtted by minimizing a regularized empirical
loss with SGD
linear_model.TheilSenRegressor([...])
Theil-Sen Estimator: robust multivariate regression model.
29.18.1 sklearn.linear_model.ARDRegression
class sklearn.linear_model.ARDRegression(n_iter=300,
tol=0.001,
alpha_1=1e-
06,
alpha_2=1e-06,
lambda_1=1e-06,
lambda_2=1e-06,
compute_score=False,
thresh-
old_lambda=10000.0, ﬁt_intercept=True, normal-
ize=False, copy_X=True, verbose=False)
Bayesian ARD regression.
Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed
to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the
weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures
(Evidence Maximization)
Read more in the User Guide.
Parametersn_iter : int, optional
Maximum number of iterations. Default is 300
tol : ﬂoat, optional
Stop the algorithm if w has converged. Default is 1.e-3.
alpha_1 : ﬂoat, optional
Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha
parameter. Default is 1.e-6.
alpha_2 : ﬂoat, optional
Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution
prior over the alpha parameter. Default is 1.e-6.
lambda_1 : ﬂoat, optional
Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda
parameter. Default is 1.e-6.
lambda_2 : ﬂoat, optional
Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution
prior over the lambda parameter. Default is 1.e-6.
compute_score : boolean, optional
If True, compute the objective function at each step of the model. Default is False.
threshold_lambda : ﬂoat, optional
threshold for removing (pruning) weights with high precision from the computation.
Default is 1.e+4.
ﬁt_intercept : boolean, optional
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered). Default is True.
1502
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
copy_X : boolean, optional, default True.
If True, X will be copied; else, it may be overwritten.
verbose : boolean, optional, default False
Verbose mode when ﬁtting the model.
Attributescoef_ : array, shape = (n_features)
Coefﬁcients of the regression model (mean of distribution)
alpha_ : ﬂoat
estimated precision of the noise.
lambda_ : array, shape = (n_features)
estimated precisions of the weights.
sigma_ : array, shape = (n_features, n_features)
estimated variance-covariance matrix of the weights
scores_ : ﬂoat
if computed, value of the objective function (to be maximized)
Notes
See examples/linear_model/plot_ard.py for an example.
Examples
>>> from sklearn import linear_model
>>> clf = linear_model.ARDRegression()
>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
...
ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,
copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,
n_iter=300, normalize=False, threshold_lambda=10000.0, tol=0.001,
verbose=False)
>>> clf.predict([[1, 1]])
array([ 1.])
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
Continued on next page
29.18. sklearn.linear_model: Generalized Linear Models
1503
scikit-learn user guide, Release 0.18.2
Table 29.138 – continued from previous page
fit(X, y)
Fit the ARDRegression model according to the given
training data and parameters.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-
06,
compute_score=False,
threshold_lambda=10000.0,
ﬁt_intercept=True,
normal-
ize=False, copy_X=True, verbose=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit the ARDRegression model according to the given training data and parameters.
Iterative procedure to maximize the evidence
ParametersX : array-like, shape = [n_samples, n_features]
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
y : array, shape = [n_samples]
Target values (integers)
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
1504
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.ARDRegression
• Automatic Relevance Determination Regression (ARD)
29.18.2 sklearn.linear_model.BayesianRidge
class sklearn.linear_model.BayesianRidge(n_iter=300,
tol=0.001,
alpha_1=1e-06,
alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-
06,
compute_score=False,
ﬁt_intercept=True,
normalize=False, copy_X=True, verbose=False)
Bayesian ridge regression
Fit a Bayesian ridge model and optimize the regularization parameters lambda (precision of the weights) and
alpha (precision of the noise).
Read more in the User Guide.
Parametersn_iter : int, optional
Maximum number of iterations. Default is 300.
tol : ﬂoat, optional
Stop the algorithm if w has converged. Default is 1.e-3.
alpha_1 : ﬂoat, optional
Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha
parameter. Default is 1.e-6
29.18. sklearn.linear_model: Generalized Linear Models
1505
scikit-learn user guide, Release 0.18.2
alpha_2 : ﬂoat, optional
Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution
prior over the alpha parameter. Default is 1.e-6.
lambda_1 : ﬂoat, optional
Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda
parameter. Default is 1.e-6.
lambda_2 : ﬂoat, optional
Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution
prior over the lambda parameter. Default is 1.e-6
compute_score : boolean, optional
If True, compute the objective function at each step of the model. Default is False
ﬁt_intercept : boolean, optional
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered). Default is True.
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
verbose : boolean, optional, default False
Verbose mode when ﬁtting the model.
Attributescoef_ : array, shape = (n_features)
Coefﬁcients of the regression model (mean of distribution)
alpha_ : ﬂoat
estimated precision of the noise.
lambda_ : ﬂoat
estimated precision of the weights.
scores_ : ﬂoat
if computed, value of the objective function (to be maximized)
Notes
See examples/linear_model/plot_bayesian_ridge.py for an example.
1506
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn import linear_model
>>> clf = linear_model.BayesianRidge()
>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
...
BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,
copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,
n_iter=300, normalize=False, tol=0.001, verbose=False)
>>> clf.predict([[1, 1]])
array([ 1.])
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y)
Fit the model
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-
06,
compute_score=False,
ﬁt_intercept=True,
normalize=False,
copy_X=True,
ver-
bose=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit the model
ParametersX : numpy array of shape [n_samples,n_features]
Training data
y : numpy array of shape [n_samples]
Target values
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
29.18. sklearn.linear_model: Generalized Linear Models
1507
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.BayesianRidge
• Feature agglomeration vs. univariate selection
• Bayesian Ridge Regression
29.18.3 sklearn.linear_model.ElasticNet
class sklearn.linear_model.ElasticNet(alpha=1.0,
l1_ratio=0.5,
ﬁt_intercept=True,
nor-
malize=False,
precompute=False,
max_iter=1000,
copy_X=True,
tol=0.0001,
warm_start=False,
posi-
tive=False, random_state=None, selection=’cyclic’)
Linear regression with combined L1 and L2 priors as regularizer.
Minimizes the objective function:
1508
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
1 / (2 * n_samples) * ||y - Xw||^2_2
+ alpha * l1_ratio * ||w||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2
If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to:
a * L1 + b * L2
where:
alpha = a + b and l1_ratio = a / (a + b)
The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda
parameter in glmnet. Speciﬁcally, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,
unless you supply your own sequence of alpha.
Read more in the User Guide.
Parametersalpha : ﬂoat, optional
Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the ex-
act mathematical meaning of this parameter.‘‘alpha = 0‘‘ is equivalent to an ordinary
least square, solved by the LinearRegression object. For numerical reasons, us-
ing alpha = 0 with the Lasso object is not advised. Given this, you should use the
LinearRegression object.
l1_ratio : ﬂoat
The ElasticNet mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio =
0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. For 0 <
l1_ratio < 1, the penalty is a combination of L1 and L2.
ﬁt_intercept : bool
Whether the intercept should be estimated or not. If False, the data is assumed to be
already centered.
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is
ignored when fit_intercept is set to False. When the regressors are normalized,
note that this makes the hyperparameters learnt more robust and almost independent of
the number of samples. The same property is not valid for standardized data. However,
if you wish to standardize, please use preprocessing.StandardScaler before
calling fit on an estimator with normalize=False.
precompute : True | False | array-like
Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix
can also be passed as argument. For sparse input this option is always True to preserve
sparsity.
max_iter : int, optional
The maximum number of iterations
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
tol : ﬂoat, optional
29.18. sklearn.linear_model: Generalized Linear Models
1509
scikit-learn user guide, Release 0.18.2
The tolerance for the optimization: if the updates are smaller than tol, the optimization
code checks the dual gap for optimality and continues until it is smaller than tol.
warm_start : bool, optional
When set to True, reuse the solution of the previous call to ﬁt as initialization, other-
wise, just erase the previous solution.
positive : bool, optional
When set to True, forces the coefﬁcients to be positive.
selection : str, default ‘cyclic’
If set to ‘random’, a random coefﬁcient is updated every iteration rather than looping
over features sequentially by default. This (setting to ‘random’) often leads to signiﬁ-
cantly faster convergence especially when tol is higher than 1e-4.
random_state : int, RandomState instance, or None (default)
The seed of the pseudo random number generator that selects a random feature to up-
date. Useful only when selection is set to ‘random’.
Attributescoef_ : array, shape (n_features,) | (n_targets, n_features)
parameter vector (w in the cost function formula)
sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) | (n_targets, n_features)
sparse_coef_ is a readonly property derived from coef_
intercept_ : ﬂoat | array, shape (n_targets,)
independent term in decision function.
n_iter_ : array-like, shape (n_targets,)
number of iterations run by the coordinate descent solver to reach the speciﬁed toler-
ance.
See also:
SGDRegressorimplements elastic net regression with incremental training.
SGDClassifierimplements
logistic
regression
with
elastic
net
penalty
(SGDClassifier(loss="log",penalty="elasticnet")).
Notes
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19
fit(X, y[, check_input])
Fit model with coordinate descent.
get_params([deep])
Get parameters for this estimator.
path(X, y[, l1_ratio, eps, n_alphas, ...])
Compute elastic net path with coordinate descent
predict(X)
Predict using the linear model
Continued on next page
1510
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.140 – continued from previous page
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1.0,
l1_ratio=0.5,
ﬁt_intercept=True,
normalize=False,
precompute=False,
max_iter=1000,
copy_X=True,
tol=0.0001,
warm_start=False,
positive=False,
ran-
dom_state=None, selection=’cyclic’)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19
Decision function of the linear model
ParametersX : numpy array or scipy.sparse matrix of shape (n_samples, n_features)
ReturnsT : array, shape (n_samples,)
The predicted decision function
fit(X, y, check_input=True)
Fit model with coordinate descent.
ParametersX : ndarray or scipy.sparse matrix, (n_samples, n_features)
Data
y : ndarray, shape (n_samples,) or (n_samples, n_targets)
Target
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Notes
Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically
convert the X input as a Fortran-contiguous numpy array if necessary.
To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
static path(X, y,
l1_ratio=0.5,
eps=0.001,
n_alphas=100,
alphas=None,
precompute=’auto’,
Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, posi-
tive=False, check_input=True, **params)
Compute elastic net path with coordinate descent
The elastic net optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
29.18. sklearn.linear_model: Generalized Linear Models
1511
scikit-learn user guide, Release 0.18.2
1 / (2 * n_samples) * ||y - Xw||^2_2
+ alpha * l1_ratio * ||w||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2
For multi-output tasks it is:
(1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+ alpha * l1_ratio * ||W||_21
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory
duplication. If y is mono-output then X can be sparse.
y : ndarray, shape (n_samples,) or (n_samples, n_outputs)
Target values
l1_ratio : ﬂoat, optional
ﬂoat between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties).
l1_ratio=1 corresponds to the Lasso
eps : ﬂoat
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3
n_alphas : int, optional
Number of alphas along the regularization path
alphas : ndarray, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
Xy : array-like, optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
coef_init : array, shape (n_features, ) | None
The initial values of the coefﬁcients.
verbose : bool or integer
Amount of verbosity.
1512
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
params : kwargs
keyword arguments passed to the coordinate descent solver.
return_n_iter : bool
whether to return the number of iterations or not.
positive : bool, default False
If set to True, forces coefﬁcients to be positive.
check_input : bool, default True
Skip input validation checks, including the Gram matrix when provided assuming there
are handled by the caller when check_input=False.
Returnsalphas : array, shape (n_alphas,)
The alphas along the path where models are computed.
coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
Coefﬁcients along the path.
dual_gaps : array, shape (n_alphas,)
The dual gaps at the end of the optimization for each alpha.
n_iters : array-like, shape (n_alphas,)
The number of iterations taken by the coordinate descent optimizer to reach the speciﬁed
tolerance for each alpha. (Is returned when return_n_iter is set to True).
See also:
MultiTaskElasticNet, MultiTaskElasticNetCV, ElasticNet, ElasticNetCV
Notes
See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
29.18. sklearn.linear_model: Generalized Linear Models
1513
scikit-learn user guide, Release 0.18.2
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sparse_coef_
sparse representation of the ﬁtted coef_
Examples using sklearn.linear_model.ElasticNet
• Lasso and Elastic Net for Sparse Signals
• Train error vs Test error
29.18.4 sklearn.linear_model.HuberRegressor
class sklearn.linear_model.HuberRegressor(epsilon=1.35,
max_iter=100,
alpha=0.0001,
warm_start=False, ﬁt_intercept=True, tol=1e-05)
Linear regression model that is robust to outliers.
The Huber Regressor optimizes the squared loss for the samples where |(y -X'w) / sigma| <
epsilon and the absolute loss for the samples where |(y -X'w) / sigma| > epsilon, where w and
sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a
certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take
into account the fact that the different features of X may be of different scales.
This makes sure that the loss function is not heavily inﬂuenced by the outliers while not completely ignoring
their effect.
Read more in the User Guide
New in version 0.18.
Parametersepsilon : ﬂoat, greater than 1.0, default 1.35
The parameter epsilon controls the number of samples that should be classiﬁed as out-
liers. The smaller the epsilon, the more robust it is to outliers.
max_iter : int, default 100
Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for.
alpha : ﬂoat, default 0.0001
Regularization parameter.
warm_start : bool, default False
1514
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
This is useful if the stored attributes of a previously used model has to be reused. If set
to False, then the coefﬁcients will be rewritten for every call to ﬁt.
ﬁt_intercept : bool, default True
Whether or not to ﬁt the intercept. This can be set to False if the data is already centered
around the origin.
tol : ﬂoat, default 1e-5
The iteration will stop when max{|proj g_i | i = 1,...,n} <= tol where
pg_i is the i-th component of the projected gradient.
Attributescoef_ : array, shape (n_features,)
Features got by optimizing the Huber loss.
intercept_ : ﬂoat
Bias.
scale_ : ﬂoat
The value by which |y -X'w -c| is scaled down.
n_iter_ : int
Number of iterations that fmin_l_bfgs_b has run for. Not available if SciPy version is
0.9 and below.
outliers_: array, shape (n_samples,) :
A boolean mask which is set to True where the samples are identiﬁed as outliers.
References
[R33], [R34]
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y[, sample_weight])
Fit the model according to the given training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, ﬁt_intercept=True, tol=1e-
05)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
29.18. sklearn.linear_model: Generalized Linear Models
1515
scikit-learn user guide, Release 0.18.2
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y, sample_weight=None)
Fit the model according to the given training data.
ParametersX : array-like, shape (n_samples, n_features)
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
y : array-like, shape (n_samples,)
Target vector relative to X.
sample_weight : array-like, shape (n_samples,)
Weight given to each sample.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
1516
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.HuberRegressor
• HuberRegressor vs Ridge on dataset with strong outliers
• Robust linear estimator ﬁtting
29.18.5 sklearn.linear_model.Lars
class sklearn.linear_model.Lars(ﬁt_intercept=True,
verbose=False,
normalize=True,
precom-
pute=’auto’, n_nonzero_coefs=500, eps=2.2204460492503131e-
16, copy_X=True, ﬁt_path=True, positive=False)
Least Angle Regression model a.k.a. LAR
Read more in the User Guide.
Parametersn_nonzero_coefs : int, optional
Target number of non-zero coefﬁcients. Use np.inf for no limit.
ﬁt_intercept : boolean
Whether to calculate the intercept for this model. If set to false, no intercept will be
used in calculations (e.g. data is expected to be already centered).
positive : boolean (default=False)
Restrict coefﬁcients to be >= 0. Be aware that you might want to remove ﬁt_intercept
which is set True by default.
verbose : boolean or integer, optional
Sets the verbosity amount
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
eps : ﬂoat, optional
29.18. sklearn.linear_model: Generalized Linear Models
1517
scikit-learn user guide, Release 0.18.2
The machine-precision regularization in the computation of the Cholesky diagonal fac-
tors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some
iterative optimization-based algorithms, this parameter does not control the tolerance of
the optimization.
ﬁt_path : boolean
If True the full path is stored in the coef_path_ attribute. If you compute the solu-
tion for a large problem or many targets, setting fit_path to False will lead to a
speedup, especially with a small alpha.
Attributesalphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays
Maximum of covariances (in absolute value) at each iteration. n_alphas is either
n_nonzero_coefs or n_features, whichever is smaller.
active_ : list, length = n_alphas | list of n_targets such lists
Indices of active variables at the end of the path.
coef_path_ : array, shape (n_features, n_alphas + 1) | list of n_targets such arrays
The varying values of the coefﬁcients along the path. It is not present if the fit_path
parameter is False.
coef_ : array, shape (n_features,) or (n_targets, n_features)
Parameter vector (w in the formulation formula).
intercept_ : ﬂoat | array, shape (n_targets,)
Independent term in decision function.
n_iter_ : array-like or int
The number of iterations taken by lars_path to ﬁnd the grid of alphas for each target.
See also:
lars_path, LarsCV, sklearn.decomposition.sparse_encode
Examples
>>> from sklearn import linear_model
>>> clf = linear_model.Lars(n_nonzero_coefs=1)
>>> clf.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])
...
Lars(copy_X=True, eps=..., fit_intercept=True, fit_path=True,
n_nonzero_coefs=1, normalize=True, positive=False, precompute='auto',
verbose=False)
>>> print(clf.coef_)
[ 0. -1.11...]
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y[, Xy])
Fit the model using X, y as training data.
get_params([deep])
Get parameters for this estimator.
Continued on next page
1518
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.142 – continued from previous page
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(ﬁt_intercept=True,
verbose=False,
normalize=True,
precompute=’auto’,
n_nonzero_coefs=500,
eps=2.2204460492503131e-16,
copy_X=True,
ﬁt_path=True,
positive=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y, Xy=None)
Fit the model using X, y as training data.
ParametersX : array-like, shape (n_samples, n_features)
Training data.
y : array-like, shape (n_samples,) or (n_samples, n_targets)
Target values.
Xy : array-like, shape (n_samples,) or (n_samples, n_targets), optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
Returnsself : object
returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
29.18. sklearn.linear_model: Generalized Linear Models
1519
scikit-learn user guide, Release 0.18.2
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.18.6 sklearn.linear_model.Lasso
class sklearn.linear_model.Lasso(alpha=1.0,
ﬁt_intercept=True,
normalize=False,
precom-
pute=False,
copy_X=True,
max_iter=1000,
tol=0.0001,
warm_start=False, positive=False, random_state=None, selec-
tion=’cyclic’)
Linear Model trained with L1 prior as regularizer (aka the Lasso)
The optimization objective for Lasso is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
Technically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio=1.0
(no L2 penalty).
Read more in the User Guide.
Parametersalpha : ﬂoat, optional
Constant that multiplies the L1 term. Defaults to 1.0. alpha = 0 is equivalent to
an ordinary least square, solved by the LinearRegression object. For numerical
reasons, using alpha = 0 with the Lasso object is not advised. Given this, you
should use the LinearRegression object.
ﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
1520
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
If True, the regressors X will be normalized before regression. This parameter is
ignored when fit_intercept is set to False. When the regressors are normalized,
note that this makes the hyperparameters learnt more robust and almost independent of
the number of samples. The same property is not valid for standardized data. However,
if you wish to standardize, please use preprocessing.StandardScaler before
calling fit on an estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
precompute : True | False | array-like, default=False
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument. For sparse input this
option is always True to preserve sparsity.
max_iter : int, optional
The maximum number of iterations
tol : ﬂoat, optional
The tolerance for the optimization: if the updates are smaller than tol, the optimization
code checks the dual gap for optimality and continues until it is smaller than tol.
warm_start : bool, optional
When set to True, reuse the solution of the previous call to ﬁt as initialization, otherwise,
just erase the previous solution.
positive : bool, optional
When set to True, forces the coefﬁcients to be positive.
selection : str, default ‘cyclic’
If set to ‘random’, a random coefﬁcient is updated every iteration rather than looping
over features sequentially by default. This (setting to ‘random’) often leads to signiﬁ-
cantly faster convergence especially when tol is higher than 1e-4.
random_state : int, RandomState instance, or None (default)
The seed of the pseudo random number generator that selects a random feature to up-
date. Useful only when selection is set to ‘random’.
Attributescoef_ : array, shape (n_features,) | (n_targets, n_features)
parameter vector (w in the cost function formula)
sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) | (n_targets, n_features)
sparse_coef_ is a readonly property derived from coef_
intercept_ : ﬂoat | array, shape (n_targets,)
independent term in decision function.
n_iter_ : int | array-like, shape (n_targets,)
number of iterations run by the coordinate descent solver to reach the speciﬁed toler-
ance.
See also:
lars_path,
lasso_path,
LassoLars,
LassoCV,
LassoLarsCV,
sklearn.decomposition.sparse_encode
29.18. sklearn.linear_model: Generalized Linear Models
1521
scikit-learn user guide, Release 0.18.2
Notes
The algorithm used to ﬁt the model is coordinate descent.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
Examples
>>> from sklearn import linear_model
>>> clf = linear_model.Lasso(alpha=0.1)
>>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, positive=False, precompute=False, random_state=None,
selection='cyclic', tol=0.0001, warm_start=False)
>>> print(clf.coef_)
[ 0.85
0.
]
>>> print(clf.intercept_)
0.15
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19
fit(X, y[, check_input])
Fit model with coordinate descent.
get_params([deep])
Get parameters for this estimator.
path(X, y[, l1_ratio, eps, n_alphas, ...])
Compute elastic net path with coordinate descent
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1.0,
ﬁt_intercept=True,
normalize=False,
precompute=False,
copy_X=True,
max_iter=1000,
tol=0.0001,
warm_start=False,
positive=False,
random_state=None,
selection=’cyclic’)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19
Decision function of the linear model
ParametersX : numpy array or scipy.sparse matrix of shape (n_samples, n_features)
ReturnsT : array, shape (n_samples,)
The predicted decision function
fit(X, y, check_input=True)
Fit model with coordinate descent.
ParametersX : ndarray or scipy.sparse matrix, (n_samples, n_features)
Data
y : ndarray, shape (n_samples,) or (n_samples, n_targets)
Target
1522
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Notes
Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically
convert the X input as a Fortran-contiguous numpy array if necessary.
To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
static path(X, y,
l1_ratio=0.5,
eps=0.001,
n_alphas=100,
alphas=None,
precompute=’auto’,
Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, posi-
tive=False, check_input=True, **params)
Compute elastic net path with coordinate descent
The elastic net optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
1 / (2 * n_samples) * ||y - Xw||^2_2
+ alpha * l1_ratio * ||w||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2
For multi-output tasks it is:
(1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+ alpha * l1_ratio * ||W||_21
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory
duplication. If y is mono-output then X can be sparse.
y : ndarray, shape (n_samples,) or (n_samples, n_outputs)
Target values
l1_ratio : ﬂoat, optional
29.18. sklearn.linear_model: Generalized Linear Models
1523
scikit-learn user guide, Release 0.18.2
ﬂoat between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties).
l1_ratio=1 corresponds to the Lasso
eps : ﬂoat
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3
n_alphas : int, optional
Number of alphas along the regularization path
alphas : ndarray, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
Xy : array-like, optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
coef_init : array, shape (n_features, ) | None
The initial values of the coefﬁcients.
verbose : bool or integer
Amount of verbosity.
params : kwargs
keyword arguments passed to the coordinate descent solver.
return_n_iter : bool
whether to return the number of iterations or not.
positive : bool, default False
If set to True, forces coefﬁcients to be positive.
check_input : bool, default True
Skip input validation checks, including the Gram matrix when provided assuming there
are handled by the caller when check_input=False.
Returnsalphas : array, shape (n_alphas,)
The alphas along the path where models are computed.
coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
Coefﬁcients along the path.
dual_gaps : array, shape (n_alphas,)
The dual gaps at the end of the optimization for each alpha.
n_iters : array-like, shape (n_alphas,)
The number of iterations taken by the coordinate descent optimizer to reach the speciﬁed
tolerance for each alpha. (Is returned when return_n_iter is set to True).
1524
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
See also:
MultiTaskElasticNet, MultiTaskElasticNetCV, ElasticNet, ElasticNetCV
Notes
See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sparse_coef_
sparse representation of the ﬁtted coef_
Examples using sklearn.linear_model.Lasso
• Compressive sensing: tomography reconstruction with L1 prior (Lasso)
• Cross-validation on diabetes Dataset Exercise
• Lasso on dense and sparse data
• Lasso and Elastic Net for Sparse Signals
29.18. sklearn.linear_model: Generalized Linear Models
1525
scikit-learn user guide, Release 0.18.2
• Joint feature selection with multi-task Lasso
29.18.7 sklearn.linear_model.LassoLars
class sklearn.linear_model.LassoLars(alpha=1.0,
ﬁt_intercept=True,
verbose=False,
nor-
malize=True,
precompute=’auto’,
max_iter=500,
eps=2.2204460492503131e-16,
copy_X=True,
ﬁt_path=True, positive=False)
Lasso model ﬁt with Least Angle Regression a.k.a. Lars
It is a Linear Model trained with an L1 prior as regularizer.
The optimization objective for Lasso is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
Read more in the User Guide.
Parametersalpha : ﬂoat
Constant that multiplies the penalty term. Defaults to 1.0. alpha = 0 is equivalent
to an ordinary least square, solved by LinearRegression. For numerical reasons,
using alpha = 0 with the LassoLars object is not advised and you should prefer the
LinearRegression object.
ﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
positive : boolean (default=False)
Restrict coefﬁcients to be >= 0. Be aware that you might want to remove ﬁt_intercept
which is set True by default. Under the positive restriction the model coefﬁcients will
not converge to the ordinary-least-squares solution for small values of alpha. Only co-
efﬁcients up to the smallest alpha value (alphas_[alphas_ > 0.].min() when
ﬁt_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congru-
ence with the solution of the coordinate descent Lasso estimator.
verbose : boolean or integer, optional
Sets the verbosity amount
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
max_iter : integer, optional
1526
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Maximum number of iterations to perform.
eps : ﬂoat, optional
The machine-precision regularization in the computation of the Cholesky diagonal fac-
tors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some
iterative optimization-based algorithms, this parameter does not control the tolerance of
the optimization.
ﬁt_path : boolean
If True the full path is stored in the coef_path_ attribute. If you compute the
solution for a large problem or many targets, setting fit_path to False will lead to
a speedup, especially with a small alpha.
Attributesalphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays
Maximum of covariances (in absolute value) at each iteration. n_alphas is either
max_iter, n_features, or the number of nodes in the path with correlation greater
than alpha, whichever is smaller.
active_ : list, length = n_alphas | list of n_targets such lists
Indices of active variables at the end of the path.
coef_path_ : array, shape (n_features, n_alphas + 1) or list
If a list is passed it’s expected to be one of n_targets such arrays. The varying values of
the coefﬁcients along the path. It is not present if the fit_path parameter is False.
coef_ : array, shape (n_features,) or (n_targets, n_features)
Parameter vector (w in the formulation formula).
intercept_ : ﬂoat | array, shape (n_targets,)
Independent term in decision function.
n_iter_ : array-like or int.
The number of iterations taken by lars_path to ﬁnd the grid of alphas for each target.
See also:
lars_path, lasso_path, Lasso, LassoCV, LassoLarsCV, sklearn.decomposition.sparse_encode
Examples
>>> from sklearn import linear_model
>>> clf = linear_model.LassoLars(alpha=0.01)
>>> clf.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])
...
LassoLars(alpha=0.01, copy_X=True, eps=..., fit_intercept=True,
fit_path=True, max_iter=500, normalize=True, positive=False,
precompute='auto', verbose=False)
>>> print(clf.coef_)
[ 0.
-0.963257...]
Methods
29.18. sklearn.linear_model: Generalized Linear Models
1527
scikit-learn user guide, Release 0.18.2
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y[, Xy])
Fit the model using X, y as training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1.0,
ﬁt_intercept=True,
verbose=False,
normalize=True,
precompute=’auto’,
max_iter=500,
eps=2.2204460492503131e-16,
copy_X=True,
ﬁt_path=True,
posi-
tive=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y, Xy=None)
Fit the model using X, y as training data.
ParametersX : array-like, shape (n_samples, n_features)
Training data.
y : array-like, shape (n_samples,) or (n_samples, n_targets)
Target values.
Xy : array-like, shape (n_samples,) or (n_samples, n_targets), optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
Returnsself : object
returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
1528
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.18.8 sklearn.linear_model.LinearRegression
class sklearn.linear_model.LinearRegression(ﬁt_intercept=True,
normalize=False,
copy_X=True, n_jobs=1)
Ordinary least squares Linear Regression.
Parametersﬁt_intercept : boolean, optional
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
n_jobs : int, optional, default 1
29.18. sklearn.linear_model: Generalized Linear Models
1529
scikit-learn user guide, Release 0.18.2
The number of jobs to use for the computation. If -1 all CPUs are used. This will only
provide speedup for n_targets > 1 and sufﬁcient large problems.
Attributescoef_ : array, shape (n_features, ) or (n_targets, n_features)
Estimated coefﬁcients for the linear regression problem. If multiple targets are passed
during the ﬁt (y 2D), this is a 2D array of shape (n_targets, n_features), while if only
one target is passed, this is a 1D array of length n_features.
residues_ : array, shape (n_targets,) or (1,) or empty
Sum of residuals. Squared Euclidean 2-norm for each target passed during the ﬁt. If
the linear regression problem is under-determined (the number of linearly independent
rows of the training matrix is less than its number of linearly independent columns), this
is an empty array. If the target vector passed during the ﬁt is 1-dimensional, this is a (1,)
shape array.
New in version 0.18.
intercept_ : array
Independent term in the linear model.
Notes
From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as
a predictor object.
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y[, sample_weight])
Fit linear model.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(ﬁt_intercept=True, normalize=False, copy_X=True, n_jobs=1)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y, sample_weight=None)
Fit linear model.
ParametersX : numpy array or sparse matrix of shape [n_samples,n_features]
1530
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Training data
y : numpy array of shape [n_samples, n_targets]
Target values
sample_weight : numpy array of shape [n_samples]
Individual weights for each sample
New in version 0.17: parameter sample_weight support to LinearRegression.
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
residues_
DEPRECATED: residues_ is deprecated and will be removed in 0.19
Get the residues of the ﬁtted model.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
29.18. sklearn.linear_model: Generalized Linear Models
1531
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.LinearRegression
• Plotting Cross-Validated Predictions
• Isotonic Regression
• Face completion with a multi-output estimators
• Automatic Relevance Determination Regression (ARD)
• Bayesian Ridge Regression
• Logistic function
• Linear Regression Example
• Sparsity Example: Fitting only features 1 and 2
• Ordinary Least Squares and Ridge Regression Variance
• Robust linear model estimation using RANSAC
• Robust linear estimator ﬁtting
• Theil-Sen Regression
• Underﬁtting vs. Overﬁtting
29.18.9 sklearn.linear_model.LogisticRegression
class sklearn.linear_model.LogisticRegression(penalty=’l2’,
dual=False,
tol=0.0001,
C=1.0,
ﬁt_intercept=True,
inter-
cept_scaling=1,
class_weight=None,
random_state=None,
solver=’liblinear’,
max_iter=100,
multi_class=’ovr’,
ver-
bose=0, warm_start=False, n_jobs=1)
Logistic Regression (aka logit, MaxEnt) classiﬁer.
In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the ‘multi_class’ option is
set to ‘ovr’, and uses the cross- entropy loss if the ‘multi_class’ option is set to ‘multinomial’. (Currently the
‘multinomial’ option is supported only by the ‘lbfgs’, ‘sag’ and ‘newton-cg’ solvers.)
This class implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’ and ‘lbfgs’
solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit
ﬂoats for optimal performance; any other input format will be converted (and copied).
The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization with primal formulation. The ‘liblin-
ear’ solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.
Read more in the User Guide.
Parameterspenalty : str, ‘l1’ or ‘l2’, default: ‘l2’
1532
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’
solvers support only l2 penalties.
dual : bool, default: False
Dual or primal formulation. Dual formulation is only implemented for l2 penalty with
liblinear solver. Prefer dual=False when n_samples > n_features.
C : ﬂoat, default: 1.0
Inverse of regularization strength; must be a positive ﬂoat. Like in support vector ma-
chines, smaller values specify stronger regularization.
ﬁt_intercept : bool, default: True
Speciﬁes if a constant (a.k.a. bias or intercept) should be added to the decision function.
intercept_scaling : ﬂoat, default 1.
Useful only when the solver ‘liblinear’ is used and self.ﬁt_intercept is set to True. In this
case, x becomes [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value
equal to intercept_scaling is appended to the instance vector. The intercept becomes
intercept_scaling * synthetic_feature_weight.
Note! the synthetic feature weight is subject to l1/l2 regularization as all other features.
To lessen the effect of regularization on synthetic feature weight (and therefore on the
intercept) intercept_scaling has to be increased.
class_weight : dict or ‘balanced’, default: None
Weights associated with classes in the form {class_label:
weight}. If not
given, all classes are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y)).
Note that these weights will be multiplied with sample_weight (passed through the ﬁt
method) if sample_weight is speciﬁed.
New
in
version
0.17:
class_weight=’balanced’
instead
of
deprecated
class_weight=’auto’.
max_iter : int, default: 100
Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations
taken for the solvers to converge.
random_state : int seed, RandomState instance, default: None
The seed of the pseudo random number generator to use when shufﬂing the data. Used
only in solvers ‘sag’ and ‘liblinear’.
solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}, default: ‘liblinear’
Algorithm to use in the optimization problem.
•For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ isfaster
for
large
ones.
•For multiclass problems, only ‘newton-cg’, ‘sag’ and ‘lbfgs’ handlemultinomial
loss; ‘liblinear’ is limited to one-versus-rest schemes.
•‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty.
29.18. sklearn.linear_model: Generalized Linear Models
1533
scikit-learn user guide, Release 0.18.2
Note that ‘sag’ fast convergence is only guaranteed on features with approximately the
same scale. You can preprocess the data with a scaler from sklearn.preprocessing.
New in version 0.17: Stochastic Average Gradient descent solver.
tol : ﬂoat, default: 1e-4
Tolerance for stopping criteria.
multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’
Multiclass option can be either ‘ovr’ or ‘multinomial’. If the option chosen is ‘ovr’,
then a binary problem is ﬁt for each label. Else the loss minimised is the multinomial
loss ﬁt across the entire probability distribution. Works only for the ‘newton-cg’, ‘sag’
and ‘lbfgs’ solver.
New in version 0.18: Stochastic Average Gradient descent solver for ‘multinomial’
case.
verbose : int, default: 0
For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.
warm_start : bool, default: False
When set to True, reuse the solution of the previous call to ﬁt as initialization, otherwise,
just erase the previous solution. Useless for liblinear solver.
New in version 0.17: warm_start to support lbfgs, newton-cg, sag solvers.
n_jobs : int, default: 1
Number of CPU cores used during the cross-validation loop. If given a value of -1, all
cores are used.
Attributescoef_ : array, shape (n_classes, n_features)
Coefﬁcient of the features in the decision function.
intercept_ : array, shape (n_classes,)
Intercept (a.k.a. bias) added to the decision function. If ﬁt_intercept is set to False, the
intercept is set to zero.
n_iter_ : array, shape (n_classes,) or (1, )
Actual number of iterations for all classes. If binary or multinomial, it returns only 1
element. For liblinear solver, only the maximum number of iteration across all classes
is given.
See also:
SGDClassifierincrementally trained logistic regression (when given the parameter loss="log").
sklearn.svm.LinearSVClearns SVM models using the same algorithm.
Notes
The underlying C implementation uses a random number generator to select features when ﬁtting the model.
It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a
smaller tol parameter.
Predict output may not match that of standalone liblinear in certain cases. See differences from liblinear in the
narrative documentation.
1534
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
LIBLINEAR – A Library for Large Linear Classiﬁcationhttp://www.csie.ntu.edu.tw/~cjlin/liblinear/
SAG – Mark Schmidt, Nicolas Le Roux, and Francis BachMinimizing Finite Sums with the Stochastic Av-
erage Gradient https://hal.inria.fr/hal-00860051/document
Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descentmethods for logistic re-
gression and maximum entropy models. Machine Learning 85(1-2):41-75. http://www.csie.ntu.edu.tw/
~cjlin/papers/maxent_dual.pdf
Methods
decision_function(X)
Predict conﬁdence scores for samples.
densify()
Convert coefﬁcient matrix to dense array format.
fit(X, y[, sample_weight])
Fit the model according to the given training data.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class labels for samples in X.
predict_log_proba(X)
Log of probability estimates.
predict_proba(X)
Probability estimates.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
sparsify()
Convert coefﬁcient matrix to sparse format.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(penalty=’l2’, dual=False, tol=0.0001, C=1.0, ﬁt_intercept=True, intercept_scaling=1,
class_weight=None,
random_state=None,
solver=’liblinear’,
max_iter=100,
multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1)
decision_function(X)
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
densify()
Convert coefﬁcient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is
required for ﬁtting, so calling this method is only required on models that have previously been sparsiﬁed;
otherwise, it is a no-op.
Returnsself: estimator :
fit(X, y, sample_weight=None)
Fit the model according to the given training data.
29.18. sklearn.linear_model: Generalized Linear Models
1535
scikit-learn user guide, Release 0.18.2
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training vector, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples,)
Target vector relative to X.
sample_weight : array-like, shape (n_samples,) optional
Array of weights that are assigned to individual samples. If not provided, then each
sample is given unit weight.
New in version 0.17: sample_weight support to LogisticRegression.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class labels for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
predict_log_proba(X)
Log of probability estimates.
The returned estimates for all classes are ordered by the label of classes.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsT : array-like, shape = [n_samples, n_classes]
1536
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returns the log-probability of the sample for each class in the model, where classes are
ordered as they are in self.classes_.
predict_proba(X)
Probability estimates.
The returned estimates for all classes are ordered by the label of classes.
For a multi_class problem, if multi_class is set to be “multinomial” the softmax function is used to ﬁnd
the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of
each class assuming it to be positive using the logistic function. and normalize these values across all the
classes.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsT : array-like, shape = [n_samples, n_classes]
Returns the probability of the sample for each class in the model, where classes are
ordered as they are in self.classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sparsify()
Convert coefﬁcient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more
memory- and storage-efﬁcient than the usual numpy.ndarray representation.
The intercept_ member is not converted.
Returnsself: estimator :
29.18. sklearn.linear_model: Generalized Linear Models
1537
scikit-learn user guide, Release 0.18.2
Notes
For non-sparse models, i.e. when there are not many zeros in coef_, this may actually increase memory
usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be
computed with (coef_ == 0).sum(), must be more than 50% for this to provide signiﬁcant beneﬁts.
After calling this method, further ﬁtting with the partial_ﬁt method (if any) will not work until you call
densify.
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.linear_model.LogisticRegression
• Pipelining: chaining a PCA and a logistic regression
• Probability Calibration curves
• Comparison of Calibration of Classiﬁers
• Plot classiﬁcation probability
• Feature transformations with ensembles of trees
• Plot class probabilities calculated by the VotingClassiﬁer
• Digits Classiﬁcation Exercise
• Logistic Regression 3-class Classiﬁer
• Logistic function
• L1 Penalty and Sparsity in Logistic Regression
• Plot multinomial and One-vs-Rest Logistic Regression
• Path with L1- Logistic Regression
• Comparing various online solvers
• Restricted Boltzmann Machine features for digit classiﬁcation
1538
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.18.10 sklearn.linear_model.MultiTaskLasso
class sklearn.linear_model.MultiTaskLasso(alpha=1.0, ﬁt_intercept=True, normalize=False,
copy_X=True,
max_iter=1000,
tol=0.0001,
warm_start=False,
random_state=None,
selec-
tion=’cyclic’)
Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer
The optimization objective for Lasso is:
(1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
Parametersalpha : ﬂoat, optional
Constant that multiplies the L1/L2 term. Defaults to 1.0
ﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is
ignored when fit_intercept is set to False. When the regressors are normalized,
note that this makes the hyperparameters learnt more robust and almost independent of
the number of samples. The same property is not valid for standardized data. However,
if you wish to standardize, please use preprocessing.StandardScaler before
calling fit on an estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
max_iter : int, optional
The maximum number of iterations
tol : ﬂoat, optional
The tolerance for the optimization: if the updates are smaller than tol, the optimization
code checks the dual gap for optimality and continues until it is smaller than tol.
warm_start : bool, optional
When set to True, reuse the solution of the previous call to ﬁt as initialization, other-
wise, just erase the previous solution.
selection : str, default ‘cyclic’
If set to ‘random’, a random coefﬁcient is updated every iteration rather than looping
over features sequentially by default. This (setting to ‘random’) often leads to signiﬁ-
cantly faster convergence especially when tol is higher than 1e-4
random_state : int, RandomState instance, or None (default)
29.18. sklearn.linear_model: Generalized Linear Models
1539
scikit-learn user guide, Release 0.18.2
The seed of the pseudo random number generator that selects a random feature to up-
date. Useful only when selection is set to ‘random’.
Attributescoef_ : array, shape (n_tasks, n_features)
parameter vector (W in the cost function formula)
intercept_ : array, shape (n_tasks,)
independent term in decision function.
n_iter_ : int
number of iterations run by the coordinate descent solver to reach the speciﬁed toler-
ance.
See also:
Lasso, MultiTaskElasticNet
Notes
The algorithm used to ﬁt the model is coordinate descent.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
Examples
>>> from sklearn import linear_model
>>> clf = linear_model.MultiTaskLasso(alpha=0.1)
>>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])
MultiTaskLasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, random_state=None, selection='cyclic', tol=0.0001,
warm_start=False)
>>> print(clf.coef_)
[[ 0.89393398
0.
]
[ 0.89393398
0.
]]
>>> print(clf.intercept_)
[ 0.10606602
0.10606602]
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19
fit(X, y)
Fit MultiTaskLasso model with coordinate descent
get_params([deep])
Get parameters for this estimator.
path(X, y[, l1_ratio, eps, n_alphas, ...])
Compute elastic net path with coordinate descent
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1.0,
ﬁt_intercept=True,
normalize=False,
copy_X=True,
max_iter=1000,
tol=0.0001, warm_start=False, random_state=None, selection=’cyclic’)
1540
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19
Decision function of the linear model
ParametersX : numpy array or scipy.sparse matrix of shape (n_samples, n_features)
ReturnsT : array, shape (n_samples,)
The predicted decision function
fit(X, y)
Fit MultiTaskLasso model with coordinate descent
ParametersX : ndarray, shape (n_samples, n_features)
Data
y : ndarray, shape (n_samples, n_tasks)
Target
Notes
Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically
convert the X input as a Fortran-contiguous numpy array if necessary.
To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute=’auto’, Xy=None,
copy_X=True,
coef_init=None,
verbose=False,
return_n_iter=False,
positive=False,
check_input=True, **params)
Compute elastic net path with coordinate descent
The elastic net optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
1 / (2 * n_samples) * ||y - Xw||^2_2
+ alpha * l1_ratio * ||w||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2
For multi-output tasks it is:
(1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+ alpha * l1_ratio * ||W||_21
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
Where:
29.18. sklearn.linear_model: Generalized Linear Models
1541
scikit-learn user guide, Release 0.18.2
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory
duplication. If y is mono-output then X can be sparse.
y : ndarray, shape (n_samples,) or (n_samples, n_outputs)
Target values
l1_ratio : ﬂoat, optional
ﬂoat between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties).
l1_ratio=1 corresponds to the Lasso
eps : ﬂoat
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3
n_alphas : int, optional
Number of alphas along the regularization path
alphas : ndarray, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
Xy : array-like, optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
coef_init : array, shape (n_features, ) | None
The initial values of the coefﬁcients.
verbose : bool or integer
Amount of verbosity.
params : kwargs
keyword arguments passed to the coordinate descent solver.
return_n_iter : bool
whether to return the number of iterations or not.
positive : bool, default False
If set to True, forces coefﬁcients to be positive.
check_input : bool, default True
1542
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Skip input validation checks, including the Gram matrix when provided assuming there
are handled by the caller when check_input=False.
Returnsalphas : array, shape (n_alphas,)
The alphas along the path where models are computed.
coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
Coefﬁcients along the path.
dual_gaps : array, shape (n_alphas,)
The dual gaps at the end of the optimization for each alpha.
n_iters : array-like, shape (n_alphas,)
The number of iterations taken by the coordinate descent optimizer to reach the speciﬁed
tolerance for each alpha. (Is returned when return_n_iter is set to True).
See also:
MultiTaskElasticNet, MultiTaskElasticNetCV, ElasticNet, ElasticNetCV
Notes
See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
29.18. sklearn.linear_model: Generalized Linear Models
1543
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sparse_coef_
sparse representation of the ﬁtted coef_
Examples using sklearn.linear_model.MultiTaskLasso
• Joint feature selection with multi-task Lasso
29.18.11 sklearn.linear_model.MultiTaskElasticNet
class sklearn.linear_model.MultiTaskElasticNet(alpha=1.0,
l1_ratio=0.5,
ﬁt_intercept=True,
normalize=False,
copy_X=True, max_iter=1000, tol=0.0001,
warm_start=False,
random_state=None,
selection=’cyclic’)
Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer
The optimization objective for MultiTaskElasticNet is:
(1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+ alpha * l1_ratio * ||W||_21
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
Parametersalpha : ﬂoat, optional
Constant that multiplies the L1/L2 term. Defaults to 1.0
l1_ratio : ﬂoat
The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 0 the penalty is
an L1/L2 penalty. For l1_ratio = 1 it is an L1 penalty. For 0 < l1_ratio < 1, the
penalty is a combination of L1/L2 and L2.
ﬁt_intercept : boolean
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is
ignored when fit_intercept is set to False. When the regressors are normalized,
note that this makes the hyperparameters learnt more robust and almost independent of
the number of samples. The same property is not valid for standardized data. However,
1544
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
if you wish to standardize, please use preprocessing.StandardScaler before
calling fit on an estimator with normalize=False.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
max_iter : int, optional
The maximum number of iterations
tol : ﬂoat, optional
The tolerance for the optimization: if the updates are smaller than tol, the optimization
code checks the dual gap for optimality and continues until it is smaller than tol.
warm_start : bool, optional
When set to True, reuse the solution of the previous call to ﬁt as initialization, other-
wise, just erase the previous solution.
selection : str, default ‘cyclic’
If set to ‘random’, a random coefﬁcient is updated every iteration rather than looping
over features sequentially by default. This (setting to ‘random’) often leads to signiﬁ-
cantly faster convergence especially when tol is higher than 1e-4.
random_state : int, RandomState instance, or None (default)
The seed of the pseudo random number generator that selects a random feature to up-
date. Useful only when selection is set to ‘random’.
Attributesintercept_ : array, shape (n_tasks,)
Independent term in decision function.
coef_ : array, shape (n_tasks, n_features)
Parameter vector (W in the cost function formula). If a 1D y is passed in at ﬁt (non
multi-task usage), coef_ is then a 1D array
n_iter_ : int
number of iterations run by the coordinate descent solver to reach the speciﬁed toler-
ance.
See also:
ElasticNet, MultiTaskLasso
Notes
The algorithm used to ﬁt the model is coordinate descent.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
Examples
29.18. sklearn.linear_model: Generalized Linear Models
1545
scikit-learn user guide, Release 0.18.2
>>> from sklearn import linear_model
>>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)
>>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])
...
MultiTaskElasticNet(alpha=0.1, copy_X=True, fit_intercept=True,
l1_ratio=0.5, max_iter=1000, normalize=False, random_state=None,
selection='cyclic', tol=0.0001, warm_start=False)
>>> print(clf.coef_)
[[ 0.45663524
0.45612256]
[ 0.45663524
0.45612256]]
>>> print(clf.intercept_)
[ 0.0872422
0.0872422]
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19
fit(X, y)
Fit MultiTaskLasso model with coordinate descent
get_params([deep])
Get parameters for this estimator.
path(X, y[, l1_ratio, eps, n_alphas, ...])
Compute elastic net path with coordinate descent
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1.0,
l1_ratio=0.5,
ﬁt_intercept=True,
normalize=False,
copy_X=True,
max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection=’cyclic’)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19
Decision function of the linear model
ParametersX : numpy array or scipy.sparse matrix of shape (n_samples, n_features)
ReturnsT : array, shape (n_samples,)
The predicted decision function
fit(X, y)
Fit MultiTaskLasso model with coordinate descent
ParametersX : ndarray, shape (n_samples, n_features)
Data
y : ndarray, shape (n_samples, n_tasks)
Target
Notes
Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically
convert the X input as a Fortran-contiguous numpy array if necessary.
To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format.
1546
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute=’auto’, Xy=None,
copy_X=True,
coef_init=None,
verbose=False,
return_n_iter=False,
positive=False,
check_input=True, **params)
Compute elastic net path with coordinate descent
The elastic net optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
1 / (2 * n_samples) * ||y - Xw||^2_2
+ alpha * l1_ratio * ||w||_1
+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2
For multi-output tasks it is:
(1 / (2 * n_samples)) * ||Y - XW||^Fro_2
+ alpha * l1_ratio * ||W||_21
+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
ParametersX : {array-like}, shape (n_samples, n_features)
Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory
duplication. If y is mono-output then X can be sparse.
y : ndarray, shape (n_samples,) or (n_samples, n_outputs)
Target values
l1_ratio : ﬂoat, optional
ﬂoat between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties).
l1_ratio=1 corresponds to the Lasso
eps : ﬂoat
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3
n_alphas : int, optional
Number of alphas along the regularization path
alphas : ndarray, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
29.18. sklearn.linear_model: Generalized Linear Models
1547
scikit-learn user guide, Release 0.18.2
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
Xy : array-like, optional
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
coef_init : array, shape (n_features, ) | None
The initial values of the coefﬁcients.
verbose : bool or integer
Amount of verbosity.
params : kwargs
keyword arguments passed to the coordinate descent solver.
return_n_iter : bool
whether to return the number of iterations or not.
positive : bool, default False
If set to True, forces coefﬁcients to be positive.
check_input : bool, default True
Skip input validation checks, including the Gram matrix when provided assuming there
are handled by the caller when check_input=False.
Returnsalphas : array, shape (n_alphas,)
The alphas along the path where models are computed.
coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
Coefﬁcients along the path.
dual_gaps : array, shape (n_alphas,)
The dual gaps at the end of the optimization for each alpha.
n_iters : array-like, shape (n_alphas,)
The number of iterations taken by the coordinate descent optimizer to reach the speciﬁed
tolerance for each alpha. (Is returned when return_n_iter is set to True).
See also:
MultiTaskElasticNet, MultiTaskElasticNetCV, ElasticNet, ElasticNetCV
Notes
See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
1548
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sparse_coef_
sparse representation of the ﬁtted coef_
29.18.12 sklearn.linear_model.OrthogonalMatchingPursuit
class sklearn.linear_model.OrthogonalMatchingPursuit(n_nonzero_coefs=None,
tol=None, ﬁt_intercept=True, nor-
malize=True, precompute=’auto’)
Orthogonal Matching Pursuit model (OMP)
Parametersn_nonzero_coefs : int, optional
Desired number of non-zero entries in the solution. If None (by default) this value is set
to 10% of n_features.
tol : ﬂoat, optional
Maximum norm of the residual. If not None, overrides n_nonzero_coefs.
ﬁt_intercept : boolean, optional
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
normalize : boolean, optional, default False
29.18. sklearn.linear_model: Generalized Linear Models
1549
scikit-learn user guide, Release 0.18.2
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
precompute : {True, False, ‘auto’}, default ‘auto’
Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves
performance when n_targets or n_samples is very large. Note that if you already have
such matrices, you can pass them directly to the ﬁt method.
Read more in the :ref:‘User Guide <omp>‘. :
Attributescoef_ : array, shape (n_features,) or (n_features, n_targets)
parameter vector (w in the formula)
intercept_ : ﬂoat or array, shape (n_targets,)
independent term in decision function.
n_iter_ : int or array-like
Number of active features across every target.
See also:
orthogonal_mp,
orthogonal_mp_gram,
lars_path,
Lars,
LassoLars,
decomposition.sparse_encode
Notes
Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang, Matching pursuits with time-frequency
dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)
This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efﬁcient Implementation of
the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008.
http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y)
Fit the model using X, y as training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_nonzero_coefs=None,
tol=None,
ﬁt_intercept=True,
normalize=True,
precom-
pute=’auto’)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
1550
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit the model using X, y as training data.
ParametersX : array-like, shape (n_samples, n_features)
Training data.
y : array-like, shape (n_samples,) or (n_samples, n_targets)
Target values.
Returnsself : object
returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
29.18. sklearn.linear_model: Generalized Linear Models
1551
scikit-learn user guide, Release 0.18.2
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.OrthogonalMatchingPursuit
• Orthogonal Matching Pursuit
29.18.13 sklearn.linear_model.PassiveAggressiveClassiﬁer
class sklearn.linear_model.PassiveAggressiveClassifier(C=1.0,
ﬁt_intercept=True,
n_iter=5,
shufﬂe=True,
verbose=0,
loss=’hinge’,
n_jobs=1, random_state=None,
warm_start=False,
class_weight=None)
Passive Aggressive Classiﬁer
Read more in the User Guide.
ParametersC : ﬂoat
Maximum step size (regularization). Defaults to 1.0.
ﬁt_intercept : bool, default=False
Whether the intercept should be estimated or not. If False, the data is assumed to be
already centered.
n_iter : int, optional
The number of passes over the training data (aka epochs). Defaults to 5.
shufﬂe : bool, default=True
Whether or not the training data should be shufﬂed after each epoch.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data.
verbose : integer, optional
The verbosity level
n_jobs : integer, optional
The number of CPUs to use to do the OVA (One Versus All, for multi-class problems)
computation. -1 means ‘all CPUs’. Defaults to 1.
loss : string, optional
The loss function to be used:
hinge:
equivalent to PA-I in the reference paper.
squared_hinge: equivalent to PA-II in the reference paper.
warm_start : bool, optional
1552
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
When set to True, reuse the solution of the previous call to ﬁt as initialization, otherwise,
just erase the previous solution.
class_weight : dict, {class_label: weight} or “balanced” or None, optional
Preset for the class_weight ﬁt parameter.
Weights associated with classes. If not given, all classes are supposed to have weight
one.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y))
New in version 0.17: parameter class_weight to automatically weight samples.
Attributescoef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes, n_features]
Weights assigned to the features.
intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]
Constants in decision function.
See also:
SGDClassifier, Perceptron
References
Online Passive-Aggressive Algorithms <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.
pdf> K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)
Methods
decision_function(X)
Predict conﬁdence scores for samples.
densify()
Convert coefﬁcient matrix to dense array format.
fit(X, y[, coef_init, intercept_init])
Fit linear model with Passive Aggressive algorithm.
get_params([deep])
Get parameters for this estimator.
partial_fit(X, y[, classes])
Fit linear model with Passive Aggressive algorithm.
predict(X)
Predict class labels for samples in X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*args, \*\*kwargs)
sparsify()
Convert coefﬁcient matrix to sparse format.
__init__(C=1.0, ﬁt_intercept=True, n_iter=5, shufﬂe=True, verbose=0, loss=’hinge’, n_jobs=1, ran-
dom_state=None, warm_start=False, class_weight=None)
decision_function(X)
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
29.18. sklearn.linear_model: Generalized Linear Models
1553
scikit-learn user guide, Release 0.18.2
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
densify()
Convert coefﬁcient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is
required for ﬁtting, so calling this method is only required on models that have previously been sparsiﬁed;
otherwise, it is a no-op.
Returnsself: estimator :
fit(X, y, coef_init=None, intercept_init=None)
Fit linear model with Passive Aggressive algorithm.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training data
y : numpy array of shape [n_samples]
Target values
coef_init : array, shape = [n_classes,n_features]
The initial coefﬁcients to warm-start the optimization.
intercept_init : array, shape = [n_classes]
The initial intercept to warm-start the optimization.
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y, classes=None)
Fit linear model with Passive Aggressive algorithm.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Subset of the training data
y : numpy array of shape [n_samples]
Subset of the target values
classes : array, shape = [n_classes]
Classes across all calls to partial_ﬁt. Can be obtained by via np.unique(y_all), where
y_all is the target vector of the entire dataset. This argument is required for the ﬁrst call
to partial_ﬁt and can be omitted in the subsequent calls. Note that y doesn’t need to
contain all labels in classes.
Returnsself : returns an instance of self.
predict(X)
Predict class labels for samples in X.
1554
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
sparsify()
Convert coefﬁcient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more
memory- and storage-efﬁcient than the usual numpy.ndarray representation.
The intercept_ member is not converted.
Returnsself: estimator :
Notes
For non-sparse models, i.e. when there are not many zeros in coef_, this may actually increase memory
usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be
computed with (coef_ == 0).sum(), must be more than 50% for this to provide signiﬁcant beneﬁts.
After calling this method, further ﬁtting with the partial_ﬁt method (if any) will not work until you call
densify.
Examples using sklearn.linear_model.PassiveAggressiveClassifier
• Out-of-core classiﬁcation of text documents
• Comparing various online solvers
• Classiﬁcation of text documents using sparse features
29.18. sklearn.linear_model: Generalized Linear Models
1555
scikit-learn user guide, Release 0.18.2
29.18.14 sklearn.linear_model.PassiveAggressiveRegressor
class sklearn.linear_model.PassiveAggressiveRegressor(C=1.0,
ﬁt_intercept=True,
n_iter=5,
shuf-
ﬂe=True,
verbose=0,
loss=’epsilon_insensitive’,
ep-
silon=0.1, random_state=None,
warm_start=False)
Passive Aggressive Regressor
Read more in the User Guide.
ParametersC : ﬂoat
Maximum step size (regularization). Defaults to 1.0.
epsilon : ﬂoat
If the difference between the current prediction and the correct label is below this thresh-
old, the model is not updated.
ﬁt_intercept : bool
Whether the intercept should be estimated or not. If False, the data is assumed to be
already centered. Defaults to True.
n_iter : int, optional
The number of passes over the training data (aka epochs). Defaults to 5.
shufﬂe : bool, default=True
Whether or not the training data should be shufﬂed after each epoch.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data.
verbose : integer, optional
The verbosity level
loss : string, optional
The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference
paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper.
warm_start : bool, optional
When set to True, reuse the solution of the previous call to ﬁt as initialization, otherwise,
just erase the previous solution.
Attributescoef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes, n_features]
Weights assigned to the features.
intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]
Constants in decision function.
See also:
SGDRegressor
1556
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
Online Passive-Aggressive Algorithms <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.
pdf> K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
densify()
Convert coefﬁcient matrix to dense array format.
fit(X, y[, coef_init, intercept_init])
Fit linear model with Passive Aggressive algorithm.
get_params([deep])
Get parameters for this estimator.
partial_fit(X, y)
Fit linear model with Passive Aggressive algorithm.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*args, \*\*kwargs)
sparsify()
Convert coefﬁcient matrix to sparse format.
__init__(C=1.0, ﬁt_intercept=True, n_iter=5, shufﬂe=True, verbose=0, loss=’epsilon_insensitive’,
epsilon=0.1, random_state=None, warm_start=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Returnsarray, shape (n_samples,) :
Predicted target values per element in X.
densify()
Convert coefﬁcient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is
required for ﬁtting, so calling this method is only required on models that have previously been sparsiﬁed;
otherwise, it is a no-op.
Returnsself: estimator :
fit(X, y, coef_init=None, intercept_init=None)
Fit linear model with Passive Aggressive algorithm.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training data
y : numpy array of shape [n_samples]
Target values
coef_init : array, shape = [n_features]
The initial coefﬁcients to warm-start the optimization.
intercept_init : array, shape = [1]
The initial intercept to warm-start the optimization.
29.18. sklearn.linear_model: Generalized Linear Models
1557
scikit-learn user guide, Release 0.18.2
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y)
Fit linear model with Passive Aggressive algorithm.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Subset of training data
y : numpy array of shape [n_samples]
Subset of target values
Returnsself : returns an instance of self.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Returnsarray, shape (n_samples,) :
Predicted target values per element in X.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
sparsify()
Convert coefﬁcient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more
memory- and storage-efﬁcient than the usual numpy.ndarray representation.
The intercept_ member is not converted.
1558
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsself: estimator :
Notes
For non-sparse models, i.e. when there are not many zeros in coef_, this may actually increase memory
usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be
computed with (coef_ == 0).sum(), must be more than 50% for this to provide signiﬁcant beneﬁts.
After calling this method, further ﬁtting with the partial_ﬁt method (if any) will not work until you call
densify.
29.18.15 sklearn.linear_model.Perceptron
class sklearn.linear_model.Perceptron(penalty=None,
alpha=0.0001,
ﬁt_intercept=True,
n_iter=5,
shufﬂe=True,
verbose=0,
eta0=1.0,
n_jobs=1,
random_state=0,
class_weight=None,
warm_start=False)
Read more in the User Guide.
Parameterspenalty : None, ‘l2’ or ‘l1’ or ‘elasticnet’
The penalty (aka regularization term) to be used. Defaults to None.
alpha : ﬂoat
Constant that multiplies the regularization term if regularization is used. Defaults to
0.0001
ﬁt_intercept : bool
Whether the intercept should be estimated or not. If False, the data is assumed to be
already centered. Defaults to True.
n_iter : int, optional
The number of passes over the training data (aka epochs). Defaults to 5.
shufﬂe : bool, optional, default True
Whether or not the training data should be shufﬂed after each epoch.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data.
verbose : integer, optional
The verbosity level
n_jobs : integer, optional
The number of CPUs to use to do the OVA (One Versus All, for multi-class problems)
computation. -1 means ‘all CPUs’. Defaults to 1.
eta0 : double
Constant by which the updates are multiplied. Defaults to 1.
class_weight : dict, {class_label: weight} or “balanced” or None, optional
29.18. sklearn.linear_model: Generalized Linear Models
1559
scikit-learn user guide, Release 0.18.2
Preset for the class_weight ﬁt parameter.
Weights associated with classes. If not given, all classes are supposed to have weight
one.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y))
warm_start : bool, optional
When set to True, reuse the solution of the previous call to ﬁt as initialization, otherwise,
just erase the previous solution.
Attributescoef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes, n_features]
Weights assigned to the features.
intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]
Constants in decision function.
See also:
SGDClassifier
Notes
Perceptron and SGDClassiﬁer share the same underlying implementation. In fact, Perceptron() is equivalent to
SGDClassiﬁer(loss=”perceptron”, eta0=1, learning_rate=”constant”, penalty=None).
References
https://en.wikipedia.org/wiki/Perceptron and references therein.
Methods
decision_function(X)
Predict conﬁdence scores for samples.
densify()
Convert coefﬁcient matrix to dense array format.
fit(X, y[, coef_init, intercept_init, ...])
Fit linear model with Stochastic Gradient Descent.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
partial_fit(X, y[, classes, sample_weight])
Fit linear model with Stochastic Gradient Descent.
predict(X)
Predict class labels for samples in X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*args, \*\*kwargs)
sparsify()
Convert coefﬁcient matrix to sparse format.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(penalty=None, alpha=0.0001, ﬁt_intercept=True, n_iter=5, shufﬂe=True, verbose=0,
eta0=1.0, n_jobs=1, random_state=0, class_weight=None, warm_start=False)
decision_function(X)
1560
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
densify()
Convert coefﬁcient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is
required for ﬁtting, so calling this method is only required on models that have previously been sparsiﬁed;
otherwise, it is a no-op.
Returnsself: estimator :
fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)
Fit linear model with Stochastic Gradient Descent.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training data
y : numpy array, shape (n_samples,)
Target values
coef_init : array, shape (n_classes, n_features)
The initial coefﬁcients to warm-start the optimization.
intercept_init : array, shape (n_classes,)
The initial intercept to warm-start the optimization.
sample_weight : array-like, shape (n_samples,), optional
Weights applied to individual samples. If not provided, uniform weights are assumed.
These weights will be multiplied with class_weight (passed through the constructor) if
class_weight is speciﬁed
Returnsself : returns an instance of self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
29.18. sklearn.linear_model: Generalized Linear Models
1561
scikit-learn user guide, Release 0.18.2
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y, classes=None, sample_weight=None)
Fit linear model with Stochastic Gradient Descent.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Subset of the training data
y : numpy array, shape (n_samples,)
Subset of the target values
classes : array, shape (n_classes,)
Classes across all calls to partial_ﬁt. Can be obtained by via np.unique(y_all), where
y_all is the target vector of the entire dataset. This argument is required for the ﬁrst call
to partial_ﬁt and can be omitted in the subsequent calls. Note that y doesn’t need to
contain all labels in classes.
sample_weight : array-like, shape (n_samples,), optional
Weights applied to individual samples. If not provided, uniform weights are assumed.
Returnsself : returns an instance of self.
predict(X)
Predict class labels for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
1562
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sparsify()
Convert coefﬁcient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more
memory- and storage-efﬁcient than the usual numpy.ndarray representation.
The intercept_ member is not converted.
Returnsself: estimator :
Notes
For non-sparse models, i.e. when there are not many zeros in coef_, this may actually increase memory
usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be
computed with (coef_ == 0).sum(), must be more than 50% for this to provide signiﬁcant beneﬁts.
After calling this method, further ﬁtting with the partial_ﬁt method (if any) will not work until you call
densify.
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.linear_model.Perceptron
• Out-of-core classiﬁcation of text documents
• Comparing various online solvers
• Classiﬁcation of text documents using sparse features
29.18. sklearn.linear_model: Generalized Linear Models
1563
scikit-learn user guide, Release 0.18.2
29.18.16 sklearn.linear_model.RandomizedLasso
class sklearn.linear_model.RandomizedLasso(alpha=’aic’, scaling=0.5, sample_fraction=0.75,
n_resampling=200,
selection_threshold=0.25,
ﬁt_intercept=True,
verbose=False,
normalize=True,
precompute=’auto’,
max_iter=500,
eps=2.2204460492503131e-
16,
random_state=None,
n_jobs=1,
pre_dispatch=‘3*n_jobs’,
mem-
ory=Memory(cachedir=None))
Randomized Lasso.
Randomized Lasso works by subsampling the training data and computing a Lasso estimate where the penalty
of a random subset of coefﬁcients has been scaled. By performing this double randomization several times, the
method assigns high scores to features that are repeatedly selected across randomizations. This is known as
stability selection. In short, features selected more often are considered good features.
Read more in the User Guide.
Parametersalpha : ﬂoat, ‘aic’, or ‘bic’, optional
The regularization parameter alpha parameter in the Lasso. Warning: this is not the
alpha parameter in the stability selection article which is scaling.
scaling : ﬂoat, optional
The s parameter used to randomly scale the penalty of different features (See User
Guide for details ). Should be between 0 and 1.
sample_fraction : ﬂoat, optional
The fraction of samples to be used in each randomized design. Should be between 0
and 1. If 1, all samples are used.
n_resampling : int, optional
Number of randomized models.
selection_threshold: ﬂoat, optional :
The score above which features should be selected.
ﬁt_intercept : boolean, optional
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
verbose : boolean or integer, optional
Sets the verbosity amount
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learned more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
precompute : True | False | ‘auto’
Whether to use a precomputed Gram matrix to speed up calculations. If set to ‘auto’ let
us decide. The Gram matrix can also be passed as argument.
1564
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
max_iter : integer, optional
Maximum number of iterations to perform in the Lars algorithm.
eps : ﬂoat, optional
The machine-precision regularization in the computation of the Cholesky diagonal fac-
tors. Increase this for very ill-conditioned systems. Unlike the ‘tol’ parameter in some
iterative optimization-based algorithms, this parameter does not control the tolerance of
the optimization.
n_jobs : integer, optional
Number of CPUs to use during the resampling. If ‘-1’, use all the CPUs
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
memory : Instance of joblib.Memory or string
Used for internal caching. By default, no caching is done. If a string is given, it is the
path to the caching directory.
Attributesscores_ : array, shape = [n_features]
Feature scores between 0 and 1.
all_scores_ : array, shape = [n_features, n_reg_parameter]
Feature scores between 0 and 1 for all values of the regularization parameter. The
reference article suggests scores_ is the max of all_scores_.
See also:
RandomizedLogisticRegression, Lasso, ElasticNet
Notes
See examples/linear_model/plot_sparse_recovery.py for an example.
References
Stability selection Nicolai Meinshausen, Peter Buhlmann Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010 DOI: 10.1111/j.1467-9868.2010.00740.x
29.18. sklearn.linear_model: Generalized Linear Models
1565
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn.linear_model import RandomizedLasso
>>> randomized_lasso = RandomizedLasso()
Methods
fit(X, y)
Fit the model using X, y as training data.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Return a mask, or list, of the features/indices selected.
inverse_transform(X)
Transform a new matrix using the selected features
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform a new matrix using the selected features
__init__(alpha=’aic’,
scaling=0.5,
sample_fraction=0.75,
n_resampling=200,
selec-
tion_threshold=0.25,
ﬁt_intercept=True,
verbose=False,
normalize=True,
precom-
pute=’auto’,
max_iter=500,
eps=2.2204460492503131e-16,
random_state=None,
n_jobs=1, pre_dispatch=‘3*n_jobs’, memory=Memory(cachedir=None))
fit(X, y)
Fit the model using X, y as training data.
ParametersX : array-like, shape = [n_samples, n_features]
Training data.
y : array-like, shape = [n_samples]
Target values.
Returnsself : object
Returns an instance of self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
1566
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Return a mask, or list, of the features/indices selected.
inverse_transform(X)
Transform a new matrix using the selected features
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transform a new matrix using the selected features
Examples using sklearn.linear_model.RandomizedLasso
• Sparse recovery: feature selection for sparse linear models
29.18.17 sklearn.linear_model.RandomizedLogisticRegression
class sklearn.linear_model.RandomizedLogisticRegression(C=1,
scaling=0.5,
sample_fraction=0.75,
n_resampling=200,
se-
lection_threshold=0.25,
tol=0.001, ﬁt_intercept=True,
verbose=False,
nor-
malize=True,
ran-
dom_state=None,
n_jobs=1,
pre_dispatch=‘3*n_jobs’,
mem-
ory=Memory(cachedir=None))
Randomized Logistic Regression
Randomized Logistic Regression works by subsampling the training data and ﬁtting a L1-penalized LogisticRe-
gression model where the penalty of a random subset of coefﬁcients has been scaled. By performing this double
randomization several times, the method assigns high scores to features that are repeatedly selected across ran-
domizations. This is known as stability selection. In short, features selected more often are considered good
features.
Read more in the User Guide.
ParametersC : ﬂoat, optional, default=1
The regularization parameter C in the LogisticRegression.
scaling : ﬂoat, optional, default=0.5
The s parameter used to randomly scale the penalty of different features (See User
Guide for details ). Should be between 0 and 1.
sample_fraction : ﬂoat, optional, default=0.75
29.18. sklearn.linear_model: Generalized Linear Models
1567
scikit-learn user guide, Release 0.18.2
The fraction of samples to be used in each randomized design. Should be between 0
and 1. If 1, all samples are used.
n_resampling : int, optional, default=200
Number of randomized models.
selection_threshold : ﬂoat, optional, default=0.25
The score above which features should be selected.
ﬁt_intercept : boolean, optional, default=True
whether to calculate the intercept for this model. If set to false, no intercept will be used
in calculations (e.g. data is expected to be already centered).
verbose : boolean or integer, optional
Sets the verbosity amount
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
tol : ﬂoat, optional, default=1e-3
tolerance for stopping criteria of LogisticRegression
n_jobs : integer, optional
Number of CPUs to use during the resampling. If ‘-1’, use all the CPUs
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
memory : Instance of joblib.Memory or string
Used for internal caching. By default, no caching is done. If a string is given, it is the
path to the caching directory.
Attributesscores_ : array, shape = [n_features]
Feature scores between 0 and 1.
all_scores_ : array, shape = [n_features, n_reg_parameter]
1568
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Feature scores between 0 and 1 for all values of the regularization parameter. The
reference article suggests scores_ is the max of all_scores_.
See also:
RandomizedLasso, LogisticRegression
Notes
See examples/linear_model/plot_sparse_recovery.py for an example.
References
Stability selection Nicolai Meinshausen, Peter Buhlmann Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010 DOI: 10.1111/j.1467-9868.2010.00740.x
Examples
>>> from sklearn.linear_model import RandomizedLogisticRegression
>>> randomized_logistic = RandomizedLogisticRegression()
Methods
fit(X, y)
Fit the model using X, y as training data.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
get_support([indices])
Return a mask, or list, of the features/indices selected.
inverse_transform(X)
Transform a new matrix using the selected features
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform a new matrix using the selected features
__init__(C=1, scaling=0.5, sample_fraction=0.75, n_resampling=200, selection_threshold=0.25,
tol=0.001,
ﬁt_intercept=True,
verbose=False,
normalize=True,
random_state=None,
n_jobs=1, pre_dispatch=‘3*n_jobs’, memory=Memory(cachedir=None))
fit(X, y)
Fit the model using X, y as training data.
ParametersX : array-like, shape = [n_samples, n_features]
Training data.
y : array-like, shape = [n_samples]
Target values.
Returnsself : object
Returns an instance of self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
29.18. sklearn.linear_model: Generalized Linear Models
1569
scikit-learn user guide, Release 0.18.2
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
get_support(indices=False)
Return a mask, or list, of the features/indices selected.
inverse_transform(X)
Transform a new matrix using the selected features
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transform a new matrix using the selected features
29.18.18 sklearn.linear_model.RANSACRegressor
class sklearn.linear_model.RANSACRegressor(base_estimator=None,
min_samples=None,
residual_threshold=None,
is_data_valid=None,
is_model_valid=None,
max_trials=100,
stop_n_inliers=inf,
stop_score=inf,
stop_probability=0.99,
residual_metric=None,
loss=’absolute_loss’, random_state=None)
RANSAC (RANdom SAmple Consensus) algorithm.
RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the
complete data set. More information can be found in the general documentation of linear models.
A detailed description of the algorithm can be found in the documentation of the linear_model sub-package.
Read more in the User Guide.
Parametersbase_estimator : object, optional
Base estimator object which implements the following methods:
1570
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
•ﬁt(X, y): Fit model to given training data and target values.
•score(X, y): Returns the mean accuracy on the given test data, which is used for the
stop criterion deﬁned by stop_score. Additionally, the score is used to decide which
of two equally large consensus sets is chosen as the better one.
If base_estimator is None, then base_estimator=sklearn.linear_model.LinearRegression()
is used for target values of dtype ﬂoat.
Note that the current implementation only supports regression estimators.
min_samples : int (>= 1) or ﬂoat ([0, 1]), optional
Minimum number of samples chosen randomly from original data.
Treated as an
absolute number of samples for min_samples >= 1, treated as a relative number
ceil(min_samples * X.shape[0]) for min_samples < 1. This is typically chosen as the
minimal number of samples necessary to estimate the given base_estimator. By default
a sklearn.linear_model.LinearRegression() estimator is assumed and
min_samples is chosen as X.shape[1] + 1.
residual_threshold : ﬂoat, optional
Maximum residual for a data sample to be classiﬁed as an inlier. By default the threshold
is chosen as the MAD (median absolute deviation) of the target values y.
is_data_valid : callable, optional
This function is called with the randomly selected data before the model is ﬁtted to it:
is_data_valid(X, y). If its return value is False the current randomly chosen sub-sample
is skipped.
is_model_valid : callable, optional
This function is called with the estimated model and the randomly selected data:
is_model_valid(model, X, y). If its return value is False the current randomly chosen
sub-sample is skipped. Rejecting samples with this function is computationally costlier
than with is_data_valid. is_model_valid should therefore only be used if the estimated
model is needed for making the rejection decision.
max_trials : int, optional
Maximum number of iterations for random sample selection.
stop_n_inliers : int, optional
Stop iteration if at least this number of inliers are found.
stop_score : ﬂoat, optional
Stop iteration if score is greater equal than this threshold.
stop_probability : ﬂoat in range [0, 1], optional
RANSAC iteration stops if at least one outlier-free set of the training data is sampled in
RANSAC. This requires to generate at least N samples (iterations):
N >= log(1 - probability) / log(1 - e**m)
where the probability (conﬁdence) is typically set to high value such as 0.99 (the default)
and e is the current fraction of inliers w.r.t. the total number of samples.
residual_metric : callable, optional
29.18. sklearn.linear_model: Generalized Linear Models
1571
scikit-learn user guide, Release 0.18.2
Metric to reduce the dimensionality of the residuals to 1 for multi-dimensional target
values y.shape[1] > 1. By default the sum of absolute differences is used:
lambda dy: np.sum(np.abs(dy), axis=1)
NOTE: residual_metric is deprecated from 0.18 and will be removed in 0.20 Use loss
instead.
loss : string, callable, optional, default “absolute_loss”
String inputs, “absolute_loss” and “squared_loss” are supported which ﬁnd the absolute
loss and squared loss per sample respectively.
If loss is a callable, then it should be a function that takes two arrays as inputs, the
true and predicted value and returns a 1-D array with the ‘‘i‘‘th value of the array cor-
responding to the loss on X[i].
If the loss on a sample is greater than the residual_threshold, then this sample
is classiﬁed as an outlier.
random_state : integer or numpy.RandomState, optional
The generator used to initialize the centers. If an integer is given, it ﬁxes the seed.
Defaults to the global numpy random number generator.
Attributesestimator_ : object
Best ﬁtted model (copy of the base_estimator object).
n_trials_ : int
Number of random selection trials until one of the stop criteria is met. It is always <=
max_trials.
inlier_mask_ : bool array of shape [n_samples]
Boolean mask of inliers classiﬁed as True.
References
[R35], [R36], [R37]
Methods
fit(X, y[, sample_weight])
Fit estimator using RANSAC algorithm.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the estimated model.
score(X, y)
Returns the score of the prediction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(base_estimator=None,
min_samples=None,
residual_threshold=None,
is_data_valid=None,
is_model_valid=None,
max_trials=100,
stop_n_inliers=inf,
stop_score=inf,
stop_probability=0.99,
residual_metric=None,
loss=’absolute_loss’,
random_state=None)
fit(X, y, sample_weight=None)
Fit estimator using RANSAC algorithm.
1572
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ParametersX : array-like or sparse matrix, shape [n_samples, n_features]
Training data.
y : array-like, shape = [n_samples] or [n_samples, n_targets]
Target values.
sample_weight : array-like, shape = [n_samples]
Individual weights for each sample raises error if sample_weight is passed and
base_estimator ﬁt method does not support it.
RaisesValueError :
If no valid consensus set could be found.
This occurs if is_data_valid and
is_model_valid return False for all max_trials randomly chosen sub-samples.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the estimated model.
This is a wrapper for estimator_.predict(X).
ParametersX : numpy array of shape [n_samples, n_features]
Returnsy : array, shape = [n_samples] or [n_samples, n_targets]
Returns predicted values.
score(X, y)
Returns the score of the prediction.
This is a wrapper for estimator_.score(X, y).
ParametersX : numpy array or sparse matrix of shape [n_samples, n_features]
Training data.
y : array, shape = [n_samples] or [n_samples, n_targets]
Target values.
Returnsz : ﬂoat
Score of the prediction.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.18. sklearn.linear_model: Generalized Linear Models
1573
scikit-learn user guide, Release 0.18.2
Examples using sklearn.linear_model.RANSACRegressor
• Robust linear model estimation using RANSAC
• Robust linear estimator ﬁtting
• Theil-Sen Regression
29.18.19 sklearn.linear_model.Ridge
class sklearn.linear_model.Ridge(alpha=1.0,
ﬁt_intercept=True,
normalize=False,
copy_X=True,
max_iter=None,
tol=0.001,
solver=’auto’,
random_state=None)
Linear least squares with l2 regularization.
This model solves a regression model where the loss function is the linear least squares function and regulariza-
tion is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has
built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).
Read more in the User Guide.
Parametersalpha : {ﬂoat, array-like}, shape (n_targets)
Regularization strength; must be a positive ﬂoat. Regularization improves the condi-
tioning of the problem and reduces the variance of the estimates. Larger values specify
stronger regularization. Alpha corresponds to C^-1 in other linear models such as
LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be
speciﬁc to the targets. Hence they must correspond in number.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
ﬁt_intercept : boolean
Whether to calculate the intercept for this model. If set to false, no intercept will be
used in calculations (e.g. data is expected to be already centered).
max_iter : int, optional
Maximum number of iterations for conjugate gradient solver. For ‘sparse_cg’ and ‘lsqr’
solvers, the default value is determined by scipy.sparse.linalg. For ‘sag’ solver, the
default value is 1000.
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
solver : {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’}
Solver to use in the computational routines:
•‘auto’ chooses the solver automatically based on the type of data.
•‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefﬁcients.
More stable for singular matrices than ‘cholesky’.
1574
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
•‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solu-
tion.
•‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As
an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale
data (possibility to set tol and max_iter).
•‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It
is the fastest but may not be available in old scipy versions. It also uses an iterative
procedure.
•‘sag’ uses a Stochastic Average Gradient descent. It also uses an iterative procedure,
and is often faster than other solvers when both n_samples and n_features are large.
Note that ‘sag’ fast convergence is only guaranteed on features with approximately
the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.
All last four solvers support both dense and sparse data. However, only ‘sag’ supports
sparse input when ﬁt_intercept is True.
New in version 0.17: Stochastic Average Gradient descent solver.
tol : ﬂoat
Precision of the solution.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data. Used
only in ‘sag’ solver.
New in version 0.17: random_state to support Stochastic Average Gradient.
Attributescoef_ : array, shape (n_features,) or (n_targets, n_features)
Weight vector(s).
intercept_ : ﬂoat | array, shape = (n_targets,)
Independent term in decision function. Set to 0.0 if fit_intercept = False.
n_iter_ : array or None, shape (n_targets,)
Actual number of iterations for each target. Available only for sag and lsqr solvers.
Other solvers will return None.
New in version 0.17.
See also:
RidgeClassifier, RidgeCV, sklearn.kernel_ridge.KernelRidge
Examples
>>> from sklearn.linear_model import Ridge
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> np.random.seed(0)
>>> y = np.random.randn(n_samples)
>>> X = np.random.randn(n_samples, n_features)
>>> clf = Ridge(alpha=1.0)
>>> clf.fit(X, y)
Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
normalize=False, random_state=None, solver='auto', tol=0.001)
29.18. sklearn.linear_model: Generalized Linear Models
1575
scikit-learn user guide, Release 0.18.2
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y[, sample_weight])
Fit Ridge regression model
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1.0, ﬁt_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001,
solver=’auto’, random_state=None)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y, sample_weight=None)
Fit Ridge regression model
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training data
y : array-like, shape = [n_samples] or [n_samples, n_targets]
Target values
sample_weight : ﬂoat or numpy array of shape [n_samples]
Individual weights for each sample
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
1576
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.Ridge
• Prediction Latency
• Compressive sensing: tomography reconstruction with L1 prior (Lasso)
• HuberRegressor vs Ridge on dataset with strong outliers
• Ordinary Least Squares and Ridge Regression Variance
• Polynomial interpolation
• Plot Ridge coefﬁcients as a function of the L2 regularization
• Plot Ridge coefﬁcients as a function of the regularization
29.18.20 sklearn.linear_model.RidgeClassiﬁer
class sklearn.linear_model.RidgeClassifier(alpha=1.0, ﬁt_intercept=True, normalize=False,
copy_X=True,
max_iter=None,
tol=0.001,
class_weight=None,
solver=’auto’,
ran-
dom_state=None)
Classiﬁer using Ridge regression.
Read more in the User Guide.
Parametersalpha : ﬂoat
29.18. sklearn.linear_model: Generalized Linear Models
1577
scikit-learn user guide, Release 0.18.2
Regularization strength; must be a positive ﬂoat. Regularization improves the condi-
tioning of the problem and reduces the variance of the estimates. Larger values specify
stronger regularization. Alpha corresponds to C^-1 in other linear models such as Lo-
gisticRegression or LinearSVC.
class_weight : dict or ‘balanced’, optional
Weights associated with classes in the form {class_label:
weight}. If not
given, all classes are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y))
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
ﬁt_intercept : boolean
Whether to calculate the intercept for this model. If set to false, no intercept will be
used in calculations (e.g. data is expected to be already centered).
max_iter : int, optional
Maximum number of iterations for conjugate gradient solver. The default value is de-
termined by scipy.sparse.linalg.
normalize : boolean, optional, default False
If True, the regressors X will be normalized before regression. This parameter is ignored
when ﬁt_intercept is set to False. When the regressors are normalized, note that this
makes the hyperparameters learnt more robust and almost independent of the number
of samples. The same property is not valid for standardized data. However, if you
wish to standardize, please use preprocessing.StandardScaler before calling ﬁt on an
estimator with normalize=False.
solver : {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’}
Solver to use in the computational routines:
•‘auto’ chooses the solver automatically based on the type of data.
•‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefﬁcients.
More stable for singular matrices than ‘cholesky’.
•‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solu-
tion.
•‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As
an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale
data (possibility to set tol and max_iter).
•‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It
is the fastest but may not be available in old scipy versions. It also uses an iterative
procedure.
•‘sag’ uses a Stochastic Average Gradient descent. It also uses an iterative procedure,
and is faster than other solvers when both n_samples and n_features are large.
New in version 0.17: Stochastic Average Gradient descent solver.
tol : ﬂoat
Precision of the solution.
1578
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data. Used
in ‘sag’ solver.
Attributescoef_ : array, shape (n_features,) or (n_classes, n_features)
Weight vector(s).
intercept_ : ﬂoat | array, shape = (n_targets,)
Independent term in decision function. Set to 0.0 if fit_intercept = False.
n_iter_ : array or None, shape (n_targets,)
Actual number of iterations for each target. Available only for sag and lsqr solvers.
Other solvers will return None.
See also:
Ridge, RidgeClassifierCV
Notes
For multi-class classiﬁcation, n_class classiﬁers are trained in a one-versus-all approach. Concretely, this is
implemented by taking advantage of the multi-variate response support in Ridge.
Methods
decision_function(X)
Predict conﬁdence scores for samples.
fit(X, y[, sample_weight])
Fit Ridge regression model.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class labels for samples in X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1.0, ﬁt_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001,
class_weight=None, solver=’auto’, random_state=None)
decision_function(X)
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
fit(X, y, sample_weight=None)
Fit Ridge regression model.
ParametersX : {array-like, sparse matrix}, shape = [n_samples,n_features]
Training data
29.18. sklearn.linear_model: Generalized Linear Models
1579
scikit-learn user guide, Release 0.18.2
y : array-like, shape = [n_samples]
Target values
sample_weight : ﬂoat or numpy array of shape (n_samples,)
Sample weight.
New in version 0.17: sample_weight support to Classiﬁer.
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class labels for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
1580
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.linear_model.RidgeClassifier
• Classiﬁcation of text documents using sparse features
29.18.21 sklearn.linear_model.SGDClassiﬁer
class sklearn.linear_model.SGDClassifier(loss=’hinge’,
penalty=’l2’,
alpha=0.0001,
l1_ratio=0.15, ﬁt_intercept=True, n_iter=5, shuf-
ﬂe=True,
verbose=0,
epsilon=0.1,
n_jobs=1,
random_state=None,
learning_rate=’optimal’,
eta0=0.0,
power_t=0.5,
class_weight=None,
warm_start=False, average=False)
Linear classiﬁers (SVM, logistic regression, a.o.) with SGD training.
This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gra-
dient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing
strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_ﬁt
method. For best results using the default learning rate schedule, the data should have zero mean and unit
variance.
This implementation works with data represented as dense or sparse arrays of ﬂoating point values for the
features. The model it ﬁts can be controlled with the loss parameter; by default, it ﬁts a linear support vector
machine (SVM).
The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector
using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If
the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for
learning sparse models and achieve online feature selection.
Read more in the User Guide.
Parametersloss : str, ‘hinge’, ‘log’, ‘modiﬁed_huber’, ‘squared_hinge’, ‘perceptron’, or a regression
loss: ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’
The loss function to be used. Defaults to ‘hinge’, which gives a linear SVM. The
‘log’ loss gives logistic regression, a probabilistic classiﬁer. ‘modiﬁed_huber’ is an-
other smooth loss that brings tolerance to outliers as well as probability estimates.
‘squared_hinge’ is like hinge but is quadratically penalized. ‘perceptron’ is the lin-
ear loss used by the perceptron algorithm. The other losses are designed for regression
but can be useful in classiﬁcation as well; see SGDRegressor for a description.
penalty : str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’
The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard
regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the
model (feature selection) not achievable with ‘l2’.
alpha : ﬂoat
Constant that multiplies the regularization term. Defaults to 0.0001 Also used to com-
pute learning_rate when set to ‘optimal’.
l1_ratio : ﬂoat
The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to
L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.
ﬁt_intercept : bool
29.18. sklearn.linear_model: Generalized Linear Models
1581
scikit-learn user guide, Release 0.18.2
Whether the intercept should be estimated or not. If False, the data is assumed to be
already centered. Defaults to True.
n_iter : int, optional
The number of passes over the training data (aka epochs). The number of iterations is
set to 1 if using partial_ﬁt. Defaults to 5.
shufﬂe : bool, optional
Whether or not the training data should be shufﬂed after each epoch. Defaults to True.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data.
verbose : integer, optional
The verbosity level
epsilon : ﬂoat
Epsilon in the epsilon-insensitive loss functions;
only if loss is ‘huber’, ‘ep-
silon_insensitive’, or ‘squared_epsilon_insensitive’. For ‘huber’, determines the thresh-
old at which it becomes less important to get the prediction exactly right. For epsilon-
insensitive, any differences between the current prediction and the correct label are
ignored if they are less than this threshold.
n_jobs : integer, optional
The number of CPUs to use to do the OVA (One Versus All, for multi-class problems)
computation. -1 means ‘all CPUs’. Defaults to 1.
learning_rate : string, optional
The learning rate schedule:
•‘constant’: eta = eta0
•‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]
•‘invscaling’: eta = eta0 / pow(t, power_t)
where t0 is chosen by a heuristic proposed by Leon Bottou.
eta0 : double
The initial learning rate for the ‘constant’ or ‘invscaling’ schedules. The default value
is 0.0 as eta0 is not used by the default schedule ‘optimal’.
power_t : double
The exponent for inverse scaling learning rate [default 0.5].
class_weight : dict, {class_label: weight} or “balanced” or None, optional
Preset for the class_weight ﬁt parameter.
Weights associated with classes. If not given, all classes are supposed to have weight
one.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y))
warm_start : bool, optional
1582
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
When set to True, reuse the solution of the previous call to ﬁt as initialization, otherwise,
just erase the previous solution.
average : bool or int, optional
When set to True, computes the averaged SGD weights and stores the result in the
coef_ attribute. If set to an int greater than 1, averaging will begin once the total
number of samples seen reaches average. So average=10 will begin averaging after
seeing 10 samples.
Attributescoef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes, n_features)
Weights assigned to the features.
intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)
Constants in decision function.
See also:
LinearSVC, LogisticRegression, Perceptron
Examples
>>> import numpy as np
>>> from sklearn import linear_model
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> Y = np.array([1, 1, 2, 2])
>>> clf = linear_model.SGDClassifier()
>>> clf.fit(X, Y)
...
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
eta0=0.0, fit_intercept=True, l1_ratio=0.15,
learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
penalty='l2', power_t=0.5, random_state=None, shuffle=True,
verbose=0, warm_start=False)
>>> print(clf.predict([[-0.8, -1]]))
[1]
Methods
decision_function(X)
Predict conﬁdence scores for samples.
densify()
Convert coefﬁcient matrix to dense array format.
fit(X, y[, coef_init, intercept_init, ...])
Fit linear model with Stochastic Gradient Descent.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
partial_fit(X, y[, classes, sample_weight])
Fit linear model with Stochastic Gradient Descent.
predict(X)
Predict class labels for samples in X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*args, \*\*kwargs)
sparsify()
Convert coefﬁcient matrix to sparse format.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
29.18. sklearn.linear_model: Generalized Linear Models
1583
scikit-learn user guide, Release 0.18.2
__init__(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, ﬁt_intercept=True, n_iter=5,
shufﬂe=True,
verbose=0,
epsilon=0.1,
n_jobs=1,
random_state=None,
learn-
ing_rate=’optimal’,
eta0=0.0,
power_t=0.5,
class_weight=None,
warm_start=False,
average=False)
decision_function(X)
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
densify()
Convert coefﬁcient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is
required for ﬁtting, so calling this method is only required on models that have previously been sparsiﬁed;
otherwise, it is a no-op.
Returnsself: estimator :
fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)
Fit linear model with Stochastic Gradient Descent.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training data
y : numpy array, shape (n_samples,)
Target values
coef_init : array, shape (n_classes, n_features)
The initial coefﬁcients to warm-start the optimization.
intercept_init : array, shape (n_classes,)
The initial intercept to warm-start the optimization.
sample_weight : array-like, shape (n_samples,), optional
Weights applied to individual samples. If not provided, uniform weights are assumed.
These weights will be multiplied with class_weight (passed through the constructor) if
class_weight is speciﬁed
Returnsself : returns an instance of self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
1584
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y, classes=None, sample_weight=None)
Fit linear model with Stochastic Gradient Descent.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Subset of the training data
y : numpy array, shape (n_samples,)
Subset of the target values
classes : array, shape (n_classes,)
Classes across all calls to partial_ﬁt. Can be obtained by via np.unique(y_all), where
y_all is the target vector of the entire dataset. This argument is required for the ﬁrst call
to partial_ﬁt and can be omitted in the subsequent calls. Note that y doesn’t need to
contain all labels in classes.
sample_weight : array-like, shape (n_samples,), optional
Weights applied to individual samples. If not provided, uniform weights are assumed.
Returnsself : returns an instance of self.
predict(X)
Predict class labels for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
predict_log_proba
Log of probability estimates.
This method is only available for log loss and modiﬁed Huber loss.
When loss=”modiﬁed_huber”, probability estimates may be hard zeros and ones, so taking the logarithm
is not possible.
See predict_proba for details.
ParametersX : array-like, shape (n_samples, n_features)
ReturnsT : array-like, shape (n_samples, n_classes)
Returns the log-probability of the sample for each class in the model, where classes are
ordered as they are in self.classes_.
29.18. sklearn.linear_model: Generalized Linear Models
1585
scikit-learn user guide, Release 0.18.2
predict_proba
Probability estimates.
This method is only available for log loss and modiﬁed Huber loss.
Multiclass probability estimates are derived from binary (one-vs.-rest) estimates by simple normalization,
as recommended by Zadrozny and Elkan.
Binary probability estimates for loss=”modiﬁed_huber” are given by (clip(decision_function(X), -1, 1) +
1) / 2. For other loss functions it is necessary to perform proper probability calibration by wrapping the
classiﬁer with sklearn.calibration.CalibratedClassifierCV instead.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Returnsarray, shape (n_samples, n_classes) :
Returns the probability of the sample for each class in the model, where classes are
ordered as they are in self.classes_.
References
Zadrozny and Elkan, “Transforming classiﬁer scores into multiclass probability estimates”, SIGKDD‘02,
http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf
The justiﬁcation for the formula in the loss=”modiﬁed_huber” case is in the appendix B in: http://jmlr.
csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
sparsify()
Convert coefﬁcient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more
memory- and storage-efﬁcient than the usual numpy.ndarray representation.
The intercept_ member is not converted.
Returnsself: estimator :
1586
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
For non-sparse models, i.e. when there are not many zeros in coef_, this may actually increase memory
usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be
computed with (coef_ == 0).sum(), must be more than 50% for this to provide signiﬁcant beneﬁts.
After calling this method, further ﬁtting with the partial_ﬁt method (if any) will not work until you call
densify.
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.linear_model.SGDClassifier
• Model Complexity Inﬂuence
• Out-of-core classiﬁcation of text documents
• Comparing various online solvers
• Plot multi-class SGD on the iris dataset
• SGD: Maximum margin separating hyperplane
• SGD: Weighted samples
• Sample pipeline for text feature extraction and evaluation
• Classiﬁcation of text documents using sparse features
• Classiﬁcation of text documents: using a MLComp dataset
29.18. sklearn.linear_model: Generalized Linear Models
1587
scikit-learn user guide, Release 0.18.2
29.18.22 sklearn.linear_model.SGDRegressor
class sklearn.linear_model.SGDRegressor(loss=’squared_loss’,
penalty=’l2’,
alpha=0.0001,
l1_ratio=0.15,
ﬁt_intercept=True,
n_iter=5,
shufﬂe=True,
verbose=0,
epsilon=0.1,
ran-
dom_state=None,
learning_rate=’invscaling’,
eta0=0.01,
power_t=0.25,
warm_start=False,
average=False)
Linear model ﬁtted by minimizing a regularized empirical loss with SGD
SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the
model is updated along the way with a decreasing strength schedule (aka learning rate).
The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector
using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If
the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for
learning sparse models and achieve online feature selection.
This implementation works with data represented as dense numpy arrays of ﬂoating point values for the features.
Read more in the User Guide.
Parametersloss : str, ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’
The loss function to be used. Defaults to ‘squared_loss’ which refers to the ordinary
least squares ﬁt. ‘huber’ modiﬁes ‘squared_loss’ to focus less on getting outliers correct
by switching from squared to linear loss past a distance of epsilon. ‘epsilon_insensitive’
ignores errors less than epsilon and is linear past that; this is the loss function used in
SVR. ‘squared_epsilon_insensitive’ is the same but becomes squared loss past a toler-
ance of epsilon.
penalty : str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’
The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard
regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the
model (feature selection) not achievable with ‘l2’.
alpha : ﬂoat
Constant that multiplies the regularization term. Defaults to 0.0001 Also used to com-
pute learning_rate when set to ‘optimal’.
l1_ratio : ﬂoat
The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to
L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.
ﬁt_intercept : bool
Whether the intercept should be estimated or not. If False, the data is assumed to be
already centered. Defaults to True.
n_iter : int, optional
The number of passes over the training data (aka epochs). The number of iterations is
set to 1 if using partial_ﬁt. Defaults to 5.
shufﬂe : bool, optional
Whether or not the training data should be shufﬂed after each epoch. Defaults to True.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data.
1588
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
verbose : integer, optional
The verbosity level.
epsilon : ﬂoat
Epsilon in the epsilon-insensitive loss functions;
only if loss is ‘huber’, ‘ep-
silon_insensitive’, or ‘squared_epsilon_insensitive’. For ‘huber’, determines the thresh-
old at which it becomes less important to get the prediction exactly right. For epsilon-
insensitive, any differences between the current prediction and the correct label are
ignored if they are less than this threshold.
learning_rate : string, optional
The learning rate schedule:
•‘constant’: eta = eta0
•‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]
•‘invscaling’: eta = eta0 / pow(t, power_t)
where t0 is chosen by a heuristic proposed by Leon Bottou.
eta0 : double, optional
The initial learning rate [default 0.01].
power_t : double, optional
The exponent for inverse scaling learning rate [default 0.25].
warm_start : bool, optional
When set to True, reuse the solution of the previous call to ﬁt as initialization, otherwise,
just erase the previous solution.
average : bool or int, optional
When set to True, computes the averaged SGD weights and stores the result in the
coef_ attribute. If set to an int greater than 1, averaging will begin once the total
number of samples seen reaches average. So average=10 will begin averaging after
seeing 10 samples.
Attributescoef_ : array, shape (n_features,)
Weights assigned to the features.
intercept_ : array, shape (1,)
The intercept term.
average_coef_ : array, shape (n_features,)
Averaged weights assigned to the features.
average_intercept_ : array, shape (1,)
The averaged intercept term.
See also:
Ridge, ElasticNet, Lasso, SVR
29.18. sklearn.linear_model: Generalized Linear Models
1589
scikit-learn user guide, Release 0.18.2
Examples
>>> import numpy as np
>>> from sklearn import linear_model
>>> n_samples, n_features = 10, 5
>>> np.random.seed(0)
>>> y = np.random.randn(n_samples)
>>> X = np.random.randn(n_samples, n_features)
>>> clf = linear_model.SGDRegressor()
>>> clf.fit(X, y)
...
SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,
fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',
loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,
random_state=None, shuffle=True, verbose=0, warm_start=False)
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
densify()
Convert coefﬁcient matrix to dense array format.
fit(X, y[, coef_init, intercept_init, ...])
Fit linear model with Stochastic Gradient Descent.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
partial_fit(X, y[, sample_weight])
Fit linear model with Stochastic Gradient Descent.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*args, \*\*kwargs)
sparsify()
Convert coefﬁcient matrix to sparse format.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(loss=’squared_loss’,
penalty=’l2’,
alpha=0.0001,
l1_ratio=0.15,
ﬁt_intercept=True,
n_iter=5,
shufﬂe=True,
verbose=0,
epsilon=0.1,
random_state=None,
learn-
ing_rate=’invscaling’, eta0=0.01, power_t=0.25, warm_start=False, average=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Returnsarray, shape (n_samples,) :
Predicted target values per element in X.
densify()
Convert coefﬁcient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is
required for ﬁtting, so calling this method is only required on models that have previously been sparsiﬁed;
otherwise, it is a no-op.
Returnsself: estimator :
1590
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)
Fit linear model with Stochastic Gradient Descent.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training data
y : numpy array, shape (n_samples,)
Target values
coef_init : array, shape (n_features,)
The initial coefﬁcients to warm-start the optimization.
intercept_init : array, shape (1,)
The initial intercept to warm-start the optimization.
sample_weight : array-like, shape (n_samples,), optional
Weights applied to individual samples (1. for unweighted).
Returnsself : returns an instance of self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y, sample_weight=None)
Fit linear model with Stochastic Gradient Descent.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Subset of training data
y : numpy array of shape (n_samples,)
Subset of target values
sample_weight : array-like, shape (n_samples,), optional
Weights applied to individual samples. If not provided, uniform weights are assumed.
Returnsself : returns an instance of self.
29.18. sklearn.linear_model: Generalized Linear Models
1591
scikit-learn user guide, Release 0.18.2
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Returnsarray, shape (n_samples,) :
Predicted target values per element in X.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
sparsify()
Convert coefﬁcient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more
memory- and storage-efﬁcient than the usual numpy.ndarray representation.
The intercept_ member is not converted.
Returnsself: estimator :
Notes
For non-sparse models, i.e. when there are not many zeros in coef_, this may actually increase memory
usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be
computed with (coef_ == 0).sum(), must be more than 50% for this to provide signiﬁcant beneﬁts.
After calling this method, further ﬁtting with the partial_ﬁt method (if any) will not work until you call
densify.
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
1592
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.linear_model.SGDRegressor
• Prediction Latency
29.18.23 sklearn.linear_model.TheilSenRegressor
class sklearn.linear_model.TheilSenRegressor(ﬁt_intercept=True,
copy_X=True,
max_subpopulation=10000.0,
n_subsamples=None,
max_iter=300,
tol=0.001,
random_state=None,
n_jobs=1,
verbose=False)
Theil-Sen Estimator: robust multivariate regression model.
The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value
of n_subsamples between the number of features and samples leads to an estimator with a compromise between
robustness and efﬁciency. Since the number of least square solutions is “n_samples choose n_subsamples”, it
can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets
are chosen randomly. In a ﬁnal step, the spatial median (or L1 median) is calculated of all least square solutions.
Read more in the User Guide.
Parametersﬁt_intercept : boolean, optional, default True
Whether to calculate the intercept for this model. If set to false, no intercept will be
used in calculations.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
max_subpopulation : int, optional, default 1e4
Instead of computing with a set of cardinality ‘n choose k’, where n is the number
of samples and k is the number of subsamples (at least number of features), consider
only a stochastic subpopulation of a given maximal size if ‘n choose k’ is larger than
max_subpopulation. For other than small problem sizes this parameter will determine
memory usage and runtime if n_subsamples is not changed.
n_subsamples : int, optional, default None
Number of samples to calculate the parameters. This is at least the number of features
(plus 1 if ﬁt_intercept=True) and the number of samples as a maximum. A lower num-
ber leads to a higher breakdown point and a low efﬁciency while a high number leads
to a low breakdown point and a high efﬁciency. If None, take the minimum number
of subsamples leading to maximal robustness. If n_subsamples is set to n_samples,
Theil-Sen is identical to least squares.
max_iter : int, optional, default 300
29.18. sklearn.linear_model: Generalized Linear Models
1593
scikit-learn user guide, Release 0.18.2
Maximum number of iterations for the calculation of spatial median.
tol : ﬂoat, optional, default 1.e-3
Tolerance when calculating spatial median.
random_state : RandomState or an int seed, optional, default None
A random number generator instance to deﬁne the state of the random permutations
generator.
n_jobs : integer, optional, default 1
Number of CPUs to use during the cross validation. If -1, use all the CPUs.
verbose : boolean, optional, default False
Verbose mode when ﬁtting the model.
Attributescoef_ : array, shape = (n_features)
Coefﬁcients of the regression model (median of distribution).
intercept_ : ﬂoat
Estimated intercept of regression model.
breakdown_ : ﬂoat
Approximated breakdown point.
n_iter_ : int
Number of iterations needed for the spatial median.
n_subpopulation_ : int
Number of combinations taken into account from ‘n choose k’, where n is the number
of samples and k is the number of subsamples.
References
•Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin
Wang and Heping Zhang http://home.olemiss.edu/~xdang/papers/MTSE.pdf
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
fit(X, y)
Fit linear model.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(ﬁt_intercept=True,
copy_X=True,
max_subpopulation=10000.0,
n_subsamples=None,
max_iter=300, tol=0.001, random_state=None, n_jobs=1, verbose=False)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
1594
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y)
Fit linear model.
ParametersX : numpy array of shape [n_samples, n_features]
Training data
y : numpy array of shape [n_samples]
Target values
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
29.18. sklearn.linear_model: Generalized Linear Models
1595
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.linear_model.TheilSenRegressor
• Robust linear estimator ﬁtting
• Theil-Sen Regression
linear_model.lars_path(X, y[, Xy, Gram, ...])
Compute Least Angle Regression or Lasso path using
LARS algorithm [1]
linear_model.lasso_path(X, y[, eps, ...])
Compute Lasso path with coordinate descent
linear_model.lasso_stability_path(X,
y[,
...])
Stability path based on randomized Lasso estimates
linear_model.logistic_regression_path(X,
y)
Compute a Logistic Regression model for a list of regular-
ization parameters.
linear_model.orthogonal_mp(X, y[, ...])
Orthogonal Matching Pursuit (OMP)
linear_model.orthogonal_mp_gram(Gram, Xy[,
...])
Gram Orthogonal Matching Pursuit (OMP)
29.18.24 sklearn.linear_model.lars_path
sklearn.linear_model.lars_path(X, y, Xy=None, Gram=None, max_iter=500, alpha_min=0,
method=’lar’,
copy_X=True,
eps=2.2204460492503131e-
16,
copy_Gram=True,
verbose=0,
return_path=True,
re-
turn_n_iter=False, positive=False)
Compute Least Angle Regression or Lasso path using LARS algorithm [1]
The optimization objective for the case method=’lasso’ is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
in the case of method=’lars’, the objective function is only known in the form of an implicit equation (see
discussion in [1])
Read more in the User Guide.
ParametersX : array, shape: (n_samples, n_features)
Input data.
y : array, shape: (n_samples)
Input targets.
positive : boolean (default=False)
Restrict coefﬁcients to be >= 0. When using this option together with method ‘lasso’
the model coefﬁcients will not converge to the ordinary-least-squares solution for small
values of alpha (neither will they when using method ‘lar’ ..). Only coefﬁcients up to
the smallest alpha value (alphas_[alphas_ > 0.].min() when ﬁt_path=True)
1596
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
reached by the stepwise Lars-Lasso algorithm are typically in congruence with the so-
lution of the coordinate descent lasso_path function.
max_iter : integer, optional (default=500)
Maximum number of iterations to perform, set to inﬁnity for no limit.
Gram : None, ‘auto’, array, shape: (n_features, n_features), optional
Precomputed Gram matrix (X’ * X), if 'auto', the Gram matrix is precomputed from
the given X, if there are more samples than features.
alpha_min : ﬂoat, optional (default=0)
Minimum correlation along the path. It corresponds to the regularization parameter
alpha parameter in the Lasso.
method : {‘lar’, ‘lasso’}, optional (default=’lar’)
Speciﬁes the returned model. Select 'lar' for Least Angle Regression, 'lasso' for
the Lasso.
eps : ﬂoat, optional (default=‘‘np.ﬁnfo(np.ﬂoat).eps‘‘)
The machine-precision regularization in the computation of the Cholesky diagonal fac-
tors. Increase this for very ill-conditioned systems.
copy_X : bool, optional (default=True)
If False, X is overwritten.
copy_Gram : bool, optional (default=True)
If False, Gram is overwritten.
verbose : int (default=0)
Controls output verbosity.
return_path : bool, optional (default=True)
If return_path==True returns the entire path, else returns only the last point of the
path.
return_n_iter : bool, optional (default=False)
Whether to return the number of iterations.
Returnsalphas : array, shape: [n_alphas + 1]
Maximum of covariances (in absolute value) at each iteration.
n_alphas is ei-
ther max_iter, n_features or the number of nodes in the path with alpha >=
alpha_min, whichever is smaller.
active : array, shape [n_alphas]
Indices of active variables at the end of the path.
coefs : array, shape (n_features, n_alphas + 1)
Coefﬁcients along the path
n_iter : int
Number of iterations run. Returned only if return_n_iter is set to True.
See also:
lasso_path, LassoLars, Lars, LassoLarsCV, LarsCV, sklearn.decomposition.sparse_encode
29.18. sklearn.linear_model: Generalized Linear Models
1597
scikit-learn user guide, Release 0.18.2
References
[R38], [R39], [R40]
Examples using sklearn.linear_model.lars_path
• Lasso path using LARS
29.18.25 sklearn.linear_model.lasso_path
sklearn.linear_model.lasso_path(X, y,
eps=0.001,
n_alphas=100,
alphas=None,
precom-
pute=’auto’, Xy=None, copy_X=True, coef_init=None, ver-
bose=False, return_n_iter=False, positive=False, **params)
Compute Lasso path with coordinate descent
The Lasso optimization function varies for mono and multi-outputs.
For mono-output tasks it is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
For multi-output tasks it is:
(1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21
Where:
||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}
i.e. the sum of norm of each row.
Read more in the User Guide.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory
duplication. If y is mono-output then X can be sparse.
y : ndarray, shape (n_samples,), or (n_samples, n_outputs)
Target values
eps : ﬂoat, optional
Length of the path. eps=1e-3 means that alpha_min / alpha_max = 1e-3
n_alphas : int, optional
Number of alphas along the regularization path
alphas : ndarray, optional
List of alphas where to compute the models. If None alphas are set automatically
precompute : True | False | ‘auto’ | array-like
Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto'
let us decide. The Gram matrix can also be passed as argument.
Xy : array-like, optional
1598
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is
precomputed.
copy_X : boolean, optional, default True
If True, X will be copied; else, it may be overwritten.
coef_init : array, shape (n_features, ) | None
The initial values of the coefﬁcients.
verbose : bool or integer
Amount of verbosity.
params : kwargs
keyword arguments passed to the coordinate descent solver.
positive : bool, default False
If set to True, forces coefﬁcients to be positive.
return_n_iter : bool
whether to return the number of iterations or not.
Returnsalphas : array, shape (n_alphas,)
The alphas along the path where models are computed.
coefs : array, shape (n_features, n_alphas) or (n_outputs, n_features, n_alphas)
Coefﬁcients along the path.
dual_gaps : array, shape (n_alphas,)
The dual gaps at the end of the optimization for each alpha.
n_iters : array-like, shape (n_alphas,)
The number of iterations taken by the coordinate descent optimizer to reach the speciﬁed
tolerance for each alpha.
See also:
lars_path, Lasso, LassoLars, LassoCV, LassoLarsCV, sklearn.decomposition.sparse_encode
Notes
See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.
To avoid unnecessary memory duplication the X argument of the ﬁt method should be directly passed as a
Fortran-contiguous numpy array.
Note that in certain cases, the Lars solver may be signiﬁcantly faster to implement this functionality. In particu-
lar, linear interpolation can be used to retrieve model coefﬁcients between the values output by lars_path
Examples
Comparing lasso_path and lars_path with interpolation:
29.18. sklearn.linear_model: Generalized Linear Models
1599
scikit-learn user guide, Release 0.18.2
>>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T
>>> y = np.array([1, 2, 3.1])
>>> # Use lasso_path to compute a coefficient path
>>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])
>>> print(coef_path)
[[ 0.
0.
0.46874778]
[ 0.2159048
0.4425765
0.23689075]]
>>> # Now use lars_path and 1D linear interpolation to compute the
>>> # same path
>>> from sklearn.linear_model import lars_path
>>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')
>>> from scipy import interpolate
>>> coef_path_continuous = interpolate.interp1d(alphas[::-1],
...
coef_path_lars[:, ::-1])
>>> print(coef_path_continuous([5., 1., .5]))
[[ 0.
0.
0.46915237]
[ 0.2159048
0.4425765
0.23668876]]
Examples using sklearn.linear_model.lasso_path
• Lasso and Elastic Net
29.18.26 sklearn.linear_model.lasso_stability_path
sklearn.linear_model.lasso_stability_path(X,
y,
scaling=0.5,
ran-
dom_state=None,
n_resampling=200,
n_grid=100,
sample_fraction=0.75,
eps=8.8817841970012523e-16,
n_jobs=1,
verbose=False)
Stability path based on randomized Lasso estimates
Read more in the User Guide.
ParametersX : array-like, shape = [n_samples, n_features]
training data.
y : array-like, shape = [n_samples]
target values.
scaling : ﬂoat, optional, default=0.5
The alpha parameter in the stability selection article used to randomly scale the features.
Should be between 0 and 1.
random_state : integer or numpy.random.RandomState, optional
The generator used to randomize the design.
n_resampling : int, optional, default=200
Number of randomized models.
n_grid : int, optional, default=100
Number of grid points. The path is linearly reinterpolated on a grid between 0 and 1
before computing the scores.
1600
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sample_fraction : ﬂoat, optional, default=0.75
The fraction of samples to be used in each randomized design. Should be between 0
and 1. If 1, all samples are used.
eps : ﬂoat, optional
Smallest value of alpha / alpha_max considered
n_jobs : integer, optional
Number of CPUs to use during the resampling. If ‘-1’, use all the CPUs
verbose : boolean or integer, optional
Sets the verbosity amount
Returnsalphas_grid : array, shape ~ [n_grid]
The grid points between 0 and 1: alpha/alpha_max
scores_path : array, shape = [n_features, n_grid]
The scores for each feature along the path.
Notes
See examples/linear_model/plot_sparse_recovery.py for an example.
Examples using sklearn.linear_model.lasso_stability_path
• Sparse recovery: feature selection for sparse linear models
29.18.27 sklearn.linear_model.logistic_regression_path
sklearn.linear_model.logistic_regression_path(X,
y,
pos_class=None,
Cs=10,
ﬁt_intercept=True,
max_iter=100,
tol=0.0001,
verbose=0,
solver=’lbfgs’,
coef=None,
copy=False,
class_weight=None,
dual=False,
penalty=’l2’,
inter-
cept_scaling=1.0,
multi_class=’ovr’,
random_state=None, check_input=True,
max_squared_sum=None,
sam-
ple_weight=None)
Compute a Logistic Regression model for a list of regularization parameters.
This is an implementation that uses the result of the previous model to speed up computations along the set of
solutions, making it faster than sequentially calling LogisticRegression for the different parameters. Note that
there will be no speedup with liblinear solver, since it does not handle warm-starting.
Read more in the User Guide.
ParametersX : array-like or sparse matrix, shape (n_samples, n_features)
Input data.
y : array-like, shape (n_samples,)
Input data, target values.
29.18. sklearn.linear_model: Generalized Linear Models
1601
scikit-learn user guide, Release 0.18.2
Cs : int | array-like, shape (n_cs,)
List of values for the regularization parameter or integer specifying the number of reg-
ularization parameters that should be used. In this case, the parameters will be chosen
in a logarithmic scale between 1e-4 and 1e4.
pos_class : int, None
The class with respect to which we perform a one-vs-all ﬁt. If None, then it is assumed
that the given problem is binary.
ﬁt_intercept : bool
Whether to ﬁt an intercept for the model. In this case the shape of the returned array is
(n_cs, n_features + 1).
max_iter : int
Maximum number of iterations for the solver.
tol : ﬂoat
Stopping criterion. For the newton-cg and lbfgs solvers, the iteration will stop when
max{|g_i | i = 1,...,n} <= tol where g_i is the i-th component of the
gradient.
verbose : int
For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.
solver : {‘lbfgs’, ‘newton-cg’, ‘liblinear’, ‘sag’}
Numerical solver to use.
coef : array-like, shape (n_features,), default None
Initialization value for coefﬁcients of logistic regression. Useless for liblinear solver.
copy : bool, default False
Whether or not to produce a copy of the data. A copy is not required anymore. This
parameter is deprecated and will be removed in 0.19.
class_weight : dict or ‘balanced’, optional
Weights associated with classes in the form {class_label:
weight}. If not
given, all classes are supposed to have weight one.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y)).
Note that these weights will be multiplied with sample_weight (passed through the ﬁt
method) if sample_weight is speciﬁed.
dual : bool
Dual or primal formulation. Dual formulation is only implemented for l2 penalty with
liblinear solver. Prefer dual=False when n_samples > n_features.
penalty : str, ‘l1’ or ‘l2’
Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’
solvers support only l2 penalties.
intercept_scaling : ﬂoat, default 1.
1602
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Useful only when the solver ‘liblinear’ is used and self.ﬁt_intercept is set to True. In this
case, x becomes [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value
equal to intercept_scaling is appended to the instance vector. The intercept becomes
intercept_scaling * synthetic_feature_weight.
Note! the synthetic feature weight is subject to l1/l2 regularization as all other features.
To lessen the effect of regularization on synthetic feature weight (and therefore on the
intercept) intercept_scaling has to be increased.
multi_class : str, {‘ovr’, ‘multinomial’}
Multiclass option can be either ‘ovr’ or ‘multinomial’. If the option chosen is ‘ovr’, then
a binary problem is ﬁt for each label. Else the loss minimised is the multinomial loss
ﬁt across the entire probability distribution. Works only for the ‘lbfgs’ and ‘newton-cg’
solvers.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data. Used
only in solvers ‘sag’ and ‘liblinear’.
check_input : bool, default True
If False, the input arrays X and y will not be checked.
max_squared_sum : ﬂoat, default None
Maximum squared sum of X over samples. Used only in SAG solver. If None, it will
be computed, going through all the samples. The value should be precomputed to speed
up cross validation.
sample_weight : array-like, shape(n_samples,) optional
Array of weights that are assigned to individual samples. If not provided, then each
sample is given unit weight.
Returnscoefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)
List of coefﬁcients for the Logistic Regression model. If ﬁt_intercept is set to True
then the second dimension will be n_features + 1, where the last item represents the
intercept.
Cs : ndarray
Grid of Cs used for cross-validation.
n_iter : array, shape (n_cs,)
Actual number of iteration for each Cs.
Notes
You might get slightly different results with the solver liblinear than with the others since this uses LIBLINEAR
which penalizes the intercept.
29.18.28 sklearn.linear_model.orthogonal_mp
sklearn.linear_model.orthogonal_mp(X,
y,
n_nonzero_coefs=None,
tol=None,
precom-
pute=False,
copy_X=True,
return_path=False,
re-
turn_n_iter=False)
Orthogonal Matching Pursuit (OMP)
29.18. sklearn.linear_model: Generalized Linear Models
1603
scikit-learn user guide, Release 0.18.2
Solves n_targets Orthogonal Matching Pursuit problems. An instance of the problem has the form:
When parametrized by the number of non-zero coefﬁcients using n_nonzero_coefs: argmin ||y - Xgamma||^2
subject to ||gamma||_0 <= n_{nonzero coefs}
When parametrized by error using the parameter tol: argmin ||gamma||_0 subject to ||y - Xgamma||^2 <= tol
Read more in the User Guide.
ParametersX : array, shape (n_samples, n_features)
Input data. Columns are assumed to have unit norm.
y : array, shape (n_samples,) or (n_samples, n_targets)
Input targets
n_nonzero_coefs : int
Desired number of non-zero entries in the solution. If None (by default) this value is set
to 10% of n_features.
tol : ﬂoat
Maximum norm of the residual. If not None, overrides n_nonzero_coefs.
precompute : {True, False, ‘auto’},
Whether to perform precomputations.
Improves performance when n_targets or
n_samples is very large.
copy_X : bool, optional
Whether the design matrix X must be copied by the algorithm. A false value is only
helpful if X is already Fortran-ordered, otherwise a copy is made anyway.
return_path : bool, optional. Default: False
Whether to return every value of the nonzero coefﬁcients along the forward path. Useful
for cross-validation.
return_n_iter : bool, optional default False
Whether or not to return the number of iterations.
Returnscoef : array, shape (n_features,) or (n_features, n_targets)
Coefﬁcients of the OMP solution. If return_path=True, this contains the whole coef-
ﬁcient path. In this case its shape is (n_features, n_features) or (n_features, n_targets,
n_features) and iterating over the last axis yields coefﬁcients in increasing order of ac-
tive features.
n_iters : array-like or int
Number of active features across every target. Returned only if return_n_iter is set to
True.
See also:
OrthogonalMatchingPursuit,
orthogonal_mp_gram,
lars_path,
decomposition.sparse_encode
1604
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang, Matching pursuits with time-frequency
dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)
This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efﬁcient Implementation of
the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008.
http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf
29.18.29 sklearn.linear_model.orthogonal_mp_gram
sklearn.linear_model.orthogonal_mp_gram(Gram,
Xy,
n_nonzero_coefs=None,
tol=None,
norms_squared=None,
copy_Gram=True,
copy_Xy=True,
return_path=False,
re-
turn_n_iter=False)
Gram Orthogonal Matching Pursuit (OMP)
Solves n_targets Orthogonal Matching Pursuit problems using only the Gram matrix X.T * X and the product
X.T * y.
Read more in the User Guide.
ParametersGram : array, shape (n_features, n_features)
Gram matrix of the input data: X.T * X
Xy : array, shape (n_features,) or (n_features, n_targets)
Input targets multiplied by X: X.T * y
n_nonzero_coefs : int
Desired number of non-zero entries in the solution. If None (by default) this value is set
to 10% of n_features.
tol : ﬂoat
Maximum norm of the residual. If not None, overrides n_nonzero_coefs.
norms_squared : array-like, shape (n_targets,)
Squared L2 norms of the lines of y. Required if tol is not None.
copy_Gram : bool, optional
Whether the gram matrix must be copied by the algorithm. A false value is only helpful
if it is already Fortran-ordered, otherwise a copy is made anyway.
copy_Xy : bool, optional
Whether the covariance vector Xy must be copied by the algorithm. If False, it may be
overwritten.
return_path : bool, optional. Default: False
Whether to return every value of the nonzero coefﬁcients along the forward path. Useful
for cross-validation.
return_n_iter : bool, optional default False
Whether or not to return the number of iterations.
Returnscoef : array, shape (n_features,) or (n_features, n_targets)
29.18. sklearn.linear_model: Generalized Linear Models
1605
scikit-learn user guide, Release 0.18.2
Coefﬁcients of the OMP solution. If return_path=True, this contains the whole coef-
ﬁcient path. In this case its shape is (n_features, n_features) or (n_features, n_targets,
n_features) and iterating over the last axis yields coefﬁcients in increasing order of ac-
tive features.
n_iters : array-like or int
Number of active features across every target. Returned only if return_n_iter is set to
True.
See also:
OrthogonalMatchingPursuit, orthogonal_mp, lars_path, decomposition.sparse_encode
Notes
Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang, Matching pursuits with time-frequency
dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)
This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efﬁcient Implementation of
the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008.
http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf
29.19 sklearn.manifold: Manifold Learning
The sklearn.manifold module implements data embedding techniques.
User guide: See the Manifold learning section for further details.
manifold.LocallyLinearEmbedding([...])
Locally Linear Embedding
manifold.Isomap([n_neighbors, n_components, ...])
Isomap Embedding
manifold.MDS([n_components, metric, n_init, ...])
Multidimensional scaling
manifold.SpectralEmbedding([n_components,
...])
Spectral embedding for non-linear dimensionality reduc-
tion.
manifold.TSNE([n_components, perplexity, ...])
t-distributed Stochastic Neighbor Embedding.
29.19.1 sklearn.manifold.LocallyLinearEmbedding
class sklearn.manifold.LocallyLinearEmbedding(n_neighbors=5,
n_components=2,
reg=0.001,
eigen_solver=’auto’,
tol=1e-
06,
max_iter=100,
method=’standard’,
hessian_tol=0.0001,
modiﬁed_tol=1e-
12,
neighbors_algorithm=’auto’,
ran-
dom_state=None, n_jobs=1)
Locally Linear Embedding
Read more in the User Guide.
Parametersn_neighbors : integer
number of neighbors to consider for each point.
n_components : integer
number of coordinates for the manifold
1606
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
reg : ﬂoat
regularization constant, multiplies the trace of the local covariance matrix of the dis-
tances.
eigen_solver : string, {‘auto’, ‘arpack’, ‘dense’}
auto : algorithm will attempt to choose the best method for input data
arpack[use arnoldi iteration in shift-invert mode.] For this method, M may be a dense
matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable
for some problems. It is best to try several random seeds in order to check results.
dense[use standard dense matrix operations for the eigenvalue] decomposition. For this
method, M must be an array or matrix type. This method should be avoided for large
problems.
tol : ﬂoat, optional
Tolerance for ‘arpack’ method Not used if eigen_solver==’dense’.
max_iter : integer
maximum
number
of
iterations
for
the
arpack
solver.
Not
used
if
eigen_solver==’dense’.
method : string (‘standard’, ‘hessian’, ‘modiﬁed’ or ‘ltsa’)
standard[use the standard locally linear embedding algorithm. see] reference [1]
hessian[use the Hessian eigenmap method. This method requires] n_neighbors >
n_components * (1 + (n_components + 1) / 2 see reference [2]
modiﬁed[use the modiﬁed locally linear embedding algorithm.] see reference [3]
ltsa[use local tangent space alignment algorithm] see reference [4]
hessian_tol : ﬂoat, optional
Tolerance for Hessian eigenmapping method. Only used if method == 'hessian'
modiﬁed_tol : ﬂoat, optional
Tolerance for modiﬁed LLE method. Only used if method == 'modified'
neighbors_algorithm : string [’auto’|’brute’|’kd_tree’|’ball_tree’]
algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors
instance
random_state: numpy.RandomState or int, optional :
The generator or seed used to determine the starting vector for arpack iterations. De-
faults to numpy.random.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run. If -1, then the number of jobs is set to the number
of CPU cores.
Attributesembedding_vectors_ : array-like, shape [n_components, n_samples]
Stores the embedding vectors
reconstruction_error_ : ﬂoat
Reconstruction error associated with embedding_vectors_
29.19. sklearn.manifold: Manifold Learning
1607
scikit-learn user guide, Release 0.18.2
nbrs_ : NearestNeighbors object
Stores nearest neighbors instance, including BallTree or KDtree if applicable.
References
[R42], [R43], [R44], [R45]
Methods
fit(X[, y])
Compute the embedding vectors for data X
fit_transform(X[, y])
Compute the embedding vectors for data X and trans-
form X.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform new points into embedding space.
__init__(n_neighbors=5,
n_components=2,
reg=0.001,
eigen_solver=’auto’,
tol=1e-06,
max_iter=100,
method=’standard’,
hessian_tol=0.0001,
modiﬁed_tol=1e-12,
neigh-
bors_algorithm=’auto’, random_state=None, n_jobs=1)
fit(X, y=None)
Compute the embedding vectors for data X
ParametersX : array-like of shape [n_samples, n_features]
training set.
Returnsself : returns an instance of self.
fit_transform(X, y=None)
Compute the embedding vectors for data X and transform X.
ParametersX : array-like of shape [n_samples, n_features]
training set.
ReturnsX_new: array-like, shape (n_samples, n_components) :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
1608
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
transform(X)
Transform new points into embedding space.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsX_new : array, shape = [n_samples, n_components]
Notes
Because of scaling performed by this method, it is discouraged to use it together with methods that are not
scale-invariant (like SVMs)
Examples using sklearn.manifold.LocallyLinearEmbedding
• Visualizing the stock market structure
• Comparison of Manifold Learning methods
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
• Manifold Learning methods on a severed sphere
29.19.2 sklearn.manifold.Isomap
class sklearn.manifold.Isomap(n_neighbors=5,
n_components=2,
eigen_solver=’auto’,
tol=0,
max_iter=None, path_method=’auto’, neighbors_algorithm=’auto’,
n_jobs=1)
Isomap Embedding
Non-linear dimensionality reduction through Isometric Mapping
Read more in the User Guide.
Parametersn_neighbors : integer
number of neighbors to consider for each point.
n_components : integer
number of coordinates for the manifold
eigen_solver : [’auto’|’arpack’|’dense’]
‘auto’ : Attempt to choose the most efﬁcient solver for the given problem.
‘arpack’ : Use Arnoldi decomposition to ﬁnd the eigenvalues and eigenvectors.
‘dense’ : Use a direct solver (i.e. LAPACK) for the eigenvalue decomposition.
tol : ﬂoat
Convergence tolerance passed to arpack or lobpcg. not used if eigen_solver == ‘dense’.
max_iter : integer
Maximum number of iterations for the arpack solver.
not used if eigen_solver ==
‘dense’.
path_method : string [’auto’|’FW’|’D’]
29.19. sklearn.manifold: Manifold Learning
1609
scikit-learn user guide, Release 0.18.2
Method to use in ﬁnding shortest path.
‘auto’ : attempt to choose the best algorithm automatically.
‘FW’ : Floyd-Warshall algorithm.
‘D’ : Dijkstra’s algorithm.
neighbors_algorithm : string [’auto’|’brute’|’kd_tree’|’ball_tree’]
Algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors
instance.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run. If -1, then the number of jobs is set to the number
of CPU cores.
Attributesembedding_ : array-like, shape (n_samples, n_components)
Stores the embedding vectors.
kernel_pca_ : object
KernelPCA object used to implement the embedding.
training_data_ : array-like, shape (n_samples, n_features)
Stores the training data.
nbrs_ : sklearn.neighbors.NearestNeighbors instance
Stores nearest neighbors instance, including BallTree or KDtree if applicable.
dist_matrix_ : array-like, shape (n_samples, n_samples)
Stores the geodesic distance matrix of training data.
References
[R41]
Methods
fit(X[, y])
Compute the embedding vectors for data X
fit_transform(X[, y])
Fit the model from data in X and transform X.
get_params([deep])
Get parameters for this estimator.
reconstruction_error()
Compute the reconstruction error for the embedding.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform X.
__init__(n_neighbors=5,
n_components=2,
eigen_solver=’auto’,
tol=0,
max_iter=None,
path_method=’auto’, neighbors_algorithm=’auto’, n_jobs=1)
fit(X, y=None)
Compute the embedding vectors for data X
ParametersX : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}
Sample data, shape = (n_samples, n_features), in the form of a numpy array, precom-
puted tree, or NearestNeighbors object.
1610
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsself : returns an instance of self.
fit_transform(X, y=None)
Fit the model from data in X and transform X.
ParametersX: {array-like, sparse matrix, BallTree, KDTree} :
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
ReturnsX_new: array-like, shape (n_samples, n_components) :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
reconstruction_error()
Compute the reconstruction error for the embedding.
Returnsreconstruction_error : ﬂoat
Notes
The cost function of an isomap embedding is
E = frobenius_norm[K(D) -K(D_fit)] / n_samples
Where D is the matrix of distances for the input data X, D_ﬁt is the matrix of distances for the output
embedding X_ﬁt, and K is the isomap kernel:
K(D) = -0.5 * (I -1/n_samples) * D^2 * (I -1/n_samples)
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transform X.
This is implemented by linking the points X into the graph of geodesic distances of the training data. First
the n_neighbors nearest neighbors of X are found in the training data, and from these the shortest geodesic
distances from each point in X to each point in the training data are computed in order to construct the
kernel. The embedding of X is the projection of this kernel onto the embedding vectors of the training set.
ParametersX: array-like, shape (n_samples, n_features) :
ReturnsX_new: array-like, shape (n_samples, n_components) :
29.19. sklearn.manifold: Manifold Learning
1611
scikit-learn user guide, Release 0.18.2
Examples using sklearn.manifold.Isomap
• Comparison of Manifold Learning methods
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
• Manifold Learning methods on a severed sphere
29.19.3 sklearn.manifold.MDS
class sklearn.manifold.MDS(n_components=2, metric=True, n_init=4, max_iter=300, verbose=0,
eps=0.001, n_jobs=1, random_state=None, dissimilarity=’euclidean’)
Multidimensional scaling
Read more in the User Guide.
Parametersmetric : boolean, optional, default: True
compute metric or nonmetric SMACOF (Scaling by Majorizing a Complicated Func-
tion) algorithm
n_components : int, optional, default: 2
number of dimension in which to immerse the similarities overridden if initial array is
provided.
n_init : int, optional, default: 4
Number of time the smacof algorithm will be run with different initialisation. The ﬁnal
results will be the best output of the n_init consecutive runs in terms of stress.
max_iter : int, optional, default: 300
Maximum number of iterations of the SMACOF algorithm for a single run
verbose : int, optional, default: 0
level of verbosity
eps : ﬂoat, optional, default: 1e-6
relative tolerance w.r.t stress to declare converge
n_jobs : int, optional, default: 1
The number of jobs to use for the computation. This works by breaking down the
pairwise matrix into n_jobs even slices and computing them in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
random_state : integer or numpy.RandomState, optional
The generator used to initialize the centers. If an integer is given, it ﬁxes the seed.
Defaults to the global numpy random number generator.
dissimilarity : string
Which dissimilarity measure to use. Supported are ‘euclidean’ and ‘precomputed’.
Attributesembedding_ : array-like, shape [n_components, n_samples]
Stores the position of the dataset in the embedding space
stress_ : ﬂoat
1612
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The ﬁnal value of the stress (sum of squared distance of the disparities and the distances
for all constrained points)
References
“Modern Multidimensional Scaling - Theory and Applications” Borg, I.; Groenen P. Springer Series in Statistics
(1997)
“Nonmetric multidimensional scaling: a numerical method” Kruskal, J. Psychometrika, 29 (1964)
“Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis” Kruskal, J. Psychometrika,
29, (1964)
Methods
fit(X[, y, init])
Computes the position of the points in the embedding
space
fit_transform(X[, y, init])
Fit the data from X, and returns the embedded coordi-
nates
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_components=2, metric=True, n_init=4, max_iter=300, verbose=0, eps=0.001, n_jobs=1,
random_state=None, dissimilarity=’euclidean’)
fit(X, y=None, init=None)
Computes the position of the points in the embedding space
ParametersX : array, shape=[n_samples, n_features], or [n_samples, n_samples] if dissimilar-
ity=’precomputed’
Input data.
init : {None or ndarray, shape (n_samples,)}, optional
If None, randomly chooses the initial conﬁguration if ndarray, initialize the SMACOF
algorithm with this array.
fit_transform(X, y=None, init=None)
Fit the data from X, and returns the embedded coordinates
ParametersX : array, shape=[n_samples, n_features], or [n_samples, n_samples] if dissimilar-
ity=’precomputed’
Input data.
init : {None or ndarray, shape (n_samples,)}, optional
If None, randomly chooses the initial conﬁguration if ndarray, initialize the SMACOF
algorithm with this array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
29.19. sklearn.manifold: Manifold Learning
1613
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.manifold.MDS
• Comparison of Manifold Learning methods
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
• Manifold Learning methods on a severed sphere
• Multi-dimensional scaling
29.19.4 sklearn.manifold.SpectralEmbedding
class sklearn.manifold.SpectralEmbedding(n_components=2,
afﬁnity=’nearest_neighbors’,
gamma=None,
random_state=None,
eigen_solver=None, n_neighbors=None, n_jobs=1)
Spectral embedding for non-linear dimensionality reduction.
Forms an afﬁnity matrix given by the speciﬁed function and applies spectral decomposition to the corresponding
graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.
Read more in the User Guide.
Parametersn_components : integer, default: 2
The dimension of the projected subspace.
eigen_solver : {None, ‘arpack’, ‘lobpcg’, or ‘amg’}
The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It
can be faster on very large, sparse problems, but may also lead to instabilities.
random_state : int seed, RandomState instance, or None, default
A pseudo random number generator used for the initialization of the lobpcg eigenvectors
decomposition when eigen_solver == ‘amg’.
afﬁnity : string or callable, default
How to construct the afﬁnity matrix.
•‘nearest_neighbors’ : construct afﬁnity matrix by knn graph
•‘rbf’ : construct afﬁnity matrix by rbf kernel
•‘precomputed’ : interpret X as precomputed afﬁnity matrix
•callable : use passed in function as afﬁnity the function takes in data matrix
(n_samples, n_features) and return afﬁnity matrix (n_samples, n_samples).
gamma : ﬂoat, optional, default
1614
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Kernel coefﬁcient for rbf kernel.
n_neighbors : int, default
Number of nearest neighbors for nearest_neighbors graph building.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run. If -1, then the number of jobs is set to the number
of CPU cores.
Attributesembedding_ : array, shape = (n_samples, n_components)
Spectral embedding of the training matrix.
afﬁnity_matrix_ : array, shape = (n_samples, n_samples)
Afﬁnity_matrix constructed from samples or precomputed.
References
•A Tutorial on Spectral Clustering, 2007 Ulrike von Luxburg http://citeseerx.ist.psu.edu/viewdoc/
summary?doi=10.1.1.165.9323
•On Spectral Clustering: Analysis and an algorithm, 2011 Andrew Y. Ng, Michael I. Jordan, Yair Weiss
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100
•Normalized cuts and image segmentation, 2000 Jianbo Shi, Jitendra Malik http://citeseer.ist.psu.edu/
viewdoc/summary?doi=10.1.1.160.2324
Methods
fit(X[, y])
Fit the model from data in X.
fit_transform(X[, y])
Fit the model from data in X and transform X.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_components=2,
afﬁnity=’nearest_neighbors’,
gamma=None,
random_state=None,
eigen_solver=None, n_neighbors=None, n_jobs=1)
fit(X, y=None)
Fit the model from data in X.
ParametersX : array-like, shape (n_samples, n_features)
Training vector, where n_samples is the number of samples and n_features is the number
of features.
If afﬁnity is “precomputed” X : array-like, shape (n_samples, n_samples), Interpret X
as precomputed adjacency graph computed from samples.
Returnsself : object
Returns the instance itself.
fit_transform(X, y=None)
Fit the model from data in X and transform X.
ParametersX: array-like, shape (n_samples, n_features) :
29.19. sklearn.manifold: Manifold Learning
1615
scikit-learn user guide, Release 0.18.2
Training vector, where n_samples is the number of samples and n_features is the number
of features.
If afﬁnity is “precomputed” X : array-like, shape (n_samples, n_samples), Interpret X
as precomputed adjacency graph computed from samples.
ReturnsX_new: array-like, shape (n_samples, n_components) :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.manifold.SpectralEmbedding
• Various Agglomerative Clustering on a 2D embedding of digits
• Comparison of Manifold Learning methods
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
• Manifold Learning methods on a severed sphere
29.19.5 sklearn.manifold.TSNE
class sklearn.manifold.TSNE(n_components=2,
perplexity=30.0,
early_exaggeration=4.0,
learn-
ing_rate=1000.0,
n_iter=1000,
n_iter_without_progress=30,
min_grad_norm=1e-07, metric=’euclidean’, init=’random’, verbose=0,
random_state=None, method=’barnes_hut’, angle=0.5)
t-distributed Stochastic Neighbor Embedding.
t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint
probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-
dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with
different initializations we can get different results.
It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or Truncat-
edSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of
features is very high. This will suppress some noise and speed up the computation of pairwise distances between
samples. For more tips see Laurens van der Maaten’s FAQ [2].
Read more in the User Guide.
Parametersn_components : int, optional (default: 2)
Dimension of the embedded space.
1616
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
perplexity : ﬂoat, optional (default: 30)
The perplexity is related to the number of nearest neighbors that is used in other man-
ifold learning algorithms. Larger datasets usually require a larger perplexity. Consider
selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is
quite insensitive to this parameter.
early_exaggeration : ﬂoat, optional (default: 4.0)
Controls how tight natural clusters in the original space are in the embedded space and
how much space will be between them. For larger values, the space between natural
clusters will be larger in the embedded space. Again, the choice of this parameter is
not very critical. If the cost function increases during initial optimization, the early
exaggeration factor or the learning rate might be too high.
learning_rate : ﬂoat, optional (default: 1000)
The learning rate can be a critical parameter. It should be between 100 and 1000. If the
cost function increases during initial optimization, the early exaggeration factor or the
learning rate might be too high. If the cost function gets stuck in a bad local minimum
increasing the learning rate helps sometimes.
n_iter : int, optional (default: 1000)
Maximum number of iterations for the optimization. Should be at least 200.
n_iter_without_progress : int, optional (default: 30)
Only used if method=’exact’ Maximum number of iterations without progress before
we abort the optimization. If method=’barnes_hut’ this parameter is ﬁxed to a value of
30 and cannot be changed.
New in version 0.17: parameter n_iter_without_progress to control stopping criteria.
min_grad_norm : ﬂoat, optional (default: 1e-7)
Only used if method=’exact’ If the gradient norm is below this threshold, the optimiza-
tion will be aborted. If method=’barnes_hut’ this parameter is ﬁxed to a value of 1e-3
and cannot be changed.
metric : string or callable, optional
The metric to use when calculating distance between instances in a feature
array.
If metric is a string,
it must be one of the options allowed by
scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pair-
wise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed
to be a distance matrix. Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable should take two
arrays from X as input and return a value indicating the distance between them. The
default is “euclidean” which is interpreted as squared euclidean distance.
init : string or numpy array, optional (default: “random”)
Initialization of embedding. Possible options are ‘random’, ‘pca’, and a numpy array of
shape (n_samples, n_components). PCA initialization cannot be used with precomputed
distances and is usually more globally stable than random initialization.
verbose : int, optional (default: 0)
Verbosity level.
random_state : int or RandomState instance or None (default)
29.19. sklearn.manifold: Manifold Learning
1617
scikit-learn user guide, Release 0.18.2
Pseudo Random Number generator seed control. If None, use the numpy.random sin-
gleton. Note that different initializations might result in different local minima of the
cost function.
method : string (default: ‘barnes_hut’)
By default the gradient calculation algorithm uses Barnes-Hut approximation running in
O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2)
time. The exact algorithm should be used when nearest-neighbor errors need to be better
than 3%. However, the exact method cannot scale to millions of examples.
New in version 0.17: Approximate optimization method via the Barnes-Hut.
angle : ﬂoat (default: 0.5)
Only used if method=’barnes_hut’ This is the trade-off between speed and accuracy for
Barnes-Hut T-SNE. ‘angle’ is the angular size (referred to as theta in [3]) of a distant
node as measured from a point. If this size is below ‘angle’ then it is used as a summary
node of all points contained within it. This method is not very sensitive to changes
in this parameter in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing
computation time and angle greater 0.8 has quickly increasing error.
Attributesembedding_ : array-like, shape (n_samples, n_components)
Stores the embedding vectors.
kl_divergence_ : ﬂoat
Kullback-Leibler divergence after optimization.
References
[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional DataUsing t-SNE. Journal of Ma-
chine Learning Research 9:2579-2605, 2008.
[2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embeddinghttp://homepage.tudelft.nl/19j49/
t-SNE.html
[3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.Journal of Machine Learning
Research 15(Oct):3221-3245, 2014. http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf
Examples
>>> import numpy as np
>>> from sklearn.manifold import TSNE
>>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
>>> model = TSNE(n_components=2, random_state=0)
>>> np.set_printoptions(suppress=True)
>>> model.fit_transform(X)
array([[ 0.00017599,
0.00003993],
[ 0.00009891,
0.00021913],
[ 0.00018554, -0.00009357],
[ 0.00009528, -0.00001407]])
Methods
1618
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
fit(X[, y])
Fit X into an embedded space.
fit_transform(X[, y])
Fit X into an embedded space and return that trans-
formed output.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_components=2,
perplexity=30.0,
early_exaggeration=4.0,
learning_rate=1000.0,
n_iter=1000, n_iter_without_progress=30, min_grad_norm=1e-07, metric=’euclidean’,
init=’random’, verbose=0, random_state=None, method=’barnes_hut’, angle=0.5)
fit(X, y=None)
Fit X into an embedded space.
ParametersX : array, shape (n_samples, n_features) or (n_samples, n_samples)
If the metric is ‘precomputed’ X must be a square distance matrix. Otherwise it contains
a sample per row. If the method is ‘exact’, X may be a sparse matrix of type ‘csr’, ‘csc’
or ‘coo’.
fit_transform(X, y=None)
Fit X into an embedded space and return that transformed output.
ParametersX : array, shape (n_samples, n_features) or (n_samples, n_samples)
If the metric is ‘precomputed’ X must be a square distance matrix. Otherwise it contains
a sample per row.
ReturnsX_new : array, shape (n_samples, n_components)
Embedding of the training data in low-dimensional space.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.manifold.TSNE
• Comparison of Manifold Learning methods
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
• Manifold Learning methods on a severed sphere
29.19. sklearn.manifold: Manifold Learning
1619
scikit-learn user guide, Release 0.18.2
manifold.locally_linear_embedding(X,
...[,
...])
Perform a Locally Linear Embedding analysis on the data.
manifold.spectral_embedding(adjacency[, ...])
Project the sample on the ﬁrst eigenvectors of the graph
Laplacian.
29.19.6 sklearn.manifold.locally_linear_embedding
sklearn.manifold.locally_linear_embedding(X,
n_neighbors,
n_components,
reg=0.001,
eigen_solver=’auto’, tol=1e-06, max_iter=100,
method=’standard’,
hessian_tol=0.0001,
modiﬁed_tol=1e-12,
random_state=None,
n_jobs=1)
Perform a Locally Linear Embedding analysis on the data.
Read more in the User Guide.
ParametersX : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}
Sample data, shape = (n_samples, n_features), in the form of a numpy array, sparse
array, precomputed tree, or NearestNeighbors object.
n_neighbors : integer
number of neighbors to consider for each point.
n_components : integer
number of coordinates for the manifold.
reg : ﬂoat
regularization constant, multiplies the trace of the local covariance matrix of the dis-
tances.
eigen_solver : string, {‘auto’, ‘arpack’, ‘dense’}
auto : algorithm will attempt to choose the best method for input data
arpack[use arnoldi iteration in shift-invert mode.] For this method, M may be a dense
matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable
for some problems. It is best to try several random seeds in order to check results.
dense[use standard dense matrix operations for the eigenvalue] decomposition. For this
method, M must be an array or matrix type. This method should be avoided for large
problems.
tol : ﬂoat, optional
Tolerance for ‘arpack’ method Not used if eigen_solver==’dense’.
max_iter : integer
maximum number of iterations for the arpack solver.
method : {‘standard’, ‘hessian’, ‘modiﬁed’, ‘ltsa’}
standard[use the standard locally linear embedding algorithm.] see reference [R46]
hessian[use the Hessian eigenmap method.
This method requires] n_neighbors >
n_components * (1 + (n_components + 1) / 2. see reference [R47]
modiﬁed[use the modiﬁed locally linear embedding algorithm.] see reference [R48]
1620
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ltsa[use local tangent space alignment algorithm] see reference [R49]
hessian_tol : ﬂoat, optional
Tolerance for Hessian eigenmapping method. Only used if method == ‘hessian’
modiﬁed_tol : ﬂoat, optional
Tolerance for modiﬁed LLE method. Only used if method == ‘modiﬁed’
random_state: numpy.RandomState or int, optional :
The generator or seed used to determine the starting vector for arpack iterations. De-
faults to numpy.random.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run for neighbors search. If -1, then the number of jobs
is set to the number of CPU cores.
ReturnsY : array-like, shape [n_samples, n_components]
Embedding vectors.
squared_error : ﬂoat
Reconstruction error for the embedding vectors.
Equivalent to norm(Y -W
Y,'fro')**2, where W are the reconstruction weights.
References
[R46], [R47], [R48], [R49]
Examples using sklearn.manifold.locally_linear_embedding
• Swiss Roll reduction with LLE
29.19.7 sklearn.manifold.spectral_embedding
sklearn.manifold.spectral_embedding(adjacency, n_components=8, eigen_solver=None, ran-
dom_state=None, eigen_tol=0.0, norm_laplacian=True,
drop_ﬁrst=True)
Project the sample on the ﬁrst eigenvectors of the graph Laplacian.
The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigen-
vectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts neces-
sary to split the graph into comparably sized components.
This embedding can also ‘work’ even if the adjacency variable is not strictly the adjacency matrix of a graph
but more generally an afﬁnity or similarity matrix between samples (for instance the heat kernel of a euclidean
distance matrix or a k-NN matrix).
However care must taken to always make the afﬁnity matrix symmetric so that the eigenvector decomposition
works as expected.
Read more in the User Guide.
Parametersadjacency : array-like or sparse matrix, shape: (n_samples, n_samples)
The adjacency matrix of the graph to embed.
29.19. sklearn.manifold: Manifold Learning
1621
scikit-learn user guide, Release 0.18.2
n_components : integer, optional, default 8
The dimension of the projection subspace.
eigen_solver : {None, ‘arpack’, ‘lobpcg’, or ‘amg’}, default None
The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It
can be faster on very large, sparse problems, but may also lead to instabilities.
random_state : int seed, RandomState instance, or None (default)
A pseudo random number generator used for the initialization of the lobpcg eigenvectors
decomposition when eigen_solver == ‘amg’. By default, arpack is used.
eigen_tol : ﬂoat, optional, default=0.0
Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack
eigen_solver.
drop_ﬁrst : bool, optional, default=True
Whether to drop the ﬁrst eigenvector. For spectral embedding, this should be True as
the ﬁrst eigenvector should be constant vector for connected graph, but for spectral
clustering, this should be kept as False to retain the ﬁrst eigenvector.
norm_laplacian : bool, optional, default=True
If True, then compute normalized Laplacian.
Returnsembedding : array, shape=(n_samples, n_components)
The reduced samples.
Notes
Spectral embedding is most useful when the graph has one connected component. If there graph has many
components, the ﬁrst few eigenvectors will simply uncover the connected components of the graph.
References
•https://en.wikipedia.org/wiki/LOBPCG
•Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gra-
dient Method Andrew V. Knyazev http://dx.doi.org/10.1137%2FS1064827500366124
29.20 sklearn.metrics: Metrics
See the Model evaluation: quantifying the quality of predictions section and the Pairwise metrics, Afﬁnities and
Kernels section of the user guide for further details.
The sklearn.metrics module includes score functions,
performance metrics and pairwise metrics and distance computations.
29.20.1 Model Selection Interface
See the The scoring parameter: deﬁning model evaluation rules section of the user guide for further details.
metrics.make_scorer(score_func[, ...])
Make a scorer from a performance metric or loss function.
Continued on next page
1622
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.169 – continued from previous page
metrics.get_scorer(scoring)
sklearn.metrics.make_scorer
sklearn.metrics.make_scorer(score_func,
greater_is_better=True,
needs_proba=False,
needs_threshold=False, **kwargs)
Make a scorer from a performance metric or loss function.
This factory function wraps scoring functions for use in GridSearchCV and cross_val_score.
It takes
a score function, such as accuracy_score, mean_squared_error, adjusted_rand_index or
average_precision and returns a callable that scores an estimator’s output.
Read more in the User Guide.
Parametersscore_func : callable,
Score
function
(or
loss
function)
with
signature
score_func(y,y_pred,**kwargs).
greater_is_better : boolean, default=True
Whether score_func is a score function (default), meaning high is good, or a loss func-
tion, meaning low is good. In the latter case, the scorer object will sign-ﬂip the outcome
of the score_func.
needs_proba : boolean, default=False
Whether score_func requires predict_proba to get probability estimates out of a classi-
ﬁer.
needs_threshold : boolean, default=False
Whether score_func takes a continuous decision certainty. This only works for binary
classiﬁcation using estimators that have either a decision_function or predict_proba
method.
For example average_precision or the area under the roc curve can not be com-
puted using discrete predictions alone.
**kwargs : additional arguments
Additional parameters to be passed to score_func.
Returnsscorer : callable
Callable object that returns a scalar score; greater is better.
Examples
>>> from sklearn.metrics import fbeta_score, make_scorer
>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
>>> ftwo_scorer
make_scorer(fbeta_score, beta=2)
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.svm import LinearSVC
>>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
...
scoring=ftwo_scorer)
29.20. sklearn.metrics: Metrics
1623
scikit-learn user guide, Release 0.18.2
sklearn.metrics.get_scorer
sklearn.metrics.get_scorer(scoring)
29.20.2 Classiﬁcation metrics
See the Classiﬁcation metrics section of the user guide for further details.
metrics.accuracy_score(y_true, y_pred[, ...])
Accuracy classiﬁcation score.
metrics.auc(x, y[, reorder])
Compute Area Under the Curve (AUC) using the trape-
zoidal rule
metrics.average_precision_score(y_true,
y_score)
Compute average precision (AP) from prediction scores
metrics.brier_score_loss(y_true, y_prob[, ...])
Compute the Brier score.
metrics.classification_report(y_true,
y_pred)
Build a text report showing the main classiﬁcation metrics
metrics.cohen_kappa_score(y1, y2[, labels, ...])
Cohen’s kappa: a statistic that measures inter-annotator
agreement.
metrics.confusion_matrix(y_true, y_pred[, ...])
Compute confusion matrix to evaluate the accuracy of a
classiﬁcation
metrics.f1_score(y_true, y_pred[, labels, ...])
Compute the F1 score, also known as balanced F-score or
F-measure
metrics.fbeta_score(y_true, y_pred, beta[, ...])
Compute the F-beta score
metrics.hamming_loss(y_true, y_pred[, ...])
Compute the average Hamming loss.
metrics.hinge_loss(y_true, pred_decision[, ...])
Average hinge loss (non-regularized)
metrics.jaccard_similarity_score(y_true,
y_pred)
Jaccard similarity coefﬁcient score
metrics.log_loss(y_true, y_pred[, eps, ...])
Log loss, aka logistic loss or cross-entropy loss.
metrics.matthews_corrcoef(y_true, y_pred[, ...])
Compute the Matthews correlation coefﬁcient (MCC) for
binary classes
metrics.precision_recall_curve(y_true, ...)
Compute precision-recall pairs for different probability
thresholds
metrics.precision_recall_fscore_support(...)Compute precision, recall, F-measure and support for each
class
metrics.precision_score(y_true, y_pred[, ...])
Compute the precision
metrics.recall_score(y_true, y_pred[, ...])
Compute the recall
metrics.roc_auc_score(y_true, y_score[, ...])
Compute Area Under the Curve (AUC) from prediction
scores
metrics.roc_curve(y_true, y_score[, ...])
Compute Receiver operating characteristic (ROC)
metrics.zero_one_loss(y_true, y_pred[, ...])
Zero-one classiﬁcation loss.
sklearn.metrics.accuracy_score
sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)
Accuracy classiﬁcation score.
In multilabel classiﬁcation, this function computes subset accuracy: the set of labels predicted for a sample must
exactly match the corresponding set of labels in y_true.
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
1624
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Ground truth (correct) labels.
y_pred : 1d array-like, or label indicator array / sparse matrix
Predicted labels, as returned by a classiﬁer.
normalize : bool, optional (default=True)
If False, return the number of correctly classiﬁed samples. Otherwise, return the
fraction of correctly classiﬁed samples.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
If normalize == True, return the correctly classiﬁed samples (ﬂoat), else it returns
the number of correctly classiﬁed samples (int).
The best performance is 1 with normalize == True and the number of samples
with normalize == False.
See also:
jaccard_similarity_score, hamming_loss, zero_one_loss
Notes
In binary and multiclass classiﬁcation, this function is equal to the jaccard_similarity_score function.
Examples
>>> import numpy as np
>>> from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)
2
In the multilabel case with binary label indicators: >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2,
2))) 0.5
Examples using sklearn.metrics.accuracy_score
• Multi-class AdaBoosted Decision Trees
• Probabilistic predictions with Gaussian process classiﬁcation (GPC)
• Classiﬁcation of text documents using sparse features
29.20. sklearn.metrics: Metrics
1625
scikit-learn user guide, Release 0.18.2
sklearn.metrics.auc
sklearn.metrics.auc(x, y, reorder=False)
Compute Area Under the Curve (AUC) using the trapezoidal rule
This is a general function, given points on a curve.
For computing the area under the ROC-curve, see
roc_auc_score.
Parametersx : array, shape = [n]
x coordinates.
y : array, shape = [n]
y coordinates.
reorder : boolean, optional (default=False)
If True, assume that the curve is ascending in the case of ties, as for an ROC curve. If
the curve is non-ascending, the result will be wrong.
Returnsauc : ﬂoat
See also:
roc_auc_scoreComputes the area under the ROC curve
precision_recall_curveCompute precision-recall pairs for different probability thresholds
Examples
>>> import numpy as np
>>> from sklearn import metrics
>>> y = np.array([1, 1, 2, 2])
>>> pred = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)
>>> metrics.auc(fpr, tpr)
0.75
Examples using sklearn.metrics.auc
• Species distribution modeling
• Sparse recovery: feature selection for sparse linear models
• Receiver Operating Characteristic (ROC)
• Receiver Operating Characteristic (ROC) with cross validation
sklearn.metrics.average_precision_score
sklearn.metrics.average_precision_score(y_true,
y_score,
average=’macro’,
sam-
ple_weight=None)
Compute average precision (AP) from prediction scores
This score corresponds to the area under the precision-recall curve.
Note: this implementation is restricted to the binary classiﬁcation task or multilabel classiﬁcation task.
1626
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Read more in the User Guide.
Parametersy_true : array, shape = [n_samples] or [n_samples, n_classes]
True binary labels in binary label indicators.
y_score : array, shape = [n_samples] or [n_samples, n_classes]
Target scores, can either be probability estimates of the positive class, conﬁdence values,
or non-thresholded measure of decisions (as returned by “decision_function” on some
classiﬁers).
average : string, [None, ‘micro’, ‘macro’ (default), ‘samples’, ‘weighted’]
If None, the scores for each class are returned. Otherwise, this determines the type of
averaging performed on the data:
'micro':Calculate metrics globally by considering each element of the label indicator
matrix as a label.
'macro':Calculate metrics for each label, and ﬁnd their unweighted mean. This does
not take label imbalance into account.
'weighted':Calculate metrics for each label, and ﬁnd their average, weighted by
support (the number of true instances for each label).
'samples':Calculate metrics for each instance, and ﬁnd their average.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsaverage_precision : ﬂoat
See also:
roc_auc_scoreArea under the ROC curve
precision_recall_curveCompute precision-recall pairs for different probability thresholds
References
[R52]
Examples
>>> import numpy as np
>>> from sklearn.metrics import average_precision_score
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> average_precision_score(y_true, y_scores)
0.79...
Examples using sklearn.metrics.average_precision_score
• Precision-Recall
29.20. sklearn.metrics: Metrics
1627
scikit-learn user guide, Release 0.18.2
sklearn.metrics.brier_score_loss
sklearn.metrics.brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None)
Compute the Brier score.
The smaller the Brier score, the better, hence the naming with “loss”.
Across all items in a set N predictions, the Brier score measures the mean squared difference between (1) the
predicted probability assigned to the possible outcomes for item i, and (2) the actual outcome. Therefore, the
lower the Brier score is for a set of predictions, the better the predictions are calibrated. Note that the Brier score
always takes on a value between zero and one, since this is the largest possible difference between a predicted
probability (which must be between zero and one) and the actual outcome (which can take on values of only 0
and 1).
The Brier score is appropriate for binary and categorical outcomes that can be structured as true or false, but
is inappropriate for ordinal variables which can take on three or more values (this is because the Brier score
assumes that all possible outcomes are equivalently “distant” from one another). Which label is considered to
be the positive label is controlled via the parameter pos_label, which defaults to 1.
Read more in the User Guide.
Parametersy_true : array, shape (n_samples,)
True targets.
y_prob : array, shape (n_samples,)
Probabilities of the positive class.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
pos_label : int or str, default=None
Label of the positive class. If None, the maximum label is used as positive class
Returnsscore : ﬂoat
Brier score
References
[R53]
Examples
>>> import numpy as np
>>> from sklearn.metrics import brier_score_loss
>>> y_true = np.array([0, 1, 1, 0])
>>> y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
>>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])
>>> brier_score_loss(y_true, y_prob)
0.037...
>>> brier_score_loss(y_true, 1-y_prob, pos_label=0)
0.037...
>>> brier_score_loss(y_true_categorical, y_prob,
pos_
˓→label="ham")
0.037...
1628
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> brier_score_loss(y_true, np.array(y_prob) > 0.5)
0.0
Examples using sklearn.metrics.brier_score_loss
• Probability calibration of classiﬁers
• Probability Calibration curves
sklearn.metrics.classiﬁcation_report
sklearn.metrics.classification_report(y_true, y_pred, labels=None, target_names=None,
sample_weight=None, digits=2)
Build a text report showing the main classiﬁcation metrics
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
Ground truth (correct) target values.
y_pred : 1d array-like, or label indicator array / sparse matrix
Estimated targets as returned by a classiﬁer.
labels : array, shape = [n_labels]
Optional list of label indices to include in the report.
target_names : list of strings
Optional display names matching the labels (same order).
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
digits : int
Number of digits for formatting output ﬂoating point values
Returnsreport : string
Text summary of the precision, recall, F1 score for each class.
Examples
>>> from sklearn.metrics import classification_report
>>> y_true = [0, 1, 2, 2, 2]
>>> y_pred = [0, 0, 2, 2, 1]
>>> target_names = ['class 0', 'class 1', 'class 2']
>>> print(classification_report(y_true, y_pred, target_names=target_names))
precision
recall
f1-score
support
class 0
0.50
1.00
0.67
1
class 1
0.00
0.00
0.00
1
class 2
1.00
0.67
0.80
3
avg / total
0.70
0.60
0.61
5
29.20. sklearn.metrics: Metrics
1629
scikit-learn user guide, Release 0.18.2
Examples using sklearn.metrics.classification_report
• Feature Union with Heterogeneous Data Sources
• Faces recognition example using eigenfaces and SVMs
• Recognizing hand-written digits
• Parameter estimation using grid search with cross-validation
• Restricted Boltzmann Machine features for digit classiﬁcation
• Label Propagation digits: Demonstrating performance
• Label Propagation digits active learning
• Classiﬁcation of text documents using sparse features
• Classiﬁcation of text documents: using a MLComp dataset
sklearn.metrics.cohen_kappa_score
sklearn.metrics.cohen_kappa_score(y1, y2, labels=None, weights=None)
Cohen’s kappa: a statistic that measures inter-annotator agreement.
This function computes Cohen’s kappa [R199], a score that expresses the level of agreement between two
annotators on a classiﬁcation problem. It is deﬁned as
𝜅= (𝑝𝑜−𝑝𝑒)/(1 −𝑝𝑒)
where 𝑝𝑜is the empirical probability of agreement on the label assigned to any sample (the observed agreement
ratio), and 𝑝𝑒is the expected agreement when both annotators assign labels randomly. 𝑝𝑒is estimated using a
per-annotator empirical prior over the class labels [R200].
Read more in the User Guide.
Parametersy1 : array, shape = [n_samples]
Labels assigned by the ﬁrst annotator.
y2 : array, shape = [n_samples]
Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping
y1 and y2 doesn’t change the value.
labels : array, shape = [n_classes], optional
List of labels to index the matrix. This may be used to select a subset of labels. If None,
all labels that appear at least once in y1 or y2 are used.
weights : str, optional
List of weighting type to calculate the score. None means no weighted; “linear” means
linear weighted; “quadratic” means quadratic weighted.
Returnskappa : ﬂoat
The kappa statistic, which is a number between -1 and 1. The maximum value means
complete agreement; zero or lower means chance agreement.
References
[R199], [R200], [R201]
1630
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.metrics.confusion_matrix
sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)
Compute confusion matrix to evaluate the accuracy of a classiﬁcation
By deﬁnition a confusion matrix 𝐶is such that 𝐶𝑖,𝑗is equal to the number of observations known to be in group
𝑖but predicted to be in group 𝑗.
Thus in binary classiﬁcation, the count of true negatives is 𝐶0,0, false negatives is 𝐶1,0, true positives is 𝐶1,1
and false positives is 𝐶0,1.
Read more in the User Guide.
Parametersy_true : array, shape = [n_samples]
Ground truth (correct) target values.
y_pred : array, shape = [n_samples]
Estimated targets as returned by a classiﬁer.
labels : array, shape = [n_classes], optional
List of labels to index the matrix. This may be used to reorder or select a subset of
labels. If none is given, those that appear at least once in y_true or y_pred are used
in sorted order.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
ReturnsC : array, shape = [n_classes, n_classes]
Confusion matrix
References
[R55]
Examples
>>> from sklearn.metrics import confusion_matrix
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
[0, 0, 1],
[1, 0, 2]])
>>> y_true = ["cat", "ant", "cat", "cat", "ant", "bird"]
>>> y_pred = ["ant", "ant", "cat", "cat", "ant", "cat"]
>>> confusion_matrix(y_true, y_pred, labels=["ant", "bird", "cat"])
array([[2, 0, 0],
[0, 0, 1],
[1, 0, 2]])
29.20. sklearn.metrics: Metrics
1631
scikit-learn user guide, Release 0.18.2
Examples using sklearn.metrics.confusion_matrix
• Faces recognition example using eigenfaces and SVMs
• Recognizing hand-written digits
• Confusion matrix
• Label Propagation digits: Demonstrating performance
• Label Propagation digits active learning
• Classiﬁcation of text documents using sparse features
• Classiﬁcation of text documents: using a MLComp dataset
sklearn.metrics.f1_score
sklearn.metrics.f1_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sam-
ple_weight=None)
Compute the F1 score, also known as balanced F-score or F-measure
The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its
best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.
The formula for the F1 score is:
F1 = 2 * (precision * recall) / (precision + recall)
In the multi-class and multi-label case, this is the weighted average of the F1 score of each class.
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
Ground truth (correct) target values.
y_pred : 1d array-like, or label indicator array / sparse matrix
Estimated targets as returned by a classiﬁer.
labels : list, optional
The set of labels to include when average != 'binary', and their order if
average is None. Labels present in the data can be excluded, for example to cal-
culate a multiclass average ignoring a majority negative class, while labels not present
in the data will result in 0 components in a macro average. For multilabel targets, labels
are column indices. By default, all labels in y_true and y_pred are used in sorted
order.
Changed in version 0.17: parameter labels improved for multiclass problem.
pos_label : str or int, 1 by default
The class to report if average='binary' and the data is binary. If the data are
multiclass or multilabel, this will be ignored; setting labels=[pos_label] and
average != 'binary' will report scores for that label only.
average : string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]
This parameter is required for multiclass/multilabel targets. If None, the scores for each
class are returned. Otherwise, this determines the type of averaging performed on the
data:
1632
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
'binary':Only report results for the class speciﬁed by pos_label. This is appli-
cable only if targets (y_{true,pred}) are binary.
'micro':Calculate metrics globally by counting the total true positives, false nega-
tives and false positives.
'macro':Calculate metrics for each label, and ﬁnd their unweighted mean. This does
not take label imbalance into account.
'weighted':Calculate metrics for each label, and ﬁnd their average, weighted by
support (the number of true instances for each label). This alters ‘macro’ to account
for label imbalance; it can result in an F-score that is not between precision and recall.
'samples':Calculate metrics for each instance, and ﬁnd their average (only mean-
ingful for multilabel classiﬁcation where this differs from accuracy_score).
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsf1_score : ﬂoat or array of ﬂoat, shape = [n_unique_labels]
F1 score of the positive class in binary classiﬁcation or weighted average of the F1
scores of each class for the multiclass task.
References
[R56]
Examples
>>> from sklearn.metrics import f1_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> f1_score(y_true, y_pred, average='macro')
0.26...
>>> f1_score(y_true, y_pred, average='micro')
0.33...
>>> f1_score(y_true, y_pred, average='weighted')
0.26...
>>> f1_score(y_true, y_pred, average=None)
array([ 0.8,
0. ,
0. ])
Examples using sklearn.metrics.f1_score
• Probability Calibration curves
sklearn.metrics.fbeta_score
sklearn.metrics.fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1, average=’binary’,
sample_weight=None)
Compute the F-beta score
The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its
worst value at 0.
29.20. sklearn.metrics: Metrics
1633
scikit-learn user guide, Release 0.18.2
The beta parameter determines the weight of precision in the combined score. beta < 1 lends more weight to
precision, while beta > 1 favors recall (beta -> 0 considers only precision, beta -> inf only recall).
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
Ground truth (correct) target values.
y_pred : 1d array-like, or label indicator array / sparse matrix
Estimated targets as returned by a classiﬁer.
beta: ﬂoat :
Weight of precision in harmonic mean.
labels : list, optional
The set of labels to include when average != 'binary', and their order if
average is None. Labels present in the data can be excluded, for example to cal-
culate a multiclass average ignoring a majority negative class, while labels not present
in the data will result in 0 components in a macro average. For multilabel targets, labels
are column indices. By default, all labels in y_true and y_pred are used in sorted
order.
Changed in version 0.17: parameter labels improved for multiclass problem.
pos_label : str or int, 1 by default
The class to report if average='binary' and the data is binary. If the data are
multiclass or multilabel, this will be ignored; setting labels=[pos_label] and
average != 'binary' will report scores for that label only.
average : string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]
This parameter is required for multiclass/multilabel targets. If None, the scores for each
class are returned. Otherwise, this determines the type of averaging performed on the
data:
'binary':Only report results for the class speciﬁed by pos_label. This is appli-
cable only if targets (y_{true,pred}) are binary.
'micro':Calculate metrics globally by counting the total true positives, false nega-
tives and false positives.
'macro':Calculate metrics for each label, and ﬁnd their unweighted mean. This does
not take label imbalance into account.
'weighted':Calculate metrics for each label, and ﬁnd their average, weighted by
support (the number of true instances for each label). This alters ‘macro’ to account
for label imbalance; it can result in an F-score that is not between precision and recall.
'samples':Calculate metrics for each instance, and ﬁnd their average (only mean-
ingful for multilabel classiﬁcation where this differs from accuracy_score).
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsfbeta_score : ﬂoat (if average is not None) or array of ﬂoat, shape = [n_unique_labels]
F-beta score of the positive class in binary classiﬁcation or weighted average of the
F-beta score of each class for the multiclass task.
1634
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
[R206], [R207]
Examples
>>> from sklearn.metrics import fbeta_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)
...
0.23...
>>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)
...
0.33...
>>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)
...
0.23...
>>> fbeta_score(y_true, y_pred, average=None, beta=0.5)
...
array([ 0.71...,
0.
,
0.
])
sklearn.metrics.hamming_loss
sklearn.metrics.hamming_loss(y_true,
y_pred,
labels=None,
sample_weight=None,
classes=None)
Compute the average Hamming loss.
The Hamming loss is the fraction of labels that are incorrectly predicted.
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
Ground truth (correct) labels.
y_pred : 1d array-like, or label indicator array / sparse matrix
Predicted labels, as returned by a classiﬁer.
labels : array, shape = [n_labels], optional (default=None)
Integer array of labels. If not provided, labels will be inferred from y_true and y_pred.
New in version 0.18.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
New in version 0.18.
classes : array, shape = [n_labels], optional
(deprecated) Integer array of labels. This parameter has beenrenamed to labels
in version 0.18 and will be removed in 0.20.
Returnsloss : ﬂoat or int,
Return the average Hamming loss between element of y_true and y_pred.
29.20. sklearn.metrics: Metrics
1635
scikit-learn user guide, Release 0.18.2
See also:
accuracy_score, jaccard_similarity_score, zero_one_loss
Notes
In multiclass classiﬁcation, the Hamming loss correspond to the Hamming distance between y_true and
y_pred which is equivalent to the subset zero_one_loss function.
In multilabel classiﬁcation, the Hamming loss is different from the subset zero-one loss. The zero-one loss
considers the entire set of labels for a given sample incorrect if it does entirely match the true set of labels.
Hamming loss is more forgiving in that it penalizes the individual labels.
The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming
loss is always between 0 and 1.
References
[R57], [R58]
Examples
>>> from sklearn.metrics import hamming_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> hamming_loss(y_true, y_pred)
0.25
In the multilabel case with binary label indicators:
>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
0.75
Examples using sklearn.metrics.hamming_loss
• Model Complexity Inﬂuence
sklearn.metrics.hinge_loss
sklearn.metrics.hinge_loss(y_true, pred_decision, labels=None, sample_weight=None)
Average hinge loss (non-regularized)
In binary class case, assuming labels in y_true are encoded with +1 and -1, when a prediction mistake is
made, margin = y_true * pred_decision is always negative (since the signs disagree), implying
1 -margin is always greater than 1. The cumulated hinge loss is therefore an upper bound of the number of
mistakes made by the classiﬁer.
In multiclass case, the function expects that either all the labels are included in y_true or an optional labels
argument is provided which contains all the labels. The multilabel margin is calculated according to Crammer-
Singer’s method. As in the binary case, the cumulated hinge loss is an upper bound of the number of mistakes
made by the classiﬁer.
Read more in the User Guide.
1636
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Parametersy_true : array, shape = [n_samples]
True target, consisting of integers of two values. The positive label must be greater than
the negative label.
pred_decision : array, shape = [n_samples] or [n_samples, n_classes]
Predicted decisions, as output by decision_function (ﬂoats).
labels : array, optional, default None
Contains all the labels for the problem. Used in multiclass hinge loss.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsloss : ﬂoat
References
[R212], [R213], [R214]
Examples
>>> from sklearn import svm
>>> from sklearn.metrics import hinge_loss
>>> X = [[0], [1]]
>>> y = [-1, 1]
>>> est = svm.LinearSVC(random_state=0)
>>> est.fit(X, y)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
intercept_scaling=1, loss='squared_hinge', max_iter=1000,
multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
verbose=0)
>>> pred_decision = est.decision_function([[-2], [3], [0.5]])
>>> pred_decision
array([-2.18...,
2.36...,
0.09...])
>>> hinge_loss([-1, 1, 1], pred_decision)
0.30...
In the multiclass case:
>>> X = np.array([[0], [1], [2], [3]])
>>> Y = np.array([0, 1, 2, 3])
>>> labels = np.array([0, 1, 2, 3])
>>> est = svm.LinearSVC()
>>> est.fit(X, Y)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
intercept_scaling=1, loss='squared_hinge', max_iter=1000,
multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
verbose=0)
>>> pred_decision = est.decision_function([[-1], [2], [3]])
>>> y_true = [0, 2, 3]
>>> hinge_loss(y_true, pred_decision, labels)
0.56...
29.20. sklearn.metrics: Metrics
1637
scikit-learn user guide, Release 0.18.2
sklearn.metrics.jaccard_similarity_score
sklearn.metrics.jaccard_similarity_score(y_true,
y_pred,
normalize=True,
sam-
ple_weight=None)
Jaccard similarity coefﬁcient score
The Jaccard index [1], or Jaccard similarity coefﬁcient, deﬁned as the size of the intersection divided by the size
of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of
labels in y_true.
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
Ground truth (correct) labels.
y_pred : 1d array-like, or label indicator array / sparse matrix
Predicted labels, as returned by a classiﬁer.
normalize : bool, optional (default=True)
If False, return the sum of the Jaccard similarity coefﬁcient over the sample set. Oth-
erwise, return the average of Jaccard similarity coefﬁcient.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
If normalize == True, return the average Jaccard similarity coefﬁcient, else it
returns the sum of the Jaccard similarity coefﬁcient over the sample set.
The best performance is 1 with normalize == True and the number of samples
with normalize == False.
See also:
accuracy_score, hamming_loss, zero_one_loss
Notes
In binary and multiclass classiﬁcation, this function is equivalent to the accuracy_score. It differs in the
multilabel classiﬁcation problem.
References
[R216]
Examples
>>> import numpy as np
>>> from sklearn.metrics import jaccard_similarity_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> jaccard_similarity_score(y_true, y_pred)
0.5
1638
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> jaccard_similarity_score(y_true, y_pred, normalize=False)
2
In the multilabel case with binary label indicators:
>>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]),
np.ones((2, 2)))
0.75
sklearn.metrics.log_loss
sklearn.metrics.log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, la-
bels=None)
Log loss, aka logistic loss or cross-entropy loss.
This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks,
deﬁned as the negative log-likelihood of the true labels given a probabilistic classiﬁer’s predictions. The log loss
is only deﬁned for two or more labels. For a single sample with true label yt in {0,1} and estimated probability
yp that yt = 1, the log loss is
-log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))
Read more in the User Guide.
Parametersy_true : array-like or label indicator matrix
Ground truth (correct) labels for n_samples samples.
y_pred : array-like of ﬂoat, shape = (n_samples, n_classes) or (n_samples,)
Predicted probabilities, as returned by a classiﬁer’s predict_proba method.
If
y_pred.shape = (n_samples,) the probabilities provided are assumed to be
that of the positive class. The labels in y_pred are assumed to be ordered alphabeti-
cally, as done by preprocessing.LabelBinarizer.
eps : ﬂoat
Log loss is undeﬁned for p=0 or p=1, so probabilities are clipped to max(eps, min(1 -
eps, p)).
normalize : bool, optional (default=True)
If true, return the mean loss per sample. Otherwise, return the sum of the per-sample
losses.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
labels : array-like, optional (default=None)
If not provided, labels will be inferred from y_true. If labels is None and y_pred
has shape (n_samples,) the labels are assumed to be binary and are inferred from
y_true. .. versionadded:: 0.18
Returnsloss : ﬂoat
Notes
The logarithm used is the natural logarithm (base-e).
29.20. sklearn.metrics: Metrics
1639
scikit-learn user guide, Release 0.18.2
References
C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209.
Examples
>>> log_loss(["spam", "ham", "ham", "spam"],
...
[[.1, .9], [.9, .1], [.8, .2], [.35, .65]])
0.21616...
Examples using sklearn.metrics.log_loss
• Probability Calibration for 3-class classiﬁcation
• Probabilistic predictions with Gaussian process classiﬁcation (GPC)
sklearn.metrics.matthews_corrcoef
sklearn.metrics.matthews_corrcoef(y_true, y_pred, sample_weight=None)
Compute the Matthews correlation coefﬁcient (MCC) for binary classes
The Matthews correlation coefﬁcient is used in machine learning as a measure of the quality of binary (two-
class) classiﬁcations. It takes into account true and false positives and negatives and is generally regarded as a
balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a
correlation coefﬁcient value between -1 and +1. A coefﬁcient of +1 represents a perfect prediction, 0 an average
random prediction and -1 an inverse prediction. The statistic is also known as the phi coefﬁcient. [source:
Wikipedia]
Only in the binary case does this relate to information about true and false positives and negatives. See references
below.
Read more in the User Guide.
Parametersy_true : array, shape = [n_samples]
Ground truth (correct) target values.
y_pred : array, shape = [n_samples]
Estimated targets as returned by a classiﬁer.
sample_weight : array-like of shape = [n_samples], default None
Sample weights.
Returnsmcc : ﬂoat
The Matthews correlation coefﬁcient (+1 represents a perfect prediction, 0 an average
random prediction and -1 and inverse prediction).
References
[R218], [R219]
1640
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn.metrics import matthews_corrcoef
>>> y_true = [+1, +1, +1, -1]
>>> y_pred = [+1, -1, +1, +1]
>>> matthews_corrcoef(y_true, y_pred)
-0.33...
sklearn.metrics.precision_recall_curve
sklearn.metrics.precision_recall_curve(y_true,
probas_pred,
pos_label=None,
sam-
ple_weight=None)
Compute precision-recall pairs for different probability thresholds
Note: this implementation is restricted to the binary classiﬁcation task.
The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of
false positives. The precision is intuitively the ability of the classiﬁer not to label as positive a sample that is
negative.
The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false
negatives. The recall is intuitively the ability of the classiﬁer to ﬁnd all the positive samples.
The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This
ensures that the graph starts on the x axis.
Read more in the User Guide.
Parametersy_true : array, shape = [n_samples]
True targets of binary classiﬁcation in range {-1, 1} or {0, 1}.
probas_pred : array, shape = [n_samples]
Estimated probabilities or decision function.
pos_label : int or str, default=None
The label of the positive class
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsprecision : array, shape = [n_thresholds + 1]
Precision values such that element i is the precision of predictions with score >= thresh-
olds[i] and the last element is 1.
recall : array, shape = [n_thresholds + 1]
Decreasing recall values such that element i is the recall of predictions with score >=
thresholds[i] and the last element is 0.
thresholds : array, shape = [n_thresholds <= len(np.unique(probas_pred))]
Increasing thresholds on the decision function used to compute precision and recall.
29.20. sklearn.metrics: Metrics
1641
scikit-learn user guide, Release 0.18.2
Examples
>>> import numpy as np
>>> from sklearn.metrics import precision_recall_curve
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> precision, recall, thresholds = precision_recall_curve(
...
y_true, y_scores)
>>> precision
array([ 0.66...,
0.5
,
1.
,
1.
])
>>> recall
array([ 1. ,
0.5,
0.5,
0. ])
>>> thresholds
array([ 0.35,
0.4 ,
0.8 ])
Examples using sklearn.metrics.precision_recall_curve
• Sparse recovery: feature selection for sparse linear models
• Precision-Recall
sklearn.metrics.precision_recall_fscore_support
sklearn.metrics.precision_recall_fscore_support(y_true,
y_pred,
beta=1.0,
la-
bels=None,
pos_label=1,
aver-
age=None,
warn_for=(‘precision’,
‘recall’,
‘f-score’),
sam-
ple_weight=None)
Compute precision, recall, F-measure and support for each class
The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of
false positives. The precision is intuitively the ability of the classiﬁer not to label as positive a sample that is
negative.
The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false
negatives. The recall is intuitively the ability of the classiﬁer to ﬁnd all the positive samples.
The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta
score reaches its best value at 1 and worst score at 0.
The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and
precision are equally important.
The support is the number of occurrences of each class in y_true.
If pos_label is None and in binary classiﬁcation, this function returns the average precision, recall and
F-measure if average is one of 'micro', 'macro', 'weighted' or 'samples'.
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
Ground truth (correct) target values.
y_pred : 1d array-like, or label indicator array / sparse matrix
Estimated targets as returned by a classiﬁer.
beta : ﬂoat, 1.0 by default
1642
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The strength of recall versus precision in the F-score.
labels : list, optional
The set of labels to include when average != 'binary', and their order if
average is None. Labels present in the data can be excluded, for example to cal-
culate a multiclass average ignoring a majority negative class, while labels not present
in the data will result in 0 components in a macro average. For multilabel targets, labels
are column indices. By default, all labels in y_true and y_pred are used in sorted
order.
pos_label : str or int, 1 by default
The class to report if average='binary' and the data is binary. If the data are
multiclass or multilabel, this will be ignored; setting labels=[pos_label] and
average != 'binary' will report scores for that label only.
average : string, [None (default), ‘binary’, ‘micro’, ‘macro’, ‘samples’, ‘weighted’]
If None, the scores for each class are returned. Otherwise, this determines the type of
averaging performed on the data:
'binary':Only report results for the class speciﬁed by pos_label. This is appli-
cable only if targets (y_{true,pred}) are binary.
'micro':Calculate metrics globally by counting the total true positives, false nega-
tives and false positives.
'macro':Calculate metrics for each label, and ﬁnd their unweighted mean. This does
not take label imbalance into account.
'weighted':Calculate metrics for each label, and ﬁnd their average, weighted by
support (the number of true instances for each label). This alters ‘macro’ to account
for label imbalance; it can result in an F-score that is not between precision and recall.
'samples':Calculate metrics for each instance, and ﬁnd their average (only mean-
ingful for multilabel classiﬁcation where this differs from accuracy_score).
warn_for : tuple or set, for internal use
This determines which warnings will be made in the case that this function is being used
to return only one of its metrics.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsprecision : ﬂoat (if average is not None) or array of ﬂoat, shape = [n_unique_labels]
recall : ﬂoat (if average is not None) or array of ﬂoat, , shape = [n_unique_labels]
fbeta_score : ﬂoat (if average is not None) or array of ﬂoat, shape = [n_unique_labels]
support : int (if average is not None) or array of int, shape = [n_unique_labels]
The number of occurrences of each label in y_true.
References
[R220], [R221], [R222]
29.20. sklearn.metrics: Metrics
1643
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn.metrics import precision_recall_fscore_support
>>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])
>>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])
>>> precision_recall_fscore_support(y_true, y_pred, average='macro')
...
(0.22..., 0.33..., 0.26..., None)
>>> precision_recall_fscore_support(y_true, y_pred, average='micro')
...
(0.33..., 0.33..., 0.33..., None)
>>> precision_recall_fscore_support(y_true, y_pred, average='weighted')
...
(0.22..., 0.33..., 0.26..., None)
It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging: >>> preci-
sion_recall_fscore_support(y_true, y_pred, average=None, ... labels=[’pig’, ‘dog’, ‘cat’]) ... # doctest: +EL-
LIPSIS,+NORMALIZE_WHITESPACE (array([ 0. , 0. , 0.66...]),
array([ 0., 0., 1.]), array([ 0. , 0. , 0.8]), array([2, 2, 2]))
sklearn.metrics.precision_score
sklearn.metrics.precision_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’,
sample_weight=None)
Compute the precision
The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of
false positives. The precision is intuitively the ability of the classiﬁer not to label as positive a sample that is
negative.
The best value is 1 and the worst value is 0.
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
Ground truth (correct) target values.
y_pred : 1d array-like, or label indicator array / sparse matrix
Estimated targets as returned by a classiﬁer.
labels : list, optional
The set of labels to include when average != 'binary', and their order if
average is None. Labels present in the data can be excluded, for example to cal-
culate a multiclass average ignoring a majority negative class, while labels not present
in the data will result in 0 components in a macro average. For multilabel targets, labels
are column indices. By default, all labels in y_true and y_pred are used in sorted
order.
Changed in version 0.17: parameter labels improved for multiclass problem.
pos_label : str or int, 1 by default
The class to report if average='binary' and the data is binary. If the data are
multiclass or multilabel, this will be ignored; setting labels=[pos_label] and
average != 'binary' will report scores for that label only.
average : string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]
1644
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
This parameter is required for multiclass/multilabel targets. If None, the scores for each
class are returned. Otherwise, this determines the type of averaging performed on the
data:
'binary':Only report results for the class speciﬁed by pos_label. This is appli-
cable only if targets (y_{true,pred}) are binary.
'micro':Calculate metrics globally by counting the total true positives, false nega-
tives and false positives.
'macro':Calculate metrics for each label, and ﬁnd their unweighted mean. This does
not take label imbalance into account.
'weighted':Calculate metrics for each label, and ﬁnd their average, weighted by
support (the number of true instances for each label). This alters ‘macro’ to account
for label imbalance; it can result in an F-score that is not between precision and recall.
'samples':Calculate metrics for each instance, and ﬁnd their average (only mean-
ingful for multilabel classiﬁcation where this differs from accuracy_score).
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsprecision : ﬂoat (if average is not None) or array of ﬂoat, shape = [n_unique_labels]
Precision of the positive class in binary classiﬁcation or weighted average of the preci-
sion of each class for the multiclass task.
Examples
>>> from sklearn.metrics import precision_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> precision_score(y_true, y_pred, average='macro')
0.22...
>>> precision_score(y_true, y_pred, average='micro')
0.33...
>>> precision_score(y_true, y_pred, average='weighted')
...
0.22...
>>> precision_score(y_true, y_pred, average=None)
array([ 0.66...,
0.
,
0.
])
Examples using sklearn.metrics.precision_score
• Probability Calibration curves
sklearn.metrics.recall_score
sklearn.metrics.recall_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sam-
ple_weight=None)
Compute the recall
The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false
negatives. The recall is intuitively the ability of the classiﬁer to ﬁnd all the positive samples.
29.20. sklearn.metrics: Metrics
1645
scikit-learn user guide, Release 0.18.2
The best value is 1 and the worst value is 0.
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
Ground truth (correct) target values.
y_pred : 1d array-like, or label indicator array / sparse matrix
Estimated targets as returned by a classiﬁer.
labels : list, optional
The set of labels to include when average != 'binary', and their order if
average is None. Labels present in the data can be excluded, for example to cal-
culate a multiclass average ignoring a majority negative class, while labels not present
in the data will result in 0 components in a macro average. For multilabel targets, labels
are column indices. By default, all labels in y_true and y_pred are used in sorted
order.
Changed in version 0.17: parameter labels improved for multiclass problem.
pos_label : str or int, 1 by default
The class to report if average='binary' and the data is binary. If the data are
multiclass or multilabel, this will be ignored; setting labels=[pos_label] and
average != 'binary' will report scores for that label only.
average : string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]
This parameter is required for multiclass/multilabel targets. If None, the scores for each
class are returned. Otherwise, this determines the type of averaging performed on the
data:
'binary':Only report results for the class speciﬁed by pos_label. This is appli-
cable only if targets (y_{true,pred}) are binary.
'micro':Calculate metrics globally by counting the total true positives, false nega-
tives and false positives.
'macro':Calculate metrics for each label, and ﬁnd their unweighted mean. This does
not take label imbalance into account.
'weighted':Calculate metrics for each label, and ﬁnd their average, weighted by
support (the number of true instances for each label). This alters ‘macro’ to account
for label imbalance; it can result in an F-score that is not between precision and recall.
'samples':Calculate metrics for each instance, and ﬁnd their average (only mean-
ingful for multilabel classiﬁcation where this differs from accuracy_score).
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsrecall : ﬂoat (if average is not None) or array of ﬂoat, shape = [n_unique_labels]
Recall of the positive class in binary classiﬁcation or weighted average of the recall of
each class for the multiclass task.
1646
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn.metrics import recall_score
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> recall_score(y_true, y_pred, average='macro')
0.33...
>>> recall_score(y_true, y_pred, average='micro')
0.33...
>>> recall_score(y_true, y_pred, average='weighted')
0.33...
>>> recall_score(y_true, y_pred, average=None)
array([ 1.,
0.,
0.])
Examples using sklearn.metrics.recall_score
• Probability Calibration curves
sklearn.metrics.roc_auc_score
sklearn.metrics.roc_auc_score(y_true, y_score, average=’macro’, sample_weight=None)
Compute Area Under the Curve (AUC) from prediction scores
Note: this implementation is restricted to the binary classiﬁcation task or multilabel classiﬁcation task in label
indicator format.
Read more in the User Guide.
Parametersy_true : array, shape = [n_samples] or [n_samples, n_classes]
True binary labels in binary label indicators.
y_score : array, shape = [n_samples] or [n_samples, n_classes]
Target scores, can either be probability estimates of the positive class, conﬁdence values,
or non-thresholded measure of decisions (as returned by “decision_function” on some
classiﬁers).
average : string, [None, ‘micro’, ‘macro’ (default), ‘samples’, ‘weighted’]
If None, the scores for each class are returned. Otherwise, this determines the type of
averaging performed on the data:
'micro':Calculate metrics globally by considering each element of the label indicator
matrix as a label.
'macro':Calculate metrics for each label, and ﬁnd their unweighted mean. This does
not take label imbalance into account.
'weighted':Calculate metrics for each label, and ﬁnd their average, weighted by
support (the number of true instances for each label).
'samples':Calculate metrics for each instance, and ﬁnd their average.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsauc : ﬂoat
29.20. sklearn.metrics: Metrics
1647
scikit-learn user guide, Release 0.18.2
See also:
average_precision_scoreArea under the precision-recall curve
roc_curveCompute Receiver operating characteristic (ROC)
References
[R224]
Examples
>>> import numpy as np
>>> from sklearn.metrics import roc_auc_score
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> roc_auc_score(y_true, y_scores)
0.75
sklearn.metrics.roc_curve
sklearn.metrics.roc_curve(y_true,
y_score,
pos_label=None,
sample_weight=None,
drop_intermediate=True)
Compute Receiver operating characteristic (ROC)
Note: this implementation is restricted to the binary classiﬁcation task.
Read more in the User Guide.
Parametersy_true : array, shape = [n_samples]
True binary labels in range {0, 1} or {-1, 1}. If labels are not binary, pos_label should
be explicitly given.
y_score : array, shape = [n_samples]
Target scores, can either be probability estimates of the positive class, conﬁdence values,
or non-thresholded measure of decisions (as returned by “decision_function” on some
classiﬁers).
pos_label : int or str, default=None
Label considered as positive and others are considered negative.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
drop_intermediate : boolean, optional (default=True)
Whether to drop some suboptimal thresholds which would not appear on a plotted ROC
curve. This is useful in order to create lighter ROC curves.
New in version 0.17: parameter drop_intermediate.
Returnsfpr : array, shape = [>2]
Increasing false positive rates such that element i is the false positive rate of predictions
with score >= thresholds[i].
1648
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
tpr : array, shape = [>2]
Increasing true positive rates such that element i is the true positive rate of predictions
with score >= thresholds[i].
thresholds : array, shape = [n_thresholds]
Decreasing thresholds on the decision function used to compute fpr and tpr. thresh-
olds[0] represents no instances being predicted and is arbitrarily set to max(y_score) +
1.
See also:
roc_auc_scoreCompute Area Under the Curve (AUC) from prediction scores
Notes
Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they
correspond to both fpr and tpr, which are sorted in reversed order during their calculation.
References
[R61]
Examples
>>> import numpy as np
>>> from sklearn import metrics
>>> y = np.array([1, 1, 2, 2])
>>> scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)
>>> fpr
array([ 0. ,
0.5,
0.5,
1. ])
>>> tpr
array([ 0.5,
0.5,
1. ,
1. ])
>>> thresholds
array([ 0.8 ,
0.4 ,
0.35,
0.1 ])
Examples using sklearn.metrics.roc_curve
• Species distribution modeling
• Feature transformations with ensembles of trees
• Receiver Operating Characteristic (ROC)
• Receiver Operating Characteristic (ROC) with cross validation
sklearn.metrics.zero_one_loss
sklearn.metrics.zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None)
Zero-one classiﬁcation loss.
29.20. sklearn.metrics: Metrics
1649
scikit-learn user guide, Release 0.18.2
If normalize is True, return the fraction of misclassiﬁcations (ﬂoat), else it returns the number of misclassiﬁca-
tions (int). The best performance is 0.
Read more in the User Guide.
Parametersy_true : 1d array-like, or label indicator array / sparse matrix
Ground truth (correct) labels.
y_pred : 1d array-like, or label indicator array / sparse matrix
Predicted labels, as returned by a classiﬁer.
normalize : bool, optional (default=True)
If False, return the number of misclassiﬁcations. Otherwise, return the fraction of
misclassiﬁcations.
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsloss : ﬂoat or int,
If normalize == True, return the fraction of misclassiﬁcations (ﬂoat), else it re-
turns the number of misclassiﬁcations (int).
See also:
accuracy_score, hamming_loss, jaccard_similarity_score
Notes
In multilabel classiﬁcation, the zero_one_loss function corresponds to the subset zero-one loss: for each sample,
the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one.
Examples
>>> from sklearn.metrics import zero_one_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> zero_one_loss(y_true, y_pred)
0.25
>>> zero_one_loss(y_true, y_pred, normalize=False)
1
In the multilabel case with binary label indicators:
>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5
Examples using sklearn.metrics.zero_one_loss
• Discrete versus Real AdaBoost
1650
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.20.3 Regression metrics
See the Regression metrics section of the user guide for further details.
metrics.explained_variance_score(y_true,
y_pred)
Explained variance regression score function
metrics.mean_absolute_error(y_true, y_pred)
Mean absolute error regression loss
metrics.mean_squared_error(y_true,
y_pred[,
...])
Mean squared error regression loss
metrics.median_absolute_error(y_true,
y_pred)
Median absolute error regression loss
metrics.r2_score(y_true, y_pred[, ...])
R^2 (coefﬁcient of determination) regression score func-
tion.
sklearn.metrics.explained_variance_score
sklearn.metrics.explained_variance_score(y_true, y_pred, sample_weight=None, multiout-
put=’uniform_average’)
Explained variance regression score function
Best possible score is 1.0, lower values are worse.
Read more in the User Guide.
Parametersy_true : array-like of shape = (n_samples) or (n_samples, n_outputs)
Ground truth (correct) target values.
y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)
Estimated target values.
sample_weight : array-like of shape = (n_samples), optional
Sample weights.
multioutput : string in [’raw_values’, ‘uniform_average’, ‘variance_weighted’] or array-like of
shape (n_outputs)
Deﬁnes aggregating of multiple output scores. Array-like value deﬁnes weights used to
average scores.
‘raw_values’ :Returns a full set of scores in case of multioutput input.
‘uniform_average’ :Scores of all outputs are averaged with uniform weight.
‘variance_weighted’ :Scores of all outputs are averaged, weighted by the variances of
each individual output.
Returnsscore : ﬂoat or ndarray of ﬂoats
The explained variance or ndarray if ‘multioutput’ is ‘raw_values’.
Notes
This is not a symmetric function.
29.20. sklearn.metrics: Metrics
1651
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn.metrics import explained_variance_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> explained_variance_score(y_true, y_pred)
0.957...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')
...
0.983...
sklearn.metrics.mean_absolute_error
sklearn.metrics.mean_absolute_error(y_true,
y_pred,
sample_weight=None,
multiout-
put=’uniform_average’)
Mean absolute error regression loss
Read more in the User Guide.
Parametersy_true : array-like of shape = (n_samples) or (n_samples, n_outputs)
Ground truth (correct) target values.
y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)
Estimated target values.
sample_weight : array-like of shape = (n_samples), optional
Sample weights.
multioutput : string in [’raw_values’, ‘uniform_average’]
or array-like of shape (n_outputs) Deﬁnes aggregating of multiple output values. Array-
like value deﬁnes weights used to average errors.
‘raw_values’ :Returns a full set of errors in case of multioutput input.
‘uniform_average’ :Errors of all outputs are averaged with uniform weight.
Returnsloss : ﬂoat or ndarray of ﬂoats
If multioutput is ‘raw_values’, then mean absolute error is returned for each output sep-
arately. If multioutput is ‘uniform_average’ or an ndarray of weights, then the weighted
average of all output errors is returned.
MAE output is non-negative ﬂoating point. The best value is 0.0.
Examples
>>> from sklearn.metrics import mean_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_error(y_true, y_pred)
0.5
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
1652
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> mean_absolute_error(y_true, y_pred)
0.75
>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')
array([ 0.5,
1. ])
>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
...
0.849...
sklearn.metrics.mean_squared_error
sklearn.metrics.mean_squared_error(y_true,
y_pred,
sample_weight=None,
multiout-
put=’uniform_average’)
Mean squared error regression loss
Read more in the User Guide.
Parametersy_true : array-like of shape = (n_samples) or (n_samples, n_outputs)
Ground truth (correct) target values.
y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)
Estimated target values.
sample_weight : array-like of shape = (n_samples), optional
Sample weights.
multioutput : string in [’raw_values’, ‘uniform_average’]
or array-like of shape (n_outputs) Deﬁnes aggregating of multiple output values. Array-
like value deﬁnes weights used to average errors.
‘raw_values’ :Returns a full set of errors in case of multioutput input.
‘uniform_average’ :Errors of all outputs are averaged with uniform weight.
Returnsloss : ﬂoat or ndarray of ﬂoats
A non-negative ﬂoating point value (the best value is 0.0), or an array of ﬂoating point
values, one for each individual target.
Examples
>>> from sklearn.metrics import mean_squared_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_squared_error(y_true, y_pred)
0.375
>>> y_true = [[0.5, 1],[-1, 1],[7, -6]]
>>> y_pred = [[0, 2],[-1, 2],[8, -5]]
>>> mean_squared_error(y_true, y_pred)
0.708...
>>> mean_squared_error(y_true, y_pred, multioutput='raw_values')
...
array([ 0.416...,
1.
])
>>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])
...
0.824...
29.20. sklearn.metrics: Metrics
1653
scikit-learn user guide, Release 0.18.2
Examples using sklearn.metrics.mean_squared_error
• Model Complexity Inﬂuence
• Gradient Boosting regression
• Plot Ridge coefﬁcients as a function of the L2 regularization
• Robust linear estimator ﬁtting
sklearn.metrics.median_absolute_error
sklearn.metrics.median_absolute_error(y_true, y_pred)
Median absolute error regression loss
Read more in the User Guide.
Parametersy_true : array-like of shape = (n_samples)
Ground truth (correct) target values.
y_pred : array-like of shape = (n_samples)
Estimated target values.
Returnsloss : ﬂoat
A positive ﬂoating point value (the best value is 0.0).
Examples
>>> from sklearn.metrics import median_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> median_absolute_error(y_true, y_pred)
0.5
sklearn.metrics.r2_score
sklearn.metrics.r2_score(y_true, y_pred, sample_weight=None, multioutput=None)
R^2 (coefﬁcient of determination) regression score function.
Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model
that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
Read more in the User Guide.
Parametersy_true : array-like of shape = (n_samples) or (n_samples, n_outputs)
Ground truth (correct) target values.
y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)
Estimated target values.
sample_weight : array-like of shape = (n_samples), optional
Sample weights.
1654
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
multioutput : string in [’raw_values’, ‘uniform_average’, ‘variance_weighted’] or None or
array-like of shape (n_outputs)
Deﬁnes aggregating of multiple output scores. Array-like value deﬁnes weights used
to average scores. Default value corresponds to ‘variance_weighted’, this behaviour is
deprecated since version 0.17 and will be changed to ‘uniform_average’ starting from
0.19.
‘raw_values’ :Returns a full set of scores in case of multioutput input.
‘uniform_average’ :Scores of all outputs are averaged with uniform weight.
‘variance_weighted’ :Scores of all outputs are averaged, weighted by the variances of
each individual output.
Returnsz : ﬂoat or ndarray of ﬂoats
The R^2 score or ndarray of scores if ‘multioutput’ is ‘raw_values’.
Notes
This is not a symmetric function.
Unlike most other scores, R^2 score may be negative (it need not actually be the square of a quantity R).
References
[R60]
Examples
>>> from sklearn.metrics import r2_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> r2_score(y_true, y_pred)
0.948...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred, multioutput='variance_weighted')
0.938...
>>> y_true = [1,2,3]
>>> y_pred = [1,2,3]
>>> r2_score(y_true, y_pred)
1.0
>>> y_true = [1,2,3]
>>> y_pred = [2,2,2]
>>> r2_score(y_true, y_pred)
0.0
>>> y_true = [1,2,3]
>>> y_pred = [3,2,1]
>>> r2_score(y_true, y_pred)
-3.0
29.20. sklearn.metrics: Metrics
1655
scikit-learn user guide, Release 0.18.2
Examples using sklearn.metrics.r2_score
• Lasso and Elastic Net for Sparse Signals
29.20.4 Multilabel ranking metrics
See the Multilabel ranking metrics section of the user guide for further details.
metrics.coverage_error(y_true, y_score[, ...])
Coverage error measure
metrics.label_ranking_average_precision_score(...)
Compute ranking-based average precision
metrics.label_ranking_loss(y_true, y_score)
Compute Ranking loss measure
sklearn.metrics.coverage_error
sklearn.metrics.coverage_error(y_true, y_score, sample_weight=None)
Coverage error measure
Compute how far we need to go through the ranked scores to cover all true labels. The best value is equal to the
average number of labels in y_true per sample.
Ties in y_scores are broken by giving maximal rank that would have been assigned to all tied values.
Read more in the User Guide.
Parametersy_true : array, shape = [n_samples, n_labels]
True binary labels in binary indicator format.
y_score : array, shape = [n_samples, n_labels]
Target scores, can either be probability estimates of the positive class, conﬁdence values,
or non-thresholded measure of decisions (as returned by “decision_function” on some
classiﬁers).
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnscoverage_error : ﬂoat
References
[R204]
sklearn.metrics.label_ranking_average_precision_score
sklearn.metrics.label_ranking_average_precision_score(y_true, y_score)
Compute ranking-based average precision
Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of
the ratio of true vs. total labels with lower score.
This metric is used in multilabel ranking problem, where the goal is to give better rank to the labels associated
to each sample.
The obtained score is always strictly greater than 0 and the best value is 1.
1656
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Read more in the User Guide.
Parametersy_true : array or sparse matrix, shape = [n_samples, n_labels]
True binary labels in binary indicator format.
y_score : array, shape = [n_samples, n_labels]
Target scores, can either be probability estimates of the positive class, conﬁdence values,
or non-thresholded measure of decisions (as returned by “decision_function” on some
classiﬁers).
Returnsscore : ﬂoat
Examples
>>> import numpy as np
>>> from sklearn.metrics import label_ranking_average_precision_score
>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> label_ranking_average_precision_score(y_true, y_score)
0.416...
sklearn.metrics.label_ranking_loss
sklearn.metrics.label_ranking_loss(y_true, y_score, sample_weight=None)
Compute Ranking loss measure
Compute the average number of label pairs that are incorrectly ordered given y_score weighted by the size of
the label set and the number of labels not in the label set.
This is similar to the error set size, but weighted by the number of relevant and irrelevant labels. The best
performance is achieved with a ranking loss of zero.
Read more in the User Guide.
New in version 0.17: A function label_ranking_loss
Parametersy_true : array or sparse matrix, shape = [n_samples, n_labels]
True binary labels in binary indicator format.
y_score : array, shape = [n_samples, n_labels]
Target scores, can either be probability estimates of the positive class, conﬁdence values,
or non-thresholded measure of decisions (as returned by “decision_function” on some
classiﬁers).
sample_weight : array-like of shape = [n_samples], optional
Sample weights.
Returnsloss : ﬂoat
References
[R217]
29.20. sklearn.metrics: Metrics
1657
scikit-learn user guide, Release 0.18.2
29.20.5 Clustering metrics
See
the
Clustering
performance
evaluation
section
of
the
user
guide
for
further
details.
The
sklearn.metrics.cluster submodule contains evaluation metrics for cluster analysis results. There are two
forms of evaluation:
• supervised, which uses a ground truth class values for each sample.
• unsupervised, which does not and measures the ‘quality’ of the model itself.
metrics.adjusted_mutual_info_score(...)
Adjusted Mutual Information between two clusterings.
metrics.adjusted_rand_score(labels_true, ...)
Rand index adjusted for chance.
metrics.calinski_harabaz_score(X, labels)
Compute the Calinski and Harabaz score.
metrics.completeness_score(labels_true, ...)
Completeness metric of a cluster labeling given a ground
truth.
metrics.fowlkes_mallows_score(labels_true,
...)
Measure the similarity of two clusterings of a set of points.
metrics.homogeneity_completeness_v_measure(...)
Compute the homogeneity and completeness and V-
Measure scores at once.
metrics.homogeneity_score(labels_true, ...)
Homogeneity metric of a cluster labeling given a ground
truth.
metrics.mutual_info_score(labels_true, ...)
Mutual Information between two clusterings.
metrics.normalized_mutual_info_score(...)
Normalized Mutual Information between two clusterings.
metrics.silhouette_score(X, labels[, ...])
Compute the mean Silhouette Coefﬁcient of all samples.
metrics.silhouette_samples(X, labels[, metric])
Compute the Silhouette Coefﬁcient for each sample.
metrics.v_measure_score(labels_true,
la-
bels_pred)
V-measure cluster labeling given a ground truth.
sklearn.metrics.adjusted_mutual_info_score
sklearn.metrics.adjusted_mutual_info_score(labels_true, labels_pred)
Adjusted Mutual Information between two clusterings.
Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for
chance. It accounts for the fact that the MI is generally higher for two clusterings with a larger number of
clusters, regardless of whether there is actually more information shared. For two clusterings 𝑈and 𝑉, the AMI
is given as:
AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [max(H(U), H(V)) - E(MI(U, V))]
This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values
won’t change the score value in any way.
This metric is furthermore symmetric: switching label_true with label_pred will return the same score
value. This can be useful to measure the agreement of two independent label assignments strategies on the same
dataset when the real ground truth is not known.
Be mindful that this function is an order of magnitude slower than other metrics, such as the Adjusted Rand
Index.
Read more in the User Guide.
Parameterslabels_true : int array, shape = [n_samples]
A clustering of the data into disjoint subsets.
labels_pred : array, shape = [n_samples]
1658
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
A clustering of the data into disjoint subsets.
Returnsami: ﬂoat(upperlimited by 1.0) :
The AMI returns a value of 1 when the two partitions are identical (ie perfectly
matched). Random partitions (independent labellings) have an expected AMI around
0 on average hence can be negative.
See also:
adjusted_rand_scoreAdjusted Rand Index
mutual_information_scoreMutual Information (not adjusted for chance)
References
[R50], [R51]
Examples
Perfect labelings are both homogeneous and complete, hence have score 1.0:
>>> from sklearn.metrics.cluster import adjusted_mutual_info_score
>>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
1.0
>>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
1.0
If classes members are completely split across different clusters, the assignment is totally in-complete, hence
the AMI is null:
>>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
0.0
Examples using sklearn.metrics.adjusted_mutual_info_score
• Adjustment for chance in clustering performance evaluation
• Demo of afﬁnity propagation clustering algorithm
• Demo of DBSCAN clustering algorithm
• A demo of K-Means clustering on the handwritten digits data
sklearn.metrics.adjusted_rand_score
sklearn.metrics.adjusted_rand_score(labels_true, labels_pred)
Rand index adjusted for chance.
The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and
counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.
The raw RI score is then “adjusted for chance” into the ARI score using the following scheme:
29.20. sklearn.metrics: Metrics
1659
scikit-learn user guide, Release 0.18.2
ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)
The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the
number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).
ARI is a symmetric measure:
adjusted_rand_score(a, b) == adjusted_rand_score(b, a)
Read more in the User Guide.
Parameterslabels_true : int array, shape = [n_samples]
Ground truth class labels to be used as a reference
labels_pred : array, shape = [n_samples]
Cluster labels to evaluate
Returnsari : ﬂoat
Similarity score between -1.0 and 1.0. Random labelings have an ARI close to 0.0. 1.0
stands for perfect match.
See also:
adjusted_mutual_info_scoreAdjusted Mutual Information
References
[Hubert1985], [wk]
Examples
Perfectly maching labelings have a score of 1 even
>>> from sklearn.metrics.cluster import adjusted_rand_score
>>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])
1.0
>>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])
1.0
Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized:
>>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])
0.57...
ARI is symmetric, so labelings that have pure clusters with members coming from the same classes but unnec-
essary splits are penalized:
>>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])
0.57...
If classes members are completely split across different clusters, the assignment is totally incomplete, hence the
ARI is very low:
1660
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])
0.0
Examples using sklearn.metrics.adjusted_rand_score
• Adjustment for chance in clustering performance evaluation
• Demo of afﬁnity propagation clustering algorithm
• Demo of DBSCAN clustering algorithm
• A demo of K-Means clustering on the handwritten digits data
• Clustering text documents using k-means
sklearn.metrics.calinski_harabaz_score
sklearn.metrics.calinski_harabaz_score(X, labels)
Compute the Calinski and Harabaz score.
The score is deﬁned as ratio between the within-cluster dispersion and the between-cluster dispersion.
Read more in the User Guide.
ParametersX : array-like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data
point.
labels : array-like, shape (n_samples,)
Predicted labels for each sample.
Returnsscore: ﬂoat :
The resulting Calinski-Harabaz score.
References
[R198]
sklearn.metrics.completeness_score
sklearn.metrics.completeness_score(labels_true, labels_pred)
Completeness metric of a cluster labeling given a ground truth.
A clustering result satisﬁes completeness if all the data points that are members of a given class are elements of
the same cluster.
This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values
won’t change the score value in any way.
This
metric
is
not
symmetric:
switching
label_true
with
label_pred
will
return
the
homogeneity_score which will be different in general.
Read more in the User Guide.
Parameterslabels_true : int array, shape = [n_samples]
29.20. sklearn.metrics: Metrics
1661
scikit-learn user guide, Release 0.18.2
ground truth class labels to be used as a reference
labels_pred : array, shape = [n_samples]
cluster labels to evaluate
Returnscompleteness: ﬂoat :
score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
See also:
homogeneity_score, v_measure_score
References
[R54]
Examples
Perfect labelings are complete:
>>> from sklearn.metrics.cluster import completeness_score
>>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])
1.0
Non-perfect labelings that assign all classes members to the same clusters are still complete:
>>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))
1.0
>>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))
1.0
If classes members are split across different clusters, the assignment cannot be complete:
>>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))
0.0
>>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))
0.0
Examples using sklearn.metrics.completeness_score
• Demo of afﬁnity propagation clustering algorithm
• Demo of DBSCAN clustering algorithm
• A demo of K-Means clustering on the handwritten digits data
• Clustering text documents using k-means
sklearn.metrics.fowlkes_mallows_score
sklearn.metrics.fowlkes_mallows_score(labels_true, labels_pred, sparse=False)
Measure the similarity of two clusterings of a set of points.
The Fowlkes-Mallows index (FMI) is deﬁned as the geometric mean between of the precision and recall:
1662
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
FMI = TP / sqrt((TP + FP) * (TP + FN))
Where TP is the number of True Positive (i.e. the number of pair of points that belongs in the same clusters
in both labels_true and labels_pred), FP is the number of False Positive (i.e. the number of pair of
points that belongs in the same clusters in labels_true and not in labels_pred) and FN is the number
of False Negative (i.e the number of pair of points that belongs in the same clusters in labels_pred and not
in labels_True).
The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.
Read more in the User Guide.
Parameterslabels_true : int array, shape = (n_samples,)
A clustering of the data into disjoint subsets.
labels_pred : array, shape = (n_samples, )
A clustering of the data into disjoint subsets.
Returnsscore : ﬂoat
The resulting Fowlkes-Mallows score.
References
[R208], [R209]
Examples
Perfect labelings are both homogeneous and complete, hence have score 1.0:
>>> from sklearn.metrics.cluster import fowlkes_mallows_score
>>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])
1.0
>>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])
1.0
If classes members are completely split across different clusters, the assignment is totally random, hence the
FMI is null:
>>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])
0.0
sklearn.metrics.homogeneity_completeness_v_measure
sklearn.metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
Compute the homogeneity and completeness and V-Measure scores at once.
Those metrics are based on normalized conditional entropy measures of the clustering labeling to evaluate given
the knowledge of a Ground Truth class labels of the same samples.
A clustering result satisﬁes homogeneity if all of its clusters contain only data points which are members of a
single class.
A clustering result satisﬁes completeness if all the data points that are members of a given class are elements of
the same cluster.
29.20. sklearn.metrics: Metrics
1663
scikit-learn user guide, Release 0.18.2
Both scores have positive values between 0.0 and 1.0, larger values being desirable.
Those 3 metrics are independent of the absolute values of the labels: a permutation of the class or cluster label
values won’t change the score values in any way.
V-Measure is furthermore symmetric: swapping labels_true and label_pred will give the same score.
This does not hold for homogeneity and completeness.
Read more in the User Guide.
Parameterslabels_true : int array, shape = [n_samples]
ground truth class labels to be used as a reference
labels_pred : array, shape = [n_samples]
cluster labels to evaluate
Returnshomogeneity : ﬂoat
score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling
completeness : ﬂoat
score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
v_measure : ﬂoat
harmonic mean of the ﬁrst two
See also:
homogeneity_score, completeness_score, v_measure_score
sklearn.metrics.homogeneity_score
sklearn.metrics.homogeneity_score(labels_true, labels_pred)
Homogeneity metric of a cluster labeling given a ground truth.
A clustering result satisﬁes homogeneity if all of its clusters contain only data points which are members of a
single class.
This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values
won’t change the score value in any way.
This
metric
is
not
symmetric:
switching
label_true
with
label_pred
will
return
the
completeness_score which will be different in general.
Read more in the User Guide.
Parameterslabels_true : int array, shape = [n_samples]
ground truth class labels to be used as a reference
labels_pred : array, shape = [n_samples]
cluster labels to evaluate
Returnshomogeneity : ﬂoat
score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling
See also:
completeness_score, v_measure_score
1664
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
[R59]
Examples
Perfect labelings are homogeneous:
>>> from sklearn.metrics.cluster import homogeneity_score
>>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])
1.0
Non-perfect labelings that further split classes into more clusters can be perfectly homogeneous:
>>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))
...
1.0...
>>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))
...
1.0...
Clusters that include samples from different classes do not make for an homogeneous labeling:
>>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))
...
0.0...
>>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))
...
0.0...
Examples using sklearn.metrics.homogeneity_score
• Demo of afﬁnity propagation clustering algorithm
• Demo of DBSCAN clustering algorithm
• A demo of K-Means clustering on the handwritten digits data
• Clustering text documents using k-means
sklearn.metrics.mutual_info_score
sklearn.metrics.mutual_info_score(labels_true, labels_pred, contingency=None)
Mutual Information between two clusterings.
The Mutual Information is a measure of the similarity between two labels of the same data. Where 𝑃(𝑖) is
the probability of a random sample occurring in cluster 𝑈𝑖and 𝑃′(𝑗) is the probability of a random sample
occurring in cluster 𝑉𝑗, the Mutual Information between clusterings 𝑈and 𝑉is given as:
𝑀𝐼(𝑈, 𝑉) =
𝑅
∑︁
𝑖=1
𝐶
∑︁
𝑗=1
𝑃(𝑖, 𝑗) log
𝑃(𝑖, 𝑗)
𝑃(𝑖)𝑃′(𝑗)
This is equal to the Kullback-Leibler divergence of the joint distribution with the product distribution of the
marginals.
29.20. sklearn.metrics: Metrics
1665
scikit-learn user guide, Release 0.18.2
This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values
won’t change the score value in any way.
This metric is furthermore symmetric: switching label_true with label_pred will return the same score
value. This can be useful to measure the agreement of two independent label assignments strategies on the same
dataset when the real ground truth is not known.
Read more in the User Guide.
Parameterslabels_true : int array, shape = [n_samples]
A clustering of the data into disjoint subsets.
labels_pred : array, shape = [n_samples]
A clustering of the data into disjoint subsets.
contingency : {None, array, sparse matrix},
shape = [n_classes_true, n_classes_pred]
A contingency matrix given by the contingency_matrix function. If value is
None, it will be computed, otherwise the given value is used, with labels_true and
labels_pred ignored.
Returnsmi : ﬂoat
Mutual information, a non-negative value
See also:
adjusted_mutual_info_scoreAdjusted against chance Mutual Information
normalized_mutual_info_scoreNormalized Mutual Information
Examples using sklearn.metrics.mutual_info_score
• Adjustment for chance in clustering performance evaluation
sklearn.metrics.normalized_mutual_info_score
sklearn.metrics.normalized_mutual_info_score(labels_true, labels_pred)
Normalized Mutual Information between two clusterings.
Normalized Mutual Information (NMI) is an normalization of the Mutual Information (MI) score to scale the
results between 0 (no mutual information) and 1 (perfect correlation). In this function, mutual information is
normalized by sqrt(H(labels_true) * H(labels_pred))
This measure is not adjusted for chance. Therefore adjusted_mustual_info_score might be preferred.
This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values
won’t change the score value in any way.
This metric is furthermore symmetric: switching label_true with label_pred will return the same score
value. This can be useful to measure the agreement of two independent label assignments strategies on the same
dataset when the real ground truth is not known.
Read more in the User Guide.
Parameterslabels_true : int array, shape = [n_samples]
A clustering of the data into disjoint subsets.
1666
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
labels_pred : array, shape = [n_samples]
A clustering of the data into disjoint subsets.
Returnsnmi : ﬂoat
score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
See also:
adjusted_rand_scoreAdjusted Rand Index
adjusted_mutual_info_scoreAdjusted Mutual Information (adjusted against chance)
Examples
Perfect labelings are both homogeneous and complete, hence have score 1.0:
>>> from sklearn.metrics.cluster import normalized_mutual_info_score
>>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
1.0
>>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
1.0
If classes members are completely split across different clusters, the assignment is totally in-complete, hence
the NMI is null:
>>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
0.0
sklearn.metrics.silhouette_score
sklearn.metrics.silhouette_score(X, labels,
metric=’euclidean’,
sample_size=None,
ran-
dom_state=None, **kwds)
Compute the mean Silhouette Coefﬁcient of all samples.
The Silhouette Coefﬁcient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster
distance (b) for each sample. The Silhouette Coefﬁcient for a sample is (b -a) / max(a,b). To clarify,
b is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette
Coefﬁcent is only deﬁned if number of labels is 2 <= n_labels <= n_samples - 1.
This function returns the mean Silhouette Coefﬁcient over all samples. To obtain the values for each sample,
use silhouette_samples.
The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values
generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.
Read more in the User Guide.
ParametersX : array [n_samples_a, n_samples_a] if metric == “precomputed”, or, [n_samples_a,
n_features] otherwise
Array of pairwise distances between samples, or a feature array.
labels : array, shape = [n_samples]
Predicted labels for each sample.
metric : string, or callable
29.20. sklearn.metrics: Metrics
1667
scikit-learn user guide, Release 0.18.2
The metric to use when calculating distance between instances in a feature
array.
If metric is a string,
it must be one of the options allowed by
metrics.pairwise.pairwise_distances. If X is the distance array itself,
use metric="precomputed".
sample_size : int or None
The size of the sample to use when computing the Silhouette Coefﬁcient on a random
subset of the data. If sample_size is None, no sampling is used.
random_state : integer or numpy.RandomState, optional
The generator used to randomly select a subset of samples if sample_size is not
None. If an integer is given, it ﬁxes the seed. Defaults to the global numpy random
number generator.
‘**kwds‘ : optional keyword parameters
Any further parameters are passed directly to the distance function.
If using a
scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy
docs for usage examples.
Returnssilhouette : ﬂoat
Mean Silhouette Coefﬁcient for all samples.
References
[R64], [R65]
Examples using sklearn.metrics.silhouette_score
• Demo of afﬁnity propagation clustering algorithm
• Demo of DBSCAN clustering algorithm
• A demo of K-Means clustering on the handwritten digits data
• Selecting the number of clusters with silhouette analysis on KMeans clustering
• Clustering text documents using k-means
sklearn.metrics.silhouette_samples
sklearn.metrics.silhouette_samples(X, labels, metric=’euclidean’, **kwds)
Compute the Silhouette Coefﬁcient for each sample.
The Silhouette Coefﬁcient is a measure of how well samples are clustered with samples that are similar to
themselves. Clustering models with a high Silhouette Coefﬁcient are said to be dense, where samples in the
same cluster are similar to each other, and well separated, where samples in different clusters are not very
similar to each other.
The Silhouette Coefﬁcient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster
distance (b) for each sample. The Silhouette Coefﬁcient for a sample is (b -a) / max(a,b). Note that
Silhouette Coefﬁcent is only deﬁned if number of labels is 2 <= n_labels <= n_samples - 1.
This function returns the Silhouette Coefﬁcient for each sample.
The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.
1668
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Read more in the User Guide.
ParametersX : array [n_samples_a, n_samples_a] if metric == “precomputed”, or, [n_samples_a,
n_features] otherwise
Array of pairwise distances between samples, or a feature array.
labels : array, shape = [n_samples]
label values for each sample
metric : string, or callable
The metric to use when calculating distance between instances in a feature
array.
If metric is a string,
it must be one of the options allowed by
sklearn.metrics.pairwise.pairwise_distances. If X is the distance
array itself, use “precomputed” as the metric.
‘**kwds‘ : optional keyword parameters
Any further parameters are passed directly to the distance function.
If using a
scipy.spatial.distance metric, the parameters are still metric dependent. See
the scipy docs for usage examples.
Returnssilhouette : array, shape = [n_samples]
Silhouette Coefﬁcient for each samples.
References
[R62], [R63]
Examples using sklearn.metrics.silhouette_samples
• Selecting the number of clusters with silhouette analysis on KMeans clustering
sklearn.metrics.v_measure_score
sklearn.metrics.v_measure_score(labels_true, labels_pred)
V-measure cluster labeling given a ground truth.
This score is identical to normalized_mutual_info_score.
The V-measure is the harmonic mean between homogeneity and completeness:
v = 2 * (homogeneity * completeness) / (homogeneity + completeness)
This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values
won’t change the score value in any way.
This metric is furthermore symmetric: switching label_true with label_pred will return the same score
value. This can be useful to measure the agreement of two independent label assignments strategies on the same
dataset when the real ground truth is not known.
Read more in the User Guide.
Parameterslabels_true : int array, shape = [n_samples]
ground truth class labels to be used as a reference
29.20. sklearn.metrics: Metrics
1669
scikit-learn user guide, Release 0.18.2
labels_pred : array, shape = [n_samples]
cluster labels to evaluate
Returnsv_measure : ﬂoat
score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
See also:
homogeneity_score, completeness_score
References
[R66]
Examples
Perfect labelings are both homogeneous and complete, hence have score 1.0:
>>> from sklearn.metrics.cluster import v_measure_score
>>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])
1.0
>>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])
1.0
Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penal-
ized:
>>> print("%.6f" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))
...
0.8...
>>> print("%.6f" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))
...
0.66...
Labelings that have pure clusters with members coming from the same classes are homogeneous but un-
necessary splits harms completeness and thus penalize V-measure as well:
>>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))
...
0.8...
>>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))
...
0.66...
If classes members are completely split across different clusters, the assignment is totally incomplete, hence the
V-Measure is null:
>>> print("%.6f" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))
...
0.0...
Clusters that include samples from totally different classes totally destroy the homogeneity of the labeling,
hence:
1670
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))
...
0.0...
Examples using sklearn.metrics.v_measure_score
• Biclustering documents with the Spectral Co-clustering algorithm
• Adjustment for chance in clustering performance evaluation
• Demo of afﬁnity propagation clustering algorithm
• Demo of DBSCAN clustering algorithm
• A demo of K-Means clustering on the handwritten digits data
• Clustering text documents using k-means
29.20.6 Biclustering metrics
See the Biclustering evaluation section of the user guide for further details.
metrics.consensus_score(a, b[, similarity])
The similarity of two sets of biclusters.
sklearn.metrics.consensus_score
sklearn.metrics.consensus_score(a, b, similarity=’jaccard’)
The similarity of two sets of biclusters.
Similarity between individual biclusters is computed. Then the best matching between sets is found using the
Hungarian algorithm. The ﬁnal score is the sum of similarities divided by the size of the larger set.
Read more in the User Guide.
Parametersa : (rows, columns)
Tuple of row and column indicators for a set of biclusters.
b : (rows, columns)
Another set of biclusters like a.
similarity : string or function, optional, default: “jaccard”
May be the string “jaccard” to use the Jaccard coefﬁcient, or any function that takes
four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows,
b_columns).
References
•Hochreiter, Bodenhofer, et. al., 2010. FABIA: factor analysis for bicluster acquisition.
Examples using sklearn.metrics.consensus_score
• A demo of the Spectral Biclustering algorithm
29.20. sklearn.metrics: Metrics
1671
scikit-learn user guide, Release 0.18.2
• A demo of the Spectral Co-Clustering algorithm
29.20.7 Pairwise metrics
See the Pairwise metrics, Afﬁnities and Kernels section of the user guide for further details.
metrics.pairwise.additive_chi2_kernel(X[,
Y])
Computes the additive chi-squared kernel between obser-
vations in X and Y
metrics.pairwise.chi2_kernel(X[, Y, gamma])
Computes the exponential chi-squared kernel X and Y.
metrics.pairwise.distance_metrics()
Valid metrics for pairwise_distances.
metrics.pairwise.euclidean_distances(X[,
Y, ...])
Considering the rows of X (and Y=X) as vectors, compute
the distance matrix between each pair of vectors.
metrics.pairwise.kernel_metrics()
Valid metrics for pairwise_kernels
metrics.pairwise.linear_kernel(X[, Y])
Compute the linear kernel between X and Y.
metrics.pairwise.manhattan_distances(X[,
Y, ...])
Compute the L1 distances between the vectors in X and Y.
metrics.pairwise.pairwise_distances(X[,
Y, ...])
Compute the distance matrix from a vector array X and op-
tional Y.
metrics.pairwise.pairwise_kernels(X[,
Y,
...])
Compute the kernel between arrays X and optional array Y.
metrics.pairwise.polynomial_kernel(X[, Y,
...])
Compute the polynomial kernel between X and Y:
metrics.pairwise.rbf_kernel(X[, Y, gamma])
Compute the rbf (gaussian) kernel between X and Y:
metrics.pairwise.sigmoid_kernel(X[, Y, ...])
Compute the sigmoid kernel between X and Y:
metrics.pairwise.cosine_similarity(X[, Y,
...])
Compute cosine similarity between samples in X and Y.
metrics.pairwise.cosine_distances(X[, Y])
Compute cosine distance between samples in X and Y.
metrics.pairwise.laplacian_kernel(X[,
Y,
gamma])
Compute the laplacian kernel between X and Y.
metrics.pairwise_distances(X[, Y, metric, ...])
Compute the distance matrix from a vector array X and op-
tional Y.
metrics.pairwise_distances_argmin(X,
Y[,
...])
Compute minimum distances between one point and a set
of points.
metrics.pairwise_distances_argmin_min(X,
Y)
Compute minimum distances between one point and a set
of points.
metrics.pairwise.paired_euclidean_distances(X,
Y)
Computes the paired euclidean distances between X and Y
metrics.pairwise.paired_manhattan_distances(X,
Y)
Compute the L1 distances between the vectors in X and Y.
metrics.pairwise.paired_cosine_distances(X,
Y)
Computes the paired cosine distances between X and Y
metrics.pairwise.paired_distances(X,
Y[,
metric])
Computes the paired distances between X and Y.
sklearn.metrics.pairwise.additive_chi2_kernel
sklearn.metrics.pairwise.additive_chi2_kernel(X, Y=None)
Computes the additive chi-squared kernel between observations in X and Y
The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative.
This kernel is most commonly applied to histograms.
1672
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The chi-squared kernel is given by:
k(x, y) = -Sum [(x - y)^2 / (x + y)]
It can be interpreted as a weighted difference per entry.
Read more in the User Guide.
ParametersX : array-like of shape (n_samples_X, n_features)
Y : array of shape (n_samples_Y, n_features)
Returnskernel_matrix : array of shape (n_samples_X, n_samples_Y)
See also:
chi2_kernelThe exponentiated version of the kernel, which is usually preferable.
sklearn.kernel_approximation.AdditiveChi2SamplerA Fourier approximation to this kernel.
Notes
As the negative of a distance, this kernel is only conditionally positive deﬁnite.
References
•Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classiﬁcation
of texture and object categories: A comprehensive study International Journal of Computer Vision 2007
http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf
sklearn.metrics.pairwise.chi2_kernel
sklearn.metrics.pairwise.chi2_kernel(X, Y=None, gamma=1.0)
Computes the exponential chi-squared kernel X and Y.
The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative.
This kernel is most commonly applied to histograms.
The chi-squared kernel is given by:
k(x, y) = exp(-gamma Sum [(x - y)^2 / (x + y)])
It can be interpreted as a weighted difference per entry.
Read more in the User Guide.
ParametersX : array-like of shape (n_samples_X, n_features)
Y : array of shape (n_samples_Y, n_features)
gamma : ﬂoat, default=1.
Scaling parameter of the chi2 kernel.
Returnskernel_matrix : array of shape (n_samples_X, n_samples_Y)
See also:
additive_chi2_kernelThe additive version of this kernel
29.20. sklearn.metrics: Metrics
1673
scikit-learn user guide, Release 0.18.2
sklearn.kernel_approximation.AdditiveChi2SamplerA Fourier approximation to the additive
version of this kernel.
References
•Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classiﬁcation
of texture and object categories: A comprehensive study International Journal of Computer Vision 2007
http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf
sklearn.metrics.pairwise.distance_metrics
sklearn.metrics.pairwise.distance_metrics()
Valid metrics for pairwise_distances.
This function simply returns the valid pairwise distance metrics. It exists to allow for a description of the
mapping for each of the valid strings.
The valid distance metrics, and the function they map to, are:
metric
Function
‘cityblock’
metrics.pairwise.manhattan_distances
‘cosine’
metrics.pairwise.cosine_distances
‘euclidean’
metrics.pairwise.euclidean_distances
‘l1’
metrics.pairwise.manhattan_distances
‘l2’
metrics.pairwise.euclidean_distances
‘manhattan’
metrics.pairwise.manhattan_distances
Read more in the User Guide.
sklearn.metrics.pairwise.euclidean_distances
sklearn.metrics.pairwise.euclidean_distances(X,
Y=None,
Y_norm_squared=None,
squared=False, X_norm_squared=None)
Considering the rows of X (and Y=X) as vectors, compute the distance matrix between each pair of vectors.
For efﬁciency reasons, the euclidean distance between a pair of row vector x and y is computed as:
dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))
This formulation has two advantages over other ways of computing distances. First, it is computationally ef-
ﬁcient when dealing with sparse data. Second, if one argument varies but the other remains unchanged, then
dot(x, x) and/or dot(y, y) can be pre-computed.
However, this is not the most precise way of doing this computation, and the distance matrix returned by this
function may not be exactly symmetric as required by, e.g., scipy.spatial.distance functions.
Read more in the User Guide.
ParametersX : {array-like, sparse matrix}, shape (n_samples_1, n_features)
Y : {array-like, sparse matrix}, shape (n_samples_2, n_features)
Y_norm_squared : array-like, shape (n_samples_2, ), optional
Pre-computed dot-products of vectors in Y (e.g., (Y**2).sum(axis=1))
squared : boolean, optional
1674
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Return squared Euclidean distances.
X_norm_squared : array-like, shape = [n_samples_1], optional
Pre-computed dot-products of vectors in X (e.g., (X**2).sum(axis=1))
Returnsdistances : {array, sparse matrix}, shape (n_samples_1, n_samples_2)
See also:
paired_distancesdistances betweens pairs of elements of X and Y.
Examples
>>> from sklearn.metrics.pairwise import euclidean_distances
>>> X = [[0, 1], [1, 1]]
>>> # distance between rows of X
>>> euclidean_distances(X, X)
array([[ 0.,
1.],
[ 1.,
0.]])
>>> # get distance to origin
>>> euclidean_distances(X, [[0, 0]])
array([[ 1.
],
[ 1.41421356]])
sklearn.metrics.pairwise.kernel_metrics
sklearn.metrics.pairwise.kernel_metrics()
Valid metrics for pairwise_kernels
This function simply returns the valid pairwise distance metrics. It exists, however, to allow for a verbose
description of the mapping for each of the valid strings.
The valid distance metrics, and the function they map to, are:
metric
Function
‘additive_chi2’
sklearn.pairwise.additive_chi2_kernel
‘chi2’
sklearn.pairwise.chi2_kernel
‘linear’
sklearn.pairwise.linear_kernel
‘poly’
sklearn.pairwise.polynomial_kernel
‘polynomial’
sklearn.pairwise.polynomial_kernel
‘rbf’
sklearn.pairwise.rbf_kernel
‘laplacian’
sklearn.pairwise.laplacian_kernel
‘sigmoid’
sklearn.pairwise.sigmoid_kernel
‘cosine’
sklearn.pairwise.cosine_similarity
Read more in the User Guide.
sklearn.metrics.pairwise.linear_kernel
sklearn.metrics.pairwise.linear_kernel(X, Y=None)
Compute the linear kernel between X and Y.
Read more in the User Guide.
ParametersX : array of shape (n_samples_1, n_features)
Y : array of shape (n_samples_2, n_features)
29.20. sklearn.metrics: Metrics
1675
scikit-learn user guide, Release 0.18.2
ReturnsGram matrix : array of shape (n_samples_1, n_samples_2)
sklearn.metrics.pairwise.manhattan_distances
sklearn.metrics.pairwise.manhattan_distances(X,
Y=None,
sum_over_features=True,
size_threshold=500000000.0)
Compute the L1 distances between the vectors in X and Y.
With sum_over_features equal to False it returns the componentwise distances.
Read more in the User Guide.
ParametersX : array_like
An array with shape (n_samples_X, n_features).
Y : array_like, optional
An array with shape (n_samples_Y, n_features).
sum_over_features : bool, default=True
If True the function returns the pairwise distance matrix else it returns the component-
wise L1 pairwise-distances. Not supported for sparse matrix inputs.
size_threshold : int, default=5e8
Unused parameter.
ReturnsD : array
If sum_over_features is False shape is (n_samples_X * n_samples_Y, n_features) and D
contains the componentwise L1 pairwise-distances (ie. absolute difference), else shape
is (n_samples_X, n_samples_Y) and D contains the pairwise L1 distances.
Examples
>>> from sklearn.metrics.pairwise import manhattan_distances
>>> manhattan_distances([[3]], [[3]])
array([[ 0.]])
>>> manhattan_distances([[3]], [[2]])
array([[ 1.]])
>>> manhattan_distances([[2]], [[3]])
array([[ 1.]])
>>> manhattan_distances([[1, 2], [3, 4]],
[[1, 2], [0, 3]])
array([[ 0.,
2.],
[ 4.,
4.]])
>>> import numpy as np
>>> X = np.ones((1, 2))
>>> y = 2 * np.ones((2, 2))
>>> manhattan_distances(X, y, sum_over_features=False)
array([[ 1.,
1.],
[ 1.,
1.]]...)
sklearn.metrics.pairwise.pairwise_distances
sklearn.metrics.pairwise.pairwise_distances(X, Y=None, metric=’euclidean’, n_jobs=1,
**kwds)
Compute the distance matrix from a vector array X and optional Y.
1676
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
This method takes either a vector array or a distance matrix, and returns a distance matrix. If the input is a vector
array, the distances are computed. If the input is a distances matrix, it is returned instead.
This method provides a safe way to take a distance matrix as input, while preserving compatibility with many
other algorithms that take a vector array.
If Y is given (default is None), then the returned matrix is the pairwise distance between the arrays from both X
and Y.
Valid values for metric are:
•From scikit-learn: [’cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]. These metrics support sparse
matrix inputs.
•From scipy.spatial.distance: [’braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jac-
card’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’,
‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’] See the documentation for scipy.spatial.distance for
details on these metrics. These metrics do not support sparse matrix inputs.
Note that in the case of ‘cityblock’, ‘cosine’ and ‘euclidean’ (which are valid scipy.spatial.distance met-
rics), the scikit-learn implementation will be used, which is faster and has support for sparse matrices (ex-
cept for ‘cityblock’).
For a verbose description of the metrics from scikit-learn, see the __doc__ of the
sklearn.pairwise.distance_metrics function.
Read more in the User Guide.
ParametersX : array [n_samples_a, n_samples_a] if metric == “precomputed”, or, [n_samples_a,
n_features] otherwise
Array of pairwise distances between samples, or a feature array.
Y : array [n_samples_b, n_features], optional
An optional second feature array. Only allowed if metric != “precomputed”.
metric : string, or callable
The metric to use when calculating distance between instances in a feature
array.
If metric is a string,
it must be one of the options allowed by
scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pair-
wise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed
to be a distance matrix. Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable should take two
arrays from X as input and return a value indicating the distance between them.
n_jobs : int
The number of jobs to use for the computation. This works by breaking down the
pairwise matrix into n_jobs even slices and computing them in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
‘**kwds‘ : optional keyword parameters
Any further parameters are passed directly to the distance function.
If using a
scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy
docs for usage examples.
ReturnsD : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]
29.20. sklearn.metrics: Metrics
1677
scikit-learn user guide, Release 0.18.2
A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of
the given matrix X, if Y is None. If Y is not None, then D_{i, j} is the distance between
the ith array from X and the jth array from Y.
sklearn.metrics.pairwise.pairwise_kernels
sklearn.metrics.pairwise.pairwise_kernels(X,
Y=None,
metric=’linear’,
ﬁl-
ter_params=False, n_jobs=1, **kwds)
Compute the kernel between arrays X and optional array Y.
This method takes either a vector array or a kernel matrix, and returns a kernel matrix. If the input is a vector
array, the kernels are computed. If the input is a kernel matrix, it is returned instead.
This method provides a safe way to take a kernel matrix as input, while preserving compatibility with many
other algorithms that take a vector array.
If Y is given (default is None), then the returned matrix is the pairwise kernel between the arrays from both X
and Y.
Valid values for metric are::[’rbf’, ‘sigmoid’, ‘polynomial’, ‘poly’, ‘linear’, ‘cosine’]
Read more in the User Guide.
ParametersX : array [n_samples_a, n_samples_a] if metric == “precomputed”, or, [n_samples_a,
n_features] otherwise
Array of pairwise kernels between samples, or a feature array.
Y : array [n_samples_b, n_features]
A second feature array only if X has shape [n_samples_a, n_features].
metric : string, or callable
The metric to use when calculating kernel between instances in a feature
array.
If
metric
is
a
string,
it
must
be
one
of
the
metrics
in
pair-
wise.PAIRWISE_KERNEL_FUNCTIONS. If metric is “precomputed”, X is assumed
to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable should take two
arrays from X as input and return a value indicating the distance between them.
n_jobs : int
The number of jobs to use for the computation. This works by breaking down the
pairwise matrix into n_jobs even slices and computing them in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
ﬁlter_params : boolean
Whether to ﬁlter invalid parameters or not.
‘**kwds‘ : optional keyword parameters
Any further parameters are passed directly to the kernel function.
ReturnsK : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]
A kernel matrix K such that K_{i, j} is the kernel between the ith and jth vectors of the
given matrix X, if Y is None. If Y is not None, then K_{i, j} is the kernel between the
ith array from X and the jth array from Y.
1678
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
If metric is ‘precomputed’, Y is ignored and X is returned.
sklearn.metrics.pairwise.polynomial_kernel
sklearn.metrics.pairwise.polynomial_kernel(X,
Y=None,
degree=3,
gamma=None,
coef0=1)
Compute the polynomial kernel between X and Y:
K(X, Y) = (gamma <X, Y> + coef0)^degree
Read more in the User Guide.
ParametersX : ndarray of shape (n_samples_1, n_features)
Y : ndarray of shape (n_samples_2, n_features)
degree : int, default 3
gamma : ﬂoat, default None
if None, defaults to 1.0 / n_samples_1
coef0 : int, default 1
ReturnsGram matrix : array of shape (n_samples_1, n_samples_2)
sklearn.metrics.pairwise.rbf_kernel
sklearn.metrics.pairwise.rbf_kernel(X, Y=None, gamma=None)
Compute the rbf (gaussian) kernel between X and Y:
K(x, y) = exp(-gamma ||x-y||^2)
for each pair of rows x in X and y in Y.
Read more in the User Guide.
ParametersX : array of shape (n_samples_X, n_features)
Y : array of shape (n_samples_Y, n_features)
gamma : ﬂoat, default None
If None, defaults to 1.0 / n_samples_X
Returnskernel_matrix : array of shape (n_samples_X, n_samples_Y)
sklearn.metrics.pairwise.sigmoid_kernel
sklearn.metrics.pairwise.sigmoid_kernel(X, Y=None, gamma=None, coef0=1)
Compute the sigmoid kernel between X and Y:
K(X, Y) = tanh(gamma <X, Y> + coef0)
Read more in the User Guide.
29.20. sklearn.metrics: Metrics
1679
scikit-learn user guide, Release 0.18.2
ParametersX : ndarray of shape (n_samples_1, n_features)
Y : ndarray of shape (n_samples_2, n_features)
gamma : ﬂoat, default None
If None, defaults to 1.0 / n_samples_1
coef0 : int, default 1
ReturnsGram matrix : array of shape (n_samples_1, n_samples_2)
sklearn.metrics.pairwise.cosine_similarity
sklearn.metrics.pairwise.cosine_similarity(X, Y=None, dense_output=True)
Compute cosine similarity between samples in X and Y.
Cosine similarity, or the cosine kernel, computes similarity as the normalized dot product of X and Y:
K(X, Y) = <X, Y> / (||X||*||Y||)
On L2-normalized data, this function is equivalent to linear_kernel.
Read more in the User Guide.
ParametersX : ndarray or sparse array, shape: (n_samples_X, n_features)
Input data.
Y : ndarray or sparse array, shape: (n_samples_Y, n_features)
Input data. If None, the output will be the pairwise similarities between all samples in
X.
dense_output : boolean (optional), default True
Whether to return dense output even when the input is sparse. If False, the output is
sparse if both input arrays are sparse.
New in version 0.17: parameter dense_output for dense output.
Returnskernel matrix : array
An array with shape (n_samples_X, n_samples_Y).
sklearn.metrics.pairwise.cosine_distances
sklearn.metrics.pairwise.cosine_distances(X, Y=None)
Compute cosine distance between samples in X and Y.
Cosine distance is deﬁned as 1.0 minus the cosine similarity.
Read more in the User Guide.
ParametersX : array_like, sparse matrix
with shape (n_samples_X, n_features).
Y : array_like, sparse matrix (optional)
with shape (n_samples_Y, n_features).
Returnsdistance matrix : array
An array with shape (n_samples_X, n_samples_Y).
1680
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
See also:
sklearn.metrics.pairwise.cosine_similarity, scipy.spatial.distance.cosine
sklearn.metrics.pairwise.laplacian_kernel
sklearn.metrics.pairwise.laplacian_kernel(X, Y=None, gamma=None)
Compute the laplacian kernel between X and Y.
The laplacian kernel is deﬁned as:
K(x, y) = exp(-gamma ||x-y||_1)
for each pair of rows x in X and y in Y. Read more in the User Guide.
New in version 0.17.
ParametersX : array of shape (n_samples_X, n_features)
Y : array of shape (n_samples_Y, n_features)
gamma : ﬂoat, default None
If None, defaults to 1.0 / n_samples_X
Returnskernel_matrix : array of shape (n_samples_X, n_samples_Y)
sklearn.metrics.pairwise_distances
sklearn.metrics.pairwise_distances(X, Y=None, metric=’euclidean’, n_jobs=1, **kwds)
Compute the distance matrix from a vector array X and optional Y.
This method takes either a vector array or a distance matrix, and returns a distance matrix. If the input is a vector
array, the distances are computed. If the input is a distances matrix, it is returned instead.
This method provides a safe way to take a distance matrix as input, while preserving compatibility with many
other algorithms that take a vector array.
If Y is given (default is None), then the returned matrix is the pairwise distance between the arrays from both X
and Y.
Valid values for metric are:
•From scikit-learn: [’cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]. These metrics support sparse
matrix inputs.
•From scipy.spatial.distance: [’braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jac-
card’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’,
‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’] See the documentation for scipy.spatial.distance for
details on these metrics. These metrics do not support sparse matrix inputs.
Note that in the case of ‘cityblock’, ‘cosine’ and ‘euclidean’ (which are valid scipy.spatial.distance met-
rics), the scikit-learn implementation will be used, which is faster and has support for sparse matrices (ex-
cept for ‘cityblock’).
For a verbose description of the metrics from scikit-learn, see the __doc__ of the
sklearn.pairwise.distance_metrics function.
Read more in the User Guide.
ParametersX : array [n_samples_a, n_samples_a] if metric == “precomputed”, or, [n_samples_a,
n_features] otherwise
Array of pairwise distances between samples, or a feature array.
29.20. sklearn.metrics: Metrics
1681
scikit-learn user guide, Release 0.18.2
Y : array [n_samples_b, n_features], optional
An optional second feature array. Only allowed if metric != “precomputed”.
metric : string, or callable
The metric to use when calculating distance between instances in a feature
array.
If metric is a string,
it must be one of the options allowed by
scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pair-
wise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed
to be a distance matrix. Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable should take two
arrays from X as input and return a value indicating the distance between them.
n_jobs : int
The number of jobs to use for the computation. This works by breaking down the
pairwise matrix into n_jobs even slices and computing them in parallel.
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which
is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
n_jobs = -2, all CPUs but one are used.
‘**kwds‘ : optional keyword parameters
Any further parameters are passed directly to the distance function.
If using a
scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy
docs for usage examples.
ReturnsD : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]
A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of
the given matrix X, if Y is None. If Y is not None, then D_{i, j} is the distance between
the ith array from X and the jth array from Y.
Examples using sklearn.metrics.pairwise_distances
• Agglomerative clustering with different metrics
sklearn.metrics.pairwise_distances_argmin
sklearn.metrics.pairwise_distances_argmin(X,
Y,
axis=1,
metric=’euclidean’,
batch_size=500, metric_kwargs=None)
Compute minimum distances between one point and a set of points.
This function computes for each row in X, the index of the row of Y which is closest (according to the speciﬁed
distance).
This is mostly equivalent to calling:
pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)
but uses much less memory, and is faster for large arrays.
This function works with dense 2D arrays only.
ParametersX : array-like
Arrays
containing
points.
Respective
shapes
(n_samples1,
n_features)
and
(n_samples2, n_features)
1682
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Y : array-like
Arrays
containing
points.
Respective
shapes
(n_samples1,
n_features)
and
(n_samples2, n_features)
batch_size : integer
To reduce memory consumption over the naive solution, data are processed in batches,
comprising batch_size rows of X and batch_size rows of Y. The default value is quite
conservative, but can be changed for ﬁne-tuning. The larger the number, the larger the
memory usage.
metric : string or callable
metric to use for distance computation.
Any metric from scikit-learn or
scipy.spatial.distance can be used.
If metric is a callable function, it is called on each pair of instances (rows) and the
resulting value recorded. The callable should take two arrays as input and return one
value indicating the distance between them. This works for Scipy’s metrics, but is less
efﬁcient than passing the metric name as a string.
Distance matrices are not supported.
Valid values for metric are:
•from scikit-learn: [’cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]
•from scipy.spatial.distance:
[’braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’,
‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’,
‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeu-
clidean’, ‘yule’]
See the documentation for scipy.spatial.distance for details on these metrics.
metric_kwargs : dict
keyword arguments to pass to speciﬁed metric function.
axis : int, optional, default 1
Axis along which the argmin and distances are to be computed.
Returnsargmin : numpy.ndarray
Y[argmin[i], :] is the row in Y that is closest to X[i, :].
See also:
sklearn.metrics.pairwise_distances, sklearn.metrics.pairwise_distances_argmin_min
Examples using sklearn.metrics.pairwise_distances_argmin
• Color Quantization using K-Means
• Comparison of the K-Means and MiniBatchKMeans clustering algorithms
sklearn.metrics.pairwise_distances_argmin_min
sklearn.metrics.pairwise_distances_argmin_min(X,
Y,
axis=1,
metric=’euclidean’,
batch_size=500, metric_kwargs=None)
Compute minimum distances between one point and a set of points.
29.20. sklearn.metrics: Metrics
1683
scikit-learn user guide, Release 0.18.2
This function computes for each row in X, the index of the row of Y which is closest (according to the speciﬁed
distance). The minimal distances are also returned.
This is mostly equivalent to calling:
(pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),pairwise_distances(X,
Y=Y,
metric=metric).min(axis=axis))
but uses much less memory, and is faster for large arrays.
ParametersX, Y : {array-like, sparse matrix}
Arrays
containing
points.
Respective
shapes
(n_samples1,
n_features)
and
(n_samples2, n_features)
batch_size : integer
To reduce memory consumption over the naive solution, data are processed in batches,
comprising batch_size rows of X and batch_size rows of Y. The default value is quite
conservative, but can be changed for ﬁne-tuning. The larger the number, the larger the
memory usage.
metric : string or callable, default ‘euclidean’
metric to use for distance computation.
Any metric from scikit-learn or
scipy.spatial.distance can be used.
If metric is a callable function, it is called on each pair of instances (rows) and the
resulting value recorded. The callable should take two arrays as input and return one
value indicating the distance between them. This works for Scipy’s metrics, but is less
efﬁcient than passing the metric name as a string.
Distance matrices are not supported.
Valid values for metric are:
•from scikit-learn: [’cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]
•from scipy.spatial.distance:
[’braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’,
‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’,
‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeu-
clidean’, ‘yule’]
See the documentation for scipy.spatial.distance for details on these metrics.
metric_kwargs : dict, optional
Keyword arguments to pass to speciﬁed metric function.
axis : int, optional, default 1
Axis along which the argmin and distances are to be computed.
Returnsargmin : numpy.ndarray
Y[argmin[i], :] is the row in Y that is closest to X[i, :].
distances : numpy.ndarray
distances[i] is the distance between the i-th row in X and the argmin[i]-th row in Y.
See also:
sklearn.metrics.pairwise_distances, sklearn.metrics.pairwise_distances_argmin
1684
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.metrics.pairwise.paired_euclidean_distances
sklearn.metrics.pairwise.paired_euclidean_distances(X, Y)
Computes the paired euclidean distances between X and Y
Read more in the User Guide.
ParametersX : array-like, shape (n_samples, n_features)
Y : array-like, shape (n_samples, n_features)
Returnsdistances : ndarray (n_samples, )
sklearn.metrics.pairwise.paired_manhattan_distances
sklearn.metrics.pairwise.paired_manhattan_distances(X, Y)
Compute the L1 distances between the vectors in X and Y.
Read more in the User Guide.
ParametersX : array-like, shape (n_samples, n_features)
Y : array-like, shape (n_samples, n_features)
Returnsdistances : ndarray (n_samples, )
sklearn.metrics.pairwise.paired_cosine_distances
sklearn.metrics.pairwise.paired_cosine_distances(X, Y)
Computes the paired cosine distances between X and Y
Read more in the User Guide.
ParametersX : array-like, shape (n_samples, n_features)
Y : array-like, shape (n_samples, n_features)
Returnsdistances : ndarray, shape (n_samples, )
Notes
The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit
norm
sklearn.metrics.pairwise.paired_distances
sklearn.metrics.pairwise.paired_distances(X, Y, metric=’euclidean’, **kwds)
Computes the paired distances between X and Y.
Computes the distances between (X[0], Y[0]), (X[1], Y[1]), etc...
Read more in the User Guide.
ParametersX : ndarray (n_samples, n_features)
Array 1 for distance computation.
Y : ndarray (n_samples, n_features)
Array 2 for distance computation.
29.20. sklearn.metrics: Metrics
1685
scikit-learn user guide, Release 0.18.2
metric : string or callable
The metric to use when calculating distance between instances in a feature array. If
metric is a string, it must be one of the options speciﬁed in PAIRED_DISTANCES,
including “euclidean”, “manhattan”, or “cosine”. Alternatively, if metric is a callable
function, it is called on each pair of instances (rows) and the resulting value recorded.
The callable should take two arrays from X as input and return a value indicating the
distance between them.
Returnsdistances : ndarray (n_samples, )
See also:
pairwise_distancespairwise distances.
Examples
>>> from sklearn.metrics.pairwise import paired_distances
>>> X = [[0, 1], [1, 1]]
>>> Y = [[0, 1], [2, 1]]
>>> paired_distances(X, Y)
array([ 0.,
1.])
29.21 sklearn.mixture: Gaussian Mixture Models
The sklearn.mixture module implements mixture modeling algorithms.
User guide: See the Gaussian mixture models section for further details.
mixture.GaussianMixture([n_components, ...])
Gaussian Mixture.
mixture.BayesianGaussianMixture([...])
Variational Bayesian estimation of a Gaussian mixture.
29.21.1 sklearn.mixture.GaussianMixture
class sklearn.mixture.GaussianMixture(n_components=1,
covariance_type=’full’,
tol=0.001,
reg_covar=1e-06,
max_iter=100,
n_init=1,
init_params=’kmeans’,
weights_init=None,
means_init=None,
precisions_init=None,
ran-
dom_state=None,
warm_start=False,
verbose=0,
verbose_interval=10)
Gaussian Mixture.
Representation of a Gaussian mixture model probability distribution. This class allows to estimate the parame-
ters of a Gaussian mixture distribution.
New in version 0.18.
GaussianMixture.
Read more in the User Guide.
Parametersn_components : int, defaults to 1.
The number of mixture components.
1686
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
covariance_type : {‘full’, ‘tied’, ‘diag’, ‘spherical’},
defaults to ‘full’.
String describing the type of covariance parameters to use. Must be one of:
'full' (each component has its own general covariance matrix),
'tied' (all components share the same general covariance
˓→matrix),
'diag' (each component has its own diagonal covariance matrix),
'spherical' (each component has its own single variance).
tol : ﬂoat, defaults to 1e-3.
The convergence threshold. EM iterations will stop when the lower bound average gain
is below this threshold.
reg_covar : ﬂoat, defaults to 0.
Non-negative regularization added to the diagonal of covariance. Allows to assure that
the covariance matrices are all positive.
max_iter : int, defaults to 100.
The number of EM iterations to perform.
n_init : int, defaults to 1.
The number of initializations to perform. The best results are kept.
init_params : {‘kmeans’, ‘random’}, defaults to ‘kmeans’.
The method used to initialize the weights, the means and the precisions. Must be one
of:
'kmeans' : responsibilities are initialized using kmeans.
'random' : responsibilities are initialized randomly.
weights_init : array-like, shape (n_components, ), optional
The user-provided initial weights, defaults to None. If it None, weights are initialized
using the init_params method.
means_init: array-like, shape (n_components, n_features), optional :
The user-provided initial means, defaults to None, If it None, means are initialized using
the init_params method.
precisions_init: array-like, optional. :
The user-provided initial precisions (inverse of the covariance matrices), defaults to
None. If it None, precisions are initialized using the ‘init_params’ method. The shape
depends on ‘covariance_type’:
(n_components,)
if 'spherical',
(n_features, n_features)
if 'tied',
(n_components, n_features)
if 'diag',
(n_components, n_features, n_features) if 'full'
random_state : RandomState or an int seed, defaults to None.
A random number generator instance.
warm_start : bool, default to False.
29.21. sklearn.mixture: Gaussian Mixture Models
1687
scikit-learn user guide, Release 0.18.2
If ‘warm_start’ is True, the solution of the last ﬁtting is used as initialization for the next
call of ﬁt(). This can speed up convergence when ﬁt is called several time on similar
problems.
verbose : int, default to 0.
Enable verbose output. If 1 then it prints the current initialization and each iteration
step. If greater than 1 then it prints also the log probability and the time needed for each
step.
verbose_interval : int, default to 10.
Number of iteration done before the next print.
Attributesweights_ : array-like, shape (n_components,)
The weights of each mixture components.
means_ : array-like, shape (n_components, n_features)
The mean of each mixture component.
covariances_ : array-like
The covariance of each mixture component. The shape depends on covariance_type:
(n_components,)
if 'spherical',
(n_features, n_features)
if 'tied',
(n_components, n_features)
if 'diag',
(n_components, n_features, n_features) if 'full'
precisions_ : array-like
The precision matrices for each component in the mixture. A precision matrix is the
inverse of a covariance matrix. A covariance matrix is symmetric positive deﬁnite so
the mixture of Gaussian can be equivalently parameterized by the precision matrices.
Storing the precision matrices instead of the covariance matrices makes it more efﬁ-
cient to compute the log-likelihood of new samples at test time. The shape depends on
covariance_type:
(n_components,)
if 'spherical',
(n_features, n_features)
if 'tied',
(n_components, n_features)
if 'diag',
(n_components, n_features, n_features) if 'full'
precisions_cholesky_ : array-like
The cholesky decomposition of the precision matrices of each mixture component. A
precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric
positive deﬁnite so the mixture of Gaussian can be equivalently parameterized by the
precision matrices. Storing the precision matrices instead of the covariance matrices
makes it more efﬁcient to compute the log-likelihood of new samples at test time. The
shape depends on covariance_type:
(n_components,)
if 'spherical',
(n_features, n_features)
if 'tied',
(n_components, n_features)
if 'diag',
(n_components, n_features, n_features) if 'full'
converged_ : bool
True when convergence was reached in ﬁt(), False otherwise.
1688
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
n_iter_ : int
Number of step used by the best ﬁt of EM to reach the convergence.
lower_bound_ : ﬂoat
Log-likelihood of the best ﬁt of EM.
See also:
BayesianGaussianMixtureGaussian mixture model ﬁt with a variational inference.
Methods
aic(X)
Akaike information criterion for the current model on
the input X.
bic(X)
Bayesian information criterion for the current model on
the input X.
fit(X[, y])
Estimate model parameters with the EM algorithm.
get_params([deep])
Get parameters for this estimator.
predict(X[, y])
Predict the labels for the data samples in X using trained
model.
predict_proba(X)
Predict posterior probability of data per each compo-
nent.
sample([n_samples])
Generate random samples from the ﬁtted Gaussian dis-
tribution.
score(X[, y])
Compute the per-sample average log-likelihood of the
given data X.
score_samples(X)
Compute the weighted log probabilities for each sam-
ple.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_components=1, covariance_type=’full’, tol=0.001, reg_covar=1e-06, max_iter=100,
n_init=1,
init_params=’kmeans’,
weights_init=None,
means_init=None,
pre-
cisions_init=None,
random_state=None,
warm_start=False,
verbose=0,
ver-
bose_interval=10)
aic(X)
Akaike information criterion for the current model on the input X.
ParametersX : array of shape (n_samples, n_dimensions)
Returnsaic: ﬂoat :
The lower the better.
bic(X)
Bayesian information criterion for the current model on the input X.
ParametersX : array of shape (n_samples, n_dimensions)
Returnsbic: ﬂoat :
The lower the better.
fit(X, y=None)
Estimate model parameters with the EM algorithm.
29.21. sklearn.mixture: Gaussian Mixture Models
1689
scikit-learn user guide, Release 0.18.2
The method ﬁt the model n_init times and set the parameters with which the model has the largest likeli-
hood or lower bound. Within each trial, the method iterates between E-step and M-step for max_iter times
until the change of likelihood or lower bound is less than tol, otherwise, a ConvergenceWarning is raised.
ParametersX : array-like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnsself :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X, y=None)
Predict the labels for the data samples in X using trained model.
ParametersX : array-like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslabels : array, shape (n_samples,)
Component labels.
predict_proba(X)
Predict posterior probability of data per each component.
ParametersX : array-like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnsresp : array, shape (n_samples, n_components)
Returns the probability of the sample for each Gaussian (state) in the model.
sample(n_samples=1)
Generate random samples from the ﬁtted Gaussian distribution.
Parametersn_samples : int, optional
Number of samples to generate. Defaults to 1.
ReturnsX : array, shape (n_samples, n_features)
Randomly generated sample
y : array, shape (nsamples,)
Component labels
score(X, y=None)
Compute the per-sample average log-likelihood of the given data X.
ParametersX : array-like, shape (n_samples, n_dimensions)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslog_likelihood : ﬂoat
Log likelihood of the Gaussian mixture given X.
1690
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
score_samples(X)
Compute the weighted log probabilities for each sample.
ParametersX : array-like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslog_prob : array, shape (n_samples,)
Log probabilities of each data point in X.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.mixture.GaussianMixture
• Gaussian Mixture Model Ellipsoids
• GMM covariances
• Density Estimation for a Gaussian mixture
• Gaussian Mixture Model Selection
• Gaussian Mixture Model Sine Curve
29.21.2 sklearn.mixture.BayesianGaussianMixture
class sklearn.mixture.BayesianGaussianMixture(n_components=1,
covariance_type=’full’,
tol=0.001, reg_covar=1e-06, max_iter=100,
n_init=1,
init_params=’kmeans’,
weight_concentration_prior_type=’dirichlet_process’,
weight_concentration_prior=None,
mean_precision_prior=None,
mean_prior=None,
de-
grees_of_freedom_prior=None,
covari-
ance_prior=None,
random_state=None,
warm_start=False,
verbose=0,
ver-
bose_interval=10)
Variational Bayesian estimation of a Gaussian mixture.
This class allows to infer an approximate posterior distribution over the parameters of a Gaussian mixture
distribution. The effective number of components can be inferred from the data.
This class implements two types of prior for the weights distribution: a ﬁnite mixture model with Dirichlet
distribution and an inﬁnite mixture model with the Dirichlet Process. In practice Dirichlet Process inference
algorithm is approximated and uses a truncated distribution with a ﬁxed maximum number of components
(called the Stick-breaking representation). The number of components actually used almost always depends on
the data.
New in version 0.18.
BayesianGaussianMixture.
Read more in the User Guide.
29.21. sklearn.mixture: Gaussian Mixture Models
1691
scikit-learn user guide, Release 0.18.2
Parametersn_components : int, defaults to 1.
The number of mixture components.
Depending on the data and the value of the
weight_concentration_prior the model can decide to not use all the components by
setting some component weights_ to values very close to zero. The number of effective
components is therefore smaller than n_components.
covariance_type : {‘full’, ‘tied’, ‘diag’, ‘spherical’}, defaults to ‘full’
String describing the type of covariance parameters to use. Must be one of:
'full' (each component has its own general covariance matrix),
'tied' (all components share the same general covariance
˓→matrix),
'diag' (each component has its own diagonal covariance matrix),
'spherical' (each component has its own single variance).
tol : ﬂoat, defaults to 1e-3.
The convergence threshold. EM iterations will stop when the lower bound average gain
on the likelihood (of the training data with respect to the model) is below this threshold.
reg_covar : ﬂoat, defaults to 1e-6.
Non-negative regularization added to the diagonal of covariance. Allows to assure that
the covariance matrices are all positive.
max_iter : int, defaults to 100.
The number of EM iterations to perform.
n_init : int, defaults to 1.
The number of initializations to perform. The result with the highest lower bound value
on the likelihood is kept.
init_params : {‘kmeans’, ‘random’}, defaults to ‘kmeans’.
The method used to initialize the weights, the means and the covariances. Must be one
of:
'kmeans' : responsibilities are initialized using kmeans.
'random' : responsibilities are initialized randomly.
weight_concentration_prior_type : str, defaults to ‘dirichlet_process’.
String describing the type of the weight concentration prior. Must be one of:
'dirichlet_process' (using the Stick-breaking representation),
'dirichlet_distribution' (can favor more uniform weights).
weight_concentration_prior : ﬂoat | None, optional.
The dirichlet concentration of each component on the weight distribution (Dirichlet).
The higher concentration puts more mass in the center and will lead to more components
being active, while a lower concentration parameter will lead to more mass at the edge
of the mixture weights simplex. The value of the parameter must be greater than 0. If it
is None, it’s set to 1.
/ n_components.
mean_precision_prior : ﬂoat | None, optional.
The precision prior on the mean distribution (Gaussian). Controls the extend to where
means can be placed. Smaller values concentrate the means of each clusters around
1692
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
mean_prior. The value of the parameter must be greater than 0. If it is None, it’s set to
1.
mean_prior : array-like, shape (n_features,), optional
The prior on the mean distribution (Gaussian). If it is None, it’s set to the mean of X.
degrees_of_freedom_prior : ﬂoat | None, optional.
The prior of the number of degrees of freedom on the covariance distributions (Wishart).
If it is None, it’s set to n_features.
covariance_prior : ﬂoat or array-like, optional
The prior on the covariance distribution (Wishart). If it is None, the emiprical co-
variance prior is initialized using the covariance of X. The shape depends on covari-
ance_type:
(n_features, n_features) if 'full',
(n_features, n_features) if 'tied',
(n_features)
if 'diag',
float
if 'spherical'
random_state: RandomState or an int seed, defaults to None. :
A random number generator instance.
warm_start : bool, default to False.
If ‘warm_start’ is True, the solution of the last ﬁtting is used as initialization for the next
call of ﬁt(). This can speed up convergence when ﬁt is called several time on similar
problems.
verbose : int, default to 0.
Enable verbose output. If 1 then it prints the current initialization and each iteration
step. If greater than 1 then it prints also the log probability and the time needed for each
step.
verbose_interval : int, default to 10.
Number of iteration done before the next print.
Attributesweights_ : array-like, shape (n_components,)
The weights of each mixture components.
means_ : array-like, shape (n_components, n_features)
The mean of each mixture component.
covariances_ : array-like
The covariance of each mixture component. The shape depends on covariance_type:
(n_components,)
if 'spherical',
(n_features, n_features)
if 'tied',
(n_components, n_features)
if 'diag',
(n_components, n_features, n_features) if 'full'
precisions_ : array-like
The precision matrices for each component in the mixture. A precision matrix is the
inverse of a covariance matrix. A covariance matrix is symmetric positive deﬁnite so
the mixture of Gaussian can be equivalently parameterized by the precision matrices.
29.21. sklearn.mixture: Gaussian Mixture Models
1693
scikit-learn user guide, Release 0.18.2
Storing the precision matrices instead of the covariance matrices makes it more efﬁ-
cient to compute the log-likelihood of new samples at test time. The shape depends on
covariance_type:
(n_components,)
if 'spherical',
(n_features, n_features)
if 'tied',
(n_components, n_features)
if 'diag',
(n_components, n_features, n_features) if 'full'
precisions_cholesky_ : array-like
The cholesky decomposition of the precision matrices of each mixture component. A
precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric
positive deﬁnite so the mixture of Gaussian can be equivalently parameterized by the
precision matrices. Storing the precision matrices instead of the covariance matrices
makes it more efﬁcient to compute the log-likelihood of new samples at test time. The
shape depends on covariance_type:
(n_components,)
if 'spherical',
(n_features, n_features)
if 'tied',
(n_components, n_features)
if 'diag',
(n_components, n_features, n_features) if 'full'
converged_ : bool
True when convergence was reached in ﬁt(), False otherwise.
n_iter_ : int
Number of step used by the best ﬁt of inference to reach the convergence.
lower_bound_ : ﬂoat
Lower bound value on the likelihood (of the training data with respect to the model) of
the best ﬁt of inference.
weight_concentration_prior_ : tuple or ﬂoat
The dirichlet concentration of each component on the weight distribution (Dirichlet).
The type depends on weight_concentration_prior_type:
(float, float) if 'dirichlet_process' (Beta parameters),
float
if 'dirichlet_distribution' (Dirichlet
˓→parameters).
The higher concentration puts more mass in the center and will lead to more components
being active, while a lower concentration parameter will lead to more mass at the edge
of the simplex.
weight_concentration_ : array-like, shape (n_components,)
The dirichlet concentration of each component on the weight distribution (Dirichlet).
mean_precision_prior : ﬂoat
The precision prior on the mean distribution (Gaussian). Controls the extend to where
means can be placed. Smaller values concentrate the means of each clusters around
mean_prior.
mean_precision_ : array-like, shape (n_components,)
The precision of each components on the mean distribution (Gaussian).
1694
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
means_prior_ : array-like, shape (n_features,)
The prior on the mean distribution (Gaussian).
degrees_of_freedom_prior_ : ﬂoat
The prior of the number of degrees of freedom on the covariance distributions (Wishart).
degrees_of_freedom_ : array-like, shape (n_components,)
The number of degrees of freedom of each components in the model.
covariance_prior_ : ﬂoat or array-like
The prior on the covariance distribution (Wishart).
The shape depends on covari-
ance_type:
(n_features, n_features) if 'full',
(n_features, n_features) if 'tied',
(n_features)
if 'diag',
float
if 'spherical'
See also:
GaussianMixtureFinite Gaussian mixture ﬁt with EM.
References
[R67], [R68], [R69]
Methods
fit(X[, y])
Estimate model parameters with the EM algorithm.
get_params([deep])
Get parameters for this estimator.
predict(X[, y])
Predict the labels for the data samples in X using trained
model.
predict_proba(X)
Predict posterior probability of data per each compo-
nent.
sample([n_samples])
Generate random samples from the ﬁtted Gaussian dis-
tribution.
score(X[, y])
Compute the per-sample average log-likelihood of the
given data X.
score_samples(X)
Compute the weighted log probabilities for each sam-
ple.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_components=1, covariance_type=’full’, tol=0.001, reg_covar=1e-06, max_iter=100,
n_init=1,
init_params=’kmeans’,
weight_concentration_prior_type=’dirichlet_process’,
weight_concentration_prior=None,
mean_precision_prior=None,
mean_prior=None,
degrees_of_freedom_prior=None,
covariance_prior=None,
random_state=None,
warm_start=False, verbose=0, verbose_interval=10)
fit(X, y=None)
Estimate model parameters with the EM algorithm.
The method ﬁt the model n_init times and set the parameters with which the model has the largest likeli-
29.21. sklearn.mixture: Gaussian Mixture Models
1695
scikit-learn user guide, Release 0.18.2
hood or lower bound. Within each trial, the method iterates between E-step and M-step for max_iter times
until the change of likelihood or lower bound is less than tol, otherwise, a ConvergenceWarning is raised.
ParametersX : array-like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnsself :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X, y=None)
Predict the labels for the data samples in X using trained model.
ParametersX : array-like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslabels : array, shape (n_samples,)
Component labels.
predict_proba(X)
Predict posterior probability of data per each component.
ParametersX : array-like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnsresp : array, shape (n_samples, n_components)
Returns the probability of the sample for each Gaussian (state) in the model.
sample(n_samples=1)
Generate random samples from the ﬁtted Gaussian distribution.
Parametersn_samples : int, optional
Number of samples to generate. Defaults to 1.
ReturnsX : array, shape (n_samples, n_features)
Randomly generated sample
y : array, shape (nsamples,)
Component labels
score(X, y=None)
Compute the per-sample average log-likelihood of the given data X.
ParametersX : array-like, shape (n_samples, n_dimensions)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslog_likelihood : ﬂoat
Log likelihood of the Gaussian mixture given X.
1696
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
score_samples(X)
Compute the weighted log probabilities for each sample.
ParametersX : array-like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslog_prob : array, shape (n_samples,)
Log probabilities of each data point in X.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.mixture.BayesianGaussianMixture
• Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture
• Gaussian Mixture Model Ellipsoids
• Gaussian Mixture Model Sine Curve
29.22 sklearn.multiclass:
Multiclass and multilabel classiﬁca-
tion
29.22.1 Multiclass and multilabel classiﬁcation strategies
This module implements multiclass learning algorithms:
• one-vs-the-rest / one-vs-all
• one-vs-one
• error correcting output codes
The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their
constructor. For example, it is possible to use these estimators to turn a binary classiﬁer or a regressor into a multiclass
classiﬁer. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime
performance improves.
All classiﬁers in scikit-learn implement multiclass classiﬁcation; you only need to use this module if you want to
experiment with custom multiclass strategies.
The one-vs-the-rest meta-classiﬁer also implements a predict_proba method, so long as such a method is implemented
by the base classiﬁer. This method returns probabilities of class membership in both the single label and multilabel
case. Note that in the multilabel case, probabilities are the marginal probability that a given sample falls in the given
class. As such, in the multilabel case the sum of these probabilities over all possible labels for a given sample will not
sum to unity, as they do in the single label case.
User guide: See the Multiclass and multilabel algorithms section for further details.
29.22. sklearn.multiclass: Multiclass and multilabel classiﬁcation
1697
scikit-learn user guide, Release 0.18.2
multiclass.OneVsRestClassifier(estimator[,
...])
One-vs-the-rest (OvR) multiclass/multilabel strategy
multiclass.OneVsOneClassifier(estimator[,
...])
One-vs-one multiclass strategy
multiclass.OutputCodeClassifier(estimator[,
...])
(Error-Correcting) Output-Code multiclass strategy
29.22.2 sklearn.multiclass.OneVsRestClassiﬁer
class sklearn.multiclass.OneVsRestClassifier(estimator, n_jobs=1)
One-vs-the-rest (OvR) multiclass/multilabel strategy
Also known as one-vs-all, this strategy consists in ﬁtting one classiﬁer per class. For each classiﬁer, the class
is ﬁtted against all the other classes. In addition to its computational efﬁciency (only n_classes classiﬁers are
needed), one advantage of this approach is its interpretability. Since each class is represented by one and one
classiﬁer only, it is possible to gain knowledge about the class by inspecting its corresponding classiﬁer. This is
the most commonly used strategy for multiclass classiﬁcation and is a fair default choice.
This strategy can also be used for multilabel learning, where a classiﬁer is used to predict multiple labels for
instance, by ﬁtting on a 2-d matrix in which cell [i, j] is 1 if sample i has label j and 0 otherwise.
In the multilabel learning literature, OvR is also known as the binary relevance method.
Read more in the User Guide.
Parametersestimator : estimator object
An estimator object implementing ﬁt and one of decision_function or predict_proba.
n_jobs : int, optional, default: 1
The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no
parallel computing code is used at all, which is useful for debugging. For n_jobs below
-1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
Attributesestimators_ : list of n_classes estimators
Estimators used for predictions.
classes_ : array, shape = [n_classes]
Class labels.
label_binarizer_ : LabelBinarizer object
Object used to transform multiclass labels to binary labels and vice-versa.
multilabel_ : boolean
Whether a OneVsRestClassiﬁer is a multilabel classiﬁer.
Methods
decision_function(X)
Returns the distance of each sample from the decision
boundary for each class.
fit(X, y)
Fit underlying estimators.
get_params([deep])
Get parameters for this estimator.
Continued on next page
1698
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.180 – continued from previous page
partial_fit(X, y[, classes])
Partially ﬁt underlying estimators
predict(X)
Predict multi-class targets using underlying estimators.
predict_proba(X)
Probability estimates.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(estimator, n_jobs=1)
decision_function(X)
Returns the distance of each sample from the decision boundary for each class. This can only be used with
estimators which implement the decision_function method.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsT : array-like, shape = [n_samples, n_classes]
fit(X, y)
Fit underlying estimators.
ParametersX : (sparse) array-like, shape = [n_samples, n_features]
Data.
y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]
Multi-class targets. An indicator matrix turns on multilabel classiﬁcation.
Returnsself :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
multilabel_
Whether this is a multilabel classiﬁer
partial_fit(X, y, classes=None)
Partially ﬁt underlying estimators
Should be used when memory is inefﬁcient to train all data. Chunks of data can be passed in several
iteration.
ParametersX : (sparse) array-like, shape = [n_samples, n_features]
Data.
y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]
Multi-class targets. An indicator matrix turns on multilabel classiﬁcation.
classes : array, shape (n_classes, )
Classes across all calls to partial_ﬁt. Can be obtained via np.unique(y_all), where y_all
is the target vector of the entire dataset. This argument is only required in the ﬁrst call
of partial_ﬁt and can be omitted in the subsequent calls.
29.22. sklearn.multiclass: Multiclass and multilabel classiﬁcation
1699
scikit-learn user guide, Release 0.18.2
Returnsself :
predict(X)
Predict multi-class targets using underlying estimators.
ParametersX : (sparse) array-like, shape = [n_samples, n_features]
Data.
Returnsy : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes].
Predicted multi-class targets.
predict_proba(X)
Probability estimates.
The returned estimates for all classes are ordered by label of classes.
Note that in the multilabel case, each sample can have any number of labels. This returns the marginal
probability that the given sample has the label in question. For example, it is entirely consistent that two
labels both have a 90% probability of applying to a given sample.
In the single label multiclass case, the rows of the returned matrix sum to 1.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsT : (sparse) array-like, shape = [n_samples, n_classes]
Returns the probability of the sample for each class in the model, where classes are
ordered as they are in self.classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.multiclass.OneVsRestClassifier
• Multilabel classiﬁcation
1700
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Precision-Recall
• Receiver Operating Characteristic (ROC)
29.22.3 sklearn.multiclass.OneVsOneClassiﬁer
class sklearn.multiclass.OneVsOneClassifier(estimator, n_jobs=1)
One-vs-one multiclass strategy
This strategy consists in ﬁtting one classiﬁer per class pair. At prediction time, the class which received the
most votes is selected. Since it requires to ﬁt n_classes * (n_classes - 1) / 2 classiﬁers, this method is usually
slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous
for algorithms such as kernel algorithms which don’t scale well with n_samples. This is because each individual
learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is
used n_classes times.
Read more in the User Guide.
Parametersestimator : estimator object
An estimator object implementing ﬁt and one of decision_function or predict_proba.
n_jobs : int, optional, default: 1
The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no
parallel computing code is used at all, which is useful for debugging. For n_jobs below
-1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
Attributesestimators_ : list of n_classes * (n_classes - 1) / 2 estimators
Estimators used for predictions.
classes_ : numpy array of shape [n_classes]
Array containing labels.
Methods
decision_function(X)
Decision function for the OneVsOneClassiﬁer.
fit(X, y)
Fit underlying estimators.
get_params([deep])
Get parameters for this estimator.
partial_fit(X, y[, classes])
Partially ﬁt underlying estimators
predict(X)
Estimate the best class label for each sample in X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(estimator, n_jobs=1)
decision_function(X)
Decision function for the OneVsOneClassiﬁer.
The decision values for the samples are computed by adding the normalized sum of pair-wise classiﬁcation
conﬁdence levels to the votes in order to disambiguate between the decision values when the votes for all
the classes are equal leading to a tie.
ParametersX : array-like, shape = [n_samples, n_features]
29.22. sklearn.multiclass: Multiclass and multilabel classiﬁcation
1701
scikit-learn user guide, Release 0.18.2
ReturnsY : array-like, shape = [n_samples, n_classes]
fit(X, y)
Fit underlying estimators.
ParametersX : (sparse) array-like, shape = [n_samples, n_features]
Data.
y : array-like, shape = [n_samples]
Multi-class targets.
Returnsself :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y, classes=None)
Partially ﬁt underlying estimators
Should be used when memory is inefﬁcient to train all data. Chunks of data can be passed in several
iteration, where the ﬁrst call should have an array of all target variables.
ParametersX : (sparse) array-like, shape = [n_samples, n_features]
Data.
y : array-like, shape = [n_samples]
Multi-class targets.
classes : array, shape (n_classes, )
Classes across all calls to partial_ﬁt. Can be obtained via np.unique(y_all), where y_all
is the target vector of the entire dataset. This argument is only required in the ﬁrst call
of partial_ﬁt and can be omitted in the subsequent calls.
Returnsself :
predict(X)
Estimate the best class label for each sample in X.
This is implemented as argmax(decision_function(X),axis=1) which will return the label of
the class with most votes by estimators predicting the outcome of a decision for each possible class pair.
ParametersX : (sparse) array-like, shape = [n_samples, n_features]
Data.
Returnsy : numpy array of shape [n_samples]
Predicted multi-class targets.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
1702
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.22.4 sklearn.multiclass.OutputCodeClassiﬁer
class sklearn.multiclass.OutputCodeClassifier(estimator,
code_size=1.5,
ran-
dom_state=None, n_jobs=1)
(Error-Correcting) Output-Code multiclass strategy
Output-code based strategies consist in representing each class with a binary code (an array of 0s and 1s). At
ﬁtting time, one binary classiﬁer per bit in the code book is ﬁtted. At prediction time, the classiﬁers are used to
project new points in the class space and the class closest to the points is chosen. The main advantage of these
strategies is that the number of classiﬁers used can be controlled by the user, either for compressing the model
(0 < code_size < 1) or for making the model more robust to errors (code_size > 1). See the documentation for
more details.
Read more in the User Guide.
Parametersestimator : estimator object
An estimator object implementing ﬁt and one of decision_function or predict_proba.
code_size : ﬂoat
Percentage of the number of classes to be used to create the code book. A number
between 0 and 1 will require fewer classiﬁers than one-vs-the-rest. A number greater
than 1 will require more classiﬁers than one-vs-the-rest.
random_state : numpy.RandomState, optional
The generator used to initialize the codebook. Defaults to numpy.random.
n_jobs : int, optional, default: 1
The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no
parallel computing code is used at all, which is useful for debugging. For n_jobs below
-1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
Attributesestimators_ : list of int(n_classes * code_size) estimators
Estimators used for predictions.
classes_ : numpy array of shape [n_classes]
29.22. sklearn.multiclass: Multiclass and multilabel classiﬁcation
1703
scikit-learn user guide, Release 0.18.2
Array containing labels.
code_book_ : numpy array of shape [n_classes, code_size]
Binary array containing the code of each class.
References
[R234], [R235], [R236]
Methods
fit(X, y)
Fit underlying estimators.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict multi-class targets using underlying estimators.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(estimator, code_size=1.5, random_state=None, n_jobs=1)
fit(X, y)
Fit underlying estimators.
ParametersX : (sparse) array-like, shape = [n_samples, n_features]
Data.
y : numpy array of shape [n_samples]
Multi-class targets.
Returnsself :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict multi-class targets using underlying estimators.
ParametersX : (sparse) array-like, shape = [n_samples, n_features]
Data.
Returnsy : numpy array of shape [n_samples]
Predicted multi-class targets.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
1704
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.23 sklearn.multioutput: Multioutput regression and classiﬁ-
cation
This module implements multioutput regression and classiﬁcation.
The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their
constructor. The meta-estimator extends single output estimators to multioutput estimators.
User guide: See the Multiclass and multilabel algorithms section for further details.
multioutput.MultiOutputRegressor(estimator)
Multi target regression
multioutput.MultiOutputClassifier(estimator) Multi target classiﬁcation
29.23.1 sklearn.multioutput.MultiOutputRegressor
class sklearn.multioutput.MultiOutputRegressor(estimator, n_jobs=1)
Multi target regression
This strategy consists of ﬁtting one regressor per target. This is a simple strategy for extending regressors that
do not natively support multi-target regression.
Parametersestimator : estimator object
An estimator object implementing ﬁt and predict.
n_jobs : int, optional, default=1
The number of jobs to run in parallel for ﬁt. If -1, then the number of jobs is set to the
number of cores. When individual estimators are fast to train or predict using n_jobs>1
can result in slower performance due to the overhead of spawning processes.
29.23. sklearn.multioutput: Multioutput regression and classiﬁcation
1705
scikit-learn user guide, Release 0.18.2
Methods
fit(X, y[, sample_weight])
Fit the model to data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict multi-output variable using a model trained for
each target variable.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(estimator, n_jobs=1)
fit(X, y, sample_weight=None)
Fit the model to data. Fit a separate model for each output variable.
ParametersX : (sparse) array-like, shape (n_samples, n_features)
Data.
y : (sparse) array-like, shape (n_samples, n_outputs)
Multi-output targets. An indicator matrix turns on multilabel estimation.
sample_weight : array-like, shape = (n_samples) or None
Sample weights. If None, then samples are equally weighted. Only supported if the
underlying regressor supports sample weights.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict multi-output variable using a modeltrained for each target variable.
ParametersX : (sparse) array-like, shape (n_samples, n_features)
Data.
Returnsy : (sparse) array-like, shape (n_samples, n_outputs)
Multi-output targets predicted across multiple predictors. Note: Separate models are
generated for each predictor.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
1706
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape (n_samples, n_features)
Test samples.
y : array-like, shape (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
Notes
R^2 is calculated by weighting all the targets equally using multioutput=’uniform_average’.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.multioutput.MultiOutputRegressor
• Comparing random forests and the multi-output meta estimator
29.23.2 sklearn.multioutput.MultiOutputClassiﬁer
class sklearn.multioutput.MultiOutputClassifier(estimator, n_jobs=1)
Multi target classiﬁcation
This strategy consists of ﬁtting one classiﬁer per target. This is a simple strategy for extending classiﬁers that
do not natively support multi-target classiﬁcation
Parametersestimator : estimator object
An estimator object implementing ﬁt, score and predict_proba.
n_jobs : int, optional, default=1
The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given,
no parallel computing code is used at all, which is useful for debugging. For n_jobs
below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are
used. The number of jobs to use for the computation. It does each target variable in y
in parallel.
Attributesestimators_ : list of n_output estimators
Estimators used for predictions.
29.23. sklearn.multioutput: Multioutput regression and classiﬁcation
1707
scikit-learn user guide, Release 0.18.2
Methods
fit(X, y[, sample_weight])
Fit the model to data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict multi-output variable using a model trained for
each target variable.
predict_proba(X)
Probability estimates.
score(X, y)
“Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(estimator, n_jobs=1)
fit(X, y, sample_weight=None)
Fit the model to data. Fit a separate model for each output variable.
ParametersX : (sparse) array-like, shape (n_samples, n_features)
Data.
y : (sparse) array-like, shape (n_samples, n_outputs)
Multi-output targets. An indicator matrix turns on multilabel estimation.
sample_weight : array-like, shape = (n_samples) or None
Sample weights. If None, then samples are equally weighted. Only supported if the
underlying regressor supports sample weights.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict multi-output variable using a modeltrained for each target variable.
ParametersX : (sparse) array-like, shape (n_samples, n_features)
Data.
Returnsy : (sparse) array-like, shape (n_samples, n_outputs)
Multi-output targets predicted across multiple predictors. Note: Separate models are
generated for each predictor.
predict_proba(X)
Probability estimates. Returns prediction probabilites for each class of each output.
ParametersX : array-like, shape (n_samples, n_features)
1708
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Data
ReturnsT : (sparse) array-like, shape = (n_samples, n_classes, n_outputs)
The class probabilities of the samples for each of the outputs
score(X, y)
“Returns the mean accuracy on the given test data and labels.
ParametersX : array-like, shape [n_samples, n_features]
Test samples
y : array-like, shape [n_samples, n_outputs]
True values for X
Returnsscores : ﬂoat
accuracy_score of self.predict(X) versus y
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.24 sklearn.naive_bayes: Naive Bayes
The sklearn.naive_bayes module implements Naive Bayes algorithms. These are supervised learning methods
based on applying Bayes’ theorem with strong (naive) feature independence assumptions.
User guide: See the Naive Bayes section for further details.
naive_bayes.GaussianNB([priors])
Gaussian Naive Bayes (GaussianNB)
naive_bayes.MultinomialNB([alpha, ...])
Naive Bayes classiﬁer for multinomial models
naive_bayes.BernoulliNB([alpha, binarize, ...])
Naive Bayes classiﬁer for multivariate Bernoulli models.
29.24.1 sklearn.naive_bayes.GaussianNB
class sklearn.naive_bayes.GaussianNB(priors=None)
Gaussian Naive Bayes (GaussianNB)
Can perform online updates to model parameters via partial_ﬁt method. For details on algorithm used to update
feature means and variance online, see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:
http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf
Read more in the User Guide.
Parameterspriors : array-like, shape (n_classes,)
Prior probabilities of the classes. If speciﬁed the priors are not adjusted according to the
data.
Attributesclass_prior_ : array, shape (n_classes,)
probability of each class.
29.24. sklearn.naive_bayes: Naive Bayes
1709
scikit-learn user guide, Release 0.18.2
class_count_ : array, shape (n_classes,)
number of training samples observed in each class.
theta_ : array, shape (n_classes, n_features)
mean of each feature per class
sigma_ : array, shape (n_classes, n_features)
variance of each feature per class
Examples
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> Y = np.array([1, 1, 1, 2, 2, 2])
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB()
>>> clf.fit(X, Y)
GaussianNB(priors=None)
>>> print(clf.predict([[-0.8, -1]]))
[1]
>>> clf_pf = GaussianNB()
>>> clf_pf.partial_fit(X, Y, np.unique(Y))
GaussianNB(priors=None)
>>> print(clf_pf.predict([[-0.8, -1]]))
[1]
Methods
fit(X, y[, sample_weight])
Fit Gaussian Naive Bayes according to X, y
get_params([deep])
Get parameters for this estimator.
partial_fit(X, y[, classes, sample_weight])
Incremental ﬁt on a batch of samples.
predict(X)
Perform classiﬁcation on an array of test vectors X.
predict_log_proba(X)
Return log-probability estimates for the test vector X.
predict_proba(X)
Return probability estimates for the test vector X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(priors=None)
fit(X, y, sample_weight=None)
Fit Gaussian Naive Bayes according to X, y
ParametersX : array-like, shape (n_samples, n_features)
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape (n_samples,)
Target values.
sample_weight : array-like, shape (n_samples,), optional (default=None)
1710
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Weights applied to individual samples (1. for unweighted).
New in version 0.17: Gaussian Naive Bayes supports ﬁtting with sample_weight.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y, classes=None, sample_weight=None)
Incremental ﬁt on a batch of samples.
This method is expected to be called several times consecutively on different chunks of a dataset so as to
implement out-of-core or online learning.
This is especially useful when the whole dataset is too big to ﬁt in memory at once.
This method has some performance and numerical stability overhead, hence it is better to call partial_ﬁt on
chunks of data that are as large as possible (as long as ﬁtting in the memory budget) to hide the overhead.
ParametersX : array-like, shape (n_samples, n_features)
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape (n_samples,)
Target values.
classes : array-like, shape (n_classes,), optional (default=None)
List of all the classes that can possibly appear in the y vector.
Must be provided at the ﬁrst call to partial_ﬁt, can be omitted in subsequent calls.
sample_weight : array-like, shape (n_samples,), optional (default=None)
Weights applied to individual samples (1. for unweighted).
New in version 0.17.
Returnsself : object
Returns self.
predict(X)
Perform classiﬁcation on an array of test vectors X.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = [n_samples]
Predicted target values for X
predict_log_proba(X)
Return log-probability estimates for the test vector X.
ParametersX : array-like, shape = [n_samples, n_features]
29.24. sklearn.naive_bayes: Naive Bayes
1711
scikit-learn user guide, Release 0.18.2
ReturnsC : array-like, shape = [n_samples, n_classes]
Returns the log-probability of the samples for each class in the model. The columns
correspond to the classes in sorted order, as they appear in the attribute classes_.
predict_proba(X)
Return probability estimates for the test vector X.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array-like, shape = [n_samples, n_classes]
Returns the probability of the samples for each class in the model. The columns corre-
spond to the classes in sorted order, as they appear in the attribute classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.naive_bayes.GaussianNB
• Probability calibration of classiﬁers
• Probability Calibration curves
• Comparison of Calibration of Classiﬁers
• Classiﬁer comparison
• Plot class probabilities calculated by the VotingClassiﬁer
• Plotting Learning Curves
1712
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.24.2 sklearn.naive_bayes.MultinomialNB
class sklearn.naive_bayes.MultinomialNB(alpha=1.0, ﬁt_prior=True, class_prior=None)
Naive Bayes classiﬁer for multinomial models
The multinomial Naive Bayes classiﬁer is suitable for classiﬁcation with discrete features (e.g., word counts for
text classiﬁcation). The multinomial distribution normally requires integer feature counts. However, in practice,
fractional counts such as tf-idf may also work.
Read more in the User Guide.
Parametersalpha : ﬂoat, optional (default=1.0)
Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).
ﬁt_prior : boolean, optional (default=True)
Whether to learn class prior probabilities or not. If false, a uniform prior will be used.
class_prior : array-like, size (n_classes,), optional (default=None)
Prior probabilities of the classes. If speciﬁed the priors are not adjusted according to the
data.
Attributesclass_log_prior_ : array, shape (n_classes, )
Smoothed empirical log probability for each class.
intercept_ : property
Mirrors class_log_prior_ for interpreting MultinomialNB as a linear model.
feature_log_prob_ : array, shape (n_classes, n_features)
Empirical log probability of features given a class, P(x_i|y).
coef_ : property
Mirrors feature_log_prob_ for interpreting MultinomialNB as a linear model.
class_count_ : array, shape (n_classes,)
Number of samples encountered for each class during ﬁtting. This value is weighted by
the sample weight when provided.
feature_count_ : array, shape (n_classes, n_features)
Number of samples encountered for each (class, feature) during ﬁtting. This value is
weighted by the sample weight when provided.
Notes
For the rationale behind the names coef_ and intercept_, i.e. naive Bayes as a linear classiﬁer, see J. Rennie et
al. (2003), Tackling the poor assumptions of naive Bayes text classiﬁers, ICML.
References
C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge Uni-
versity Press, pp. 234-265. http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classiﬁcation-1.
html
29.24. sklearn.naive_bayes: Naive Bayes
1713
scikit-learn user guide, Release 0.18.2
Examples
>>> import numpy as np
>>> X = np.random.randint(5, size=(6, 100))
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB()
>>> clf.fit(X, y)
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2:3]))
[3]
Methods
fit(X, y[, sample_weight])
Fit Naive Bayes classiﬁer according to X, y
get_params([deep])
Get parameters for this estimator.
partial_fit(X, y[, classes, sample_weight])
Incremental ﬁt on a batch of samples.
predict(X)
Perform classiﬁcation on an array of test vectors X.
predict_log_proba(X)
Return log-probability estimates for the test vector X.
predict_proba(X)
Return probability estimates for the test vector X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1.0, ﬁt_prior=True, class_prior=None)
fit(X, y, sample_weight=None)
Fit Naive Bayes classiﬁer according to X, y
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples]
Target values.
sample_weight : array-like, shape = [n_samples], optional (default=None)
Weights applied to individual samples (1. for unweighted).
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
1714
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
partial_fit(X, y, classes=None, sample_weight=None)
Incremental ﬁt on a batch of samples.
This method is expected to be called several times consecutively on different chunks of a dataset so as to
implement out-of-core or online learning.
This is especially useful when the whole dataset is too big to ﬁt in memory at once.
This method has some performance overhead hence it is better to call partial_ﬁt on chunks of data that are
as large as possible (as long as ﬁtting in the memory budget) to hide the overhead.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples]
Target values.
classes : array-like, shape = [n_classes], optional (default=None)
List of all the classes that can possibly appear in the y vector.
Must be provided at the ﬁrst call to partial_ﬁt, can be omitted in subsequent calls.
sample_weight : array-like, shape = [n_samples], optional (default=None)
Weights applied to individual samples (1. for unweighted).
Returnsself : object
Returns self.
predict(X)
Perform classiﬁcation on an array of test vectors X.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = [n_samples]
Predicted target values for X
predict_log_proba(X)
Return log-probability estimates for the test vector X.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array-like, shape = [n_samples, n_classes]
Returns the log-probability of the samples for each class in the model. The columns
correspond to the classes in sorted order, as they appear in the attribute classes_.
predict_proba(X)
Return probability estimates for the test vector X.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array-like, shape = [n_samples, n_classes]
Returns the probability of the samples for each class in the model. The columns corre-
spond to the classes in sorted order, as they appear in the attribute classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
29.24. sklearn.naive_bayes: Naive Bayes
1715
scikit-learn user guide, Release 0.18.2
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.naive_bayes.MultinomialNB
• Out-of-core classiﬁcation of text documents
• Classiﬁcation of text documents using sparse features
• Classiﬁcation of text documents: using a MLComp dataset
29.24.3 sklearn.naive_bayes.BernoulliNB
class sklearn.naive_bayes.BernoulliNB(alpha=1.0,
binarize=0.0,
ﬁt_prior=True,
class_prior=None)
Naive Bayes classiﬁer for multivariate Bernoulli models.
Like MultinomialNB, this classiﬁer is suitable for discrete data. The difference is that while MultinomialNB
works with occurrence counts, BernoulliNB is designed for binary/boolean features.
Read more in the User Guide.
Parametersalpha : ﬂoat, optional (default=1.0)
Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).
binarize : ﬂoat or None, optional (default=0.0)
Threshold for binarizing (mapping to booleans) of sample features. If None, input is
presumed to already consist of binary vectors.
ﬁt_prior : boolean, optional (default=True)
Whether to learn class prior probabilities or not. If false, a uniform prior will be used.
class_prior : array-like, size=[n_classes,], optional (default=None)
Prior probabilities of the classes. If speciﬁed the priors are not adjusted according to the
data.
Attributesclass_log_prior_ : array, shape = [n_classes]
Log probability of each class (smoothed).
1716
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
feature_log_prob_ : array, shape = [n_classes, n_features]
Empirical log probability of features given a class, P(x_i|y).
class_count_ : array, shape = [n_classes]
Number of samples encountered for each class during ﬁtting. This value is weighted by
the sample weight when provided.
feature_count_ : array, shape = [n_classes, n_features]
Number of samples encountered for each (class, feature) during ﬁtting. This value is
weighted by the sample weight when provided.
References
C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge Univer-
sity Press, pp. 234-265. http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html
A. McCallum and K. Nigam (1998). A comparison of event models for naive Bayes text classiﬁcation. Proc.
AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.
V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam ﬁltering with naive Bayes – Which naive Bayes?
3rd Conf. on Email and Anti-Spam (CEAS).
Examples
>>> import numpy as np
>>> X = np.random.randint(2, size=(6, 100))
>>> Y = np.array([1, 2, 3, 4, 4, 5])
>>> from sklearn.naive_bayes import BernoulliNB
>>> clf = BernoulliNB()
>>> clf.fit(X, Y)
BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2:3]))
[3]
Methods
fit(X, y[, sample_weight])
Fit Naive Bayes classiﬁer according to X, y
get_params([deep])
Get parameters for this estimator.
partial_fit(X, y[, classes, sample_weight])
Incremental ﬁt on a batch of samples.
predict(X)
Perform classiﬁcation on an array of test vectors X.
predict_log_proba(X)
Return log-probability estimates for the test vector X.
predict_proba(X)
Return probability estimates for the test vector X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(alpha=1.0, binarize=0.0, ﬁt_prior=True, class_prior=None)
fit(X, y, sample_weight=None)
Fit Naive Bayes classiﬁer according to X, y
29.24. sklearn.naive_bayes: Naive Bayes
1717
scikit-learn user guide, Release 0.18.2
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples]
Target values.
sample_weight : array-like, shape = [n_samples], optional (default=None)
Weights applied to individual samples (1. for unweighted).
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit(X, y, classes=None, sample_weight=None)
Incremental ﬁt on a batch of samples.
This method is expected to be called several times consecutively on different chunks of a dataset so as to
implement out-of-core or online learning.
This is especially useful when the whole dataset is too big to ﬁt in memory at once.
This method has some performance overhead hence it is better to call partial_ﬁt on chunks of data that are
as large as possible (as long as ﬁtting in the memory budget) to hide the overhead.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples]
Target values.
classes : array-like, shape = [n_classes], optional (default=None)
List of all the classes that can possibly appear in the y vector.
Must be provided at the ﬁrst call to partial_ﬁt, can be omitted in subsequent calls.
sample_weight : array-like, shape = [n_samples], optional (default=None)
Weights applied to individual samples (1. for unweighted).
Returnsself : object
Returns self.
predict(X)
Perform classiﬁcation on an array of test vectors X.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = [n_samples]
1718
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Predicted target values for X
predict_log_proba(X)
Return log-probability estimates for the test vector X.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array-like, shape = [n_samples, n_classes]
Returns the log-probability of the samples for each class in the model. The columns
correspond to the classes in sorted order, as they appear in the attribute classes_.
predict_proba(X)
Return probability estimates for the test vector X.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array-like, shape = [n_samples, n_classes]
Returns the probability of the samples for each class in the model. The columns corre-
spond to the classes in sorted order, as they appear in the attribute classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.naive_bayes.BernoulliNB
• Hashing feature transformation using Totally Random Trees
• Classiﬁcation of text documents using sparse features
29.25 sklearn.neighbors: Nearest Neighbors
The sklearn.neighbors module implements the k-nearest neighbors algorithm.
29.25. sklearn.neighbors: Nearest Neighbors
1719
scikit-learn user guide, Release 0.18.2
User guide: See the Nearest Neighbors section for further details.
neighbors.NearestNeighbors([n_neighbors, ...])
Unsupervised learner for implementing neighbor searches.
neighbors.KNeighborsClassifier([...])
Classiﬁer implementing the k-nearest neighbors vote.
neighbors.RadiusNeighborsClassifier([...])
Classiﬁer implementing a vote among neighbors within a
given radius
neighbors.KNeighborsRegressor([n_neighbors,
...])
Regression based on k-nearest neighbors.
neighbors.RadiusNeighborsRegressor([radius,
...])
Regression based on neighbors within a ﬁxed radius.
neighbors.NearestCentroid([metric, ...])
Nearest centroid classiﬁer.
neighbors.BallTree
BallTree for fast generalized N-point problems
neighbors.KDTree
KDTree for fast generalized N-point problems
neighbors.LSHForest([n_estimators, radius, ...])
Performs approximate nearest neighbor search using LSH
forest.
neighbors.DistanceMetric
DistanceMetric class
neighbors.KernelDensity([bandwidth, ...])
Kernel Density Estimation
29.25.1 sklearn.neighbors.NearestNeighbors
class sklearn.neighbors.NearestNeighbors(n_neighbors=5,
radius=1.0,
algorithm=’auto’,
leaf_size=30,
metric=’minkowski’,
p=2,
met-
ric_params=None, n_jobs=1, **kwargs)
Unsupervised learner for implementing neighbor searches.
Read more in the User Guide.
Parametersn_neighbors : int, optional (default = 5)
Number of neighbors to use by default for k_neighbors queries.
radius : ﬂoat, optional (default = 1.0)
Range of parameter space to use by default for radius_neighbors queries.
algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
Algorithm used to compute the nearest neighbors:
•‘ball_tree’ will use BallTree
•‘kd_tree’ will use KDtree
•‘brute’ will use a brute-force search.
•‘auto’ will attempt to decide the most appropriate algorithm based on the values
passed to fit method.
Note: ﬁtting on sparse input will override the setting of this parameter, using brute force.
leaf_size : int, optional (default = 30)
Leaf size passed to BallTree or KDTree. This can affect the speed of the construction
and query, as well as the memory required to store the tree. The optimal value depends
on the nature of the problem.
p: integer, optional (default = 2) :
Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances.
When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
1720
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
metric : string or callable, default ‘minkowski’
metric to use for distance computation.
Any metric from scikit-learn or
scipy.spatial.distance can be used.
If metric is a callable function, it is called on each pair of instances (rows) and the
resulting value recorded. The callable should take two arrays as input and return one
value indicating the distance between them. This works for Scipy’s metrics, but is less
efﬁcient than passing the metric name as a string.
Distance matrices are not supported.
Valid values for metric are:
•from scikit-learn: [’cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]
•from scipy.spatial.distance:
[’braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’,
‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’,
‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeu-
clidean’, ‘yule’]
See the documentation for scipy.spatial.distance for details on these metrics.
metric_params : dict, optional (default = None)
Additional keyword arguments for the metric function.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run for neighbors search.
If -1, then the num-
ber of jobs is set to the number of CPU cores.
Affects only k_neighbors and
kneighbors_graph methods.
See also:
KNeighborsClassifier,
RadiusNeighborsClassifier,
KNeighborsRegressor,
RadiusNeighborsRegressor, BallTree
Notes
See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and
leaf_size.
https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm
Examples
>>> import numpy as np
>>> from sklearn.neighbors import NearestNeighbors
>>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]
>>> neigh = NearestNeighbors(2, 0.4)
>>> neigh.fit(samples)
NearestNeighbors(...)
>>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)
...
array([[2, 0]]...)
29.25. sklearn.neighbors: Nearest Neighbors
1721
scikit-learn user guide, Release 0.18.2
>>> nbrs = neigh.radius_neighbors([[0, 0, 1.3]], 0.4, return_distance=False)
>>> np.asarray(nbrs[0][0])
array(2)
Methods
fit(X[, y])
Fit the model using X as training data
get_params([deep])
Get parameters for this estimator.
kneighbors([X, n_neighbors, return_distance])
Finds the K-neighbors of a point.
kneighbors_graph([X, n_neighbors, mode])
Computes the (weighted) graph of k-Neighbors for
points in X
radius_neighbors([X, radius, return_distance])
Finds the neighbors within a given radius of a point or
points.
radius_neighbors_graph([X, radius, mode])
Computes the (weighted) graph of Neighbors for points
in X
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_neighbors=5, radius=1.0, algorithm=’auto’, leaf_size=30, metric=’minkowski’, p=2,
metric_params=None, n_jobs=1, **kwargs)
fit(X, y=None)
Fit the model using X as training data
ParametersX : {array-like, sparse matrix, BallTree, KDTree}
Training data.
If array or matrix, shape [n_samples, n_features], or [n_samples,
n_samples] if metric=’precomputed’.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
kneighbors(X=None, n_neighbors=None, return_distance=True)
Finds the K-neighbors of a point.
Returns indices of and distances to the neighbors of each point.
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
n_neighbors : int
Number of neighbors to get (default is the value passed to the constructor).
return_distance : boolean, optional. Defaults to True.
If False, distances will not be returned
1722
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsdist : array
Array representing the lengths to points, only present if return_distance=True
ind : array
Indices of the nearest points in the population matrix.
Examples
In the following example, we construct a NeighborsClassiﬁer class from an array representing our data set
and ask who’s the closest point to [1,1,1]
>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=1)
>>> neigh.fit(samples)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> print(neigh.kneighbors([[1., 1., 1.]]))
(array([[ 0.5]]), array([[2]]...))
As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third
element of samples (indexes start at 0). You can also query for multiple points:
>>> X = [[0., 1., 0.], [1., 0., 1.]]
>>> neigh.kneighbors(X, return_distance=False)
array([[1],
[2]]...)
kneighbors_graph(X=None, n_neighbors=None, mode=’connectivity’)
Computes the (weighted) graph of k-Neighbors for points in X
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
n_neighbors : int
Number of neighbors for each sample. (default is value passed to the constructor).
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, in ‘distance’ the edges are Euclidean distance between points.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples_ﬁt]
n_samples_ﬁt is the number of samples in the ﬁtted data A[i, j] is assigned the weight
of edge that connects i to j.
See also:
NearestNeighbors.radius_neighbors_graph
29.25. sklearn.neighbors: Nearest Neighbors
1723
scikit-learn user guide, Release 0.18.2
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=2)
>>> neigh.fit(X)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> A = neigh.kneighbors_graph(X)
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
1.],
[ 1.,
0.,
1.]])
radius_neighbors(X=None, radius=None, return_distance=True)
Finds the neighbors within a given radius of a point or points.
Return the indices and distances of each point from the dataset lying in a ball with size radius around
the points of the query array. Points lying on the boundary are included in the results.
The result points are not necessarily sorted by distance to their query point.
ParametersX : array-like, (n_samples, n_features), optional
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
radius : ﬂoat
Limiting distance of neighbors to return. (default is the value passed to the constructor).
return_distance : boolean, optional. Defaults to True.
If False, distances will not be returned
Returnsdist : array, shape (n_samples,) of arrays
Array representing the distances to each point, only present if return_distance=True.
The distance values are computed according to the metric constructor parameter.
ind : array, shape (n_samples,) of arrays
An array of arrays of indices of the approximate nearest points from the population
matrix that lie within a ball of size radius around the query points.
Notes
Because the number of neighbors of each point is not necessarily equal, the results for multiple query
points cannot be ﬁt in a standard data array. For efﬁciency, radius_neighbors returns arrays of objects,
where each object is a 1D array of indices or distances.
Examples
In the following example, we construct a NeighborsClassiﬁer class from an array representing our data set
and ask who’s the closest point to [1, 1, 1]:
>>> import numpy as np
>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
>>> from sklearn.neighbors import NearestNeighbors
1724
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> neigh = NearestNeighbors(radius=1.6)
>>> neigh.fit(samples)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> rng = neigh.radius_neighbors([[1., 1., 1.]])
>>> print(np.asarray(rng[0][0]))
[ 1.5
0.5]
>>> print(np.asarray(rng[1][0]))
[1 2]
The ﬁrst array returned contains the distances to all points which are closer than 1.6, while the second
array returned contains their indices. In general, multiple points can be queried at the same time.
radius_neighbors_graph(X=None, radius=None, mode=’connectivity’)
Computes the (weighted) graph of Neighbors for points in X
Neighborhoods are restricted the points at a distance lower than radius.
ParametersX : array-like, shape = [n_samples, n_features], optional
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
radius : ﬂoat
Radius of neighborhoods. (default is the value passed to the constructor).
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, in ‘distance’ the edges are Euclidean distance between points.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples]
A[i, j] is assigned the weight of edge that connects i to j.
See also:
kneighbors_graph
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(radius=1.5)
>>> neigh.fit(X)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> A = neigh.radius_neighbors_graph(X)
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
0.],
[ 1.,
0.,
1.]])
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.25. sklearn.neighbors: Nearest Neighbors
1725
scikit-learn user guide, Release 0.18.2
Examples using sklearn.neighbors.NearestNeighbors
• Hyper-parameters of Approximate Nearest Neighbors
• Scalability of Approximate Nearest Neighbors
29.25.2 sklearn.neighbors.KNeighborsClassiﬁer
class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,
weights=’uniform’,
al-
gorithm=’auto’,
leaf_size=30,
p=2,
met-
ric=’minkowski’,
metric_params=None,
n_jobs=1, **kwargs)
Classiﬁer implementing the k-nearest neighbors vote.
Read more in the User Guide.
Parametersn_neighbors : int, optional (default = 5)
Number of neighbors to use by default for k_neighbors queries.
weights : str or callable, optional (default = ‘uniform’)
weight function used in prediction. Possible values:
•‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.
•‘distance’ : weight points by the inverse of their distance. in this case, closer neigh-
bors of a query point will have a greater inﬂuence than neighbors which are further
away.
•[callable] : a user-deﬁned function which accepts an array of distances, and returns
an array of the same shape containing the weights.
algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
Algorithm used to compute the nearest neighbors:
•‘ball_tree’ will use BallTree
•‘kd_tree’ will use KDTree
•‘brute’ will use a brute-force search.
•‘auto’ will attempt to decide the most appropriate algorithm based on the values
passed to fit method.
Note: ﬁtting on sparse input will override the setting of this parameter, using brute force.
leaf_size : int, optional (default = 30)
Leaf size passed to BallTree or KDTree. This can affect the speed of the construction
and query, as well as the memory required to store the tree. The optimal value depends
on the nature of the problem.
metric : string or DistanceMetric object (default = ‘minkowski’)
the distance metric to use for the tree. The default metric is minkowski, and with p=2
is equivalent to the standard Euclidean metric. See the documentation of the Distance-
Metric class for a list of available metrics.
p : integer, optional (default = 2)
1726
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Power parameter for the Minkowski metric. When p = 1, this is equivalent to us-
ing manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p,
minkowski_distance (l_p) is used.
metric_params : dict, optional (default = None)
Additional keyword arguments for the metric function.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run for neighbors search. If -1, then the number of jobs
is set to the number of CPU cores. Doesn’t affect fit method.
See also:
RadiusNeighborsClassifier,
KNeighborsRegressor,
RadiusNeighborsRegressor,
NearestNeighbors
Notes
See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and
leaf_size.
Warning: Regarding the Nearest Neighbors algorithms, if it is found that two neighbors, neighbor k+1 and
k, have identical distances but different labels, the results will depend on the ordering of the training data.
https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm
Examples
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsClassifier
>>> neigh = KNeighborsClassifier(n_neighbors=3)
>>> neigh.fit(X, y)
KNeighborsClassifier(...)
>>> print(neigh.predict([[1.1]]))
[0]
>>> print(neigh.predict_proba([[0.9]]))
[[ 0.66666667
0.33333333]]
Methods
fit(X, y)
Fit the model using X as training data and y as target
values
get_params([deep])
Get parameters for this estimator.
kneighbors([X, n_neighbors, return_distance])
Finds the K-neighbors of a point.
kneighbors_graph([X, n_neighbors, mode])
Computes the (weighted) graph of k-Neighbors for
points in X
predict(X)
Predict the class labels for the provided data
predict_proba(X)
Return probability estimates for the test data X.
Continued on next page
29.25. sklearn.neighbors: Nearest Neighbors
1727
scikit-learn user guide, Release 0.18.2
Table 29.192 – continued from previous page
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_neighbors=5,
weights=’uniform’,
algorithm=’auto’,
leaf_size=30,
p=2,
met-
ric=’minkowski’, metric_params=None, n_jobs=1, **kwargs)
fit(X, y)
Fit the model using X as training data and y as target values
ParametersX : {array-like, sparse matrix, BallTree, KDTree}
Training data.
If array or matrix, shape [n_samples, n_features], or [n_samples,
n_samples] if metric=’precomputed’.
y : {array-like, sparse matrix}
Target values of shape = [n_samples] or [n_samples, n_outputs]
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
kneighbors(X=None, n_neighbors=None, return_distance=True)
Finds the K-neighbors of a point.
Returns indices of and distances to the neighbors of each point.
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
n_neighbors : int
Number of neighbors to get (default is the value passed to the constructor).
return_distance : boolean, optional. Defaults to True.
If False, distances will not be returned
Returnsdist : array
Array representing the lengths to points, only present if return_distance=True
ind : array
Indices of the nearest points in the population matrix.
Examples
In the following example, we construct a NeighborsClassiﬁer class from an array representing our data set
and ask who’s the closest point to [1,1,1]
1728
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=1)
>>> neigh.fit(samples)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> print(neigh.kneighbors([[1., 1., 1.]]))
(array([[ 0.5]]), array([[2]]...))
As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third
element of samples (indexes start at 0). You can also query for multiple points:
>>> X = [[0., 1., 0.], [1., 0., 1.]]
>>> neigh.kneighbors(X, return_distance=False)
array([[1],
[2]]...)
kneighbors_graph(X=None, n_neighbors=None, mode=’connectivity’)
Computes the (weighted) graph of k-Neighbors for points in X
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
n_neighbors : int
Number of neighbors for each sample. (default is value passed to the constructor).
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, in ‘distance’ the edges are Euclidean distance between points.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples_ﬁt]
n_samples_ﬁt is the number of samples in the ﬁtted data A[i, j] is assigned the weight
of edge that connects i to j.
See also:
NearestNeighbors.radius_neighbors_graph
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=2)
>>> neigh.fit(X)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> A = neigh.kneighbors_graph(X)
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
1.],
[ 1.,
0.,
1.]])
predict(X)
Predict the class labels for the provided data
29.25. sklearn.neighbors: Nearest Neighbors
1729
scikit-learn user guide, Release 0.18.2
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
Test samples.
Returnsy : array of shape [n_samples] or [n_samples, n_outputs]
Class labels for each data sample.
predict_proba(X)
Return probability estimates for the test data X.
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
Test samples.
Returnsp : array of shape = [n_samples, n_classes], or a list of n_outputs
of such arrays if n_outputs > 1. The class probabilities of the input samples. Classes are
ordered by lexicographic order.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.neighbors.KNeighborsClassifier
• Classiﬁer comparison
• Plot the decision boundaries of a VotingClassiﬁer
• Digits Classiﬁcation Exercise
• Nearest Neighbors Classiﬁcation
• Robust Scaling on Toy Data
• Classiﬁcation of text documents using sparse features
1730
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.25.3 sklearn.neighbors.RadiusNeighborsClassiﬁer
class sklearn.neighbors.RadiusNeighborsClassifier(radius=1.0, weights=’uniform’, algo-
rithm=’auto’, leaf_size=30, p=2, met-
ric=’minkowski’, outlier_label=None,
metric_params=None, **kwargs)
Classiﬁer implementing a vote among neighbors within a given radius
Read more in the User Guide.
Parametersradius : ﬂoat, optional (default = 1.0)
Range of parameter space to use by default for :meth‘radius_neighbors‘ queries.
weights : str or callable
weight function used in prediction. Possible values:
•‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.
•‘distance’ : weight points by the inverse of their distance. in this case, closer neigh-
bors of a query point will have a greater inﬂuence than neighbors which are further
away.
•[callable] : a user-deﬁned function which accepts an array of distances, and returns
an array of the same shape containing the weights.
Uniform weights are used by default.
algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
Algorithm used to compute the nearest neighbors:
•‘ball_tree’ will use BallTree
•‘kd_tree’ will use KDtree
•‘brute’ will use a brute-force search.
•‘auto’ will attempt to decide the most appropriate algorithm based on the values
passed to fit method.
Note: ﬁtting on sparse input will override the setting of this parameter, using brute force.
leaf_size : int, optional (default = 30)
Leaf size passed to BallTree or KDTree. This can affect the speed of the construction
and query, as well as the memory required to store the tree. The optimal value depends
on the nature of the problem.
metric : string or DistanceMetric object (default=’minkowski’)
the distance metric to use for the tree. The default metric is minkowski, and with p=2
is equivalent to the standard Euclidean metric. See the documentation of the Distance-
Metric class for a list of available metrics.
p : integer, optional (default = 2)
Power parameter for the Minkowski metric. When p = 1, this is equivalent to us-
ing manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p,
minkowski_distance (l_p) is used.
outlier_label : int, optional (default = None)
Label, which is given for outlier samples (samples with no neighbors on given radius).
If set to None, ValueError is raised, when outlier is detected.
29.25. sklearn.neighbors: Nearest Neighbors
1731
scikit-learn user guide, Release 0.18.2
metric_params : dict, optional (default = None)
Additional keyword arguments for the metric function.
See also:
KNeighborsClassifier,
RadiusNeighborsRegressor,
KNeighborsRegressor,
NearestNeighbors
Notes
See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and
leaf_size.
https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm
Examples
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import RadiusNeighborsClassifier
>>> neigh = RadiusNeighborsClassifier(radius=1.0)
>>> neigh.fit(X, y)
RadiusNeighborsClassifier(...)
>>> print(neigh.predict([[1.5]]))
[0]
Methods
fit(X, y)
Fit the model using X as training data and y as target
values
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict the class labels for the provided data
radius_neighbors([X, radius, return_distance])
Finds the neighbors within a given radius of a point or
points.
radius_neighbors_graph([X, radius, mode])
Computes the (weighted) graph of Neighbors for points
in X
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(radius=1.0, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’,
outlier_label=None, metric_params=None, **kwargs)
fit(X, y)
Fit the model using X as training data and y as target values
ParametersX : {array-like, sparse matrix, BallTree, KDTree}
Training data.
If array or matrix, shape [n_samples, n_features], or [n_samples,
n_samples] if metric=’precomputed’.
y : {array-like, sparse matrix}
1732
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Target values of shape = [n_samples] or [n_samples, n_outputs]
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict the class labels for the provided data
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
Test samples.
Returnsy : array of shape [n_samples] or [n_samples, n_outputs]
Class labels for each data sample.
radius_neighbors(X=None, radius=None, return_distance=True)
Finds the neighbors within a given radius of a point or points.
Return the indices and distances of each point from the dataset lying in a ball with size radius around
the points of the query array. Points lying on the boundary are included in the results.
The result points are not necessarily sorted by distance to their query point.
ParametersX : array-like, (n_samples, n_features), optional
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
radius : ﬂoat
Limiting distance of neighbors to return. (default is the value passed to the constructor).
return_distance : boolean, optional. Defaults to True.
If False, distances will not be returned
Returnsdist : array, shape (n_samples,) of arrays
Array representing the distances to each point, only present if return_distance=True.
The distance values are computed according to the metric constructor parameter.
ind : array, shape (n_samples,) of arrays
An array of arrays of indices of the approximate nearest points from the population
matrix that lie within a ball of size radius around the query points.
Notes
Because the number of neighbors of each point is not necessarily equal, the results for multiple query
points cannot be ﬁt in a standard data array. For efﬁciency, radius_neighbors returns arrays of objects,
where each object is a 1D array of indices or distances.
29.25. sklearn.neighbors: Nearest Neighbors
1733
scikit-learn user guide, Release 0.18.2
Examples
In the following example, we construct a NeighborsClassiﬁer class from an array representing our data set
and ask who’s the closest point to [1, 1, 1]:
>>> import numpy as np
>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(radius=1.6)
>>> neigh.fit(samples)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> rng = neigh.radius_neighbors([[1., 1., 1.]])
>>> print(np.asarray(rng[0][0]))
[ 1.5
0.5]
>>> print(np.asarray(rng[1][0]))
[1 2]
The ﬁrst array returned contains the distances to all points which are closer than 1.6, while the second
array returned contains their indices. In general, multiple points can be queried at the same time.
radius_neighbors_graph(X=None, radius=None, mode=’connectivity’)
Computes the (weighted) graph of Neighbors for points in X
Neighborhoods are restricted the points at a distance lower than radius.
ParametersX : array-like, shape = [n_samples, n_features], optional
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
radius : ﬂoat
Radius of neighborhoods. (default is the value passed to the constructor).
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, in ‘distance’ the edges are Euclidean distance between points.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples]
A[i, j] is assigned the weight of edge that connects i to j.
See also:
kneighbors_graph
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(radius=1.5)
>>> neigh.fit(X)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> A = neigh.radius_neighbors_graph(X)
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
0.],
[ 1.,
0.,
1.]])
1734
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.25.4 sklearn.neighbors.KNeighborsRegressor
class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5,
weights=’uniform’,
algo-
rithm=’auto’,
leaf_size=30,
p=2,
met-
ric=’minkowski’,
metric_params=None,
n_jobs=1, **kwargs)
Regression based on k-nearest neighbors.
The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.
Read more in the User Guide.
Parametersn_neighbors : int, optional (default = 5)
Number of neighbors to use by default for k_neighbors queries.
weights : str or callable
weight function used in prediction. Possible values:
•‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.
•‘distance’ : weight points by the inverse of their distance. in this case, closer neigh-
bors of a query point will have a greater inﬂuence than neighbors which are further
away.
•[callable] : a user-deﬁned function which accepts an array of distances, and returns
an array of the same shape containing the weights.
Uniform weights are used by default.
algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
Algorithm used to compute the nearest neighbors:
29.25. sklearn.neighbors: Nearest Neighbors
1735
scikit-learn user guide, Release 0.18.2
•‘ball_tree’ will use BallTree
•‘kd_tree’ will use KDtree
•‘brute’ will use a brute-force search.
•‘auto’ will attempt to decide the most appropriate algorithm based on the values
passed to fit method.
Note: ﬁtting on sparse input will override the setting of this parameter, using brute force.
leaf_size : int, optional (default = 30)
Leaf size passed to BallTree or KDTree. This can affect the speed of the construction
and query, as well as the memory required to store the tree. The optimal value depends
on the nature of the problem.
metric : string or DistanceMetric object (default=’minkowski’)
the distance metric to use for the tree. The default metric is minkowski, and with p=2
is equivalent to the standard Euclidean metric. See the documentation of the Distance-
Metric class for a list of available metrics.
p : integer, optional (default = 2)
Power parameter for the Minkowski metric. When p = 1, this is equivalent to us-
ing manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p,
minkowski_distance (l_p) is used.
metric_params : dict, optional (default = None)
Additional keyword arguments for the metric function.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run for neighbors search. If -1, then the number of jobs
is set to the number of CPU cores. Doesn’t affect fit method.
See also:
NearestNeighbors,
RadiusNeighborsRegressor,
KNeighborsClassifier,
RadiusNeighborsClassifier
Notes
See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and
leaf_size.
Warning:
Regarding the Nearest Neighbors algorithms, if it is found that two neighbors, neighbor k+1
and k, have identical distances but but different labels, the results will depend on the ordering of the training
data.
https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm
Examples
1736
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsRegressor
>>> neigh = KNeighborsRegressor(n_neighbors=2)
>>> neigh.fit(X, y)
KNeighborsRegressor(...)
>>> print(neigh.predict([[1.5]]))
[ 0.5]
Methods
fit(X, y)
Fit the model using X as training data and y as target
values
get_params([deep])
Get parameters for this estimator.
kneighbors([X, n_neighbors, return_distance])
Finds the K-neighbors of a point.
kneighbors_graph([X, n_neighbors, mode])
Computes the (weighted) graph of k-Neighbors for
points in X
predict(X)
Predict the target for the provided data
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_neighbors=5,
weights=’uniform’,
algorithm=’auto’,
leaf_size=30,
p=2,
met-
ric=’minkowski’, metric_params=None, n_jobs=1, **kwargs)
fit(X, y)
Fit the model using X as training data and y as target values
ParametersX : {array-like, sparse matrix, BallTree, KDTree}
Training data.
If array or matrix, shape [n_samples, n_features], or [n_samples,
n_samples] if metric=’precomputed’.
y : {array-like, sparse matrix}
Target values, array of ﬂoat values, shape = [n_samples]or [n_samples, n_outputs]
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
kneighbors(X=None, n_neighbors=None, return_distance=True)
Finds the K-neighbors of a point.
Returns indices of and distances to the neighbors of each point.
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
29.25. sklearn.neighbors: Nearest Neighbors
1737
scikit-learn user guide, Release 0.18.2
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
n_neighbors : int
Number of neighbors to get (default is the value passed to the constructor).
return_distance : boolean, optional. Defaults to True.
If False, distances will not be returned
Returnsdist : array
Array representing the lengths to points, only present if return_distance=True
ind : array
Indices of the nearest points in the population matrix.
Examples
In the following example, we construct a NeighborsClassiﬁer class from an array representing our data set
and ask who’s the closest point to [1,1,1]
>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=1)
>>> neigh.fit(samples)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> print(neigh.kneighbors([[1., 1., 1.]]))
(array([[ 0.5]]), array([[2]]...))
As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third
element of samples (indexes start at 0). You can also query for multiple points:
>>> X = [[0., 1., 0.], [1., 0., 1.]]
>>> neigh.kneighbors(X, return_distance=False)
array([[1],
[2]]...)
kneighbors_graph(X=None, n_neighbors=None, mode=’connectivity’)
Computes the (weighted) graph of k-Neighbors for points in X
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
n_neighbors : int
Number of neighbors for each sample. (default is value passed to the constructor).
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, in ‘distance’ the edges are Euclidean distance between points.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples_ﬁt]
n_samples_ﬁt is the number of samples in the ﬁtted data A[i, j] is assigned the weight
of edge that connects i to j.
1738
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
See also:
NearestNeighbors.radius_neighbors_graph
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=2)
>>> neigh.fit(X)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> A = neigh.kneighbors_graph(X)
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
1.],
[ 1.,
0.,
1.]])
predict(X)
Predict the target for the provided data
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
Test samples.
Returnsy : array of int, shape = [n_samples] or [n_samples, n_outputs]
Target values
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.25. sklearn.neighbors: Nearest Neighbors
1739
scikit-learn user guide, Release 0.18.2
Examples using sklearn.neighbors.KNeighborsRegressor
• Face completion with a multi-output estimators
• Nearest Neighbors regression
29.25.5 sklearn.neighbors.RadiusNeighborsRegressor
class sklearn.neighbors.RadiusNeighborsRegressor(radius=1.0,
weights=’uniform’,
algo-
rithm=’auto’, leaf_size=30, p=2, met-
ric=’minkowski’, metric_params=None,
**kwargs)
Regression based on neighbors within a ﬁxed radius.
The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.
Read more in the User Guide.
Parametersradius : ﬂoat, optional (default = 1.0)
Range of parameter space to use by default for radius_neighbors queries.
weights : str or callable
weight function used in prediction. Possible values:
•‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.
•‘distance’ : weight points by the inverse of their distance. in this case, closer neigh-
bors of a query point will have a greater inﬂuence than neighbors which are further
away.
•[callable] : a user-deﬁned function which accepts an array of distances, and returns
an array of the same shape containing the weights.
Uniform weights are used by default.
algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
Algorithm used to compute the nearest neighbors:
•‘ball_tree’ will use BallTree
•‘kd_tree’ will use KDtree
•‘brute’ will use a brute-force search.
•‘auto’ will attempt to decide the most appropriate algorithm based on the values
passed to fit method.
Note: ﬁtting on sparse input will override the setting of this parameter, using brute force.
leaf_size : int, optional (default = 30)
Leaf size passed to BallTree or KDTree. This can affect the speed of the construction
and query, as well as the memory required to store the tree. The optimal value depends
on the nature of the problem.
metric : string or DistanceMetric object (default=’minkowski’)
the distance metric to use for the tree. The default metric is minkowski, and with p=2
is equivalent to the standard Euclidean metric. See the documentation of the Distance-
Metric class for a list of available metrics.
p : integer, optional (default = 2)
1740
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Power parameter for the Minkowski metric. When p = 1, this is equivalent to us-
ing manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p,
minkowski_distance (l_p) is used.
metric_params : dict, optional (default = None)
Additional keyword arguments for the metric function.
See also:
NearestNeighbors,
KNeighborsRegressor,
KNeighborsClassifier,
RadiusNeighborsClassifier
Notes
See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and
leaf_size.
https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm
Examples
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import RadiusNeighborsRegressor
>>> neigh = RadiusNeighborsRegressor(radius=1.0)
>>> neigh.fit(X, y)
RadiusNeighborsRegressor(...)
>>> print(neigh.predict([[1.5]]))
[ 0.5]
Methods
fit(X, y)
Fit the model using X as training data and y as target
values
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict the target for the provided data
radius_neighbors([X, radius, return_distance])
Finds the neighbors within a given radius of a point or
points.
radius_neighbors_graph([X, radius, mode])
Computes the (weighted) graph of Neighbors for points
in X
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(radius=1.0, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’,
metric_params=None, **kwargs)
fit(X, y)
Fit the model using X as training data and y as target values
ParametersX : {array-like, sparse matrix, BallTree, KDTree}
29.25. sklearn.neighbors: Nearest Neighbors
1741
scikit-learn user guide, Release 0.18.2
Training data.
If array or matrix, shape [n_samples, n_features], or [n_samples,
n_samples] if metric=’precomputed’.
y : {array-like, sparse matrix}
Target values, array of ﬂoat values, shape = [n_samples]or [n_samples, n_outputs]
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict the target for the provided data
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
Test samples.
Returnsy : array of int, shape = [n_samples] or [n_samples, n_outputs]
Target values
radius_neighbors(X=None, radius=None, return_distance=True)
Finds the neighbors within a given radius of a point or points.
Return the indices and distances of each point from the dataset lying in a ball with size radius around
the points of the query array. Points lying on the boundary are included in the results.
The result points are not necessarily sorted by distance to their query point.
ParametersX : array-like, (n_samples, n_features), optional
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
radius : ﬂoat
Limiting distance of neighbors to return. (default is the value passed to the constructor).
return_distance : boolean, optional. Defaults to True.
If False, distances will not be returned
Returnsdist : array, shape (n_samples,) of arrays
Array representing the distances to each point, only present if return_distance=True.
The distance values are computed according to the metric constructor parameter.
ind : array, shape (n_samples,) of arrays
An array of arrays of indices of the approximate nearest points from the population
matrix that lie within a ball of size radius around the query points.
1742
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
Because the number of neighbors of each point is not necessarily equal, the results for multiple query
points cannot be ﬁt in a standard data array. For efﬁciency, radius_neighbors returns arrays of objects,
where each object is a 1D array of indices or distances.
Examples
In the following example, we construct a NeighborsClassiﬁer class from an array representing our data set
and ask who’s the closest point to [1, 1, 1]:
>>> import numpy as np
>>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(radius=1.6)
>>> neigh.fit(samples)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> rng = neigh.radius_neighbors([[1., 1., 1.]])
>>> print(np.asarray(rng[0][0]))
[ 1.5
0.5]
>>> print(np.asarray(rng[1][0]))
[1 2]
The ﬁrst array returned contains the distances to all points which are closer than 1.6, while the second
array returned contains their indices. In general, multiple points can be queried at the same time.
radius_neighbors_graph(X=None, radius=None, mode=’connectivity’)
Computes the (weighted) graph of Neighbors for points in X
Neighborhoods are restricted the points at a distance lower than radius.
ParametersX : array-like, shape = [n_samples, n_features], optional
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
radius : ﬂoat
Radius of neighborhoods. (default is the value passed to the constructor).
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, in ‘distance’ the edges are Euclidean distance between points.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples]
A[i, j] is assigned the weight of edge that connects i to j.
See also:
kneighbors_graph
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(radius=1.5)
29.25. sklearn.neighbors: Nearest Neighbors
1743
scikit-learn user guide, Release 0.18.2
>>> neigh.fit(X)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> A = neigh.radius_neighbors_graph(X)
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
0.],
[ 1.,
0.,
1.]])
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.25.6 sklearn.neighbors.NearestCentroid
class sklearn.neighbors.NearestCentroid(metric=’euclidean’, shrink_threshold=None)
Nearest centroid classiﬁer.
Each class is represented by its centroid, with test samples classiﬁed to the class with the nearest centroid.
Read more in the User Guide.
Parametersmetric : string, or callable
The metric to use when calculating distance between instances in a feature array.
If metric is a string or callable, it must be one of the options allowed by met-
rics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples
corresponding to each class is the point from which the sum of the distances (according
to the metric) of all samples that belong to that particular class are minimized. If the
“manhattan” metric is provided, this centroid is the median and for all other metrics, the
centroid is now set to be the mean.
shrink_threshold : ﬂoat, optional (default = None)
1744
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Threshold for shrinking centroids to remove features.
Attributescentroids_ : array-like, shape = [n_classes, n_features]
Centroid of each class
See also:
sklearn.neighbors.KNeighborsClassifiernearest neighbors classiﬁer
Notes
When used for text classiﬁcation with tf-idf vectors, this classiﬁer is also known as the Rocchio classiﬁer.
References
Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of multiple cancer types by shrunken
centroids of gene expression. Proceedings of the National Academy of Sciences of the United States of America,
99(10), 6567-6572. The National Academy of Sciences.
Examples
>>> from sklearn.neighbors.nearest_centroid import NearestCentroid
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> clf = NearestCentroid()
>>> clf.fit(X, y)
NearestCentroid(metric='euclidean', shrink_threshold=None)
>>> print(clf.predict([[-0.8, -1]]))
[1]
Methods
fit(X, y)
Fit the NearestCentroid model according to the given
training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform classiﬁcation on an array of test vectors X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(metric=’euclidean’, shrink_threshold=None)
fit(X, y)
Fit the NearestCentroid model according to the given training data.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features. Note that centroid shrinking cannot be used with sparse matrices.
29.25. sklearn.neighbors: Nearest Neighbors
1745
scikit-learn user guide, Release 0.18.2
y : array, shape = [n_samples]
Target values (integers)
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform classiﬁcation on an array of test vectors X.
The predicted class C for each sample in X is returned.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = [n_samples]
Notes
If the metric constructor parameter is “precomputed”, X is assumed to be the distance matrix between the
data to be predicted and self.centroids_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
1746
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.neighbors.NearestCentroid
• Nearest Centroid Classiﬁcation
• Classiﬁcation of text documents using sparse features
29.25.7 sklearn.neighbors.BallTree
class sklearn.neighbors.BallTree
BallTree for fast generalized N-point problems
BallTree(X, leaf_size=40, metric=’minkowski’, **kwargs)
ParametersX : array-like, shape = [n_samples, n_features]
n_samples is the number of points in the data set, and n_features is the dimension of
the parameter space. Note: if X is a C-contiguous array of doubles then data will not be
copied. Otherwise, an internal copy will be made.
leaf_size : positive integer (default = 40)
Number of points at which to switch to brute-force. Changing leaf_size will not affect
the results of a query, but can signiﬁcantly impact the speed of a query and the memory
required to store the constructed tree. The amount of memory needed to store the tree
scales as approximately n_samples / leaf_size. For a speciﬁed leaf_size, a leaf
node is guaranteed to satisfy leaf_size <= n_points <= 2 * leaf_size,
except in the case that n_samples < leaf_size.
metric : string or DistanceMetric object
the distance metric to use for the tree. Default=’minkowski’ with p=2 (that is, a eu-
clidean metric). See the documentation of the DistanceMetric class for a list of available
metrics. ball_tree.valid_metrics gives a list of the metrics which are valid for BallTree.
Additional keywords are passed to the distance metric class. :
Attributesdata : np.ndarray
The training data
Examples
Query for k-nearest neighbors
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = BallTree(X, leaf_size=2)
>>> dist, ind = tree.query([X[0]], k=3)
>>> print ind
# indices of 3 closest neighbors
[0 3 1]
>>> print dist
# distances to 3 closest neighbors
[ 0.
0.19662693
0.29473397]
Pickle and Unpickle a tree. Note that the state of the tree is saved in the pickle operation: the tree needs not be
rebuilt upon unpickling.
29.25. sklearn.neighbors: Nearest Neighbors
1747
scikit-learn user guide, Release 0.18.2
>>> import numpy as np
>>> import pickle
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = BallTree(X, leaf_size=2)
>>> s = pickle.dumps(tree)
>>> tree_copy = pickle.loads(s)
>>> dist, ind = tree_copy.query(X[0], k=3)
>>> print ind
# indices of 3 closest neighbors
[0 3 1]
>>> print dist
# distances to 3 closest neighbors
[ 0.
0.19662693
0.29473397]
Query for neighbors within a given radius
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = BallTree(X, leaf_size=2)
>>> print tree.query_radius(X[0], r=0.3, count_only=True)
3
>>> ind = tree.query_radius(X[0], r=0.3)
>>> print ind
# indices of neighbors within distance 0.3
[3 0 1]
Compute a gaussian kernel density estimate:
>>> import numpy as np
>>> np.random.seed(1)
>>> X = np.random.random((100, 3))
>>> tree = BallTree(X)
>>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')
array([ 6.94114649,
7.83281226,
7.2071716 ])
Compute a two-point auto-correlation function
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((30, 3))
>>> r = np.linspace(0, 1, 5)
>>> tree = BallTree(X)
>>> tree.two_point_correlation(X, r)
array([ 30,
62, 278, 580, 820])
Methods
get_arrays
get_n_calls
get_tree_stats
kernel_density(self, X, h[, kernel, atol, ...])
Compute the kernel density estimate at points X with
the given kernel, using the distance metric speciﬁed at
tree creation.
query(X[, k, return_distance, dualtree, ...])
query the tree for the k nearest neighbors
Continued on next page
1748
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.197 – continued from previous page
query_radius
query_radius(self, X, r, count_only = False):
reset_n_calls
two_point_correlation
Compute the two-point correlation function
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
kernel_density(self,
X, h,
kernel=’gaussian’,
atol=0,
rtol=1E-8,
breadth_ﬁrst=True,
re-
turn_log=False)
Compute the kernel density estimate at points X with the given kernel, using the distance metric speciﬁed
at tree creation.
ParametersX : array_like
An array of points to query. Last dimension should match dimension of training data.
h : ﬂoat
the bandwidth of the kernel
kernel : string
specify the kernel to use. Options are - ‘gaussian’ - ‘tophat’ - ‘epanechnikov’ - ‘expo-
nential’ - ‘linear’ - ‘cosine’ Default is kernel = ‘gaussian’
atol, rtol : ﬂoat (default = 0)
Specify the desired relative and absolute tolerance of the result. If the true result is
K_true, then the returned result K_ret satisﬁes abs(K_true -K_ret) < atol +
rtol * K_ret The default is zero (i.e. machine precision) for both.
breadth_ﬁrst : boolean (default = False)
if True, use a breadth-ﬁrst search. If False (default) use a depth-ﬁrst search. Breadth-
ﬁrst is generally faster for compact kernels and/or high tolerances.
return_log : boolean (default = False)
return the logarithm of the result. This can be more accurate than returning the result
itself for narrow kernels.
Returnsdensity : ndarray
The array of (log)-density evaluations, shape = X.shape[:-1]
Examples
Compute a gaussian kernel density estimate:
>>> import numpy as np
>>> np.random.seed(1)
>>> X = np.random.random((100, 3))
>>> tree = BinaryTree(X)
>>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')
array([ 6.94114649,
7.83281226,
7.2071716 ])
query(X, k=1, return_distance=True, dualtree=False, breadth_ﬁrst=False)
query the tree for the k nearest neighbors
ParametersX : array-like, last dimension self.dim
29.25. sklearn.neighbors: Nearest Neighbors
1749
scikit-learn user guide, Release 0.18.2
An array of points to query
k : integer (default = 1)
The number of nearest neighbors to return
return_distance : boolean (default = True)
if True, return a tuple (d, i) of distances and indices if False, return array i
dualtree : boolean (default = False)
if True, use the dual tree formalism for the query: a tree is built for the query points,
and the pair of trees is used to efﬁciently search this space. This can lead to better
performance as the number of points grows large.
breadth_ﬁrst : boolean (default = False)
if True, then query the nodes in a breadth-ﬁrst manner. Otherwise, query the nodes in a
depth-ﬁrst manner.
sort_results : boolean (default = True)
if True, then distances and indices of each point are sorted on return, so that the ﬁrst
column contains the closest points. Otherwise, neighbors are returned in an arbitrary
order.
Returnsi : if return_distance == False
(d,i) : if return_distance == True
d : array of doubles - shape: x.shape[:-1] + (k,)
each entry gives the list of distances to the neighbors of the corresponding point
i : array of integers - shape: x.shape[:-1] + (k,)
each entry gives the list of indices of neighbors of the corresponding point
Examples
Query for k-nearest neighbors
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = BinaryTree(X, leaf_size=2)
>>> dist, ind = tree.query(X[0], k=3)
>>> print ind
# indices of 3 closest neighbors
[0 3 1]
>>> print dist
# distances to 3 closest neighbors
[ 0.
0.19662693
0.29473397]
query_radius()
query_radius(self, X, r, count_only = False):
query the tree for neighbors within a radius r
ParametersX : array-like, last dimension self.dim
An array of points to query
r : distance within which neighbors are returned
1750
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
r can be a single value, or an array of values of shape x.shape[:-1] if different radii are
desired for each point.
return_distance : boolean (default = False)
if True, return distances to neighbors of each point if False, return only neighbors Note
that unlike the query() method, setting return_distance=True here adds to the computa-
tion time. Not all distances need to be calculated explicitly for return_distance=False.
Results are not sorted by default: see sort_results keyword.
count_only : boolean (default = False)
if True, return only the count of points within distance r if False, return the indices of all
points within distance r If return_distance==True, setting count_only=True will result
in an error.
sort_results : boolean (default = False)
if True, the distances and indices will be sorted before being returned. If False, the
results will not be sorted. If return_distance == False, setting sort_results = True will
result in an error.
Returnscount : if count_only == True
ind : if count_only == False and return_distance == False
(ind, dist) : if count_only == False and return_distance == True
count : array of integers, shape = X.shape[:-1]
each entry gives the number of neighbors within a distance r of the corresponding point.
ind : array of objects, shape = X.shape[:-1]
each element is a numpy integer array listing the indices of neighbors of the correspond-
ing point. Note that unlike the results of a k-neighbors query, the returned neighbors are
not sorted by distance by default.
dist : array of objects, shape = X.shape[:-1]
each element is a numpy double array listing the distances corresponding to indices in
i.
Examples
Query for neighbors in a given radius
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = BinaryTree(X, leaf_size=2)
>>> print tree.query_radius(X[0], r=0.3, count_only=True)
3
>>> ind = tree.query_radius(X[0], r=0.3)
>>> print ind
# indices of neighbors within distance 0.3
[3 0 1]
two_point_correlation()
Compute the two-point correlation function
ParametersX : array_like
29.25. sklearn.neighbors: Nearest Neighbors
1751
scikit-learn user guide, Release 0.18.2
An array of points to query. Last dimension should match dimension of training data.
r : array_like
A one-dimensional array of distances
dualtree : boolean (default = False)
If true, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree
algorithms can have better scaling for large N.
Returnscounts : ndarray
counts[i] contains the number of pairs of points with distance less than or equal to r[i]
Examples
Compute the two-point autocorrelation function of X:
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((30, 3))
>>> r = np.linspace(0, 1, 5)
>>> tree = BinaryTree(X)
>>> tree.two_point_correlation(X, r)
array([ 30,
62, 278, 580, 820])
29.25.8 sklearn.neighbors.KDTree
class sklearn.neighbors.KDTree
KDTree for fast generalized N-point problems
KDTree(X, leaf_size=40, metric=’minkowski’, **kwargs)
ParametersX : array-like, shape = [n_samples, n_features]
n_samples is the number of points in the data set, and n_features is the dimension of
the parameter space. Note: if X is a C-contiguous array of doubles then data will not be
copied. Otherwise, an internal copy will be made.
leaf_size : positive integer (default = 40)
Number of points at which to switch to brute-force. Changing leaf_size will not affect
the results of a query, but can signiﬁcantly impact the speed of a query and the memory
required to store the constructed tree. The amount of memory needed to store the tree
scales as approximately n_samples / leaf_size. For a speciﬁed leaf_size, a leaf
node is guaranteed to satisfy leaf_size <= n_points <= 2 * leaf_size,
except in the case that n_samples < leaf_size.
metric : string or DistanceMetric object
the distance metric to use for the tree. Default=’minkowski’ with p=2 (that is, a eu-
clidean metric). See the documentation of the DistanceMetric class for a list of available
metrics. kd_tree.valid_metrics gives a list of the metrics which are valid for KDTree.
Additional keywords are passed to the distance metric class. :
Attributesdata : np.ndarray
The training data
1752
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
Query for k-nearest neighbors
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = KDTree(X, leaf_size=2)
>>> dist, ind = tree.query([X[0]], k=3)
>>> print ind
# indices of 3 closest neighbors
[0 3 1]
>>> print dist
# distances to 3 closest neighbors
[ 0.
0.19662693
0.29473397]
Pickle and Unpickle a tree. Note that the state of the tree is saved in the pickle operation: the tree needs not be
rebuilt upon unpickling.
>>> import numpy as np
>>> import pickle
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = KDTree(X, leaf_size=2)
>>> s = pickle.dumps(tree)
>>> tree_copy = pickle.loads(s)
>>> dist, ind = tree_copy.query(X[0], k=3)
>>> print ind
# indices of 3 closest neighbors
[0 3 1]
>>> print dist
# distances to 3 closest neighbors
[ 0.
0.19662693
0.29473397]
Query for neighbors within a given radius
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = KDTree(X, leaf_size=2)
>>> print tree.query_radius(X[0], r=0.3, count_only=True)
3
>>> ind = tree.query_radius(X[0], r=0.3)
>>> print ind
# indices of neighbors within distance 0.3
[3 0 1]
Compute a gaussian kernel density estimate:
>>> import numpy as np
>>> np.random.seed(1)
>>> X = np.random.random((100, 3))
>>> tree = KDTree(X)
>>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')
array([ 6.94114649,
7.83281226,
7.2071716 ])
Compute a two-point auto-correlation function
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((30, 3))
>>> r = np.linspace(0, 1, 5)
>>> tree = KDTree(X)
29.25. sklearn.neighbors: Nearest Neighbors
1753
scikit-learn user guide, Release 0.18.2
>>> tree.two_point_correlation(X, r)
array([ 30,
62, 278, 580, 820])
Methods
get_arrays
get_n_calls
get_tree_stats
kernel_density(self, X, h[, kernel, atol, ...])
Compute the kernel density estimate at points X with
the given kernel, using the distance metric speciﬁed at
tree creation.
query(X[, k, return_distance, dualtree, ...])
query the tree for the k nearest neighbors
query_radius
query_radius(self, X, r, count_only = False):
reset_n_calls
two_point_correlation
Compute the two-point correlation function
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
kernel_density(self,
X, h,
kernel=’gaussian’,
atol=0,
rtol=1E-8,
breadth_ﬁrst=True,
re-
turn_log=False)
Compute the kernel density estimate at points X with the given kernel, using the distance metric speciﬁed
at tree creation.
ParametersX : array_like
An array of points to query. Last dimension should match dimension of training data.
h : ﬂoat
the bandwidth of the kernel
kernel : string
specify the kernel to use. Options are - ‘gaussian’ - ‘tophat’ - ‘epanechnikov’ - ‘expo-
nential’ - ‘linear’ - ‘cosine’ Default is kernel = ‘gaussian’
atol, rtol : ﬂoat (default = 0)
Specify the desired relative and absolute tolerance of the result. If the true result is
K_true, then the returned result K_ret satisﬁes abs(K_true -K_ret) < atol +
rtol * K_ret The default is zero (i.e. machine precision) for both.
breadth_ﬁrst : boolean (default = False)
if True, use a breadth-ﬁrst search. If False (default) use a depth-ﬁrst search. Breadth-
ﬁrst is generally faster for compact kernels and/or high tolerances.
return_log : boolean (default = False)
return the logarithm of the result. This can be more accurate than returning the result
itself for narrow kernels.
Returnsdensity : ndarray
The array of (log)-density evaluations, shape = X.shape[:-1]
1754
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
Compute a gaussian kernel density estimate:
>>> import numpy as np
>>> np.random.seed(1)
>>> X = np.random.random((100, 3))
>>> tree = BinaryTree(X)
>>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')
array([ 6.94114649,
7.83281226,
7.2071716 ])
query(X, k=1, return_distance=True, dualtree=False, breadth_ﬁrst=False)
query the tree for the k nearest neighbors
ParametersX : array-like, last dimension self.dim
An array of points to query
k : integer (default = 1)
The number of nearest neighbors to return
return_distance : boolean (default = True)
if True, return a tuple (d, i) of distances and indices if False, return array i
dualtree : boolean (default = False)
if True, use the dual tree formalism for the query: a tree is built for the query points,
and the pair of trees is used to efﬁciently search this space. This can lead to better
performance as the number of points grows large.
breadth_ﬁrst : boolean (default = False)
if True, then query the nodes in a breadth-ﬁrst manner. Otherwise, query the nodes in a
depth-ﬁrst manner.
sort_results : boolean (default = True)
if True, then distances and indices of each point are sorted on return, so that the ﬁrst
column contains the closest points. Otherwise, neighbors are returned in an arbitrary
order.
Returnsi : if return_distance == False
(d,i) : if return_distance == True
d : array of doubles - shape: x.shape[:-1] + (k,)
each entry gives the list of distances to the neighbors of the corresponding point
i : array of integers - shape: x.shape[:-1] + (k,)
each entry gives the list of indices of neighbors of the corresponding point
Examples
Query for k-nearest neighbors
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = BinaryTree(X, leaf_size=2)
29.25. sklearn.neighbors: Nearest Neighbors
1755
scikit-learn user guide, Release 0.18.2
>>> dist, ind = tree.query(X[0], k=3)
>>> print ind
# indices of 3 closest neighbors
[0 3 1]
>>> print dist
# distances to 3 closest neighbors
[ 0.
0.19662693
0.29473397]
query_radius()
query_radius(self, X, r, count_only = False):
query the tree for neighbors within a radius r
ParametersX : array-like, last dimension self.dim
An array of points to query
r : distance within which neighbors are returned
r can be a single value, or an array of values of shape x.shape[:-1] if different radii are
desired for each point.
return_distance : boolean (default = False)
if True, return distances to neighbors of each point if False, return only neighbors Note
that unlike the query() method, setting return_distance=True here adds to the computa-
tion time. Not all distances need to be calculated explicitly for return_distance=False.
Results are not sorted by default: see sort_results keyword.
count_only : boolean (default = False)
if True, return only the count of points within distance r if False, return the indices of all
points within distance r If return_distance==True, setting count_only=True will result
in an error.
sort_results : boolean (default = False)
if True, the distances and indices will be sorted before being returned. If False, the
results will not be sorted. If return_distance == False, setting sort_results = True will
result in an error.
Returnscount : if count_only == True
ind : if count_only == False and return_distance == False
(ind, dist) : if count_only == False and return_distance == True
count : array of integers, shape = X.shape[:-1]
each entry gives the number of neighbors within a distance r of the corresponding point.
ind : array of objects, shape = X.shape[:-1]
each element is a numpy integer array listing the indices of neighbors of the correspond-
ing point. Note that unlike the results of a k-neighbors query, the returned neighbors are
not sorted by distance by default.
dist : array of objects, shape = X.shape[:-1]
each element is a numpy double array listing the distances corresponding to indices in
i.
Examples
Query for neighbors in a given radius
1756
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((10, 3))
# 10 points in 3 dimensions
>>> tree = BinaryTree(X, leaf_size=2)
>>> print tree.query_radius(X[0], r=0.3, count_only=True)
3
>>> ind = tree.query_radius(X[0], r=0.3)
>>> print ind
# indices of neighbors within distance 0.3
[3 0 1]
two_point_correlation()
Compute the two-point correlation function
ParametersX : array_like
An array of points to query. Last dimension should match dimension of training data.
r : array_like
A one-dimensional array of distances
dualtree : boolean (default = False)
If true, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree
algorithms can have better scaling for large N.
Returnscounts : ndarray
counts[i] contains the number of pairs of points with distance less than or equal to r[i]
Examples
Compute the two-point autocorrelation function of X:
>>> import numpy as np
>>> np.random.seed(0)
>>> X = np.random.random((30, 3))
>>> r = np.linspace(0, 1, 5)
>>> tree = BinaryTree(X)
>>> tree.two_point_correlation(X, r)
array([ 30,
62, 278, 580, 820])
29.25.9 sklearn.neighbors.LSHForest
class sklearn.neighbors.LSHForest(n_estimators=10,
radius=1.0,
n_candidates=50,
n_neighbors=5, min_hash_match=4, radius_cutoff_ratio=0.9,
random_state=None)
Performs approximate nearest neighbor search using LSH forest.
LSH Forest: Locality Sensitive Hashing forest [1] is an alternative method for vanilla approximate nearest
neighbor search methods. LSH forest data structure has been implemented using sorted arrays and binary
search and 32 bit ﬁxed-length hashes. Random projection is used as the hash family which approximates cosine
distance.
The cosine distance is deﬁned as 1 -cosine_similarity: the lowest value is 0 (identical point) but it is
bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only
on their relative angles.
29.25. sklearn.neighbors: Nearest Neighbors
1757
scikit-learn user guide, Release 0.18.2
Read more in the User Guide.
Parametersn_estimators : int (default = 10)
Number of trees in the LSH Forest.
min_hash_match : int (default = 4)
lowest hash length to be searched when candidate selection is performed for nearest
neighbors.
n_candidates : int (default = 10)
Minimum number of candidates evaluated per estimator, assuming enough items meet
the min_hash_match constraint.
n_neighbors : int (default = 5)
Number of neighbors to be returned from query function when it is not provided to the
kneighbors method.
radius : ﬂoat, optinal (default = 1.0)
Radius from the data point to its neighbors. This is the parameter space to use by default
for the :meth‘radius_neighbors‘ queries.
radius_cutoff_ratio : ﬂoat, optional (default = 0.9)
A value ranges from 0 to 1. Radius neighbors will be searched until the ratio between
total neighbors within the radius and the total candidates becomes less than this value
unless it is terminated by hash length reaching min_hash_match.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
Attributeshash_functions_ : list of GaussianRandomProjectionHash objects
Hash function g(p,x) for a tree is an array of 32 randomly generated ﬂoat arrays with the
same dimension as the data set. This array is stored in GaussianRandomProjectionHash
object and can be obtained from components_ attribute.
trees_ : array, shape (n_estimators, n_samples)
Each tree (corresponding to a hash function) contains an array of sorted hashed values.
The array representation may change in future versions.
original_indices_ : array, shape (n_estimators, n_samples)
Original indices of sorted hashed values in the ﬁtted index.
References
[R70]
Examples
>>> from sklearn.neighbors import LSHForest
1758
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> X_train = [[5, 5, 2], [21, 5, 5], [1, 1, 1], [8, 9, 1], [6, 10, 2]]
>>> X_test = [[9, 1, 6], [3, 1, 10], [7, 10, 3]]
>>> lshf = LSHForest(random_state=42)
>>> lshf.fit(X_train)
LSHForest(min_hash_match=4, n_candidates=50, n_estimators=10,
n_neighbors=5, radius=1.0, radius_cutoff_ratio=0.9,
random_state=42)
>>> distances, indices = lshf.kneighbors(X_test, n_neighbors=2)
>>> distances
array([[ 0.069...,
0.149...],
[ 0.229...,
0.481...],
[ 0.004...,
0.014...]])
>>> indices
array([[1, 2],
[2, 0],
[4, 0]])
Methods
fit(X[, y])
Fit the LSH forest on the data.
get_params([deep])
Get parameters for this estimator.
kneighbors(X[, n_neighbors, return_distance])
Returns n_neighbors of approximate nearest neighbors.
kneighbors_graph([X, n_neighbors, mode])
Computes the (weighted) graph of k-Neighbors for
points in X
partial_fit(X[, y])
Inserts new data into the already ﬁtted LSH Forest.
radius_neighbors(X[, radius, return_distance])
Finds the neighbors within a given radius of a point or
points.
radius_neighbors_graph([X, radius, mode])
Computes the (weighted) graph of Neighbors for points
in X
set_params(\*\*params)
Set the parameters of this estimator.
__init__(n_estimators=10, radius=1.0, n_candidates=50, n_neighbors=5, min_hash_match=4, ra-
dius_cutoff_ratio=0.9, random_state=None)
fit(X, y=None)
Fit the LSH forest on the data.
This creates binary hashes of input data points by getting the dot product of input points and hash_function
then transforming the projection into a binary string array based on the sign (positive/negative) of the
projection. A sorted array of binary hashes is created.
ParametersX : array_like or sparse (CSR) matrix, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
29.25. sklearn.neighbors: Nearest Neighbors
1759
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
kneighbors(X, n_neighbors=None, return_distance=True)
Returns n_neighbors of approximate nearest neighbors.
ParametersX : array_like or sparse (CSR) matrix, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single query.
n_neighbors : int, opitonal (default = None)
Number of neighbors required. If not provided, this will return the number speciﬁed at
the initialization.
return_distance : boolean, optional (default = True)
Returns the distances of neighbors if set to True.
Returnsdist : array, shape (n_samples, n_neighbors)
Array representing the cosine distances to each point,
only present if re-
turn_distance=True.
ind : array, shape (n_samples, n_neighbors)
Indices of the approximate nearest points in the population matrix.
kneighbors_graph(X=None, n_neighbors=None, mode=’connectivity’)
Computes the (weighted) graph of k-Neighbors for points in X
ParametersX : array-like, shape (n_query, n_features), or (n_query, n_indexed) if metric ==
‘precomputed’
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
n_neighbors : int
Number of neighbors for each sample. (default is value passed to the constructor).
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, in ‘distance’ the edges are Euclidean distance between points.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples_ﬁt]
n_samples_ﬁt is the number of samples in the ﬁtted data A[i, j] is assigned the weight
of edge that connects i to j.
See also:
NearestNeighbors.radius_neighbors_graph
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(n_neighbors=2)
>>> neigh.fit(X)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> A = neigh.kneighbors_graph(X)
1760
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
1.],
[ 1.,
0.,
1.]])
partial_fit(X, y=None)
Inserts new data into the already ﬁtted LSH Forest. Cost is proportional to new total size, so additions
should be batched.
ParametersX : array_like or sparse (CSR) matrix, shape (n_samples, n_features)
New data point to be inserted into the LSH Forest.
radius_neighbors(X, radius=None, return_distance=True)
Finds the neighbors within a given radius of a point or points.
Return the indices and distances of some points from the dataset lying in a ball with size radius around
the points of the query array. Points lying on the boundary are included in the results.
The result points are not necessarily sorted by distance to their query point.
LSH Forest being an approximate method, some true neighbors from the indexed dataset might be missing
from the results.
ParametersX : array_like or sparse (CSR) matrix, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single query.
radius : ﬂoat
Limiting distance of neighbors to return. (default is the value passed to the constructor).
return_distance : boolean, optional (default = False)
Returns the distances of neighbors if set to True.
Returnsdist : array, shape (n_samples,) of arrays
Each element is an array representing the cosine distances to some points found within
radius of the respective query. Only present if return_distance=True.
ind : array, shape (n_samples,) of arrays
Each element is an array of indices for neighbors within radius of the respective
query.
radius_neighbors_graph(X=None, radius=None, mode=’connectivity’)
Computes the (weighted) graph of Neighbors for points in X
Neighborhoods are restricted the points at a distance lower than radius.
ParametersX : array-like, shape = [n_samples, n_features], optional
The query point or points. If not provided, neighbors of each indexed point are returned.
In this case, the query point is not considered its own neighbor.
radius : ﬂoat
Radius of neighborhoods. (default is the value passed to the constructor).
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, in ‘distance’ the edges are Euclidean distance between points.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples]
29.25. sklearn.neighbors: Nearest Neighbors
1761
scikit-learn user guide, Release 0.18.2
A[i, j] is assigned the weight of edge that connects i to j.
See also:
kneighbors_graph
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import NearestNeighbors
>>> neigh = NearestNeighbors(radius=1.5)
>>> neigh.fit(X)
NearestNeighbors(algorithm='auto', leaf_size=30, ...)
>>> A = neigh.radius_neighbors_graph(X)
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
0.],
[ 1.,
0.,
1.]])
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.neighbors.LSHForest
• Hyper-parameters of Approximate Nearest Neighbors
• Scalability of Approximate Nearest Neighbors
29.25.10 sklearn.neighbors.DistanceMetric
class sklearn.neighbors.DistanceMetric
DistanceMetric class
This class provides a uniform interface to fast distance metric functions. The various metrics can be accessed
via the get_metric class method and the metric string identiﬁer (see below). For example, to use the Euclidean
distance:
>>> dist = DistanceMetric.get_metric('euclidean')
>>> X = [[0, 1, 2],
[3, 4, 5]])
>>> dist.pairwise(X)
array([[ 0.
,
5.19615242],
[ 5.19615242,
0.
]])
Available Metrics The following lists the string metric identiﬁers and the associated distance metric classes:
Metrics intended for real-valued vector spaces:
1762
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
identiﬁer
class name
args
distance function
“euclidean”
EuclideanDistance
•
sqrt(sum((x
-y)^2))
“manhattan”
ManhattanDistance
•
sum(|x -y|)
“chebyshev”
ChebyshevDistance
•
max(|x -y|)
“minkowski”
MinkowskiDistance
p
sum(|x
-y|^p)^(1/p)
“wminkowski”
WMinkowskiDistance
p, w
sum(w * |x
-y|^p)^(1/p)
“seuclidean”
SEuclideanDistance
V
sqrt(sum((x -y)^2
/ V))
“mahalanobis”
MahalanobisDistance
V or VI
sqrt((x -y)' V^-1
(x -y))
Metrics intended for two-dimensional vector spaces: Note that the haversine distance metric requires data in
the form of [latitude, longitude] and both inputs and outputs are in units of radians.
identiﬁer
class name
distance function
“haversine”
HaversineDistance
2 arcsin(sqrt(sin^2(0.5*dx)
•
cos(x1)cos(x2)sin^2(0.5*dy)))
Metrics intended for integer-valued vector spaces: Though intended for integer-valued vectors, these are also
valid metrics in the case of real-valued vectors.
identiﬁer
class name
distance function
“hamming”
HammingDistance
N_unequal(x,y) / N_tot
“canberra”
CanberraDistance
sum(|x -y| / (|x| + |y|))
“braycurtis”
BrayCurtisDistance
sum(|x -y|) / (sum(|x|) + sum(|y|))
Metrics intended for boolean-valued vector spaces: Any nonzero entry is evaluated to “True”. In the listings
below, the following abbreviations are used:
•N : number of dimensions
•NTT : number of dims in which both values are True
•NTF : number of dims in which the ﬁrst value is True, second is False
•NFT : number of dims in which the ﬁrst value is False, second is True
•NFF : number of dims in which both values are False
•NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT
•NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT
29.25. sklearn.neighbors: Nearest Neighbors
1763
scikit-learn user guide, Release 0.18.2
identiﬁer
class name
distance function
“jaccard”
JaccardDistance
NNEQ / NNZ
“matching”
MatchingDistance
NNEQ / N
“dice”
DiceDistance
NNEQ / (NTT + NNZ)
“kulsinski”
KulsinskiDistance
(NNEQ + N - NTT) / (NNEQ + N)
“rogerstanimoto”
RogersTanimotoDistance
2 * NNEQ / (N + NNEQ)
“russellrao”
RussellRaoDistance
NNZ / N
“sokalmichener”
SokalMichenerDistance
2 * NNEQ / (N + NNEQ)
“sokalsneath”
SokalSneathDistance
NNEQ / (NNEQ + 0.5 * NTT)
User-deﬁned distance:
identiﬁer
class name
args
“pyfunc”
PyFuncDistance
func
Here func is a function which takes two one-dimensional numpy arrays, and returns a distance. Note that
in order to be used within the BallTree, the distance must be a true metric: i.e. it must satisfy the following
properties
1.Non-negativity: d(x, y) >= 0
2.Identity: d(x, y) = 0 if and only if x == y
3.Symmetry: d(x, y) = d(y, x)
4.Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)
Because of the Python object overhead involved in calling the python function, this will be fairly slow, but it
will have the same scaling as other distances.
Methods
dist_to_rdist
Convert the true distance to the reduced distance.
get_metric
Get the given distance metric from the string identiﬁer.
pairwise
Compute the pairwise distances between X and Y
rdist_to_dist
Convert the Reduced distance to the true distance.
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
dist_to_rdist()
Convert the true distance to the reduced distance.
The reduced distance, deﬁned for some metrics, is a computationally more efﬁcent measure which pre-
serves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is
the squared-euclidean distance.
get_metric()
Get the given distance metric from the string identiﬁer.
See the docstring of DistanceMetric for a list of available metrics.
Parametersmetric : string or class name
The distance metric to use
**kwargs :
additional arguments will be passed to the requested metric
1764
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
pairwise()
Compute the pairwise distances between X and Y
This is a convenience routine for the sake of testing.
For many metrics,
the utilities in
scipy.spatial.distance.cdist and scipy.spatial.distance.pdist will be faster.
ParametersX : array_like
Array of shape (Nx, D), representing Nx points in D dimensions.
Y : array_like (optional)
Array of shape (Ny, D), representing Ny points in D dimensions. If not speciﬁed, then
Y=X.
Returns :
——- :
dist : ndarray
The shape (Nx, Ny) array of pairwise distances between points in X and Y.
rdist_to_dist()
Convert the Reduced distance to the true distance.
The reduced distance, deﬁned for some metrics, is a computationally more efﬁcent measure which pre-
serves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is
the squared-euclidean distance.
29.25.11 sklearn.neighbors.KernelDensity
class sklearn.neighbors.KernelDensity(bandwidth=1.0,
algorithm=’auto’,
kernel=’gaussian’,
metric=’euclidean’, atol=0, rtol=0, breadth_ﬁrst=True,
leaf_size=40, metric_params=None)
Kernel Density Estimation
Read more in the User Guide.
Parametersbandwidth : ﬂoat
The bandwidth of the kernel.
algorithm : string
The tree algorithm to use. Valid options are [’kd_tree’|’ball_tree’|’auto’]. Default is
‘auto’.
kernel : string
The
kernel
to
use.
Valid
kernels
are
[’gaus-
sian’|’tophat’|’epanechnikov’|’exponential’|’linear’|’cosine’] Default is ‘gaussian’.
metric : string
The distance metric to use. Note that not all metrics are valid with all algorithms.
Refer to the documentation of BallTree and KDTree for a description of available
algorithms. Note that the normalization of the density output is correct only for the
Euclidean distance metric. Default is ‘euclidean’.
atol : ﬂoat
The desired absolute tolerance of the result. A larger tolerance will generally lead to
faster execution. Default is 0.
29.25. sklearn.neighbors: Nearest Neighbors
1765
scikit-learn user guide, Release 0.18.2
rtol : ﬂoat
The desired relative tolerance of the result. A larger tolerance will generally lead to
faster execution. Default is 1E-8.
breadth_ﬁrst : boolean
If true (default), use a breadth-ﬁrst approach to the problem. Otherwise use a depth-ﬁrst
approach.
leaf_size : int
Specify the leaf size of the underlying tree. See BallTree or KDTree for details.
Default is 40.
metric_params : dict
Additional parameters to be passed to the tree for use with the metric. For more infor-
mation, see the documentation of BallTree or KDTree.
Methods
fit(X[, y])
Fit the Kernel Density model on the data.
get_params([deep])
Get parameters for this estimator.
sample([n_samples, random_state])
Generate random samples from the model.
score(X[, y])
Compute the total log probability under the model.
score_samples(X)
Evaluate the density model on the data.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(bandwidth=1.0, algorithm=’auto’, kernel=’gaussian’, metric=’euclidean’, atol=0, rtol=0,
breadth_ﬁrst=True, leaf_size=40, metric_params=None)
fit(X, y=None)
Fit the Kernel Density model on the data.
ParametersX : array_like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
sample(n_samples=1, random_state=None)
Generate random samples from the model.
Currently, this is implemented only for gaussian and tophat kernels.
Parametersn_samples : int, optional
Number of samples to generate. Defaults to 1.
random_state : RandomState or an int seed (0 by default)
1766
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
A random number generator instance.
ReturnsX : array_like, shape (n_samples, n_features)
List of samples.
score(X, y=None)
Compute the total log probability under the model.
ParametersX : array_like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslogprob : ﬂoat
Total log-likelihood of the data in X.
score_samples(X)
Evaluate the density model on the data.
ParametersX : array_like, shape (n_samples, n_features)
An array of points to query. Last dimension should match dimension of training data
(n_features).
Returnsdensity : ndarray, shape (n_samples,)
The array of log(density) evaluations.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.neighbors.KernelDensity
• Kernel Density Estimation
• Simple 1D Kernel Density Estimation
• Kernel Density Estimate of Species Distributions
neighbors.kneighbors_graph(X,
n_neighbors[,
...])
Computes the (weighted) graph of k-Neighbors for points
in X
neighbors.radius_neighbors_graph(X, radius)
Computes the (weighted) graph of Neighbors for points in
X
29.25.12 sklearn.neighbors.kneighbors_graph
sklearn.neighbors.kneighbors_graph(X,
n_neighbors,
mode=’connectivity’,
met-
ric=’minkowski’,
p=2,
metric_params=None,
in-
clude_self=False, n_jobs=1)
Computes the (weighted) graph of k-Neighbors for points in X
Read more in the User Guide.
ParametersX : array-like or BallTree, shape = [n_samples, n_features]
29.25. sklearn.neighbors: Nearest Neighbors
1767
scikit-learn user guide, Release 0.18.2
Sample data, in the form of a numpy array or a precomputed BallTree.
n_neighbors : int
Number of neighbors for each sample.
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, and ‘distance’ will return the distances between neighbors according to the
given metric.
metric : string, default ‘minkowski’
The distance metric used to calculate the k-Neighbors for each sample point. The Dis-
tanceMetric class gives a list of available metrics. The default distance is ‘euclidean’
(‘minkowski’ metric with the p param equal to 2.)
include_self: bool, default=False. :
Whether or not to mark each sample as the ﬁrst nearest neighbor to itself. If None,
then True is used for mode=’connectivity’ and False for mode=’distance’ as this will
preserve backwards compatibilty.
p : int, default 2
Power parameter for the Minkowski metric. When p = 1, this is equivalent to us-
ing manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p,
minkowski_distance (l_p) is used.
metric_params: dict, optional :
additional keyword arguments for the metric function.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run for neighbors search. If -1, then the number of jobs
is set to the number of CPU cores.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples]
A[i, j] is assigned the weight of edge that connects i to j.
See also:
radius_neighbors_graph
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import kneighbors_graph
>>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
1.],
[ 1.,
0.,
1.]])
Examples using sklearn.neighbors.kneighbors_graph
• Agglomerative clustering with and without structure
1768
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
• Comparing different clustering algorithms on toy datasets
• Hierarchical clustering: structured vs unstructured ward
29.25.13 sklearn.neighbors.radius_neighbors_graph
sklearn.neighbors.radius_neighbors_graph(X,
radius,
mode=’connectivity’,
met-
ric=’minkowski’,
p=2,
metric_params=None,
include_self=False, n_jobs=1)
Computes the (weighted) graph of Neighbors for points in X
Neighborhoods are restricted the points at a distance lower than radius.
Read more in the User Guide.
ParametersX : array-like or BallTree, shape = [n_samples, n_features]
Sample data, in the form of a numpy array or a precomputed BallTree.
radius : ﬂoat
Radius of neighborhoods.
mode : {‘connectivity’, ‘distance’}, optional
Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones
and zeros, and ‘distance’ will return the distances between neighbors according to the
given metric.
metric : string, default ‘minkowski’
The distance metric used to calculate the neighbors within a given radius for each sam-
ple point. The DistanceMetric class gives a list of available metrics. The default distance
is ‘euclidean’ (‘minkowski’ metric with the param equal to 2.)
include_self: bool, default=False :
Whether or not to mark each sample as the ﬁrst nearest neighbor to itself. If None,
then True is used for mode=’connectivity’ and False for mode=’distance’ as this will
preserve backwards compatibilty.
p : int, default 2
Power parameter for the Minkowski metric. When p = 1, this is equivalent to us-
ing manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p,
minkowski_distance (l_p) is used.
metric_params: dict, optional :
additional keyword arguments for the metric function.
n_jobs : int, optional (default = 1)
The number of parallel jobs to run for neighbors search. If -1, then the number of jobs
is set to the number of CPU cores.
ReturnsA : sparse matrix in CSR format, shape = [n_samples, n_samples]
A[i, j] is assigned the weight of edge that connects i to j.
See also:
kneighbors_graph
29.25. sklearn.neighbors: Nearest Neighbors
1769
scikit-learn user guide, Release 0.18.2
Examples
>>> X = [[0], [3], [1]]
>>> from sklearn.neighbors import radius_neighbors_graph
>>> A = radius_neighbors_graph(X, 1.5, mode='connectivity', include_self=True)
>>> A.toarray()
array([[ 1.,
0.,
1.],
[ 0.,
1.,
0.],
[ 1.,
0.,
1.]])
29.26 sklearn.neural_network: Neural network models
The sklearn.neural_network module includes models based on neural networks.
User guide: See the Neural network models (supervised) and Neural network models (unsupervised) sections for
further details.
neural_network.BernoulliRBM([n_components,
...])
Bernoulli Restricted Boltzmann Machine (RBM).
neural_network.MLPClassifier([...])
Multi-layer Perceptron classiﬁer.
neural_network.MLPRegressor([...])
Multi-layer Perceptron regressor.
29.26.1 sklearn.neural_network.BernoulliRBM
class sklearn.neural_network.BernoulliRBM(n_components=256,
learning_rate=0.1,
batch_size=10,
n_iter=10,
verbose=0,
ran-
dom_state=None)
Bernoulli Restricted Boltzmann Machine (RBM).
A Restricted Boltzmann Machine with binary visible units and binary hidden units. Parameters are estimated
using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD) [2].
The time complexity of this implementation is O(d ** 2) assuming d ~ n_features ~ n_components.
Read more in the User Guide.
Parametersn_components : int, optional
Number of binary hidden units.
learning_rate : ﬂoat, optional
The learning rate for weight updates. It is highly recommended to tune this hyper-
parameter. Reasonable values are in the 10**[0., -3.] range.
batch_size : int, optional
Number of examples per minibatch.
n_iter : int, optional
Number of iterations/sweeps over the training dataset to perform during training.
verbose : int, optional
The verbosity level. The default, zero, means silent mode.
random_state : integer or numpy.RandomState, optional
1770
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
A random number generator instance to deﬁne the state of the random permutations
generator. If an integer is given, it ﬁxes the seed. Defaults to the global numpy random
number generator.
Attributesintercept_hidden_ : array-like, shape (n_components,)
Biases of the hidden units.
intercept_visible_ : array-like, shape (n_features,)
Biases of the visible units.
components_ : array-like, shape (n_components, n_features)
Weight matrix, where n_features in the number of visible units and n_components is
the number of hidden units.
References
[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm fordeep belief nets. Neural Computa-
tion 18, pp 1527-1554. http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf
[2] Tieleman, T. Training Restricted Boltzmann Machines usingApproximations to the Likelihood Gradi-
ent. International Conference on Machine Learning (ICML) 2008
Examples
>>> import numpy as np
>>> from sklearn.neural_network import BernoulliRBM
>>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
>>> model = BernoulliRBM(n_components=2)
>>> model.fit(X)
BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,
random_state=None, verbose=0)
Methods
fit(X[, y])
Fit the model to the data X.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
gibbs(v)
Perform one Gibbs sampling step.
partial_fit(X[, y])
Fit the model to the data X which should contain a par-
tial segment of the data.
score_samples(X)
Compute the pseudo-likelihood of X.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Compute the hidden layer activation probabilities,
P(h=1|v=X).
__init__(n_components=256,
learning_rate=0.1,
batch_size=10,
n_iter=10,
verbose=0,
ran-
dom_state=None)
fit(X, y=None)
Fit the model to the data X.
29.26. sklearn.neural_network: Neural network models
1771
scikit-learn user guide, Release 0.18.2
ParametersX : {array-like, sparse matrix} shape (n_samples, n_features)
Training data.
Returnsself : BernoulliRBM
The ﬁtted model.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
gibbs(v)
Perform one Gibbs sampling step.
Parametersv : array-like, shape (n_samples, n_features)
Values of the visible layer to start from.
Returnsv_new : array-like, shape (n_samples, n_features)
Values of the visible layer after one Gibbs step.
partial_fit(X, y=None)
Fit the model to the data X which should contain a partial segment of the data.
ParametersX : array-like, shape (n_samples, n_features)
Training data.
Returnsself : BernoulliRBM
The ﬁtted model.
score_samples(X)
Compute the pseudo-likelihood of X.
ParametersX : {array-like, sparse matrix} shape (n_samples, n_features)
Values of the visible layer. Must be all-boolean (not checked).
Returnspseudo_likelihood : array-like, shape (n_samples,)
Value of the pseudo-likelihood (proxy for likelihood).
1772
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
This method is not deterministic: it computes a quantity called the free energy on X, then on a randomly
corrupted version of X, and returns the log of the logistic function of the difference.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Compute the hidden layer activation probabilities, P(h=1|v=X).
ParametersX : {array-like, sparse matrix} shape (n_samples, n_features)
The data to be transformed.
Returnsh : array, shape (n_samples, n_components)
Latent representations of the data.
Examples using sklearn.neural_network.BernoulliRBM
• Restricted Boltzmann Machine features for digit classiﬁcation
29.26.2 sklearn.neural_network.MLPClassiﬁer
class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100,
),
activa-
tion=’relu’,
solver=’adam’,
alpha=0.0001,
batch_size=’auto’,
learning_rate=’constant’,
learning_rate_init=0.001,
power_t=0.5,
max_iter=200,
shufﬂe=True,
ran-
dom_state=None,
tol=0.0001,
verbose=False,
warm_start=False,
momentum=0.9,
nes-
terovs_momentum=True,
early_stopping=False,
validation_fraction=0.1,
beta_1=0.9,
beta_2=0.999, epsilon=1e-08)
Multi-layer Perceptron classiﬁer.
This model optimizes the log-loss function using LBFGS or stochastic gradient descent.
New in version 0.18.
Parametershidden_layer_sizes : tuple, length = n_layers - 2, default (100,)
The ith element represents the number of neurons in the ith hidden layer.
activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’
Activation function for the hidden layer.
•‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x
•‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).
•‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).
29.26. sklearn.neural_network: Neural network models
1773
scikit-learn user guide, Release 0.18.2
•‘relu’, the rectiﬁed linear unit function, returns f(x) = max(0, x)
solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’
The solver for weight optimization.
•‘lbfgs’ is an optimizer in the family of quasi-Newton methods.
•‘sgd’ refers to stochastic gradient descent.
•‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik,
and Jimmy Ba
Note: The default solver ‘adam’ works pretty well on relatively large datasets (with
thousands of training samples or more) in terms of both training time and validation
score. For small datasets, however, ‘lbfgs’ can converge faster and perform better.
alpha : ﬂoat, optional, default 0.0001
L2 penalty (regularization term) parameter.
batch_size : int, optional, default ‘auto’
Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classiﬁer will
not use minibatch. When set to “auto”, batch_size=min(200, n_samples)
learning_rate : {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’
Learning rate schedule for weight updates.
•‘constant’ is a constant learning rate given by ‘learning_rate_init’.
•‘invscaling’ gradually decreases the learning rate learning_rate_ at each time
step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate =
learning_rate_init / pow(t, power_t)
•‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training
loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss
by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is
on, the current learning rate is divided by 5.
Only used when solver='sgd'.
max_iter : int, optional, default 200
Maximum number of iterations. The solver iterates until convergence (determined by
‘tol’) or this number of iterations.
random_state : int or RandomState, optional, default None
State or seed for random number generator.
shufﬂe : bool, optional, default True
Whether to shufﬂe samples in each iteration. Only used when solver=’sgd’ or ‘adam’.
tol : ﬂoat, optional, default 1e-4
Tolerance for the optimization. When the loss or score is not improving by at least tol
for two consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is
considered to be reached and training stops.
learning_rate_init : double, optional, default 0.001
The initial learning rate used. It controls the step-size in updating the weights. Only
used when solver=’sgd’ or ‘adam’.
1774
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
power_t : double, optional, default 0.5
The exponent for inverse scaling learning rate. It is used in updating effective learning
rate when the learning_rate is set to ‘invscaling’. Only used when solver=’sgd’.
verbose : bool, optional, default False
Whether to print progress messages to stdout.
warm_start : bool, optional, default False
When set to True, reuse the solution of the previous call to ﬁt as initialization, otherwise,
just erase the previous solution.
momentum : ﬂoat, default 0.9
Momentum for gradient descent update. Should be between 0 and 1. Only used when
solver=’sgd’.
nesterovs_momentum : boolean, default True
Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum >
0.
early_stopping : bool, default False
Whether to use early stopping to terminate training when validation score is not im-
proving. If set to true, it will automatically set aside 10% of training data as validation
and terminate training when validation score is not improving by at least tol for two
consecutive epochs. Only effective when solver=’sgd’ or ‘adam’
validation_fraction : ﬂoat, optional, default 0.1
The proportion of training data to set aside as validation set for early stopping. Must be
between 0 and 1. Only used if early_stopping is True
beta_1 : ﬂoat, optional, default 0.9
Exponential decay rate for estimates of ﬁrst moment vector in adam, should be in [0,
1). Only used when solver=’adam’
beta_2 : ﬂoat, optional, default 0.999
Exponential decay rate for estimates of second moment vector in adam, should be in [0,
1). Only used when solver=’adam’
epsilon : ﬂoat, optional, default 1e-8
Value for numerical stability in adam. Only used when solver=’adam’
Attributes‘classes_‘ : array or list of array of shape (n_classes,)
Class labels for each output.
‘loss_‘ : ﬂoat
The current loss computed with the loss function.
‘coefs_‘ : list, length n_layers - 1
The ith element in the list represents the weight matrix corresponding to layer i.
‘intercepts_‘ : list, length n_layers - 1
The ith element in the list represents the bias vector corresponding to layer i + 1.
n_iter_ : int,
29.26. sklearn.neural_network: Neural network models
1775
scikit-learn user guide, Release 0.18.2
The number of iterations the solver has ran.
n_layers_ : int
Number of layers.
‘n_outputs_‘ : int
Number of outputs.
‘out_activation_‘ : string
Name of the output activation function.
Notes
MLPClassiﬁer trains iteratively since at each time step the partial derivatives of the loss function with respect to
the model parameters are computed to update the parameters.
It can also have a regularization term added to the loss function that shrinks model parameters to prevent over-
ﬁtting.
This implementation works with data represented as dense numpy arrays or sparse scipy arrays of ﬂoating point
values.
References
Hinton, Geoffrey E.“Connectionist learning procedures.” Artiﬁcial intelligence 40.1 (1989): 185-234.
Glorot, Xavier, and Yoshua Bengio. “Understanding the difﬁculty oftraining deep feedforward neural net-
works.” International Conference on Artiﬁcial Intelligence and Statistics. 2010.
He, Kaiming, et al. “Delving deep into rectiﬁers: Surpassing human-levelperformance on imagenet classi-
ﬁcation.” arXiv preprint arXiv:1502.01852 (2015).
Kingma, Diederik, and Jimmy Ba. “Adam: A method for stochasticoptimization.”
arXiv
preprint
arXiv:1412.6980 (2014).
Methods
fit(X, y)
Fit the model to data matrix X and target y.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the multi-layer perceptron classiﬁer
predict_log_proba(X)
Return the log of probability estimates.
predict_proba(X)
Probability estimates.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(hidden_layer_sizes=(100,
),
activation=’relu’,
solver=’adam’,
alpha=0.0001,
batch_size=’auto’,
learning_rate=’constant’,
learning_rate_init=0.001,
power_t=0.5,
max_iter=200,
shufﬂe=True,
random_state=None,
tol=0.0001,
verbose=False,
warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,
validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
fit(X, y)
1776
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Fit the model to data matrix X and target y.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
The input data.
y : array-like, shape (n_samples,)
The target values.
Returnsself : returns a trained MLP model.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit
Fit the model to data matrix X and target y.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
The input data.
y : array-like, shape (n_samples,)
The target values.
classes : array, shape (n_classes)
Classes across all calls to partial_ﬁt. Can be obtained via np.unique(y_all), where y_all
is the target vector of the entire dataset. This argument is required for the ﬁrst call to
partial_ﬁt and can be omitted in the subsequent calls. Note that y doesn’t need to contain
all labels in classes.
Returnsself : returns a trained MLP model.
predict(X)
Predict using the multi-layer perceptron classiﬁer
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
The input data.
Returnsy : array-like, shape (n_samples,) or (n_samples, n_classes)
The predicted classes.
predict_log_proba(X)
Return the log of probability estimates.
ParametersX : array-like, shape (n_samples, n_features)
The input data.
Returnslog_y_prob : array-like, shape (n_samples, n_classes)
The predicted log-probability of the sample for each class in the model, where classes
are ordered as they are in self.classes_. Equivalent to log(predict_proba(X))
29.26. sklearn.neural_network: Neural network models
1777
scikit-learn user guide, Release 0.18.2
predict_proba(X)
Probability estimates.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
The input data.
Returnsy_prob : array-like, shape (n_samples, n_classes)
The predicted probability of the sample for each class in the model, where classes are
ordered as they are in self.classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.neural_network.MLPClassifier
• Classiﬁer comparison
• Varying regularization in Multi-layer Perceptron
• Compare Stochastic learning strategies for MLPClassiﬁer
• Visualization of MLP weights on MNIST
1778
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.26.3 sklearn.neural_network.MLPRegressor
class sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(100,
),
activa-
tion=’relu’,
solver=’adam’,
alpha=0.0001,
batch_size=’auto’,
learning_rate=’constant’,
learning_rate_init=0.001,
power_t=0.5,
max_iter=200, shufﬂe=True, random_state=None,
tol=0.0001,
verbose=False,
warm_start=False,
momentum=0.9,
nesterovs_momentum=True,
early_stopping=False,
validation_fraction=0.1,
beta_1=0.9, beta_2=0.999, epsilon=1e-08)
Multi-layer Perceptron regressor.
This model optimizes the squared-loss using LBFGS or stochastic gradient descent.
New in version 0.18.
Parametershidden_layer_sizes : tuple, length = n_layers - 2, default (100,)
The ith element represents the number of neurons in the ith hidden layer.
activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’
Activation function for the hidden layer.
•‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x
•‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).
•‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).
•‘relu’, the rectiﬁed linear unit function, returns f(x) = max(0, x)
solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’
The solver for weight optimization.
•‘lbfgs’ is an optimizer in the family of quasi-Newton methods.
•‘sgd’ refers to stochastic gradient descent.
•‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik,
and Jimmy Ba
Note: The default solver ‘adam’ works pretty well on relatively large datasets (with
thousands of training samples or more) in terms of both training time and validation
score. For small datasets, however, ‘lbfgs’ can converge faster and perform better.
alpha : ﬂoat, optional, default 0.0001
L2 penalty (regularization term) parameter.
batch_size : int, optional, default ‘auto’
Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classiﬁer will
not use minibatch. When set to “auto”, batch_size=min(200, n_samples)
learning_rate : {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’
Learning rate schedule for weight updates.
•‘constant’ is a constant learning rate given by ‘learning_rate_init’.
•‘invscaling’ gradually decreases the learning rate learning_rate_ at each time
step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate =
learning_rate_init / pow(t, power_t)
29.26. sklearn.neural_network: Neural network models
1779
scikit-learn user guide, Release 0.18.2
•‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training
loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss
by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is
on, the current learning rate is divided by 5.
Only used when solver=’sgd’.
max_iter : int, optional, default 200
Maximum number of iterations. The solver iterates until convergence (determined by
‘tol’) or this number of iterations.
random_state : int or RandomState, optional, default None
State or seed for random number generator.
shufﬂe : bool, optional, default True
Whether to shufﬂe samples in each iteration. Only used when solver=’sgd’ or ‘adam’.
tol : ﬂoat, optional, default 1e-4
Tolerance for the optimization. When the loss or score is not improving by at least tol
for two consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is
considered to be reached and training stops.
learning_rate_init : double, optional, default 0.001
The initial learning rate used. It controls the step-size in updating the weights. Only
used when solver=’sgd’ or ‘adam’.
power_t : double, optional, default 0.5
The exponent for inverse scaling learning rate. It is used in updating effective learning
rate when the learning_rate is set to ‘invscaling’. Only used when solver=’sgd’.
verbose : bool, optional, default False
Whether to print progress messages to stdout.
warm_start : bool, optional, default False
When set to True, reuse the solution of the previous call to ﬁt as initialization, otherwise,
just erase the previous solution.
momentum : ﬂoat, default 0.9
Momentum for gradient descent update. Should be between 0 and 1. Only used when
solver=’sgd’.
nesterovs_momentum : boolean, default True
Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum >
0.
early_stopping : bool, default False
Whether to use early stopping to terminate training when validation score is not im-
proving. If set to true, it will automatically set aside 10% of training data as validation
and terminate training when validation score is not improving by at least tol for two
consecutive epochs. Only effective when solver=’sgd’ or ‘adam’
validation_fraction : ﬂoat, optional, default 0.1
The proportion of training data to set aside as validation set for early stopping. Must be
between 0 and 1. Only used if early_stopping is True
1780
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
beta_1 : ﬂoat, optional, default 0.9
Exponential decay rate for estimates of ﬁrst moment vector in adam, should be in [0,
1). Only used when solver=’adam’
beta_2 : ﬂoat, optional, default 0.999
Exponential decay rate for estimates of second moment vector in adam, should be in [0,
1). Only used when solver=’adam’
epsilon : ﬂoat, optional, default 1e-8
Value for numerical stability in adam. Only used when solver=’adam’
Attributes‘loss_‘ : ﬂoat
The current loss computed with the loss function.
‘coefs_‘ : list, length n_layers - 1
The ith element in the list represents the weight matrix corresponding to layer i.
‘intercepts_‘ : list, length n_layers - 1
The ith element in the list represents the bias vector corresponding to layer i + 1.
n_iter_ : int,
The number of iterations the solver has ran.
n_layers_ : int
Number of layers.
‘n_outputs_‘ : int
Number of outputs.
‘out_activation_‘ : string
Name of the output activation function.
Notes
MLPRegressor trains iteratively since at each time step the partial derivatives of the loss function with respect
to the model parameters are computed to update the parameters.
It can also have a regularization term added to the loss function that shrinks model parameters to prevent over-
ﬁtting.
This implementation works with data represented as dense and sparse numpy arrays of ﬂoating point values.
References
Hinton, Geoffrey E.“Connectionist learning procedures.” Artiﬁcial intelligence 40.1 (1989): 185-234.
Glorot, Xavier, and Yoshua Bengio. “Understanding the difﬁculty oftraining deep feedforward neural net-
works.” International Conference on Artiﬁcial Intelligence and Statistics. 2010.
He, Kaiming, et al. “Delving deep into rectiﬁers: Surpassing human-levelperformance on imagenet classi-
ﬁcation.” arXiv preprint arXiv:1502.01852 (2015).
Kingma, Diederik, and Jimmy Ba. “Adam: A method for stochasticoptimization.”
arXiv
preprint
arXiv:1412.6980 (2014).
29.26. sklearn.neural_network: Neural network models
1781
scikit-learn user guide, Release 0.18.2
Methods
fit(X, y)
Fit the model to data matrix X and target y.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the multi-layer perceptron model.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(hidden_layer_sizes=(100,
),
activation=’relu’,
solver=’adam’,
alpha=0.0001,
batch_size=’auto’,
learning_rate=’constant’,
learning_rate_init=0.001,
power_t=0.5,
max_iter=200,
shufﬂe=True,
random_state=None,
tol=0.0001,
verbose=False,
warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,
validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
fit(X, y)
Fit the model to data matrix X and target y.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
The input data.
y : array-like, shape (n_samples,)
The target values.
Returnsself : returns a trained MLP model.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
partial_fit
Fit the model to data matrix X and target y.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
The input data.
y : array-like, shape (n_samples,)
The target values.
Returnsself : returns a trained MLP model.
predict(X)
Predict using the multi-layer perceptron model.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
The input data.
Returnsy : array-like, shape (n_samples, n_outputs)
The predicted values.
1782
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.27 sklearn.calibration: Probability Calibration
Calibration of predicted probabilities.
User guide: See the Probability calibration section for further details.
calibration.CalibratedClassifierCV([...])
Probability calibration with isotonic regression or sigmoid.
29.27.1 sklearn.calibration.CalibratedClassiﬁerCV
class sklearn.calibration.CalibratedClassifierCV(base_estimator=None,
method=’sigmoid’, cv=3)
Probability calibration with isotonic regression or sigmoid.
With this class, the base_estimator is ﬁt on the train set of the cross-validation generator and the test set is used
for calibration. The probabilities for each of the folds are then averaged for prediction. In case that cv=”preﬁt”
is passed to __init__, it is assumed that base_estimator has been ﬁtted already and all data is used for calibration.
Note that data for ﬁtting the classiﬁer and for calibrating it must be disjoint.
Read more in the User Guide.
Parametersbase_estimator : instance BaseEstimator
The classiﬁer whose output decision function needs to be calibrated to offer more ac-
curate predict_proba outputs. If cv=preﬁt, the classiﬁer must have been ﬁt already on
data.
29.27. sklearn.calibration: Probability Calibration
1783
scikit-learn user guide, Release 0.18.2
method : ‘sigmoid’ or ‘isotonic’
The method to use for calibration.
Can be ‘sigmoid’ which corresponds to Platt’s
method or ‘isotonic’ which is a non-parametric approach. It is not advised to use iso-
tonic calibration with too few calibration samples (<<1000) since it tends to overﬁt.
Use sigmoids (Platt’s calibration) in this case.
cv : integer, cross-validation generator, iterable or “preﬁt”, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For
integer/None
inputs,
if
y
is
binary
or
multiclass,
sklearn.model_selection.StratifiedKFold is used.
If y is neither
binary nor multiclass, sklearn.model_selection.KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
If “preﬁt” is passed, it is assumed that base_estimator has been ﬁtted already and all
data is used for calibration.
Attributesclasses_ : array, shape (n_classes)
The class labels.
calibrated_classiﬁers_: list (len() equal to cv or 1 if cv == “preﬁt”) :
The list of calibrated classiﬁers, one for each crossvalidation fold, which has been ﬁtted
on all but the validation fold and calibrated on the validation fold.
References
[R1], [R2], [R3], [R4]
Methods
fit(X, y[, sample_weight])
Fit the calibrated model
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict the target of new samples.
predict_proba(X)
Posterior probabilities of classiﬁcation
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(base_estimator=None, method=’sigmoid’, cv=3)
fit(X, y, sample_weight=None)
Fit the calibrated model
ParametersX : array-like, shape (n_samples, n_features)
Training data.
1784
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
y : array-like, shape (n_samples,)
Target values.
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted.
Returnsself : object
Returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict the target of new samples. Can be different from the prediction of the uncalibrated classiﬁer.
ParametersX : array-like, shape (n_samples, n_features)
The samples.
ReturnsC : array, shape (n_samples,)
The predicted class.
predict_proba(X)
Posterior probabilities of classiﬁcation
This function returns posterior probabilities of classiﬁcation according to each class on an array of test
vectors X.
ParametersX : array-like, shape (n_samples, n_features)
The samples.
ReturnsC : array, shape (n_samples, n_classes)
The predicted probas.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
29.27. sklearn.calibration: Probability Calibration
1785
scikit-learn user guide, Release 0.18.2
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.calibration.CalibratedClassifierCV
• Probability calibration of classiﬁers
• Probability Calibration curves
• Probability Calibration for 3-class classiﬁcation
calibration.calibration_curve(y_true,
y_prob)
Compute true and predicted probabilities for a calibration
curve.
29.27.2 sklearn.calibration.calibration_curve
sklearn.calibration.calibration_curve(y_true, y_prob, normalize=False, n_bins=5)
Compute true and predicted probabilities for a calibration curve.
Read more in the User Guide.
Parametersy_true : array, shape (n_samples,)
True targets.
y_prob : array, shape (n_samples,)
Probabilities of the positive class.
normalize : bool, optional, default=False
Whether y_prob needs to be normalized into the bin [0, 1], i.e. is not a proper proba-
bility. If True, the smallest value in y_prob is mapped onto 0 and the largest one onto
1.
n_bins : int
Number of bins. A bigger number requires more data.
Returnsprob_true : array, shape (n_bins,)
The true probability in each bin (fraction of positives).
prob_pred : array, shape (n_bins,)
The mean predicted probability in each bin.
References
Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good Probabilities With Supervised Learning,
in Proceedings of the 22nd International Conference on Machine Learning (ICML). See section 4 (Qualitative
Analysis of Predictions).
1786
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.calibration.calibration_curve
• Probability Calibration curves
• Comparison of Calibration of Classiﬁers
29.28 sklearn.cross_decomposition: Cross decomposition
User guide: See the Cross decomposition section for further details.
cross_decomposition.PLSRegression([...])
PLS regression
cross_decomposition.PLSCanonical([...])
PLSCanonical implements the 2 blocks canonical PLS of
the original Wold algorithm [Tenenhaus 1998] p.204, re-
ferred as PLS-C2A in [Wegelin 2000].
cross_decomposition.CCA([n_components, ...])
CCA Canonical Correlation Analysis.
cross_decomposition.PLSSVD([n_components,
...])
Partial Least Square SVD
29.28.1 sklearn.cross_decomposition.PLSRegression
class sklearn.cross_decomposition.PLSRegression(n_components=2,
scale=True,
max_iter=500, tol=1e-06, copy=True)
PLS regression
PLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1 in case of one dimensional
response. This class inherits from _PLS with mode=”A”, deﬂation_mode=”regression”, norm_y_weights=False
and algorithm=”nipals”.
Read more in the User Guide.
Parametersn_components : int, (default 2)
Number of components to keep.
scale : boolean, (default True)
whether to scale the data
max_iter : an integer, (default 500)
the maximum number of iterations of the NIPALS inner loop (used only if algo-
rithm=”nipals”)
tol : non-negative real
Tolerance used in the iterative algorithm default 1e-06.
copy : boolean, default True
Whether the deﬂation should be done on a copy. Let the default value to True unless
you don’t care about side effect
Attributesx_weights_ : array, [p, n_components]
X block weights vectors.
y_weights_ : array, [q, n_components]
Y block weights vectors.
29.28. sklearn.cross_decomposition: Cross decomposition
1787
scikit-learn user guide, Release 0.18.2
x_loadings_ : array, [p, n_components]
X block loadings vectors.
y_loadings_ : array, [q, n_components]
Y block loadings vectors.
x_scores_ : array, [n_samples, n_components]
X scores.
y_scores_ : array, [n_samples, n_components]
Y scores.
x_rotations_ : array, [p, n_components]
X block to latents rotations.
y_rotations_ : array, [q, n_components]
Y block to latents rotations.
coef_: array, [p, q] :
The coefﬁcients of the linear model: Y = X coef_ + Err
n_iter_ : array-like
Number of iterations of the NIPALS inner loop for each component.
Notes
Matrices:
T: x_scores_
U: y_scores_
W: x_weights_
C: y_weights_
P: x_loadings_
Q: y_loadings__
Are computed such that:
X = T P.T + Err and Y = U Q.T + Err
T[:, k] = Xk W[:, k] for k in range(n_components)
U[:, k] = Yk C[:, k] for k in range(n_components)
x_rotations_ = W (P.T W)^(-1)
y_rotations_ = C (Q.T C)^(-1)
where Xk and Yk are residual matrices at iteration k.
Slides explaining PLS <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>
For each component k, ﬁnd weights u, v that optimizes:
max corr(Xk u,Yk v) * std(Xk u)
std(Yk u), such that |u| = 1
Note that it maximizes both the correlations between the scores and the intra-block variances.
The residual matrix of X (Xk+1) block is obtained by the deﬂation on the current X score: x_score.
The residual matrix of Y (Yk+1) block is obtained by deﬂation on the current X score. This performs the PLS
regression known as PLS2. This mode is prediction oriented.
1788
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
This implementation provides the same results that 3 PLS packages provided in the R language (R-project):
•“mixOmics” with function pls(X, Y, mode = “regression”)
•“plspm ” with function plsreg2(X, Y)
•“pls” with function oscorespls.ﬁt(X, Y)
References
Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case.
Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.
In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions
Technic.
Examples
>>> from sklearn.cross_decomposition import PLSRegression
>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]
>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
>>> pls2 = PLSRegression(n_components=2)
>>> pls2.fit(X, Y)
...
PLSRegression(copy=True, max_iter=500, n_components=2, scale=True,
tol=1e-06)
>>> Y_pred = pls2.predict(X)
Methods
fit(X, Y)
Fit model to data.
fit_transform(X[, y])
Learn and apply the dimension reduction on the train
data.
get_params([deep])
Get parameters for this estimator.
predict(X[, copy])
Apply the dimension reduction learned on the train data.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, Y, copy])
Apply the dimension reduction learned on the train data.
__init__(n_components=2, scale=True, max_iter=500, tol=1e-06, copy=True)
fit(X, Y)
Fit model to data.
ParametersX : array-like, shape = [n_samples, n_features]
Training vectors, where n_samples in the number of samples and n_features is the num-
ber of predictors.
Y : array-like of response, shape = [n_samples, n_targets]
Target vectors, where n_samples in the number of samples and n_targets is the number
of response variables.
29.28. sklearn.cross_decomposition: Cross decomposition
1789
scikit-learn user guide, Release 0.18.2
fit_transform(X, y=None, **ﬁt_params)
Learn and apply the dimension reduction on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
Y : array-like of response, shape = [n_samples, q], optional
Training vectors, where n_samples in the number of samples and q is the number of
response variables.
copy : boolean, default True
Whether to copy X and Y, or perform in-place normalization.
Returnsx_scores if Y is not given, (x_scores, y_scores) otherwise. :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X, copy=True)
Apply the dimension reduction learned on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
copy : boolean, default True
Whether to copy X and Y, or perform in-place normalization.
Notes
This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
1790
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, Y=None, copy=True)
Apply the dimension reduction learned on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
Y : array-like of response, shape = [n_samples, q], optional
Training vectors, where n_samples in the number of samples and q is the number of
response variables.
copy : boolean, default True
Whether to copy X and Y, or perform in-place normalization.
Returnsx_scores if Y is not given, (x_scores, y_scores) otherwise. :
Examples using sklearn.cross_decomposition.PLSRegression
• Compare cross decomposition methods
29.28.2 sklearn.cross_decomposition.PLSCanonical
class sklearn.cross_decomposition.PLSCanonical(n_components=2,
scale=True,
algo-
rithm=’nipals’, max_iter=500, tol=1e-06,
copy=True)
PLSCanonical implements the 2 blocks canonical PLS of the original Wold algorithm [Tenenhaus 1998] p.204,
referred as PLS-C2A in [Wegelin 2000].
This class inherits from PLS with mode=”A” and deﬂation_mode=”canonical”, norm_y_weights=True and al-
gorithm=”nipals”, but svd should provide similar results up to numerical errors.
Read more in the User Guide.
Parametersscale : boolean, scale data? (default True)
algorithm : string, “nipals” or “svd”
The algorithm used to estimate the weights. It will be called n_components times, i.e.
once for each iteration of the outer loop.
max_iter : an integer, (default 500)
the maximum number of iterations of the NIPALS inner loop (used only if algo-
rithm=”nipals”)
29.28. sklearn.cross_decomposition: Cross decomposition
1791
scikit-learn user guide, Release 0.18.2
tol : non-negative real, default 1e-06
the tolerance used in the iterative algorithm
copy : boolean, default True
Whether the deﬂation should be done on a copy. Let the default value to True unless
you don’t care about side effect
n_components : int, number of components to keep. (default 2).
Attributesx_weights_ : array, shape = [p, n_components]
X block weights vectors.
y_weights_ : array, shape = [q, n_components]
Y block weights vectors.
x_loadings_ : array, shape = [p, n_components]
X block loadings vectors.
y_loadings_ : array, shape = [q, n_components]
Y block loadings vectors.
x_scores_ : array, shape = [n_samples, n_components]
X scores.
y_scores_ : array, shape = [n_samples, n_components]
Y scores.
x_rotations_ : array, shape = [p, n_components]
X block to latents rotations.
y_rotations_ : array, shape = [q, n_components]
Y block to latents rotations.
n_iter_ : array-like
Number of iterations of the NIPALS inner loop for each component. Not useful if the
algorithm provided is “svd”.
See also:
CCA, PLSSVD
Notes
Matrices:
T: x_scores_
U: y_scores_
W: x_weights_
C: y_weights_
P: x_loadings_
Q: y_loadings__
Are computed such that:
1792
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
X = T P.T + Err and Y = U Q.T + Err
T[:, k] = Xk W[:, k] for k in range(n_components)
U[:, k] = Yk C[:, k] for k in range(n_components)
x_rotations_ = W (P.T W)^(-1)
y_rotations_ = C (Q.T C)^(-1)
where Xk and Yk are residual matrices at iteration k.
Slides explaining PLS <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>
For each component k, ﬁnd weights u, v that optimize:
max corr(Xk u, Yk v) * std(Xk u) std(Yk u), such that ``|u| = |v| = 1``
Note that it maximizes both the correlations between the scores and the intra-block variances.
The residual matrix of X (Xk+1) block is obtained by the deﬂation on the current X score: x_score.
The residual matrix of Y (Yk+1) block is obtained by deﬂation on the current Y score. This performs a canonical
symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.
This implementation provides the same results that the “plspm” package provided in the R language (R-
project), using the function plsca(X, Y). Results are equal or collinear with the function pls(...,mode =
"canonical") of the “mixOmics” package. The difference relies in the fact that mixOmics implementation
does not exactly implement the Wold algorithm since it does not normalize y_weights to one.
References
Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case.
Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.
Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.
Examples
>>> from sklearn.cross_decomposition import PLSCanonical
>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]
>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
>>> plsca = PLSCanonical(n_components=2)
>>> plsca.fit(X, Y)
...
PLSCanonical(algorithm='nipals', copy=True, max_iter=500, n_components=2,
scale=True, tol=1e-06)
>>> X_c, Y_c = plsca.transform(X, Y)
Methods
fit(X, Y)
Fit model to data.
fit_transform(X[, y])
Learn and apply the dimension reduction on the train
data.
get_params([deep])
Get parameters for this estimator.
predict(X[, copy])
Apply the dimension reduction learned on the train data.
Continued on next page
29.28. sklearn.cross_decomposition: Cross decomposition
1793
scikit-learn user guide, Release 0.18.2
Table 29.212 – continued from previous page
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, Y, copy])
Apply the dimension reduction learned on the train data.
__init__(n_components=2, scale=True, algorithm=’nipals’, max_iter=500, tol=1e-06, copy=True)
fit(X, Y)
Fit model to data.
ParametersX : array-like, shape = [n_samples, n_features]
Training vectors, where n_samples in the number of samples and n_features is the num-
ber of predictors.
Y : array-like of response, shape = [n_samples, n_targets]
Target vectors, where n_samples in the number of samples and n_targets is the number
of response variables.
fit_transform(X, y=None, **ﬁt_params)
Learn and apply the dimension reduction on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
Y : array-like of response, shape = [n_samples, q], optional
Training vectors, where n_samples in the number of samples and q is the number of
response variables.
copy : boolean, default True
Whether to copy X and Y, or perform in-place normalization.
Returnsx_scores if Y is not given, (x_scores, y_scores) otherwise. :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X, copy=True)
Apply the dimension reduction learned on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
copy : boolean, default True
Whether to copy X and Y, or perform in-place normalization.
1794
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, Y=None, copy=True)
Apply the dimension reduction learned on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
Y : array-like of response, shape = [n_samples, q], optional
Training vectors, where n_samples in the number of samples and q is the number of
response variables.
copy : boolean, default True
Whether to copy X and Y, or perform in-place normalization.
Returnsx_scores if Y is not given, (x_scores, y_scores) otherwise. :
Examples using sklearn.cross_decomposition.PLSCanonical
• Compare cross decomposition methods
29.28. sklearn.cross_decomposition: Cross decomposition
1795
scikit-learn user guide, Release 0.18.2
29.28.3 sklearn.cross_decomposition.CCA
class sklearn.cross_decomposition.CCA(n_components=2, scale=True, max_iter=500, tol=1e-06,
copy=True)
CCA Canonical Correlation Analysis.
CCA inherits from PLS with mode=”B” and deﬂation_mode=”canonical”.
Read more in the User Guide.
Parametersn_components : int, (default 2).
number of components to keep.
scale : boolean, (default True)
whether to scale the data?
max_iter : an integer, (default 500)
the maximum number of iterations of the NIPALS inner loop
tol : non-negative real, default 1e-06.
the tolerance used in the iterative algorithm
copy : boolean
Whether the deﬂation be done on a copy. Let the default value to True unless you don’t
care about side effects
Attributesx_weights_ : array, [p, n_components]
X block weights vectors.
y_weights_ : array, [q, n_components]
Y block weights vectors.
x_loadings_ : array, [p, n_components]
X block loadings vectors.
y_loadings_ : array, [q, n_components]
Y block loadings vectors.
x_scores_ : array, [n_samples, n_components]
X scores.
y_scores_ : array, [n_samples, n_components]
Y scores.
x_rotations_ : array, [p, n_components]
X block to latents rotations.
y_rotations_ : array, [q, n_components]
Y block to latents rotations.
n_iter_ : array-like
Number of iterations of the NIPALS inner loop for each component.
See also:
PLSCanonical, PLSSVD
1796
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
For each component k, ﬁnd the weights u, v that maximizes max corr(Xk u, Yk v), such that |u| = |v| =
1
Note that it maximizes only the correlations between the scores.
The residual matrix of X (Xk+1) block is obtained by the deﬂation on the current X score: x_score.
The residual matrix of Y (Yk+1) block is obtained by deﬂation on the current Y score.
References
Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case.
Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.
In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions
Technic.
Examples
>>> from sklearn.cross_decomposition import CCA
>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]
>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
>>> cca = CCA(n_components=1)
>>> cca.fit(X, Y)
...
CCA(copy=True, max_iter=500, n_components=1, scale=True, tol=1e-06)
>>> X_c, Y_c = cca.transform(X, Y)
Methods
fit(X, Y)
Fit model to data.
fit_transform(X[, y])
Learn and apply the dimension reduction on the train
data.
get_params([deep])
Get parameters for this estimator.
predict(X[, copy])
Apply the dimension reduction learned on the train data.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, Y, copy])
Apply the dimension reduction learned on the train data.
__init__(n_components=2, scale=True, max_iter=500, tol=1e-06, copy=True)
fit(X, Y)
Fit model to data.
ParametersX : array-like, shape = [n_samples, n_features]
Training vectors, where n_samples in the number of samples and n_features is the num-
ber of predictors.
Y : array-like of response, shape = [n_samples, n_targets]
29.28. sklearn.cross_decomposition: Cross decomposition
1797
scikit-learn user guide, Release 0.18.2
Target vectors, where n_samples in the number of samples and n_targets is the number
of response variables.
fit_transform(X, y=None, **ﬁt_params)
Learn and apply the dimension reduction on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
Y : array-like of response, shape = [n_samples, q], optional
Training vectors, where n_samples in the number of samples and q is the number of
response variables.
copy : boolean, default True
Whether to copy X and Y, or perform in-place normalization.
Returnsx_scores if Y is not given, (x_scores, y_scores) otherwise. :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X, copy=True)
Apply the dimension reduction learned on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
copy : boolean, default True
Whether to copy X and Y, or perform in-place normalization.
Notes
This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
1798
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, Y=None, copy=True)
Apply the dimension reduction learned on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
Y : array-like of response, shape = [n_samples, q], optional
Training vectors, where n_samples in the number of samples and q is the number of
response variables.
copy : boolean, default True
Whether to copy X and Y, or perform in-place normalization.
Returnsx_scores if Y is not given, (x_scores, y_scores) otherwise. :
Examples using sklearn.cross_decomposition.CCA
• Multilabel classiﬁcation
• Compare cross decomposition methods
29.28.4 sklearn.cross_decomposition.PLSSVD
class sklearn.cross_decomposition.PLSSVD(n_components=2, scale=True, copy=True)
Partial Least Square SVD
Simply perform a svd on the crosscovariance matrix: X’Y There are no iterative deﬂation here.
Read more in the User Guide.
Parametersn_components : int, default 2
Number of components to keep.
scale : boolean, default True
Whether to scale X and Y.
copy : boolean, default True
Whether to copy X and Y, or perform in-place computations.
Attributesx_weights_ : array, [p, n_components]
29.28. sklearn.cross_decomposition: Cross decomposition
1799
scikit-learn user guide, Release 0.18.2
X block weights vectors.
y_weights_ : array, [q, n_components]
Y block weights vectors.
x_scores_ : array, [n_samples, n_components]
X scores.
y_scores_ : array, [n_samples, n_components]
Y scores.
See also:
PLSCanonical, CCA
Methods
fit(X, Y)
fit_transform(X[, y])
Learn and apply the dimension reduction on the train
data.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, Y])
Apply the dimension reduction learned on the train data.
__init__(n_components=2, scale=True, copy=True)
fit_transform(X, y=None, **ﬁt_params)
Learn and apply the dimension reduction on the train data.
ParametersX : array-like of predictors, shape = [n_samples, p]
Training vectors, where n_samples in the number of samples and p is the number of
predictors.
Y : array-like of response, shape = [n_samples, q], optional
Training vectors, where n_samples in the number of samples and q is the number of
response variables.
Returnsx_scores if Y is not given, (x_scores, y_scores) otherwise. :
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
1800
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsself :
transform(X, Y=None)
Apply the dimension reduction learned on the train data.
29.29 sklearn.pipeline: Pipeline
The sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and
estimators.
pipeline.Pipeline(steps)
Pipeline of transforms with a ﬁnal estimator.
pipeline.FeatureUnion(transformer_list[, ...])
Concatenates results of multiple transformer objects.
29.29.1 sklearn.pipeline.Pipeline
class sklearn.pipeline.Pipeline(steps)
Pipeline of transforms with a ﬁnal estimator.
Sequentially apply a list of transforms and a ﬁnal estimator. Intermediate steps of the pipeline must be ‘trans-
forms’, that is, they must implement ﬁt and transform methods. The ﬁnal estimator only needs to implement
ﬁt.
The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting differ-
ent parameters. For this, it enables setting parameters of the various steps using their names and the parameter
name separated by a ‘__’, as in the example below. A step’s estimator may be replaced entirely by setting the
parameter with its name to another estimator, or a transformer removed by setting to None.
Read more in the User Guide.
Parameterssteps : list
List of (name, transform) tuples (implementing ﬁt/transform) that are chained, in the
order in which they are chained, with the last object an estimator.
Attributesnamed_steps : dict
Read-only attribute to access any step parameter by user given name. Keys are step
names and values are steps parameters.
Examples
>>> from sklearn import svm
>>> from sklearn.datasets import samples_generator
>>> from sklearn.feature_selection import SelectKBest
>>> from sklearn.feature_selection import f_regression
>>> from sklearn.pipeline import Pipeline
>>> # generate some data to play with
>>> X, y = samples_generator.make_classification(
...
n_informative=5, n_redundant=0, random_state=42)
>>> # ANOVA SVM-C
>>> anova_filter = SelectKBest(f_regression, k=5)
>>> clf = svm.SVC(kernel='linear')
>>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])
>>> # You can set the parameters using the names issued
>>> # For instance, fit using a k of 10 in the SelectKBest
29.29. sklearn.pipeline: Pipeline
1801
scikit-learn user guide, Release 0.18.2
>>> # and a parameter 'C' of the svm
>>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
...
Pipeline(steps=[...])
>>> prediction = anova_svm.predict(X)
>>> anova_svm.score(X, y)
0.829...
>>> # getting the selected features chosen by anova_filter
>>> anova_svm.named_steps['anova'].get_support()
...
array([False, False,
True,
True, False, False, True,
True, False,
True,
False,
True,
True, False, True,
False, True, True,
False, False], dtype=bool)
Methods
decision_function(\*args, \*\*kwargs)
Apply transforms, and decision_function of the ﬁnal es-
timator
fit(X[, y])
Fit the model
fit_predict(\*args, \*\*kwargs)
Applies ﬁt_predict of last step in pipeline after trans-
forms.
fit_transform(X[, y])
Fit the model and transform with the ﬁnal estimator
get_params([deep])
Get parameters for this estimator.
predict(\*args, \*\*kwargs)
Apply transforms to the data, and predict with the ﬁnal
estimator
predict_log_proba(\*args, \*\*kwargs)
Apply transforms, and predict_log_proba of the ﬁnal es-
timator
predict_proba(\*args, \*\*kwargs)
Apply transforms, and predict_proba of the ﬁnal estima-
tor
score(\*args, \*\*kwargs)
Apply transforms, and score with the ﬁnal estimator
set_params(\*\*kwargs)
Set the parameters of this estimator.
__init__(steps)
decision_function(*args, **kwargs)
Apply transforms, and decision_function of the ﬁnal estimator
ParametersX : iterable
Data to predict on. Must fulﬁll input requirements of ﬁrst step of the pipeline.
Returnsy_score : array-like, shape = [n_samples, n_classes]
fit(X, y=None, **ﬁt_params)
Fit the model
Fit all the transforms one after the other and transform the data, then ﬁt the transformed data using the ﬁnal
estimator.
ParametersX : iterable
Training data. Must fulﬁll input requirements of ﬁrst step of the pipeline.
y : iterable, default=None
Training targets. Must fulﬁll label requirements for all steps of the pipeline.
1802
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
**ﬁt_params : dict of string -> object
Parameters passed to the fit method of each step, where each parameter name is pre-
ﬁxed such that parameter p for step s has key s__p.
Returnsself : Pipeline
This estimator
fit_predict(*args, **kwargs)
Applies ﬁt_predict of last step in pipeline after transforms.
Applies ﬁt_transforms of a pipeline to the data, followed by the ﬁt_predict method of the ﬁnal estimator in
the pipeline. Valid only if the ﬁnal estimator implements ﬁt_predict.
ParametersX : iterable
Training data. Must fulﬁll input requirements of ﬁrst step of the pipeline.
y : iterable, default=None
Training targets. Must fulﬁll label requirements for all steps of the pipeline.
**ﬁt_params : dict of string -> object
Parameters passed to the fit method of each step, where each parameter name is pre-
ﬁxed such that parameter p for step s has key s__p.
Returnsy_pred : array-like
fit_transform(X, y=None, **ﬁt_params)
Fit the model and transform with the ﬁnal estimator
Fits all the transforms one after the other and transforms the data, then uses ﬁt_transform on transformed
data with the ﬁnal estimator.
ParametersX : iterable
Training data. Must fulﬁll input requirements of ﬁrst step of the pipeline.
y : iterable, default=None
Training targets. Must fulﬁll label requirements for all steps of the pipeline.
**ﬁt_params : dict of string -> object
Parameters passed to the fit method of each step, where each parameter name is pre-
ﬁxed such that parameter p for step s has key s__p.
ReturnsXt : array-like, shape = [n_samples, n_transformed_features]
Transformed samples
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform
Apply inverse transformations in reverse order
29.29. sklearn.pipeline: Pipeline
1803
scikit-learn user guide, Release 0.18.2
All estimators in the pipeline must support inverse_transform.
ParametersXt : array-like, shape = [n_samples, n_transformed_features]
Data samples, where n_samples is the number of samples and n_features is
the number of features.
Must fulﬁll input requirements of last step of pipeline’s
inverse_transform method.
ReturnsXt : array-like, shape = [n_samples, n_features]
predict(*args, **kwargs)
Apply transforms to the data, and predict with the ﬁnal estimator
ParametersX : iterable
Data to predict on. Must fulﬁll input requirements of ﬁrst step of the pipeline.
Returnsy_pred : array-like
predict_log_proba(*args, **kwargs)
Apply transforms, and predict_log_proba of the ﬁnal estimator
ParametersX : iterable
Data to predict on. Must fulﬁll input requirements of ﬁrst step of the pipeline.
Returnsy_score : array-like, shape = [n_samples, n_classes]
predict_proba(*args, **kwargs)
Apply transforms, and predict_proba of the ﬁnal estimator
ParametersX : iterable
Data to predict on. Must fulﬁll input requirements of ﬁrst step of the pipeline.
Returnsy_proba : array-like, shape = [n_samples, n_classes]
score(*args, **kwargs)
Apply transforms, and score with the ﬁnal estimator
ParametersX : iterable
Data to predict on. Must fulﬁll input requirements of ﬁrst step of the pipeline.
y : iterable, default=None
Targets used for scoring. Must fulﬁll label requirements for all steps of the pipeline.
Returnsscore : ﬂoat
set_params(**kwargs)
Set the parameters of this estimator.
Valid parameter keys can be listed with get_params().
Returnsself :
transform
Apply transforms, and transform with the ﬁnal estimator
This also works where ﬁnal estimator is None: all prior transformations are applied.
ParametersX : iterable
Data to transform. Must fulﬁll input requirements of ﬁrst step of the pipeline.
ReturnsXt : array-like, shape = [n_samples, n_transformed_features]
1804
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.pipeline.Pipeline
• Concatenating multiple feature extraction methods
• Feature Union with Heterogeneous Data Sources
• Imputing missing values before building an estimator
• Selecting dimensionality reduction with Pipeline and GridSearchCV
• Pipelining: chaining a PCA and a logistic regression
• Explicit feature map approximation for RBF kernels
• Feature agglomeration vs. univariate selection
• Sample pipeline for text feature extraction and evaluation
• Underﬁtting vs. Overﬁtting
• Restricted Boltzmann Machine features for digit classiﬁcation
• SVM-Anova: SVM with univariate feature selection
• Classiﬁcation of text documents using sparse features
29.29.2 sklearn.pipeline.FeatureUnion
class sklearn.pipeline.FeatureUnion(transformer_list, n_jobs=1, transformer_weights=None)
Concatenates results of multiple transformer objects.
This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results.
This is useful to combine several feature extraction mechanisms into a single transformer.
Parameters of the transformers may be set using its name and the parameter name separated by a ‘__’. A
transformer may be replaced entirely by setting the parameter with its name to another transformer, or removed
by setting to None.
Read more in the User Guide.
Parameterstransformer_list: list of (string, transformer) tuples :
List of transformer objects to be applied to the data. The ﬁrst half of each tuple is the
name of the transformer.
n_jobs: int, optional :
Number of jobs to run in parallel (default 1).
transformer_weights: dict, optional :
Multiplicative weights for features per transformer. Keys are transformer names, values
the weights.
Methods
fit(X[, y])
Fit all transformers using X.
fit_transform(X[, y])
Fit all transformers, transform the data and concatenate
results.
get_feature_names()
Get feature names from all transformers.
Continued on next page
29.29. sklearn.pipeline: Pipeline
1805
scikit-learn user guide, Release 0.18.2
Table 29.217 – continued from previous page
get_params([deep])
Get parameters for this estimator.
set_params(\*\*kwargs)
Set the parameters of this estimator.
transform(X)
Transform X separately by each transformer, concate-
nate results.
__init__(transformer_list, n_jobs=1, transformer_weights=None)
fit(X, y=None)
Fit all transformers using X.
ParametersX : iterable or array-like, depending on transformers
Input data, used to ﬁt transformers.
y : array-like, shape (n_samples, ...), optional
Targets for supervised learning.
Returnsself : FeatureUnion
This estimator
fit_transform(X, y=None, **ﬁt_params)
Fit all transformers, transform the data and concatenate results.
ParametersX : iterable or array-like, depending on transformers
Input data to be transformed.
y : array-like, shape (n_samples, ...), optional
Targets for supervised learning.
ReturnsX_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
hstack of results of transformers. sum_n_components is the sum of n_components (out-
put dimension) over transformers.
get_feature_names()
Get feature names from all transformers.
Returnsfeature_names : list of strings
Names of the features produced by transform.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep: boolean, optional :
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**kwargs)
Set the parameters of this estimator.
Valid parameter keys can be listed with get_params().
Returnsself :
1806
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
transform(X)
Transform X separately by each transformer, concatenate results.
ParametersX : iterable or array-like, depending on transformers
Input data to be transformed.
ReturnsX_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
hstack of results of transformers. sum_n_components is the sum of n_components (out-
put dimension) over transformers.
Examples using sklearn.pipeline.FeatureUnion
• Concatenating multiple feature extraction methods
• Feature Union with Heterogeneous Data Sources
pipeline.make_pipeline(\*steps)
Construct a Pipeline from the given estimators.
pipeline.make_union(\*transformers)
Construct a FeatureUnion from the given transformers.
29.29.3 sklearn.pipeline.make_pipeline
sklearn.pipeline.make_pipeline(*steps)
Construct a Pipeline from the given estimators.
This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators.
Instead, their names will be set to the lowercase of their types automatically.
Returnsp : Pipeline
Examples
>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.preprocessing import StandardScaler
>>> make_pipeline(StandardScaler(), GaussianNB(priors=None))
...
Pipeline(steps=[('standardscaler',
StandardScaler(copy=True, with_mean=True, with_std=True)),
('gaussiannb', GaussianNB(priors=None))])
Examples using sklearn.pipeline.make_pipeline
• Feature transformations with ensembles of trees
• Pipeline Anova SVM
• Polynomial interpolation
• Robust linear estimator ﬁtting
• Using FunctionTransformer to select columns
• Clustering text documents using k-means
29.29. sklearn.pipeline: Pipeline
1807
scikit-learn user guide, Release 0.18.2
29.29.4 sklearn.pipeline.make_union
sklearn.pipeline.make_union(*transformers)
Construct a FeatureUnion from the given transformers.
This is a shorthand for the FeatureUnion constructor; it does not require, and does not permit, naming the
transformers. Instead, they will be given names automatically based on their types. It also does not allow
weighting.
Returnsf : FeatureUnion
Examples
>>> from sklearn.decomposition import PCA, TruncatedSVD
>>> make_union(PCA(), TruncatedSVD())
FeatureUnion(n_jobs=1,
transformer_list=[('pca',
PCA(copy=True, iterated_power='auto',
n_components=None, random_state=None,
svd_solver='auto', tol=0.0, whiten=False)),
('truncatedsvd',
TruncatedSVD(algorithm='randomized',
n_components=2, n_iter=5,
random_state=None, tol=0.0))],
transformer_weights=None)
29.30 sklearn.preprocessing: Preprocessing and Normalization
The sklearn.preprocessing module includes scaling, centering, normalization, binarization and imputation
methods.
User guide: See the Preprocessing data section for further details.
preprocessing.Binarizer([threshold, copy])
Binarize data (set feature values to 0 or 1) according to a
threshold
preprocessing.FunctionTransformer([func,
...])
Constructs a transformer from an arbitrary callable.
preprocessing.Imputer([missing_values, ...])
Imputation transformer for completing missing values.
preprocessing.KernelCenterer
Center a kernel matrix
preprocessing.LabelBinarizer([neg_label, ...])
Binarize labels in a one-vs-all fashion
preprocessing.LabelEncoder
Encode labels with value between 0 and n_classes-1.
preprocessing.MultiLabelBinarizer([classes,
...])
Transform between iterable of iterables and a multilabel
format
preprocessing.MaxAbsScaler([copy])
Scale each feature by its maximum absolute value.
preprocessing.MinMaxScaler([feature_range,
copy])
Transforms features by scaling each feature to a given
range.
preprocessing.Normalizer([norm, copy])
Normalize samples individually to unit norm.
preprocessing.OneHotEncoder([n_values, ...])
Encode categorical integer features using a one-hot aka
one-of-K scheme.
preprocessing.PolynomialFeatures([degree,
...])
Generate polynomial and interaction features.
Continued on next page
1808
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.219 – continued from previous page
preprocessing.RobustScaler([with_centering,
...])
Scale features using statistics that are robust to outliers.
preprocessing.StandardScaler([copy, ...])
Standardize features by removing the mean and scaling to
unit variance
29.30.1 sklearn.preprocessing.Binarizer
class sklearn.preprocessing.Binarizer(threshold=0.0, copy=True)
Binarize data (set feature values to 0 or 1) according to a threshold
Values greater than the threshold map to 1, while values less than or equal to the threshold map to 0. With the
default threshold of 0, only positive values map to 1.
Binarization is a common operation on text count data where the analyst can decide to only consider the presence
or absence of a feature rather than a quantiﬁed number of occurrences for instance.
It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled
using the Bernoulli distribution in a Bayesian setting).
Read more in the User Guide.
Parametersthreshold : ﬂoat, optional (0.0 by default)
Feature values below or equal to this are replaced by 0, above it by 1. Threshold may
not be less than 0 for operations on sparse matrices.
copy : boolean, optional, default True
set to False to perform inplace binarization and avoid a copy (if the input is already a
numpy array or a scipy.sparse CSR matrix).
See also:
binarizeEquivalent function without the object oriented API.
Notes
If the input is a sparse matrix, only the non-zero values are subject to update by the Binarizer class.
This estimator is stateless (besides constructor parameters), the ﬁt method does nothing but is useful when used
in a pipeline.
Methods
fit(X[, y])
Do nothing and return the estimator unchanged
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y, copy])
Binarize each element of X
__init__(threshold=0.0, copy=True)
fit(X, y=None)
Do nothing and return the estimator unchanged
29.30. sklearn.preprocessing: Preprocessing and Normalization
1809
scikit-learn user guide, Release 0.18.2
This method is just there to implement the usual API and hence work in pipelines.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None, copy=None)
Binarize each element of X
ParametersX : {array-like, sparse matrix}, shape [n_samples, n_features]
The data to binarize, element by element. scipy.sparse matrices should be in CSR format
to avoid an un-necessary copy.
29.30.2 sklearn.preprocessing.FunctionTransformer
class sklearn.preprocessing.FunctionTransformer(func=None,
inverse_func=None,
val-
idate=True,
accept_sparse=False,
pass_y=False,
kw_args=None,
inv_kw_args=None)
Constructs a transformer from an arbitrary callable.
A FunctionTransformer forwards its X (and optionally y) arguments to a user-deﬁned function or function
object and returns the result of this function. This is useful for stateless transformations such as taking the log
of frequencies, doing custom scaling, etc.
A FunctionTransformer will not do any checks on its function’s output.
Note: If a lambda is used as the function, then the resulting transformer will not be pickleable.
New in version 0.17.
1810
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Read more in the User Guide.
Parametersfunc : callable, optional default=None
The callable to use for the transformation. This will be passed the same arguments
as transform, with args and kwargs forwarded. If func is None, then func will be the
identity function.
inverse_func : callable, optional default=None
The callable to use for the inverse transformation. This will be passed the same argu-
ments as inverse transform, with args and kwargs forwarded. If inverse_func is None,
then inverse_func will be the identity function.
validate : bool, optional default=True
Indicate that the input X array should be checked before calling func. If validate is false,
there will be no input validation. If it is true, then X will be converted to a 2-dimensional
NumPy array or sparse matrix. If this conversion is not possible or X contains NaN or
inﬁnity, an exception is raised.
accept_sparse : boolean, optional
Indicate that func accepts a sparse matrix as input. If validate is False, this has no effect.
Otherwise, if accept_sparse is false, sparse matrix inputs will cause an exception to be
raised.
pass_y : bool, optional default=False
Indicate that transform should forward the y argument to the inner callable.
kw_args : dict, optional
Dictionary of additional keyword arguments to pass to func.
inv_kw_args : dict, optional
Dictionary of additional keyword arguments to pass to inverse_func.
Methods
fit(X[, y])
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X[, y])
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
__init__(func=None,
inverse_func=None,
validate=True,
accept_sparse=False,
pass_y=False,
kw_args=None, inv_kw_args=None)
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
29.30. sklearn.preprocessing: Preprocessing and Normalization
1811
scikit-learn user guide, Release 0.18.2
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.preprocessing.FunctionTransformer
• Using FunctionTransformer to select columns
29.30.3 sklearn.preprocessing.Imputer
class sklearn.preprocessing.Imputer(missing_values=’NaN’,
strategy=’mean’,
axis=0,
ver-
bose=0, copy=True)
Imputation transformer for completing missing values.
Read more in the User Guide.
Parametersmissing_values : integer or “NaN”, optional (default=”NaN”)
The placeholder for the missing values. All occurrences of missing_values will be im-
puted. For missing values encoded as np.nan, use the string value “NaN”.
strategy : string, optional (default=”mean”)
The imputation strategy.
•If “mean”, then replace missing values using the mean along the axis.
•If “median”, then replace missing values using the median along the axis.
•If “most_frequent”, then replace missing using the most frequent value along the axis.
axis : integer, optional (default=0)
The axis along which to impute.
•If axis=0, then impute along columns.
•If axis=1, then impute along rows.
verbose : integer, optional (default=0)
Controls the verbosity of the imputer.
1812
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
copy : boolean, optional (default=True)
If True, a copy of X will be created. If False, imputation will be done in-place whenever
possible. Note that, in the following cases, a new copy will always be made, even if
copy=False:
•If X is not an array of ﬂoating values;
•If X is sparse and missing_values=0;
•If axis=0 and X is encoded as a CSR matrix;
•If axis=1 and X is encoded as a CSC matrix.
Attributesstatistics_ : array of shape (n_features,)
The imputation ﬁll value for each feature if axis == 0.
Notes
•When axis=0, columns which only contained missing values at ﬁt are discarded upon transform.
•When axis=1, an exception is raised if there are rows for which it is not possible to ﬁll in the missing
values (e.g., because they only contain missing values).
Methods
fit(X[, y])
Fit the imputer on X.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Impute all missing values in X.
__init__(missing_values=’NaN’, strategy=’mean’, axis=0, verbose=0, copy=True)
fit(X, y=None)
Fit the imputer on X.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Input data, where n_samples is the number of samples and n_features is the
number of features.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
29.30. sklearn.preprocessing: Preprocessing and Normalization
1813
scikit-learn user guide, Release 0.18.2
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Impute all missing values in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
The input data to complete.
Examples using sklearn.preprocessing.Imputer
• Imputing missing values before building an estimator
29.30.4 sklearn.preprocessing.KernelCenterer
class sklearn.preprocessing.KernelCenterer
Center a kernel matrix
Let K(x, z) be a kernel deﬁned by phi(x)^T phi(z), where phi is a function mapping x to a Hilbert space.
KernelCenterer centers (i.e., normalize to have zero mean) the data without explicitly computing phi(x). It is
equivalent to centering phi(x) with sklearn.preprocessing.StandardScaler(with_std=False).
Read more in the User Guide.
Methods
fit(K[, y])
Fit KernelCenterer
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(K[, y, copy])
Center kernel matrix.
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
1814
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
fit(K, y=None)
Fit KernelCenterer
ParametersK : numpy array of shape [n_samples, n_samples]
Kernel matrix.
Returnsself : returns an instance of self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(K, y=None, copy=True)
Center kernel matrix.
ParametersK : numpy array of shape [n_samples1, n_samples2]
Kernel matrix.
copy : boolean, optional, default True
Set to False to perform inplace computation.
ReturnsK_new : numpy array of shape [n_samples1, n_samples2]
29.30.5 sklearn.preprocessing.LabelBinarizer
class sklearn.preprocessing.LabelBinarizer(neg_label=0,
pos_label=1,
sparse_output=False)
Binarize labels in a one-vs-all fashion
29.30. sklearn.preprocessing: Preprocessing and Normalization
1815
scikit-learn user guide, Release 0.18.2
Several regression and binary classiﬁcation algorithms are available in the scikit. A simple way to extend these
algorithms to the multi-class classiﬁcation case is to use the so-called one-vs-all scheme.
At learning time, this simply consists in learning one regressor or binary classiﬁer per class. In doing so, one
needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer
makes this process easy with the transform method.
At prediction time, one assigns the class for which the corresponding model gave the greatest conﬁdence. La-
belBinarizer makes this easy with the inverse_transform method.
Read more in the User Guide.
Parametersneg_label : int (default: 0)
Value with which negative labels must be encoded.
pos_label : int (default: 1)
Value with which positive labels must be encoded.
sparse_output : boolean (default: False)
True if the returned array from transform is desired to be in sparse CSR format.
Attributesclasses_ : array of shape [n_class]
Holds the label for each class.
y_type_ : str,
Represents the type of the target data as evaluated by utils.multiclass.type_of_target.
Possible type are ‘continuous’,
‘continuous-multioutput’,
‘binary’,
‘multiclass’,
‘multiclass-multioutput’, ‘multilabel-indicator’, and ‘unknown’.
sparse_input_ : boolean,
True if the input data to transform is given as a sparse matrix, False otherwise.
See also:
label_binarizefunction to perform the transform operation of LabelBinarizer with ﬁxed classes.
sklearn.preprocessing.OneHotEncoderencode categorical integer features using a one-hot aka
one-of-K scheme.
Examples
>>> from sklearn import preprocessing
>>> lb = preprocessing.LabelBinarizer()
>>> lb.fit([1, 2, 6, 4, 2])
LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)
>>> lb.classes_
array([1, 2, 4, 6])
>>> lb.transform([1, 6])
array([[1, 0, 0, 0],
[0, 0, 0, 1]])
Binary targets transform to a column vector
>>> lb = preprocessing.LabelBinarizer()
>>> lb.fit_transform(['yes', 'no', 'no', 'yes'])
array([[1],
1816
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
[0],
[0],
[1]])
Passing a 2D matrix for multilabel classiﬁcation
>>> import numpy as np
>>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))
LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)
>>> lb.classes_
array([0, 1, 2])
>>> lb.transform([0, 1, 2, 1])
array([[1, 0, 0],
[0, 1, 0],
[0, 0, 1],
[0, 1, 0]])
Methods
fit(y)
Fit label binarizer
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
inverse_transform(Y[, threshold])
Transform binary labels back to multi-class labels
set_params(\*\*params)
Set the parameters of this estimator.
transform(y)
Transform multi-class labels to binary labels
__init__(neg_label=0, pos_label=1, sparse_output=False)
fit(y)
Fit label binarizer
Parametersy : numpy array of shape (n_samples,) or (n_samples, n_classes)
Target values. The 2-d matrix should only contain 0 and 1, represents multilabel classi-
ﬁcation.
Returnsself : returns an instance of self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
29.30. sklearn.preprocessing: Preprocessing and Normalization
1817
scikit-learn user guide, Release 0.18.2
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(Y, threshold=None)
Transform binary labels back to multi-class labels
ParametersY : numpy array or sparse matrix with shape [n_samples, n_classes]
Target values. All sparse matrices are converted to CSR before inverse transformation.
threshold : ﬂoat or None
Threshold used in the binary and multi-label cases.
Use 0 when:
•Y contains the output of decision_function (classiﬁer)
Use 0.5 when:
•Y contains the output of predict_proba
If None, the threshold is assumed to be half way between neg_label and pos_label.
Returnsy : numpy array or CSR matrix of shape [n_samples] Target values.
Notes
In the case when the binary labels are fractional (probabilistic), inverse_transform chooses the class with
the greatest value. Typically, this allows to use the output of a linear model’s decision_function method
directly as the input of inverse_transform.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(y)
Transform multi-class labels to binary labels
The output of transform is sometimes referred to by some authors as the 1-of-K coding scheme.
Parametersy : numpy array or sparse matrix of shape (n_samples,) or
(n_samples, n_classes) Target values. The 2-d matrix should only contain 0 and 1,
represents multilabel classiﬁcation. Sparse matrix can be CSR, CSC, COO, DOK, or
LIL.
ReturnsY : numpy array or CSR matrix of shape [n_samples, n_classes]
Shape will be [n_samples, 1] for binary problems.
1818
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.30.6 sklearn.preprocessing.LabelEncoder
class sklearn.preprocessing.LabelEncoder
Encode labels with value between 0 and n_classes-1.
Read more in the User Guide.
Attributesclasses_ : array of shape (n_class,)
Holds the label for each class.
See also:
sklearn.preprocessing.OneHotEncoderencode categorical integer features using a one-hot aka
one-of-K scheme.
Examples
LabelEncoder can be used to normalize labels.
>>> from sklearn import preprocessing
>>> le = preprocessing.LabelEncoder()
>>> le.fit([1, 2, 2, 6])
LabelEncoder()
>>> le.classes_
array([1, 2, 6])
>>> le.transform([1, 1, 2, 6])
array([0, 0, 1, 2]...)
>>> le.inverse_transform([0, 0, 1, 2])
array([1, 1, 2, 6])
It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical
labels.
>>> le = preprocessing.LabelEncoder()
>>> le.fit(["paris", "paris", "tokyo", "amsterdam"])
LabelEncoder()
>>> list(le.classes_)
['amsterdam', 'paris', 'tokyo']
>>> le.transform(["tokyo", "tokyo", "paris"])
array([2, 2, 1]...)
>>> list(le.inverse_transform([2, 2, 1]))
['tokyo', 'tokyo', 'paris']
Methods
fit(y)
Fit label encoder
fit_transform(y)
Fit label encoder and return encoded labels
get_params([deep])
Get parameters for this estimator.
inverse_transform(y)
Transform labels back to original encoding.
set_params(\*\*params)
Set the parameters of this estimator.
transform(y)
Transform labels to normalized encoding.
29.30. sklearn.preprocessing: Preprocessing and Normalization
1819
scikit-learn user guide, Release 0.18.2
__init__()
x.__init__(...) initializes x; see help(type(x)) for signature
fit(y)
Fit label encoder
Parametersy : array-like of shape (n_samples,)
Target values.
Returnsself : returns an instance of self.
fit_transform(y)
Fit label encoder and return encoded labels
Parametersy : array-like of shape [n_samples]
Target values.
Returnsy : array-like of shape [n_samples]
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(y)
Transform labels back to original encoding.
Parametersy : numpy array of shape [n_samples]
Target values.
Returnsy : numpy array of shape [n_samples]
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(y)
Transform labels to normalized encoding.
Parametersy : array-like of shape [n_samples]
Target values.
Returnsy : array-like of shape [n_samples]
29.30.7 sklearn.preprocessing.MultiLabelBinarizer
class sklearn.preprocessing.MultiLabelBinarizer(classes=None, sparse_output=False)
Transform between iterable of iterables and a multilabel format
1820
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Although a list of sets or tuples is a very intuitive format for multilabel data, it is unwieldy to process. This
transformer converts between this intuitive format and the supported multilabel format: a (samples x classes)
binary matrix indicating the presence of a class label.
Parametersclasses : array-like of shape [n_classes] (optional)
Indicates an ordering for the class labels
sparse_output : boolean (default: False),
Set to true if output binary array is desired in CSR sparse format
Attributesclasses_ : array of labels
A copy of the classes parameter where provided, or otherwise, the sorted set of classes
found when ﬁtting.
See also:
sklearn.preprocessing.OneHotEncoderencode categorical integer features using a one-hot aka
one-of-K scheme.
Examples
>>> from sklearn.preprocessing import MultiLabelBinarizer
>>> mlb = MultiLabelBinarizer()
>>> mlb.fit_transform([(1, 2), (3,)])
array([[1, 1, 0],
[0, 0, 1]])
>>> mlb.classes_
array([1, 2, 3])
>>> mlb.fit_transform([set(['sci-fi', 'thriller']), set(['comedy'])])
array([[0, 1, 1],
[1, 0, 0]])
>>> list(mlb.classes_)
['comedy', 'sci-fi', 'thriller']
Methods
fit(y)
Fit the label sets binarizer, storing classes_
fit_transform(y)
Fit the label sets binarizer and transform the given label
sets
get_params([deep])
Get parameters for this estimator.
inverse_transform(yt)
Transform the given indicator matrix into label sets
set_params(\*\*params)
Set the parameters of this estimator.
transform(y)
Transform the given label sets
__init__(classes=None, sparse_output=False)
fit(y)
Fit the label sets binarizer, storing classes_
Parametersy : iterable of iterables
29.30. sklearn.preprocessing: Preprocessing and Normalization
1821
scikit-learn user guide, Release 0.18.2
A set of labels (any orderable and hashable object) for each sample. If the classes
parameter is set, y will not be iterated.
Returnsself : returns this MultiLabelBinarizer instance
fit_transform(y)
Fit the label sets binarizer and transform the given label sets
Parametersy : iterable of iterables
A set of labels (any orderable and hashable object) for each sample. If the classes
parameter is set, y will not be iterated.
Returnsy_indicator : array or CSR matrix, shape (n_samples, n_classes)
A matrix such that y_indicator[i, j] = 1 iff classes_[j] is in y[i], and 0 otherwise.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(yt)
Transform the given indicator matrix into label sets
Parametersyt : array or sparse matrix of shape (n_samples, n_classes)
A matrix containing only 1s ands 0s.
Returnsy : list of tuples
The set of labels for each sample such that y[i] consists of classes_[j] for each yt[i, j]
== 1.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(y)
Transform the given label sets
Parametersy : iterable of iterables
A set of labels (any orderable and hashable object) for each sample. If the classes
parameter is set, y will not be iterated.
Returnsy_indicator : array or CSR matrix, shape (n_samples, n_classes)
A matrix such that y_indicator[i, j] = 1 iff classes_[j] is in y[i], and 0 otherwise.
1822
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.30.8 sklearn.preprocessing.MaxAbsScaler
class sklearn.preprocessing.MaxAbsScaler(copy=True)
Scale each feature by its maximum absolute value.
This estimator scales and translates each feature individually such that the maximal absolute value of each
feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.
This scaler can also be applied to sparse CSR or CSC matrices.
New in version 0.17.
Parameterscopy : boolean, optional, default is True
Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy
array).
Attributesscale_ : ndarray, shape (n_features,)
Per feature relative scaling of the data.
New in version 0.17: scale_ attribute.
max_abs_ : ndarray, shape (n_features,)
Per feature maximum absolute value.
n_samples_seen_ : int
The number of samples processed by the estimator. Will be reset on new calls to ﬁt, but
increments across partial_fit calls.
See also:
maxabs_scaleEquivalent function without the object oriented API.
Methods
fit(X[, y])
Compute the maximum absolute value to be used for
later scaling.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X)
Scale back the data to the original representation
partial_fit(X[, y])
Online computation of max absolute value of X for later
scaling.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Scale the data
__init__(copy=True)
fit(X, y=None)
Compute the maximum absolute value to be used for later scaling.
ParametersX : {array-like, sparse matrix}, shape [n_samples, n_features]
The data used to compute the per-feature minimum and maximum used for later scaling
along the features axis.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
29.30. sklearn.preprocessing: Preprocessing and Normalization
1823
scikit-learn user guide, Release 0.18.2
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(X)
Scale back the data to the original representation
ParametersX : {array-like, sparse matrix}
The data that should be transformed back.
partial_fit(X, y=None)
Online computation of max absolute value of X for later scaling. All of X is processed as a single batch.
This is intended for cases when ﬁt is not feasible due to very large number of n_samples or because X is
read from a continuous stream.
ParametersX : {array-like, sparse matrix}, shape [n_samples, n_features]
The data used to compute the mean and standard deviation used for later scaling along
the features axis.
y: Passthrough for ‘‘Pipeline‘‘ compatibility. :
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Scale the data
ParametersX : {array-like, sparse matrix}
The data that should be scaled.
29.30.9 sklearn.preprocessing.MinMaxScaler
class sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)
Transforms features by scaling each feature to a given range.
1824
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
This estimator scales and translates each feature individually such that it is in the given range on the training set,
i.e. between zero and one.
The transformation is given by:
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_scaled = X_std * (max - min) + min
where min, max = feature_range.
This transformation is often used as an alternative to zero mean, unit variance scaling.
Read more in the User Guide.
Parametersfeature_range : tuple (min, max), default=(0, 1)
Desired range of transformed data.
copy : boolean, optional, default True
Set to False to perform inplace row normalization and avoid a copy (if the input is
already a numpy array).
Attributesmin_ : ndarray, shape (n_features,)
Per feature adjustment for minimum.
scale_ : ndarray, shape (n_features,)
Per feature relative scaling of the data.
New in version 0.17: scale_ attribute.
data_min_ : ndarray, shape (n_features,)
Per feature minimum seen in the data
New in version 0.17: data_min_ instead of deprecated data_min.
data_max_ : ndarray, shape (n_features,)
Per feature maximum seen in the data
New in version 0.17: data_max_ instead of deprecated data_max.
data_range_ : ndarray, shape (n_features,)
Per feature range (data_max_ -data_min_) seen in the data
New in version 0.17: data_range_ instead of deprecated data_range.
See also:
minmax_scaleEquivalent function without the object oriented API.
Methods
fit(X[, y])
Compute the minimum and maximum to be used for
later scaling.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X)
Undo the scaling of X according to feature_range.
Continued on next page
29.30. sklearn.preprocessing: Preprocessing and Normalization
1825
scikit-learn user guide, Release 0.18.2
Table 29.228 – continued from previous page
partial_fit(X[, y])
Online computation of min and max on X for later scal-
ing.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Scaling features of X according to feature_range.
__init__(feature_range=(0, 1), copy=True)
data_min
DEPRECATED: Attribute data_min will be removed in 0.19. Use data_min_ instead
data_range
DEPRECATED: Attribute data_range will be removed in 0.19. Use data_range_ instead
fit(X, y=None)
Compute the minimum and maximum to be used for later scaling.
ParametersX : array-like, shape [n_samples, n_features]
The data used to compute the per-feature minimum and maximum used for later scaling
along the features axis.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(X)
Undo the scaling of X according to feature_range.
ParametersX : array-like, shape [n_samples, n_features]
Input data that will be transformed. It cannot be sparse.
partial_fit(X, y=None)
Online computation of min and max on X for later scaling. All of X is processed as a single batch. This
is intended for cases when ﬁt is not feasible due to very large number of n_samples or because X is read
from a continuous stream.
ParametersX : array-like, shape [n_samples, n_features]
1826
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The data used to compute the mean and standard deviation used for later scaling along
the features axis.
y : Passthrough for Pipeline compatibility.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Scaling features of X according to feature_range.
ParametersX : array-like, shape [n_samples, n_features]
Input data that will be transformed.
Examples using sklearn.preprocessing.MinMaxScaler
• Compare Stochastic learning strategies for MLPClassiﬁer
29.30.10 sklearn.preprocessing.Normalizer
class sklearn.preprocessing.Normalizer(norm=’l2’, copy=True)
Normalize samples individually to unit norm.
Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently
of other samples so that its norm (l1 or l2) equals one.
This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you
want to avoid the burden of a copy / conversion).
Scaling inputs to unit norms is a common operation for text classiﬁcation or clustering for instance. For instance
the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base
similarity metric for the Vector Space Model commonly used by the Information Retrieval community.
Read more in the User Guide.
Parametersnorm : ‘l1’, ‘l2’, or ‘max’, optional (‘l2’ by default)
The norm to use to normalize each non zero sample.
copy : boolean, optional, default True
set to False to perform inplace row normalization and avoid a copy (if the input is
already a numpy array or a scipy.sparse CSR matrix).
See also:
normalizeEquivalent function without the object oriented API.
Notes
This estimator is stateless (besides constructor parameters), the ﬁt method does nothing but is useful when used
in a pipeline.
29.30. sklearn.preprocessing: Preprocessing and Normalization
1827
scikit-learn user guide, Release 0.18.2
Methods
fit(X[, y])
Do nothing and return the estimator unchanged
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y, copy])
Scale each non zero row of X to unit norm
__init__(norm=’l2’, copy=True)
fit(X, y=None)
Do nothing and return the estimator unchanged
This method is just there to implement the usual API and hence work in pipelines.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None, copy=None)
Scale each non zero row of X to unit norm
ParametersX : {array-like, sparse matrix}, shape [n_samples, n_features]
The data to normalize, row by row. scipy.sparse matrices should be in CSR format to
avoid an un-necessary copy.
1828
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.preprocessing.Normalizer
• Clustering text documents using k-means
29.30.11 sklearn.preprocessing.OneHotEncoder
class sklearn.preprocessing.OneHotEncoder(n_values=’auto’,
categorical_features=’all’,
dtype=<type
‘numpy.ﬂoat64’>,
sparse=True,
handle_unknown=’error’)
Encode categorical integer features using a one-hot aka one-of-K scheme.
The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete)
features. The output will be a sparse matrix where each column corresponds to one possible value of one feature.
It is assumed that input features take on values in the range [0, n_values).
This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and
SVMs with the standard kernels.
Note: a one-hot encoding of y labels should use a LabelBinarizer instead.
Read more in the User Guide.
Parametersn_values : ‘auto’, int or array of ints
Number of values per feature.
•‘auto’ : determine value range from training data.
•int[number of categorical values per feature.]
Each feature value should be in
range(n_values)
•array[n_values[i] is the number of categorical values in] X[:,i]. Each feature
value should be in range(n_values[i])
categorical_features : “all” or array of indices or mask
Specify what features are treated as categorical.
•‘all’ (default): All features are treated as categorical.
•array of indices: Array of categorical feature indices.
•mask: Array of length n_features and with dtype=bool.
Non-categorical features are always stacked to the right of the matrix.
dtype : number type, default=np.ﬂoat
Desired dtype of output.
sparse : boolean, default=True
Will return sparse matrix if set True else will return an array.
handle_unknown : str, ‘error’ or ‘ignore’
Whether to raise an error or ignore if a unknown categorical feature is present during
transform.
Attributesactive_features_ : array
Indices for active features, meaning values that actually occur in the training set. Only
available when n_values is 'auto'.
feature_indices_ : array of shape (n_features,)
29.30. sklearn.preprocessing: Preprocessing and Normalization
1829
scikit-learn user guide, Release 0.18.2
Indices to feature ranges. Feature i in the original data is mapped to features from
feature_indices_[i] to feature_indices_[i+1] (and then potentially
masked by active_features_ afterwards)
n_values_ : array of shape (n_features,)
Maximum number of values per feature.
See also:
sklearn.feature_extraction.DictVectorizerperforms a one-hot encoding of dictionary items
(also handles string-valued features).
sklearn.feature_extraction.FeatureHasherperforms an approximate one-hot encoding of dic-
tionary items or strings.
sklearn.preprocessing.LabelBinarizerbinarizes labels in a one-vs-all fashion.
sklearn.preprocessing.MultiLabelBinarizertransforms between iterable of iterables and a
multilabel format, e.g. a (samples x classes) binary matrix indicating the presence of a class label.
sklearn.preprocessing.LabelEncoderencodes labels with values between 0 and n_classes-1.
Examples
Given a dataset with three features and two samples, we let the encoder ﬁnd the maximum value per feature and
transform the data to a binary one-hot encoding.
>>> from sklearn.preprocessing import OneHotEncoder
>>> enc = OneHotEncoder()
>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>,
handle_unknown='error', n_values='auto', sparse=True)
>>> enc.n_values_
array([2, 3, 4])
>>> enc.feature_indices_
array([0, 2, 5, 9])
>>> enc.transform([[0, 1, 1]]).toarray()
array([[ 1.,
0.,
0.,
1.,
0.,
0.,
1.,
0.,
0.]])
Methods
fit(X[, y])
Fit OneHotEncoder to X.
fit_transform(X[, y])
Fit OneHotEncoder to X, then transform X.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Transform X using one-hot encoding.
__init__(n_values=’auto’, categorical_features=’all’, dtype=<type ‘numpy.ﬂoat64’>, sparse=True,
handle_unknown=’error’)
fit(X, y=None)
Fit OneHotEncoder to X.
ParametersX : array-like, shape [n_samples, n_feature]
1830
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Input array of type int.
Returnsself :
fit_transform(X, y=None)
Fit OneHotEncoder to X, then transform X.
Equivalent to self.ﬁt(X).transform(X), but more convenient and more efﬁcient. See ﬁt for the parameters,
transform for the return value.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Transform X using one-hot encoding.
ParametersX : array-like, shape [n_samples, n_features]
Input array of type int.
ReturnsX_out : sparse matrix if sparse=True else a 2-d array, dtype=int
Transformed input.
Examples using sklearn.preprocessing.OneHotEncoder
• Feature transformations with ensembles of trees
29.30.12 sklearn.preprocessing.PolynomialFeatures
class sklearn.preprocessing.PolynomialFeatures(degree=2,
interaction_only=False,
in-
clude_bias=True)
Generate polynomial and interaction features.
Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than
or equal to the speciﬁed degree. For example, if an input sample is two dimensional and of the form [a, b], the
degree-2 polynomial features are [1, a, b, a^2, ab, b^2].
Parametersdegree : integer
The degree of the polynomial features. Default = 2.
interaction_only : boolean, default = False
29.30. sklearn.preprocessing: Preprocessing and Normalization
1831
scikit-learn user guide, Release 0.18.2
If true, only interaction features are produced: features that are products of at most
degree distinct input features (so not x[1] ** 2, x[0] * x[2] ** 3, etc.).
include_bias : boolean
If True (default), then include a bias column, the feature in which all polynomial powers
are zero (i.e. a column of ones - acts as an intercept term in a linear model).
Attributespowers_ : array, shape (n_output_features, n_input_features)
powers_[i, j] is the exponent of the jth input in the ith output.
n_input_features_ : int
The total number of input features.
n_output_features_ : int
The total number of polynomial output features. The number of output features is com-
puted by iterating over all suitably sized combinations of input features.
Notes
Be aware that the number of features in the output array scales polynomially in the number of features of the
input array, and exponentially in the degree. High degrees can cause overﬁtting.
See examples/linear_model/plot_polynomial_interpolation.py
Examples
>>> X = np.arange(6).reshape(3, 2)
>>> X
array([[0, 1],
[2, 3],
[4, 5]])
>>> poly = PolynomialFeatures(2)
>>> poly.fit_transform(X)
array([[
1.,
0.,
1.,
0.,
0.,
1.],
[
1.,
2.,
3.,
4.,
6.,
9.],
[
1.,
4.,
5.,
16.,
20.,
25.]])
>>> poly = PolynomialFeatures(interaction_only=True)
>>> poly.fit_transform(X)
array([[
1.,
0.,
1.,
0.],
[
1.,
2.,
3.,
6.],
[
1.,
4.,
5.,
20.]])
Methods
fit(X[, y])
Compute number of output features.
fit_transform(X[, y])
Fit to data, then transform it.
get_feature_names([input_features])
Return feature names for output features
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Transform data to polynomial features
1832
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
__init__(degree=2, interaction_only=False, include_bias=True)
fit(X, y=None)
Compute number of output features.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_feature_names(input_features=None)
Return feature names for output features
Parametersinput_features : list of string, length n_features, optional
String names for input features if available. By default, “x0”, “x1”, ... “xn_features” is
used.
Returnsoutput_feature_names : list of string, length n_output_features
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Transform data to polynomial features
ParametersX : array-like, shape [n_samples, n_features]
The data to transform, row by row.
ReturnsXP : np.ndarray shape [n_samples, NP]
The matrix of features, where NP is the number of polynomial features generated from
the combination of inputs.
29.30. sklearn.preprocessing: Preprocessing and Normalization
1833
scikit-learn user guide, Release 0.18.2
Examples using sklearn.preprocessing.PolynomialFeatures
• Polynomial interpolation
• Robust linear estimator ﬁtting
• Underﬁtting vs. Overﬁtting
29.30.13 sklearn.preprocessing.RobustScaler
class sklearn.preprocessing.RobustScaler(with_centering=True,
with_scaling=True,
quan-
tile_range=(25.0, 75.0), copy=True)
Scale features using statistics that are robust to outliers.
This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile
Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).
Centering and scaling happen independently on each feature (or each sample, depending on the axis argument)
by computing the relevant statistics on the samples in the training set. Median and interquartile range are then
stored to be used on later data using the transform method.
Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is
done by removing the mean and scaling to unit variance. However, outliers can often inﬂuence the sample mean
/ variance in a negative way. In such cases, the median and the interquartile range often give better results.
New in version 0.17.
Read more in the User Guide.
Parameterswith_centering : boolean, True by default
If True, center the data before scaling. This does not work (and will raise an exception)
when attempted on sparse matrices, because centering them entails building a dense
matrix which in common use cases is likely to be too large to ﬁt in memory.
with_scaling : boolean, True by default
If True, scale the data to interquartile range.
quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0
Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR Quantile range used to calcu-
late scale_.
New in version 0.18.
copy : boolean, optional, default is True
If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to
always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix,
a copy may still be returned.
Attributescenter_ : array of ﬂoats
The median value for each feature in the training set.
scale_ : array of ﬂoats
The (scaled) interquartile range for each feature in the training set.
New in version 0.17: scale_ attribute.
See also:
1834
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
robust_scaleEquivalent function without the object oriented API.
sklearn.decomposition.PCAFurther removes the linear correlation across features with ‘whiten=True’.
Notes
See examples/preprocessing/plot_robust_scaling.py for an example.
https://en.wikipedia.org/wiki/Median_(statistics) https://en.wikipedia.org/wiki/Interquartile_range
Methods
fit(X[, y])
Compute the median and quantiles to be used for scal-
ing.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X)
Scale back the data to the original representation
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Center and scale the data
__init__(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)
fit(X, y=None)
Compute the median and quantiles to be used for scaling.
ParametersX : array-like, shape [n_samples, n_features]
The data used to compute the median and quantiles used for later scaling along the
features axis.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
29.30. sklearn.preprocessing: Preprocessing and Normalization
1835
scikit-learn user guide, Release 0.18.2
inverse_transform(X)
Scale back the data to the original representation
ParametersX : array-like
The data used to scale along the speciﬁed axis.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Center and scale the data
ParametersX : array-like
The data used to scale along the speciﬁed axis.
Examples using sklearn.preprocessing.RobustScaler
• Robust Scaling on Toy Data
29.30.14 sklearn.preprocessing.StandardScaler
class sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)
Standardize features by removing the mean and scaling to unit variance
Centering and scaling happen independently on each feature by computing the relevant statistics on the samples
in the training set. Mean and standard deviation are then stored to be used on later data using the transform
method.
Standardization of a dataset is a common requirement for many machine learning estimators: they might behave
badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian
with 0 mean and unit variance).
For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of
Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered
around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger
that others, it might dominate the objective function and make the estimator unable to learn from other features
correctly as expected.
This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking
the sparsity structure of the data.
Read more in the User Guide.
Parameterswith_mean : boolean, True by default
If True, center the data before scaling. This does not work (and will raise an exception)
when attempted on sparse matrices, because centering them entails building a dense
matrix which in common use cases is likely to be too large to ﬁt in memory.
with_std : boolean, True by default
If True, scale the data to unit variance (or equivalently, unit standard deviation).
1836
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
copy : boolean, optional, default True
If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to
always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix,
a copy may still be returned.
Attributesscale_ : ndarray, shape (n_features,)
Per feature relative scaling of the data.
New in version 0.17: scale_ is recommended instead of deprecated std_.
mean_ : array of ﬂoats with shape [n_features]
The mean value for each feature in the training set.
var_ : array of ﬂoats with shape [n_features]
The variance for each feature in the training set. Used to compute scale_
n_samples_seen_ : int
The number of samples processed by the estimator. Will be reset on new calls to ﬁt, but
increments across partial_fit calls.
See also:
scaleEquivalent function without the object oriented API.
sklearn.decomposition.PCAFurther removes the linear correlation across features with ‘whiten=True’.
Methods
fit(X[, y])
Compute the mean and std to be used for later scaling.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X[, copy])
Scale back the data to the original representation
partial_fit(X[, y])
Online computation of mean and std on X for later scal-
ing.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y, copy])
Perform standardization by centering and scaling
__init__(copy=True, with_mean=True, with_std=True)
fit(X, y=None)
Compute the mean and std to be used for later scaling.
ParametersX : {array-like, sparse matrix}, shape [n_samples, n_features]
The data used to compute the mean and standard deviation used for later scaling along
the features axis.
y: Passthrough for ‘‘Pipeline‘‘ compatibility. :
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
29.30. sklearn.preprocessing: Preprocessing and Normalization
1837
scikit-learn user guide, Release 0.18.2
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(X, copy=None)
Scale back the data to the original representation
ParametersX : array-like, shape [n_samples, n_features]
The data used to scale along the features axis.
partial_fit(X, y=None)
Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This
is intended for cases when ﬁt is not feasible due to very large number of n_samples or because X is read
from a continuous stream.
The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub,
and Randall J. LeVeque. “Algorithms for computing the sample variance: Analysis and recommendations.”
The American Statistician 37.3 (1983): 242-247:
ParametersX : {array-like, sparse matrix}, shape [n_samples, n_features]
The data used to compute the mean and standard deviation used for later scaling along
the features axis.
y: Passthrough for ‘‘Pipeline‘‘ compatibility. :
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
std_
DEPRECATED: Attribute std_ will be removed in 0.19. Use scale_ instead
transform(X, y=None, copy=None)
Perform standardization by centering and scaling
ParametersX : array-like, shape [n_samples, n_features]
The data used to scale along the features axis.
1838
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.preprocessing.StandardScaler
• Prediction Latency
• Classiﬁer comparison
• Comparing different clustering algorithms on toy datasets
• Demo of DBSCAN clustering algorithm
• L1 Penalty and Sparsity in Logistic Regression
• Sparse recovery: feature selection for sparse linear models
• Varying regularization in Multi-layer Perceptron
• Robust Scaling on Toy Data
• RBF SVM parameters
preprocessing.add_dummy_feature(X[, value])
Augment dataset with an additional dummy feature.
preprocessing.binarize(X[, threshold, copy])
Boolean thresholding of array-like or scipy.sparse matrix
preprocessing.label_binarize(y, classes[, ...])
Binarize labels in a one-vs-all fashion
preprocessing.maxabs_scale(X[, axis, copy])
Scale each feature to the [-1, 1] range without breaking the
sparsity.
preprocessing.minmax_scale(X[, ...])
Transforms features by scaling each feature to a given
range.
preprocessing.normalize(X[, norm, axis, ...])
Scale input vectors individually to unit norm (vector
length).
preprocessing.robust_scale(X[, axis, ...])
Standardize a dataset along any axis
preprocessing.scale(X[, axis, with_mean, ...])
Standardize a dataset along any axis
29.30.15 sklearn.preprocessing.add_dummy_feature
sklearn.preprocessing.add_dummy_feature(X, value=1.0)
Augment dataset with an additional dummy feature.
This is useful for ﬁtting an intercept term with implementations which cannot otherwise ﬁt it directly.
ParametersX : {array-like, sparse matrix}, shape [n_samples, n_features]
Data.
value : ﬂoat
Value to use for the dummy feature.
ReturnsX : {array, sparse matrix}, shape [n_samples, n_features + 1]
Same data with dummy feature added as ﬁrst column.
Examples
>>> from sklearn.preprocessing import add_dummy_feature
>>> add_dummy_feature([[0, 1], [1, 0]])
array([[ 1.,
0.,
1.],
[ 1.,
1.,
0.]])
29.30. sklearn.preprocessing: Preprocessing and Normalization
1839
scikit-learn user guide, Release 0.18.2
29.30.16 sklearn.preprocessing.binarize
sklearn.preprocessing.binarize(X, threshold=0.0, copy=True)
Boolean thresholding of array-like or scipy.sparse matrix
Read more in the User Guide.
ParametersX : {array-like, sparse matrix}, shape [n_samples, n_features]
The data to binarize, element by element. scipy.sparse matrices should be in CSR or
CSC format to avoid an un-necessary copy.
threshold : ﬂoat, optional (0.0 by default)
Feature values below or equal to this are replaced by 0, above it by 1. Threshold may
not be less than 0 for operations on sparse matrices.
copy : boolean, optional, default True
set to False to perform inplace binarization and avoid a copy (if the input is already a
numpy array or a scipy.sparse CSR / CSC matrix and if axis is 1).
See also:
BinarizerPerforms binarization using the Transformer API (e.g.
as part of a preprocessing
sklearn.pipeline.Pipeline).
29.30.17 sklearn.preprocessing.label_binarize
sklearn.preprocessing.label_binarize(y,
classes,
neg_label=0,
pos_label=1,
sparse_output=False)
Binarize labels in a one-vs-all fashion
Several regression and binary classiﬁcation algorithms are available in the scikit. A simple way to extend these
algorithms to the multi-class classiﬁcation case is to use the so-called one-vs-all scheme.
This function makes it possible to compute this transformation for a ﬁxed set of class labels known ahead of
time.
Parametersy : array-like
Sequence of integer labels or multilabel data to encode.
classes : array-like of shape [n_classes]
Uniquely holds the label for each class.
neg_label : int (default: 0)
Value with which negative labels must be encoded.
pos_label : int (default: 1)
Value with which positive labels must be encoded.
sparse_output : boolean (default: False),
Set to true if output binary array is desired in CSR sparse format
ReturnsY : numpy array or CSR matrix of shape [n_samples, n_classes]
Shape will be [n_samples, 1] for binary problems.
See also:
1840
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
LabelBinarizerclass used to wrap the functionality of label_binarize and allow for ﬁtting to classes inde-
pendently of the transform operation
Examples
>>> from sklearn.preprocessing import label_binarize
>>> label_binarize([1, 6], classes=[1, 2, 4, 6])
array([[1, 0, 0, 0],
[0, 0, 0, 1]])
The class ordering is preserved:
>>> label_binarize([1, 6], classes=[1, 6, 4, 2])
array([[1, 0, 0, 0],
[0, 1, 0, 0]])
Binary targets transform to a column vector
>>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])
array([[1],
[0],
[0],
[1]])
Examples using sklearn.preprocessing.label_binarize
• Precision-Recall
• Receiver Operating Characteristic (ROC)
29.30.18 sklearn.preprocessing.maxabs_scale
sklearn.preprocessing.maxabs_scale(X, axis=0, copy=True)
Scale each feature to the [-1, 1] range without breaking the sparsity.
This estimator scales each feature individually such that the maximal absolute value of each feature in the
training set will be 1.0.
This scaler can also be applied to sparse CSR or CSC matrices.
Parametersaxis : int (0 by default)
axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale
each sample.
copy : boolean, optional, default is True
Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy
array).
See also:
MaxAbsScalerPerforms scaling to the [-1, 1] range using the‘‘Transformer‘‘ API (e.g. as part of a prepro-
cessing sklearn.pipeline.Pipeline).
29.30. sklearn.preprocessing: Preprocessing and Normalization
1841
scikit-learn user guide, Release 0.18.2
29.30.19 sklearn.preprocessing.minmax_scale
sklearn.preprocessing.minmax_scale(X, feature_range=(0, 1), axis=0, copy=True)
Transforms features by scaling each feature to a given range.
This estimator scales and translates each feature individually such that it is in the given range on the training set,
i.e. between zero and one.
The transformation is given by:
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_scaled = X_std * (max - min) + min
where min, max = feature_range.
This transformation is often used as an alternative to zero mean, unit variance scaling.
Read more in the User Guide.
New in version 0.17: minmax_scale function interface to sklearn.preprocessing.MinMaxScaler.
Parametersfeature_range : tuple (min, max), default=(0, 1)
Desired range of transformed data.
axis : int (0 by default)
axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale
each sample.
copy : boolean, optional, default is True
Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy
array).
See also:
MinMaxScalerPerforms scaling to a given range using the‘‘Transformer‘‘ API (e.g. as part of a preprocessing
sklearn.pipeline.Pipeline).
29.30.20 sklearn.preprocessing.normalize
sklearn.preprocessing.normalize(X, norm=’l2’, axis=1, copy=True, return_norm=False)
Scale input vectors individually to unit norm (vector length).
Read more in the User Guide.
ParametersX : {array-like, sparse matrix}, shape [n_samples, n_features]
The data to normalize, element by element. scipy.sparse matrices should be in CSR
format to avoid an un-necessary copy.
norm : ‘l1’, ‘l2’, or ‘max’, optional (‘l2’ by default)
The norm to use to normalize each non zero sample (or each non-zero feature if axis is
0).
axis : 0 or 1, optional (1 by default)
axis used to normalize the data along. If 1, independently normalize each sample, oth-
erwise (if 0) normalize each feature.
copy : boolean, optional, default True
1842
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
set to False to perform inplace row normalization and avoid a copy (if the input is
already a numpy array or a scipy.sparse CSR matrix and if axis is 1).
return_norm : boolean, default False
whether to return the computed norms
See also:
NormalizerPerforms normalization using the Transformer API (e.g.
as part of a preprocessing
sklearn.pipeline.Pipeline).
29.30.21 sklearn.preprocessing.robust_scale
sklearn.preprocessing.robust_scale(X,
axis=0,
with_centering=True,
with_scaling=True,
quantile_range=(25.0, 75.0), copy=True)
Standardize a dataset along any axis
Center to the median and component wise scale according to the interquartile range.
Read more in the User Guide.
ParametersX : array-like
The data to center and scale.
axis : int (0 by default)
axis used to compute the medians and IQR along. If 0, independently scale each feature,
otherwise (if 1) scale each sample.
with_centering : boolean, True by default
If True, center the data before scaling.
with_scaling : boolean, True by default
If True, scale the data to unit variance (or equivalently, unit standard deviation).
quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0
Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR Quantile range used to calcu-
late scale_.
New in version 0.18.
copy : boolean, optional, default is True
set to False to perform inplace row normalization and avoid a copy (if the input is
already a numpy array or a scipy.sparse CSR matrix and if axis is 1).
See also:
RobustScalerPerforms centering and scaling using the Transformer API (e.g. as part of a preprocessing
sklearn.pipeline.Pipeline).
Notes
This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would
potentially crash the program with memory exhaustion problems.
29.30. sklearn.preprocessing: Preprocessing and Normalization
1843
scikit-learn user guide, Release 0.18.2
Instead the caller is expected to either set explicitly with_centering=False (in that case, only variance scaling
will be performed on the features of the CSR matrix) or to call X.toarray() if he/she expects the materialized
dense array to ﬁt in memory.
To avoid memory copy the caller should pass a CSR matrix.
29.30.22 sklearn.preprocessing.scale
sklearn.preprocessing.scale(X, axis=0, with_mean=True, with_std=True, copy=True)
Standardize a dataset along any axis
Center to the mean and component wise scale to unit variance.
Read more in the User Guide.
ParametersX : {array-like, sparse matrix}
The data to center and scale.
axis : int (0 by default)
axis used to compute the means and standard deviations along. If 0, independently
standardize each feature, otherwise (if 1) standardize each sample.
with_mean : boolean, True by default
If True, center the data before scaling.
with_std : boolean, True by default
If True, scale the data to unit variance (or equivalently, unit standard deviation).
copy : boolean, optional, default True
set to False to perform inplace row normalization and avoid a copy (if the input is
already a numpy array or a scipy.sparse CSC matrix and if axis is 1).
See also:
StandardScalerPerforms scaling to unit variance using the‘‘Transformer‘‘ API (e.g. as part of a prepro-
cessing sklearn.pipeline.Pipeline).
Notes
This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would
potentially crash the program with memory exhaustion problems.
Instead the caller is expected to either set explicitly with_mean=False (in that case, only variance scaling will
be performed on the features of the CSC matrix) or to call X.toarray() if he/she expects the materialized dense
array to ﬁt in memory.
To avoid memory copy the caller should pass a CSC matrix.
Examples using sklearn.preprocessing.scale
• A demo of K-Means clustering on the handwritten digits data
1844
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.31 sklearn.random_projection: Random projection
Random Projection transformers
Random Projections are a simple and computationally efﬁcient way to reduce the dimensionality of the data by trading
a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes.
The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances
between any two samples of the dataset.
The main theoretical result behind the efﬁciency of random projection is the Johnson-Lindenstrauss lemma (quoting
Wikipedia):
In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of
points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set
of points in a high-dimensional space can be embedded into a space of much lower dimension in such a
way that distances between the points are nearly preserved. The map used for the embedding is at least
Lipschitz, and can even be taken to be an orthogonal projection.
User guide: See the Random Projection section for further details.
random_projection.GaussianRandomProjection([...])
Reduce dimensionality through Gaussian random projec-
tion
random_projection.SparseRandomProjection([...])
Reduce dimensionality through sparse random projection
29.31.1 sklearn.random_projection.GaussianRandomProjection
class sklearn.random_projection.GaussianRandomProjection(n_components=’auto’,
eps=0.1,
ran-
dom_state=None)
Reduce dimensionality through Gaussian random projection
The components of the random matrix are drawn from N(0, 1 / n_components).
Read more in the User Guide.
Parametersn_components : int or ‘auto’, optional (default = ‘auto’)
Dimensionality of the target projection space.
n_components can be automatically adjusted according to the number of samples in
the dataset and the bound given by the Johnson-Lindenstrauss lemma. In that case the
quality of the embedding is controlled by the eps parameter.
It should be noted that Johnson-Lindenstrauss lemma can yield very conservative esti-
mated of the required number of components as it makes no assumption on the structure
of the dataset.
eps : strictly positive ﬂoat, optional (default=0.1)
Parameter to control the quality of the embedding according to the Johnson-
Lindenstrauss lemma when n_components is set to ‘auto’.
Smaller values lead to better embedding and higher number of dimensions
(n_components) in the target projection space.
random_state : integer, RandomState instance or None (default=None)
Control the pseudo random number generator used to generate the matrix at ﬁt time.
Attributesn_component_ : int
29.31. sklearn.random_projection: Random projection
1845
scikit-learn user guide, Release 0.18.2
Concrete number of components computed when n_components=”auto”.
components_ : numpy array of shape [n_components, n_features]
Random matrix used for the projection.
See also:
SparseRandomProjection
Methods
fit(X[, y])
Generate a sparse random projection matrix
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Project the data by using matrix product with the ran-
dom matrix
__init__(n_components=’auto’, eps=0.1, random_state=None)
fit(X, y=None)
Generate a sparse random projection matrix
ParametersX : numpy array or scipy.sparse of shape [n_samples, n_features]
Training set: only the shape is used to ﬁnd optimal random matrix dimensions based on
the theory referenced in the afore mentioned papers.
y : is not used: placeholder to allow for usage in a Pipeline.
Returnsself :
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
1846
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Project the data by using matrix product with the random matrix
ParametersX : numpy array or scipy.sparse of shape [n_samples, n_features]
The input data to project into a smaller dimensional space.
y : is not used: placeholder to allow for usage in a Pipeline.
ReturnsX_new : numpy array or scipy sparse of shape [n_samples, n_components]
Projected array.
29.31.2 sklearn.random_projection.SparseRandomProjection
class sklearn.random_projection.SparseRandomProjection(n_components=’auto’,
density=’auto’,
eps=0.1,
dense_output=False,
ran-
dom_state=None)
Reduce dimensionality through sparse random projection
Sparse random matrix is an alternative to dense random projection matrix that guarantees similar embedding
quality while being much more memory efﬁcient and allowing faster computation of the projected data.
If we note s = 1 / density the components of the random matrix are drawn from:
•-sqrt(s) / sqrt(n_components) with probability 1 / 2s
•0 with probability 1 - 1 / s
•+sqrt(s) / sqrt(n_components) with probability 1 / 2s
Read more in the User Guide.
Parametersn_components : int or ‘auto’, optional (default = ‘auto’)
Dimensionality of the target projection space.
n_components can be automatically adjusted according to the number of samples in
the dataset and the bound given by the Johnson-Lindenstrauss lemma. In that case the
quality of the embedding is controlled by the eps parameter.
It should be noted that Johnson-Lindenstrauss lemma can yield very conservative esti-
mated of the required number of components as it makes no assumption on the structure
of the dataset.
density : ﬂoat in range ]0, 1], optional (default=’auto’)
Ratio of non-zero component in the random projection matrix.
If density = ‘auto’, the value is set to the minimum density as recommended by Ping Li
et al.: 1 / sqrt(n_features).
Use density = 1 / 3.0 if you want to reproduce the results from Achlioptas, 2001.
eps : strictly positive ﬂoat, optional, (default=0.1)
29.31. sklearn.random_projection: Random projection
1847
scikit-learn user guide, Release 0.18.2
Parameter to control the quality of the embedding according to the Johnson-
Lindenstrauss lemma when n_components is set to ‘auto’.
Smaller values lead to better embedding and higher number of dimensions
(n_components) in the target projection space.
dense_output : boolean, optional (default=False)
If True, ensure that the output of the random projection is a dense numpy array even if
the input and random projection matrix are both sparse. In practice, if the number of
components is small the number of zero components in the projected data will be very
small and it will be more CPU and memory efﬁcient to use a dense representation.
If False, the projected data uses a sparse representation if the input is sparse.
random_state : integer, RandomState instance or None (default=None)
Control the pseudo random number generator used to generate the matrix at ﬁt time.
Attributesn_component_ : int
Concrete number of components computed when n_components=”auto”.
components_ : CSR matrix with shape [n_components, n_features]
Random matrix used for the projection.
density_ : ﬂoat in range 0.0 - 1.0
Concrete density computed from when density = “auto”.
See also:
GaussianRandomProjection
References
[R71], [R72]
Methods
fit(X[, y])
Generate a sparse random projection matrix
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Project the data by using matrix product with the ran-
dom matrix
__init__(n_components=’auto’,
density=’auto’,
eps=0.1,
dense_output=False,
ran-
dom_state=None)
fit(X, y=None)
Generate a sparse random projection matrix
ParametersX : numpy array or scipy.sparse of shape [n_samples, n_features]
Training set: only the shape is used to ﬁnd optimal random matrix dimensions based on
the theory referenced in the afore mentioned papers.
y : is not used: placeholder to allow for usage in a Pipeline.
1848
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsself :
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Project the data by using matrix product with the random matrix
ParametersX : numpy array or scipy.sparse of shape [n_samples, n_features]
The input data to project into a smaller dimensional space.
y : is not used: placeholder to allow for usage in a Pipeline.
ReturnsX_new : numpy array or scipy sparse of shape [n_samples, n_components]
Projected array.
Examples using sklearn.random_projection.SparseRandomProjection
• The Johnson-Lindenstrauss bound for embedding with random projections
• Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
random_projection.johnson_lindenstrauss_min_dim(...)
Find a ‘safe’ number of components to randomly project to
29.31. sklearn.random_projection: Random projection
1849
scikit-learn user guide, Release 0.18.2
29.31.3 sklearn.random_projection.johnson_lindenstrauss_min_dim
sklearn.random_projection.johnson_lindenstrauss_min_dim(n_samples, eps=0.1)
Find a ‘safe’ number of components to randomly project to
The distortion introduced by a random projection p only changes the distance between two points by a factor (1
+- eps) in an euclidean space with good probability. The projection p is an eps-embedding as deﬁned by:
(1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2
Where u and v are any rows taken from a dataset of shape [n_samples, n_features], eps is in ]0, 1[ and p is a
projection by a random Gaussian N(0, 1) matrix with shape [n_components, n_features] (or a sparse Achlioptas
matrix).
The minimum number of components to guarantee the eps-embedding is given by:
n_components >= 4 log(n_samples) / (eps^2 / 2 - eps^3 / 3)
Note that the number of dimensions is independent of the original number of features but instead depends on
the size of the dataset: the larger the dataset, the higher is the minimal dimensionality of an eps-embedding.
Read more in the User Guide.
Parametersn_samples : int or numpy array of int greater than 0,
Number of samples. If an array is given, it will compute a safe number of components
array-wise.
eps : ﬂoat or numpy array of ﬂoat in ]0,1[, optional (default=0.1)
Maximum distortion rate as deﬁned by the Johnson-Lindenstrauss lemma. If an array is
given, it will compute a safe number of components array-wise.
Returnsn_components : int or numpy array of int,
The minimal number of components to guarantee with good probability an eps-
embedding with n_samples.
References
[R73], [R74]
Examples
>>> johnson_lindenstrauss_min_dim(1e6, eps=0.5)
663
>>> johnson_lindenstrauss_min_dim(1e6, eps=[0.5, 0.1, 0.01])
array([
663,
11841, 1112658])
>>> johnson_lindenstrauss_min_dim([1e4, 1e5, 1e6], eps=0.1)
array([ 7894,
9868, 11841])
Examples using sklearn.random_projection.johnson_lindenstrauss_min_dim
• The Johnson-Lindenstrauss bound for embedding with random projections
1850
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.32 sklearn.semi_supervised Semi-Supervised Learning
The sklearn.semi_supervised module implements semi-supervised learning algorithms. These algorithms
utilized small amounts of labeled data and large amounts of unlabeled data for classiﬁcation tasks. This module
includes Label Propagation.
User guide: See the Semi-Supervised section for further details.
semi_supervised.LabelPropagation([kernel,
...])
Label Propagation classiﬁer
semi_supervised.LabelSpreading([kernel, ...])
LabelSpreading model for semi-supervised learning
29.32.1 sklearn.semi_supervised.LabelPropagation
class sklearn.semi_supervised.LabelPropagation(kernel=’rbf’, gamma=20, n_neighbors=7,
alpha=1,
max_iter=30,
tol=0.001,
n_jobs=1)
Label Propagation classiﬁer
Read more in the User Guide.
Parameterskernel : {‘knn’, ‘rbf’}
String identiﬁer for kernel function to use. Only ‘rbf’ and ‘knn’ kernels are currently
supported..
gamma : ﬂoat
Parameter for rbf kernel
n_neighbors : integer > 0
Parameter for knn kernel
alpha : ﬂoat
Clamping factor
max_iter : ﬂoat
Change maximum number of iterations allowed
tol : ﬂoat
Convergence tolerance: threshold to consider the system at steady state
AttributesX_ : array, shape = [n_samples, n_features]
Input array.
classes_ : array, shape = [n_classes]
The distinct labels used in classifying instances.
label_distributions_ : array, shape = [n_samples, n_classes]
Categorical distribution for each item.
transduction_ : array, shape = [n_samples]
Label assigned to each item via the transduction.
n_iter_ : int
29.32. sklearn.semi_supervised Semi-Supervised Learning
1851
scikit-learn user guide, Release 0.18.2
Number of iterations run.
See also:
LabelSpreadingAlternate label propagation strategy more robust to noise
References
Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Tech-
nical Report CMU-CALD-02-107, Carnegie Mellon University, 2002 http://pages.cs.wisc.edu/~jerryzhu/pub/
CMU-CALD-02-107.pdf
Examples
>>> from sklearn import datasets
>>> from sklearn.semi_supervised import LabelPropagation
>>> label_prop_model = LabelPropagation()
>>> iris = datasets.load_iris()
>>> random_unlabeled_points = np.where(np.random.randint(0, 2,
...
size=len(iris.target)))
>>> labels = np.copy(iris.target)
>>> labels[random_unlabeled_points] = -1
>>> label_prop_model.fit(iris.data, labels)
...
LabelPropagation(...)
Methods
fit(X, y)
Fit a semi-supervised label propagation model based
get_params([deep])
Get parameters for this estimator.
predict(X)
Performs inductive inference across the model.
predict_proba(X)
Predict probability for each possible outcome.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(kernel=’rbf’, gamma=20, n_neighbors=7, alpha=1, max_iter=30, tol=0.001, n_jobs=1)
fit(X, y)
Fit a semi-supervised label propagation model based
All the input data is provided matrix X (labeled and unlabeled) and corresponding label matrix y with a
dedicated marker value for unlabeled samples.
ParametersX : array-like, shape = [n_samples, n_features]
A {n_samples by n_samples} size matrix will be created from this
y : array_like, shape = [n_samples]
n_labeled_samples (unlabeled points are marked as -1) All unlabeled samples will be
transductively assigned labels
Returnsself : returns an instance of self.
1852
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Performs inductive inference across the model.
ParametersX : array_like, shape = [n_samples, n_features]
Returnsy : array_like, shape = [n_samples]
Predictions for input data
predict_proba(X)
Predict probability for each possible outcome.
Compute the probability estimates for each single sample in X and each possible outcome seen during
training (categorical distribution).
ParametersX : array_like, shape = [n_samples, n_features]
Returnsprobabilities : array, shape = [n_samples, n_classes]
Normalized probability distributions across class labels
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.32. sklearn.semi_supervised Semi-Supervised Learning
1853
scikit-learn user guide, Release 0.18.2
29.32.2 sklearn.semi_supervised.LabelSpreading
class sklearn.semi_supervised.LabelSpreading(kernel=’rbf’, gamma=20, n_neighbors=7, al-
pha=0.2, max_iter=30, tol=0.001, n_jobs=1)
LabelSpreading model for semi-supervised learning
This model is similar to the basic Label Propgation algorithm, but uses afﬁnity matrix based on the normalized
graph Laplacian and soft clamping across the labels.
Read more in the User Guide.
Parameterskernel : {‘knn’, ‘rbf’}
String identiﬁer for kernel function to use. Only ‘rbf’ and ‘knn’ kernels are currently
supported.
gamma : ﬂoat
parameter for rbf kernel
n_neighbors : integer > 0
parameter for knn kernel
alpha : ﬂoat
clamping factor
max_iter : ﬂoat
maximum number of iterations allowed
tol : ﬂoat
Convergence tolerance: threshold to consider the system at steady state
n_jobs : int, optional (default = 1)
The number of parallel jobs to run. If -1, then the number of jobs is set to the number
of CPU cores.
AttributesX_ : array, shape = [n_samples, n_features]
Input array.
classes_ : array, shape = [n_classes]
The distinct labels used in classifying instances.
label_distributions_ : array, shape = [n_samples, n_classes]
Categorical distribution for each item.
transduction_ : array, shape = [n_samples]
Label assigned to each item via the transduction.
n_iter_ : int
Number of iterations run.
See also:
LabelPropagationUnregularized graph based semi-supervised learning
1854
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
References
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, Bernhard Schoelkopf. Learning with
local and global consistency (2004) http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219
Examples
>>> from sklearn import datasets
>>> from sklearn.semi_supervised import LabelSpreading
>>> label_prop_model = LabelSpreading()
>>> iris = datasets.load_iris()
>>> random_unlabeled_points = np.where(np.random.randint(0, 2,
...
size=len(iris.target)))
>>> labels = np.copy(iris.target)
>>> labels[random_unlabeled_points] = -1
>>> label_prop_model.fit(iris.data, labels)
...
LabelSpreading(...)
Methods
fit(X, y)
Fit a semi-supervised label propagation model based
get_params([deep])
Get parameters for this estimator.
predict(X)
Performs inductive inference across the model.
predict_proba(X)
Predict probability for each possible outcome.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(kernel=’rbf’, gamma=20, n_neighbors=7, alpha=0.2, max_iter=30, tol=0.001, n_jobs=1)
fit(X, y)
Fit a semi-supervised label propagation model based
All the input data is provided matrix X (labeled and unlabeled) and corresponding label matrix y with a
dedicated marker value for unlabeled samples.
ParametersX : array-like, shape = [n_samples, n_features]
A {n_samples by n_samples} size matrix will be created from this
y : array_like, shape = [n_samples]
n_labeled_samples (unlabeled points are marked as -1) All unlabeled samples will be
transductively assigned labels
Returnsself : returns an instance of self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
29.32. sklearn.semi_supervised Semi-Supervised Learning
1855
scikit-learn user guide, Release 0.18.2
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Performs inductive inference across the model.
ParametersX : array_like, shape = [n_samples, n_features]
Returnsy : array_like, shape = [n_samples]
Predictions for input data
predict_proba(X)
Predict probability for each possible outcome.
Compute the probability estimates for each single sample in X and each possible outcome seen during
training (categorical distribution).
ParametersX : array_like, shape = [n_samples, n_features]
Returnsprobabilities : array, shape = [n_samples, n_classes]
Normalized probability distributions across class labels
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.semi_supervised.LabelSpreading
• Label Propagation digits: Demonstrating performance
• Label Propagation digits active learning
• Label Propagation learning a complex structure
• Decision boundary of label propagation versus SVM on the Iris dataset
1856
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.33 sklearn.svm: Support Vector Machines
The sklearn.svm module includes Support Vector Machine algorithms.
User guide: See the Support Vector Machines section for further details.
29.33.1 Estimators
svm.SVC([C, kernel, degree, gamma, coef0, ...])
C-Support Vector Classiﬁcation.
svm.LinearSVC([penalty, loss, dual, tol, C, ...])
Linear Support Vector Classiﬁcation.
svm.NuSVC([nu, kernel, degree, gamma, ...])
Nu-Support Vector Classiﬁcation.
svm.SVR([kernel, degree, gamma, coef0, tol, ...])
Epsilon-Support Vector Regression.
svm.LinearSVR([epsilon, tol, C, loss, ...])
Linear Support Vector Regression.
svm.NuSVR([nu, C, kernel, degree, gamma, ...])
Nu Support Vector Regression.
svm.OneClassSVM([kernel, degree, gamma, ...])
Unsupervised Outlier Detection.
sklearn.svm.SVC
class sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True,
probability=False,
tol=0.001,
cache_size=200,
class_weight=None,
verbose=False,
max_iter=-1,
decision_function_shape=None,
ran-
dom_state=None)
C-Support Vector Classiﬁcation.
The implementation is based on libsvm. The ﬁt time complexity is more than quadratic with the number of
samples which makes it hard to scale to dataset with more than a couple of 10000 samples.
The multiclass support is handled according to a one-vs-one scheme.
For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and
degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.
Read more in the User Guide.
ParametersC : ﬂoat, optional (default=1.0)
Penalty parameter C of the error term.
kernel : string, optional (default=’rbf’)
Speciﬁes the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’,
‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If
a callable is given it is used to pre-compute the kernel matrix from data matrices; that
matrix should be an array of shape (n_samples,n_samples).
degree : int, optional (default=3)
Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.
gamma : ﬂoat, optional (default=’auto’)
Kernel coefﬁcient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is ‘auto’ then 1/n_features
will be used instead.
coef0 : ﬂoat, optional (default=0.0)
Independent term in kernel function. It is only signiﬁcant in ‘poly’ and ‘sigmoid’.
probability : boolean, optional (default=False)
29.33. sklearn.svm: Support Vector Machines
1857
scikit-learn user guide, Release 0.18.2
Whether to enable probability estimates. This must be enabled prior to calling ﬁt, and
will slow down that method.
shrinking : boolean, optional (default=True)
Whether to use the shrinking heuristic.
tol : ﬂoat, optional (default=1e-3)
Tolerance for stopping criterion.
cache_size : ﬂoat, optional
Specify the size of the kernel cache (in MB).
class_weight : {dict, ‘balanced’}, optional
Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes
are supposed to have weight one. The “balanced” mode uses the values of y to auto-
matically adjust weights inversely proportional to class frequencies in the input data as
n_samples / (n_classes * np.bincount(y))
verbose : bool, default: False
Enable verbose output. Note that this setting takes advantage of a per-process runtime
setting in libsvm that, if enabled, may not work properly in a multithreaded context.
max_iter : int, optional (default=-1)
Hard limit on iterations within solver, or -1 for no limit.
decision_function_shape : ‘ovo’, ‘ovr’ or None, default=None
Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples,
n_classes) as all other classiﬁers, or the original one-vs-one (‘ovo’) decision function
of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). The default of
None will currently behave as ‘ovo’ for backward compatibility and raise a deprecation
warning, but will change ‘ovr’ in 0.19.
New in version 0.17: decision_function_shape=’ovr’ is recommended.
Changed in version 0.17: Deprecated decision_function_shape=’ovo’ and None.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data for
probability estimation.
Attributessupport_ : array-like, shape = [n_SV]
Indices of support vectors.
support_vectors_ : array-like, shape = [n_SV, n_features]
Support vectors.
n_support_ : array-like, dtype=int32, shape = [n_class]
Number of support vectors for each class.
dual_coef_ : array, shape = [n_class-1, n_SV]
Coefﬁcients of the support vector in the decision function. For multiclass, coefﬁcient for
all 1-vs-1 classiﬁers. The layout of the coefﬁcients in the multiclass case is somewhat
non-trivial. See the section about multi-class classiﬁcation in the SVM section of the
User Guide for details.
1858
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
coef_ : array, shape = [n_class-1, n_features]
Weights assigned to the features (coefﬁcients in the primal problem). This is only avail-
able in the case of a linear kernel.
coef_ is a readonly property derived from dual_coef_ and support_vectors_.
intercept_ : array, shape = [n_class * (n_class-1) / 2]
Constants in decision function.
See also:
SVRSupport Vector Machine for Regression implemented using libsvm.
LinearSVCScalable Linear Support Vector Machine for classiﬁcation implemented using liblinear. Check the
See also section of LinearSVC for more comparison element.
Examples
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> y = np.array([1, 1, 2, 2])
>>> from sklearn.svm import SVC
>>> clf = SVC()
>>> clf.fit(X, y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False)
>>> print(clf.predict([[-0.8, -1]]))
[1]
Methods
decision_function(X)
Distance of the samples X to the separating hyperplane.
fit(X, y[, sample_weight])
Fit the SVM model according to the given training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform classiﬁcation on samples in X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probabil-
ity=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1,
decision_function_shape=None, random_state=None)
decision_function(X)
Distance of the samples X to the separating hyperplane.
ParametersX : array-like, shape (n_samples, n_features)
ReturnsX : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)
Returns the decision function of the sample for each class in the model.
If deci-
sion_function_shape=’ovr’, the shape is (n_samples, n_classes)
29.33. sklearn.svm: Support Vector Machines
1859
scikit-learn user guide, Release 0.18.2
fit(X, y, sample_weight=None)
Fit the SVM model according to the given training data.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features. For kernel=”precomputed”, the expected shape of X is (n_samples,
n_samples).
y : array-like, shape (n_samples,)
Target values (class labels in classiﬁcation, real numbers in regression)
sample_weight : array-like, shape (n_samples,)
Per-sample weights. Rescale C per sample. Higher weights force the classiﬁer to put
more emphasis on these points.
Returnsself : object
Returns self.
Notes
If X and y are not C-ordered and contiguous arrays of np.ﬂoat64 and X is not a scipy.sparse.csr_matrix, X
and/or y may be copied.
If X is a dense array, then the other methods will not support sparse matrices as input.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform classiﬁcation on samples in X.
For an one-class model, +1 or -1 is returned.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
[n_samples_test,
n_samples_train]
Returnsy_pred : array, shape (n_samples,)
Class labels for samples in X.
predict_log_proba
Compute log probabilities of possible outcomes for samples in X.
The model need to have probability information computed at training time: ﬁt with attribute probability
set to True.
ParametersX : array-like, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
[n_samples_test,
n_samples_train]
1860
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ReturnsT : array-like, shape (n_samples, n_classes)
Returns the log-probabilities of the sample for each class in the model. The columns
correspond to the classes in sorted order, as they appear in the attribute classes_.
Notes
The probability model is created using cross validation, so the results can be slightly different than those
obtained by predict. Also, it will produce meaningless results on very small datasets.
predict_proba
Compute probabilities of possible outcomes for samples in X.
The model need to have probability information computed at training time: ﬁt with attribute probability
set to True.
ParametersX : array-like, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
[n_samples_test,
n_samples_train]
ReturnsT : array-like, shape (n_samples, n_classes)
Returns the probability of the sample for each class in the model. The columns corre-
spond to the classes in sorted order, as they appear in the attribute classes_.
Notes
The probability model is created using cross validation, so the results can be slightly different than those
obtained by predict. Also, it will produce meaningless results on very small datasets.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
29.33. sklearn.svm: Support Vector Machines
1861
scikit-learn user guide, Release 0.18.2
Examples using sklearn.svm.SVC
• Concatenating multiple feature extraction methods
• Feature Union with Heterogeneous Data Sources
• Explicit feature map approximation for RBF kernels
• Multilabel classiﬁcation
• Faces recognition example using eigenfaces and SVMs
• Libsvm GUI
• Plot classiﬁcation probability
• Classiﬁer comparison
• Recognizing hand-written digits
• Plot the decision boundaries of a VotingClassiﬁer
• Cross-validation on Digits Dataset Exercise
• SVM Exercise
• Pipeline Anova SVM
• Univariate Feature Selection
• Test with permutations the signiﬁcance of a classiﬁcation score
• Recursive feature elimination
• Recursive feature elimination with cross-validation
• Parameter estimation using grid search with cross-validation
• Confusion matrix
• Plotting Learning Curves
• Nested versus non-nested cross-validation
• Precision-Recall
• Receiver Operating Characteristic (ROC)
• Receiver Operating Characteristic (ROC) with cross validation
• Plotting Validation Curves
• Decision boundary of label propagation versus SVM on the Iris dataset
• SVM with custom kernel
• Plot different SVM classiﬁers in the iris dataset
• RBF SVM parameters
• SVM: Maximum margin separating hyperplane
• SVM: Separating hyperplane for unbalanced classes
• SVM-Anova: SVM with univariate feature selection
• SVM-Kernels
• SVM Margins Example
• SVM: Weighted samples
1862
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.svm.LinearSVC
class sklearn.svm.LinearSVC(penalty=’l2’,
loss=’squared_hinge’,
dual=True,
tol=0.0001,
C=1.0,
multi_class=’ovr’,
ﬁt_intercept=True,
intercept_scaling=1,
class_weight=None, verbose=0, random_state=None, max_iter=1000)
Linear Support Vector Classiﬁcation.
Similar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it
has more ﬂexibility in the choice of penalties and loss functions and should scale better to large numbers of
samples.
This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-
rest scheme.
Read more in the User Guide.
ParametersC : ﬂoat, optional (default=1.0)
Penalty parameter C of the error term.
loss : string, ‘hinge’ or ‘squared_hinge’ (default=’squared_hinge’)
Speciﬁes the loss function. ‘hinge’ is the standard SVM loss (used e.g. by the SVC
class) while ‘squared_hinge’ is the square of the hinge loss.
penalty : string, ‘l1’ or ‘l2’ (default=’l2’)
Speciﬁes the norm used in the penalization. The ‘l2’ penalty is the standard used in
SVC. The ‘l1’ leads to coef_ vectors that are sparse.
dual : bool, (default=True)
Select the algorithm to either solve the dual or primal optimization problem. Prefer
dual=False when n_samples > n_features.
tol : ﬂoat, optional (default=1e-4)
Tolerance for stopping criteria.
multi_class: string, ‘ovr’ or ‘crammer_singer’ (default=’ovr’) :
Determines the multi-class strategy if y contains more than two classes. "ovr" trains
n_classes one-vs-rest classiﬁers, while "crammer_singer" optimizes a joint objec-
tive over all classes. While crammer_singer is interesting from a theoretical perspective
as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and
is more expensive to compute. If "crammer_singer" is chosen, the options loss,
penalty and dual will be ignored.
ﬁt_intercept : boolean, optional (default=True)
Whether to calculate the intercept for this model. If set to false, no intercept will be
used in calculations (i.e. data is expected to be already centered).
intercept_scaling : ﬂoat, optional (default=1)
When
self.ﬁt_intercept
is
True,
instance
vector
x
becomes
[x,self.intercept_scaling], i.e.
a “synthetic” feature with constant
value equals to intercept_scaling is appended to the instance vector. The intercept be-
comes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is
subject to l1/l2 regularization as all other features. To lessen the effect of regularization
on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be
increased.
class_weight : {dict, ‘balanced’}, optional
29.33. sklearn.svm: Support Vector Machines
1863
scikit-learn user guide, Release 0.18.2
Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all
classes are supposed to have weight one. The “balanced” mode uses the values of y
to automatically adjust weights inversely proportional to class frequencies in the input
data as n_samples / (n_classes * np.bincount(y))
verbose : int, (default=0)
Enable verbose output. Note that this setting takes advantage of a per-process runtime
setting in liblinear that, if enabled, may not work properly in a multithreaded context.
random_state : int seed, RandomState instance, or None (default=None)
The seed of the pseudo random number generator to use when shufﬂing the data.
max_iter : int, (default=1000)
The maximum number of iterations to be run.
Attributescoef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]
Weights assigned to the features (coefﬁcients in the primal problem). This is only avail-
able in the case of a linear kernel.
coef_ is a readonly property derived from raw_coef_ that follows the internal mem-
ory layout of liblinear.
intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]
Constants in decision function.
See also:
SVCImplementation of Support Vector Machine classiﬁer using libsvm: the kernel can be non-linear but its
SMO algorithm does not scale to large number of samples as LinearSVC does. Furthermore SVC multi-
class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest. It is possible to
implement one vs the rest with SVC by using the sklearn.multiclass.OneVsRestClassifier
wrapper. Finally SVC can ﬁt dense data without memory copy if the input is C-contiguous. Sparse data
will still incur memory copy though.
sklearn.linear_model.SGDClassifierSGDClassiﬁer can optimize the same cost function as Lin-
earSVC by adjusting the penalty and loss parameters. In addition it requires less memory, allows incre-
mental (online) learning, and implements various loss functions and regularization regimes.
Notes
The underlying C implementation uses a random number generator to select features when ﬁtting the model.
It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a
smaller tol parameter.
The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a
memory copy.
Predict output may not match that of standalone liblinear in certain cases. See differences from liblinear in the
narrative documentation.
References
LIBLINEAR: A Library for Large Linear Classiﬁcation
1864
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Methods
decision_function(X)
Predict conﬁdence scores for samples.
densify()
Convert coefﬁcient matrix to dense array format.
fit(X, y[, sample_weight])
Fit the model according to the given training data.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class labels for samples in X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
sparsify()
Convert coefﬁcient matrix to sparse format.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(penalty=’l2’, loss=’squared_hinge’, dual=True, tol=0.0001, C=1.0, multi_class=’ovr’,
ﬁt_intercept=True,
intercept_scaling=1,
class_weight=None,
verbose=0,
ran-
dom_state=None, max_iter=1000)
decision_function(X)
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
densify()
Convert coefﬁcient matrix to dense array format.
Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is
required for ﬁtting, so calling this method is only required on models that have previously been sparsiﬁed;
otherwise, it is a no-op.
Returnsself: estimator :
fit(X, y, sample_weight=None)
Fit the model according to the given training data.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples]
Target vector relative to X
sample_weight : array-like, shape = [n_samples], optional
Array of weights that are assigned to individual samples. If not provided, then each
sample is given unit weight.
Returnsself : object
29.33. sklearn.svm: Support Vector Machines
1865
scikit-learn user guide, Release 0.18.2
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class labels for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
1866
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sparsify()
Convert coefﬁcient matrix to sparse format.
Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more
memory- and storage-efﬁcient than the usual numpy.ndarray representation.
The intercept_ member is not converted.
Returnsself: estimator :
Notes
For non-sparse models, i.e. when there are not many zeros in coef_, this may actually increase memory
usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be
computed with (coef_ == 0).sum(), must be more than 50% for this to provide signiﬁcant beneﬁts.
After calling this method, further ﬁtting with the partial_ﬁt method (if any) will not work until you call
densify.
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.svm.LinearSVC
• Selecting dimensionality reduction with Pipeline and GridSearchCV
• Explicit feature map approximation for RBF kernels
• Probability Calibration curves
• Comparison of Calibration of Classiﬁers
29.33. sklearn.svm: Support Vector Machines
1867
scikit-learn user guide, Release 0.18.2
• Plot different SVM classiﬁers in the iris dataset
• Scaling the regularization parameter for SVCs
• Classiﬁcation of text documents using sparse features
sklearn.svm.NuSVC
class sklearn.svm.NuSVC(nu=0.5, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True,
probability=False,
tol=0.001,
cache_size=200,
class_weight=None,
verbose=False,
max_iter=-1,
decision_function_shape=None,
ran-
dom_state=None)
Nu-Support Vector Classiﬁcation.
Similar to SVC but uses a parameter to control the number of support vectors.
The implementation is based on libsvm.
Read more in the User Guide.
Parametersnu : ﬂoat, optional (default=0.5)
An upper bound on the fraction of training errors and a lower bound of the fraction of
support vectors. Should be in the interval (0, 1].
kernel : string, optional (default=’rbf’)
Speciﬁes the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’,
‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a
callable is given it is used to precompute the kernel matrix.
degree : int, optional (default=3)
Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.
gamma : ﬂoat, optional (default=’auto’)
Kernel coefﬁcient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is ‘auto’ then 1/n_features
will be used instead.
coef0 : ﬂoat, optional (default=0.0)
Independent term in kernel function. It is only signiﬁcant in ‘poly’ and ‘sigmoid’.
probability : boolean, optional (default=False)
Whether to enable probability estimates. This must be enabled prior to calling ﬁt, and
will slow down that method.
shrinking : boolean, optional (default=True)
Whether to use the shrinking heuristic.
tol : ﬂoat, optional (default=1e-3)
Tolerance for stopping criterion.
cache_size : ﬂoat, optional
Specify the size of the kernel cache (in MB).
class_weight : {dict, ‘balanced’}, optional
1868
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes
are supposed to have weight one. The “balanced” mode uses the values of y to auto-
matically adjust weights inversely proportional to class frequencies as n_samples /
(n_classes * np.bincount(y))
verbose : bool, default: False
Enable verbose output. Note that this setting takes advantage of a per-process runtime
setting in libsvm that, if enabled, may not work properly in a multithreaded context.
max_iter : int, optional (default=-1)
Hard limit on iterations within solver, or -1 for no limit.
decision_function_shape : ‘ovo’, ‘ovr’ or None, default=None
Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples,
n_classes) as all other classiﬁers, or the original one-vs-one (‘ovo’) decision function
of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). The default of
None will currently behave as ‘ovo’ for backward compatibility and raise a deprecation
warning, but will change ‘ovr’ in 0.19.
New in version 0.17: decision_function_shape=’ovr’ is recommended.
Changed in version 0.17: Deprecated decision_function_shape=’ovo’ and None.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data for
probability estimation.
Attributessupport_ : array-like, shape = [n_SV]
Indices of support vectors.
support_vectors_ : array-like, shape = [n_SV, n_features]
Support vectors.
n_support_ : array-like, dtype=int32, shape = [n_class]
Number of support vectors for each class.
dual_coef_ : array, shape = [n_class-1, n_SV]
Coefﬁcients of the support vector in the decision function. For multiclass, coefﬁcient for
all 1-vs-1 classiﬁers. The layout of the coefﬁcients in the multiclass case is somewhat
non-trivial. See the section about multi-class classiﬁcation in the SVM section of the
User Guide for details.
coef_ : array, shape = [n_class-1, n_features]
Weights assigned to the features (coefﬁcients in the primal problem). This is only avail-
able in the case of a linear kernel.
coef_ is readonly property derived from dual_coef_ and support_vectors_.
intercept_ : array, shape = [n_class * (n_class-1) / 2]
Constants in decision function.
See also:
SVCSupport Vector Machine for classiﬁcation using libsvm.
LinearSVCScalable linear Support Vector Machine for classiﬁcation using liblinear.
29.33. sklearn.svm: Support Vector Machines
1869
scikit-learn user guide, Release 0.18.2
Examples
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>>> y = np.array([1, 1, 2, 2])
>>> from sklearn.svm import NuSVC
>>> clf = NuSVC()
>>> clf.fit(X, y)
NuSVC(cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, nu=0.5, probability=False, random_state=None,
shrinking=True, tol=0.001, verbose=False)
>>> print(clf.predict([[-0.8, -1]]))
[1]
Methods
decision_function(X)
Distance of the samples X to the separating hyperplane.
fit(X, y[, sample_weight])
Fit the SVM model according to the given training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform classiﬁcation on samples in X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(nu=0.5, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probabil-
ity=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1,
decision_function_shape=None, random_state=None)
decision_function(X)
Distance of the samples X to the separating hyperplane.
ParametersX : array-like, shape (n_samples, n_features)
ReturnsX : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)
Returns the decision function of the sample for each class in the model.
If deci-
sion_function_shape=’ovr’, the shape is (n_samples, n_classes)
fit(X, y, sample_weight=None)
Fit the SVM model according to the given training data.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features. For kernel=”precomputed”, the expected shape of X is (n_samples,
n_samples).
y : array-like, shape (n_samples,)
Target values (class labels in classiﬁcation, real numbers in regression)
sample_weight : array-like, shape (n_samples,)
Per-sample weights. Rescale C per sample. Higher weights force the classiﬁer to put
more emphasis on these points.
1870
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsself : object
Returns self.
Notes
If X and y are not C-ordered and contiguous arrays of np.ﬂoat64 and X is not a scipy.sparse.csr_matrix, X
and/or y may be copied.
If X is a dense array, then the other methods will not support sparse matrices as input.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform classiﬁcation on samples in X.
For an one-class model, +1 or -1 is returned.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
[n_samples_test,
n_samples_train]
Returnsy_pred : array, shape (n_samples,)
Class labels for samples in X.
predict_log_proba
Compute log probabilities of possible outcomes for samples in X.
The model need to have probability information computed at training time: ﬁt with attribute probability
set to True.
ParametersX : array-like, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
[n_samples_test,
n_samples_train]
ReturnsT : array-like, shape (n_samples, n_classes)
Returns the log-probabilities of the sample for each class in the model. The columns
correspond to the classes in sorted order, as they appear in the attribute classes_.
Notes
The probability model is created using cross validation, so the results can be slightly different than those
obtained by predict. Also, it will produce meaningless results on very small datasets.
predict_proba
Compute probabilities of possible outcomes for samples in X.
29.33. sklearn.svm: Support Vector Machines
1871
scikit-learn user guide, Release 0.18.2
The model need to have probability information computed at training time: ﬁt with attribute probability
set to True.
ParametersX : array-like, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
[n_samples_test,
n_samples_train]
ReturnsT : array-like, shape (n_samples, n_classes)
Returns the probability of the sample for each class in the model. The columns corre-
spond to the classes in sorted order, as they appear in the attribute classes_.
Notes
The probability model is created using cross validation, so the results can be slightly different than those
obtained by predict. Also, it will produce meaningless results on very small datasets.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.svm.NuSVC
• Non-linear SVM
sklearn.svm.SVR
class sklearn.svm.SVR(kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, tol=0.001, C=1.0, ep-
silon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)
Epsilon-Support Vector Regression.
The free parameters in the model are C and epsilon.
1872
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The implementation is based on libsvm.
Read more in the User Guide.
ParametersC : ﬂoat, optional (default=1.0)
Penalty parameter C of the error term.
epsilon : ﬂoat, optional (default=0.1)
Epsilon in the epsilon-SVR model. It speciﬁes the epsilon-tube within which no penalty
is associated in the training loss function with points predicted within a distance epsilon
from the actual value.
kernel : string, optional (default=’rbf’)
Speciﬁes the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’,
‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a
callable is given it is used to precompute the kernel matrix.
degree : int, optional (default=3)
Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.
gamma : ﬂoat, optional (default=’auto’)
Kernel coefﬁcient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is ‘auto’ then 1/n_features
will be used instead.
coef0 : ﬂoat, optional (default=0.0)
Independent term in kernel function. It is only signiﬁcant in ‘poly’ and ‘sigmoid’.
shrinking : boolean, optional (default=True)
Whether to use the shrinking heuristic.
tol : ﬂoat, optional (default=1e-3)
Tolerance for stopping criterion.
cache_size : ﬂoat, optional
Specify the size of the kernel cache (in MB).
verbose : bool, default: False
Enable verbose output. Note that this setting takes advantage of a per-process runtime
setting in libsvm that, if enabled, may not work properly in a multithreaded context.
max_iter : int, optional (default=-1)
Hard limit on iterations within solver, or -1 for no limit.
Attributessupport_ : array-like, shape = [n_SV]
Indices of support vectors.
support_vectors_ : array-like, shape = [nSV, n_features]
Support vectors.
dual_coef_ : array, shape = [1, n_SV]
Coefﬁcients of the support vector in the decision function.
coef_ : array, shape = [1, n_features]
29.33. sklearn.svm: Support Vector Machines
1873
scikit-learn user guide, Release 0.18.2
Weights assigned to the features (coefﬁcients in the primal problem). This is only avail-
able in the case of a linear kernel.
coef_ is readonly property derived from dual_coef_ and support_vectors_.
intercept_ : array, shape = [1]
Constants in decision function.
sample_weight : array-like, shape = [n_samples]
Individual weights for each sample
See also:
NuSVRSupport Vector Machine for regression implemented using libsvm using a parameter to control the num-
ber of support vectors.
LinearSVRScalable Linear Support Vector Machine for regression implemented using liblinear.
Examples
>>> from sklearn.svm import SVR
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> np.random.seed(0)
>>> y = np.random.randn(n_samples)
>>> X = np.random.randn(n_samples, n_features)
>>> clf = SVR(C=1.0, epsilon=0.2)
>>> clf.fit(X, y)
SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='auto',
kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19
fit(X, y[, sample_weight])
Fit the SVM model according to the given training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform regression on samples in X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrink-
ing=True, cache_size=200, verbose=False, max_iter=-1)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19
Distance of the samples X to the separating hyperplane.
ParametersX : array-like, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
[n_samples_test,
n_samples_train].
ReturnsX : array-like, shape (n_samples, n_class * (n_class-1) / 2)
1874
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returns the decision function of the sample for each class in the model.
fit(X, y, sample_weight=None)
Fit the SVM model according to the given training data.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features. For kernel=”precomputed”, the expected shape of X is (n_samples,
n_samples).
y : array-like, shape (n_samples,)
Target values (class labels in classiﬁcation, real numbers in regression)
sample_weight : array-like, shape (n_samples,)
Per-sample weights. Rescale C per sample. Higher weights force the classiﬁer to put
more emphasis on these points.
Returnsself : object
Returns self.
Notes
If X and y are not C-ordered and contiguous arrays of np.ﬂoat64 and X is not a scipy.sparse.csr_matrix, X
and/or y may be copied.
If X is a dense array, then the other methods will not support sparse matrices as input.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform regression on samples in X.
For an one-class model, +1 or -1 is returned.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
(n_samples_test,
n_samples_train).
Returnsy_pred : array, shape (n_samples,)
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
29.33. sklearn.svm: Support Vector Machines
1875
scikit-learn user guide, Release 0.18.2
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.svm.SVR
• Comparison of kernel ridge regression and SVR
• Prediction Latency
• Support Vector Regression (SVR) using linear and non-linear kernels
sklearn.svm.LinearSVR
class sklearn.svm.LinearSVR(epsilon=0.0,
tol=0.0001,
C=1.0,
loss=’epsilon_insensitive’,
ﬁt_intercept=True,
intercept_scaling=1.0,
dual=True,
verbose=0,
random_state=None, max_iter=1000)
Linear Support Vector Regression.
Similar to SVR with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it
has more ﬂexibility in the choice of penalties and loss functions and should scale better to large numbers of
samples.
This class supports both dense and sparse input.
Read more in the User Guide.
ParametersC : ﬂoat, optional (default=1.0)
Penalty parameter C of the error term. The penalty is a squared l2 penalty. The bigger
this parameter, the less regularization is used.
loss
:
string,
‘epsilon_insensitive’
or
‘squared_epsilon_insensitive’
(de-
fault=’epsilon_insensitive’)
Speciﬁes the loss function. ‘l1’ is the epsilon-insensitive loss (standard SVR) while ‘l2’
is the squared epsilon-insensitive loss.
epsilon : ﬂoat, optional (default=0.1)
Epsilon parameter in the epsilon-insensitive loss function. Note that the value of this
parameter depends on the scale of the target variable y. If unsure, set epsilon=0.
dual : bool, (default=True)
1876
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Select the algorithm to either solve the dual or primal optimization problem. Prefer
dual=False when n_samples > n_features.
tol : ﬂoat, optional (default=1e-4)
Tolerance for stopping criteria.
ﬁt_intercept : boolean, optional (default=True)
Whether to calculate the intercept for this model. If set to false, no intercept will be
used in calculations (i.e. data is expected to be already centered).
intercept_scaling : ﬂoat, optional (default=1)
When self.ﬁt_intercept is True, instance vector x becomes [x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equals to intercept_scaling is appended to
the instance vector. The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization as all other features.
To lessen the effect of regularization on synthetic feature weight (and therefore on the
intercept) intercept_scaling has to be increased.
verbose : int, (default=0)
Enable verbose output. Note that this setting takes advantage of a per-process runtime
setting in liblinear that, if enabled, may not work properly in a multithreaded context.
random_state : int seed, RandomState instance, or None (default=None)
The seed of the pseudo random number generator to use when shufﬂing the data.
max_iter : int, (default=1000)
The maximum number of iterations to be run.
Attributescoef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]
Weights assigned to the features (coefﬁcients in the primal problem). This is only avail-
able in the case of a linear kernel.
coef_ is a readonly property derived from raw_coef_ that follows the internal memory
layout of liblinear.
intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]
Constants in decision function.
See also:
LinearSVCImplementation of Support Vector Machine classiﬁer using the same library as this class (liblin-
ear).
SVRImplementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its
SMO algorithm does not scale to large number of samples as LinearSVC does.
sklearn.linear_model.SGDRegressorSGDRegressor can optimize the same cost function as Lin-
earSVR by adjusting the penalty and loss parameters. In addition it requires less memory, allows incre-
mental (online) learning, and implements various loss functions and regularization regimes.
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19.
Continued on next page
29.33. sklearn.svm: Support Vector Machines
1877
scikit-learn user guide, Release 0.18.2
Table 29.247 – continued from previous page
fit(X, y[, sample_weight])
Fit the model according to the given training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict using the linear model
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(epsilon=0.0, tol=0.0001, C=1.0, loss=’epsilon_insensitive’, ﬁt_intercept=True, inter-
cept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19.
Decision function of the linear model.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
Returns predicted values.
fit(X, y, sample_weight=None)
Fit the model according to the given training data.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples]
Target vector relative to X
sample_weight : array-like, shape = [n_samples], optional
Array of weights that are assigned to individual samples. If not provided, then each
sample is given unit weight.
Returnsself : object
Returns self.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict using the linear model
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
ReturnsC : array, shape = (n_samples,)
1878
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returns predicted values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sklearn.svm.NuSVR
class sklearn.svm.NuSVR(nu=0.5, C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrink-
ing=True, tol=0.001, cache_size=200, verbose=False, max_iter=-1)
Nu Support Vector Regression.
Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However,
unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR.
The implementation is based on libsvm.
Read more in the User Guide.
ParametersC : ﬂoat, optional (default=1.0)
Penalty parameter C of the error term.
nu : ﬂoat, optional
An upper bound on the fraction of training errors and a lower bound of the fraction of
support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.
kernel : string, optional (default=’rbf’)
Speciﬁes the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’,
‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a
callable is given it is used to precompute the kernel matrix.
degree : int, optional (default=3)
29.33. sklearn.svm: Support Vector Machines
1879
scikit-learn user guide, Release 0.18.2
Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.
gamma : ﬂoat, optional (default=’auto’)
Kernel coefﬁcient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is ‘auto’ then 1/n_features
will be used instead.
coef0 : ﬂoat, optional (default=0.0)
Independent term in kernel function. It is only signiﬁcant in ‘poly’ and ‘sigmoid’.
shrinking : boolean, optional (default=True)
Whether to use the shrinking heuristic.
tol : ﬂoat, optional (default=1e-3)
Tolerance for stopping criterion.
cache_size : ﬂoat, optional
Specify the size of the kernel cache (in MB).
verbose : bool, default: False
Enable verbose output. Note that this setting takes advantage of a per-process runtime
setting in libsvm that, if enabled, may not work properly in a multithreaded context.
max_iter : int, optional (default=-1)
Hard limit on iterations within solver, or -1 for no limit.
Attributessupport_ : array-like, shape = [n_SV]
Indices of support vectors.
support_vectors_ : array-like, shape = [nSV, n_features]
Support vectors.
dual_coef_ : array, shape = [1, n_SV]
Coefﬁcients of the support vector in the decision function.
coef_ : array, shape = [1, n_features]
Weights assigned to the features (coefﬁcients in the primal problem). This is only avail-
able in the case of a linear kernel.
coef_ is readonly property derived from dual_coef_ and support_vectors_.
intercept_ : array, shape = [1]
Constants in decision function.
See also:
NuSVCSupport Vector Machine for classiﬁcation implemented with libsvm with a parameter to control the
number of support vectors.
SVRepsilon Support Vector Machine for regression implemented with libsvm.
Examples
1880
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> from sklearn.svm import NuSVR
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> np.random.seed(0)
>>> y = np.random.randn(n_samples)
>>> X = np.random.randn(n_samples, n_features)
>>> clf = NuSVR(C=1.0, nu=0.1)
>>> clf.fit(X, y)
NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma='auto',
kernel='rbf', max_iter=-1, nu=0.1, shrinking=True, tol=0.001,
verbose=False)
Methods
decision_function(\*args, \*\*kwargs)
DEPRECATED: and will be removed in 0.19
fit(X, y[, sample_weight])
Fit the SVM model according to the given training data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform regression on samples in X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(nu=0.5, C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True,
tol=0.001, cache_size=200, verbose=False, max_iter=-1)
decision_function(*args, **kwargs)
DEPRECATED: and will be removed in 0.19
Distance of the samples X to the separating hyperplane.
ParametersX : array-like, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
[n_samples_test,
n_samples_train].
ReturnsX : array-like, shape (n_samples, n_class * (n_class-1) / 2)
Returns the decision function of the sample for each class in the model.
fit(X, y, sample_weight=None)
Fit the SVM model according to the given training data.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Training vectors, where n_samples is the number of samples and n_features is the num-
ber of features. For kernel=”precomputed”, the expected shape of X is (n_samples,
n_samples).
y : array-like, shape (n_samples,)
Target values (class labels in classiﬁcation, real numbers in regression)
sample_weight : array-like, shape (n_samples,)
Per-sample weights. Rescale C per sample. Higher weights force the classiﬁer to put
more emphasis on these points.
Returnsself : object
29.33. sklearn.svm: Support Vector Machines
1881
scikit-learn user guide, Release 0.18.2
Returns self.
Notes
If X and y are not C-ordered and contiguous arrays of np.ﬂoat64 and X is not a scipy.sparse.csr_matrix, X
and/or y may be copied.
If X is a dense array, then the other methods will not support sparse matrices as input.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform regression on samples in X.
For an one-class model, +1 or -1 is returned.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
(n_samples_test,
n_samples_train).
Returnsy_pred : array, shape (n_samples,)
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
1882
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples using sklearn.svm.NuSVR
• Model Complexity Inﬂuence
sklearn.svm.OneClassSVM
class sklearn.svm.OneClassSVM(kernel=’rbf’,
degree=3,
gamma=’auto’,
coef0=0.0,
tol=0.001,
nu=0.5,
shrinking=True,
cache_size=200,
verbose=False,
max_iter=-1, random_state=None)
Unsupervised Outlier Detection.
Estimate the support of a high-dimensional distribution.
The implementation is based on libsvm.
Read more in the User Guide.
Parameterskernel : string, optional (default=’rbf’)
Speciﬁes the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’,
‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a
callable is given it is used to precompute the kernel matrix.
nu : ﬂoat, optional
An upper bound on the fraction of training errors and a lower bound of the fraction of
support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.
degree : int, optional (default=3)
Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.
gamma : ﬂoat, optional (default=’auto’)
Kernel coefﬁcient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is ‘auto’ then 1/n_features
will be used instead.
coef0 : ﬂoat, optional (default=0.0)
Independent term in kernel function. It is only signiﬁcant in ‘poly’ and ‘sigmoid’.
tol : ﬂoat, optional
Tolerance for stopping criterion.
shrinking : boolean, optional
Whether to use the shrinking heuristic.
cache_size : ﬂoat, optional
Specify the size of the kernel cache (in MB).
verbose : bool, default: False
Enable verbose output. Note that this setting takes advantage of a per-process runtime
setting in libsvm that, if enabled, may not work properly in a multithreaded context.
max_iter : int, optional (default=-1)
Hard limit on iterations within solver, or -1 for no limit.
random_state : int seed, RandomState instance, or None (default)
The seed of the pseudo random number generator to use when shufﬂing the data for
probability estimation.
29.33. sklearn.svm: Support Vector Machines
1883
scikit-learn user guide, Release 0.18.2
Attributessupport_ : array-like, shape = [n_SV]
Indices of support vectors.
support_vectors_ : array-like, shape = [nSV, n_features]
Support vectors.
dual_coef_ : array, shape = [n_classes-1, n_SV]
Coefﬁcients of the support vectors in the decision function.
coef_ : array, shape = [n_classes-1, n_features]
Weights assigned to the features (coefﬁcients in the primal problem). This is only avail-
able in the case of a linear kernel.
coef_ is readonly property derived from dual_coef_ and support_vectors_
intercept_ : array, shape = [n_classes-1]
Constants in decision function.
Methods
decision_function(X)
Distance of the samples X to the separating hyperplane.
fit(X[, y, sample_weight])
Detects the soft boundary of the set of samples X.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform regression on samples in X.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, tol=0.001, nu=0.5, shrinking=True,
cache_size=200, verbose=False, max_iter=-1, random_state=None)
decision_function(X)
Distance of the samples X to the separating hyperplane.
ParametersX : array-like, shape (n_samples, n_features)
ReturnsX : array-like, shape (n_samples,)
Returns the decision function of the samples.
fit(X, y=None, sample_weight=None, **params)
Detects the soft boundary of the set of samples X.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
Set of samples, where n_samples is the number of samples and n_features is the number
of features.
sample_weight : array-like, shape (n_samples,)
Per-sample weights. Rescale C per sample. Higher weights force the classiﬁer to put
more emphasis on these points.
Returnsself : object
Returns self.
1884
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
If X is not a C-ordered contiguous array it is copied.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform regression on samples in X.
For an one-class model, +1 or -1 is returned.
ParametersX : {array-like, sparse matrix}, shape (n_samples, n_features)
For
kernel=”precomputed”,
the
expected
shape
of
X
is
(n_samples_test,
n_samples_train).
Returnsy_pred : array, shape (n_samples,)
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
Examples using sklearn.svm.OneClassSVM
• Outlier detection on a real data set
• Species distribution modeling
• Libsvm GUI
• Outlier detection with several methods.
• One-class SVM with non-linear kernel (RBF)
svm.l1_min_c(X, y[, loss, ﬁt_intercept, ...])
Return the lowest bound for C such that for C in (l1_min_C,
inﬁnity) the model is guaranteed not to be empty.
sklearn.svm.l1_min_c
sklearn.svm.l1_min_c(X, y, loss=’squared_hinge’, ﬁt_intercept=True, intercept_scaling=1.0)
Return the lowest bound for C such that for C in (l1_min_C, inﬁnity) the model is guaranteed not
to be empty.
This applies to l1 penalized classiﬁers, such as LinearSVC with penalty=’l1’ and lin-
ear_model.LogisticRegression with penalty=’l1’.
This value is valid if class_weight parameter in ﬁt() is not set.
29.33. sklearn.svm: Support Vector Machines
1885
scikit-learn user guide, Release 0.18.2
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
y : array, shape = [n_samples]
Target vector relative to X
loss : {‘squared_hinge’, ‘log’}, default ‘squared_hinge’
Speciﬁes the loss function. With ‘squared_hinge’ it is the squared hinge loss (a.k.a. L2
loss). With ‘log’ it is the loss of logistic regression models. ‘l2’ is accepted as an alias
for ‘squared_hinge’, for backward compatibility reasons, but should not be used in new
code.
ﬁt_intercept : bool, default: True
Speciﬁes if the intercept should be ﬁtted by the model. It must match the ﬁt() method
parameter.
intercept_scaling : ﬂoat, default: 1
when ﬁt_intercept is True, instance vector x becomes [x, intercept_scaling], i.e. a “syn-
thetic” feature with constant value equals to intercept_scaling is appended to the in-
stance vector. It must match the ﬁt() method parameter.
Returnsl1_min_c: ﬂoat :
minimum value for C
Examples using sklearn.svm.l1_min_c
• Path with L1- Logistic Regression
29.33.2 Low-level methods
svm.libsvm.fit
Train the model using libsvm (low-level method)
svm.libsvm.decision_function
Predict margin (libsvm name for this is predict_values)
svm.libsvm.predict
Predict target values of X given a model (low-level method)
svm.libsvm.predict_proba
Predict probabilities
svm.libsvm.cross_validation
Binding of the cross-validation routine (low-level routine)
sklearn.svm.libsvm.ﬁt
sklearn.svm.libsvm.fit()
Train the model using libsvm (low-level method)
ParametersX : array-like, dtype=ﬂoat64, size=[n_samples, n_features]
Y : array, dtype=ﬂoat64, size=[n_samples]
target vector
svm_type : {0, 1, 2, 3, 4}, optional
Type of SVM: C_SVC, NuSVC, OneClassSVM, EpsilonSVR or NuSVR respectively.
0 by default.
1886
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
kernel : {‘linear’, ‘rbf’, ‘poly’, ‘sigmoid’, ‘precomputed’}, optional
Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed. ‘rbf’ by
default.
degree : int32, optional
Degree of the polynomial kernel (only relevant if kernel is set to polynomial), 3 by
default.
gamma : ﬂoat64, optional
Gamma parameter in RBF kernel (only relevant if kernel is set to RBF). 0.1 by default.
coef0 : ﬂoat64, optional
Independent parameter in poly/sigmoid kernel. 0 by default.
tol : ﬂoat64, optional
Numeric stopping criterion (WRITEME). 1e-3 by default.
C : ﬂoat64, optional
C parameter in C-Support Vector Classiﬁcation. 1 by default.
nu : ﬂoat64, optional
0.5 by default.
epsilon : double, optional
0.1 by default.
class_weight : array, dtype ﬂoat64, shape (n_classes,), optional
np.empty(0) by default.
sample_weight : array, dtype ﬂoat64, shape (n_samples,), optional
np.empty(0) by default.
shrinking : int, optional
1 by default.
probability : int, optional
0 by default.
cache_size : ﬂoat64, optional
Cache size for gram matrix columns (in megabytes). 100 by default.
max_iter : int (-1 for no limit), optional.
Stop solver after this many iterations regardless of accuracy (XXX Currently there is no
API to know whether this kicked in.) -1 by default.
random_seed : int, optional
Seed for the random number generator used for probability estimates. 0 by default.
Returnssupport : array, shape=[n_support]
index of support vectors
support_vectors : array, shape=[n_support, n_features]
29.33. sklearn.svm: Support Vector Machines
1887
scikit-learn user guide, Release 0.18.2
support vectors (equivalent to X[support]). Will return an empty array in the case of
precomputed kernel.
n_class_SV : array
number of support vectors in each class.
sv_coef : array
coefﬁcients of support vectors in decision function.
intercept : array
intercept in decision function
probA, probB : array
probability estimates, empty array for probability=False
sklearn.svm.libsvm.decision_function
sklearn.svm.libsvm.decision_function()
Predict margin (libsvm name for this is predict_values)
We have to reconstruct model and parameters to make sure we stay in sync with the python object.
sklearn.svm.libsvm.predict
sklearn.svm.libsvm.predict()
Predict target values of X given a model (low-level method)
ParametersX: array-like, dtype=ﬂoat, size=[n_samples, n_features] :
svm_type : {0, 1, 2, 3, 4}
Type of SVM: C SVC, nu SVC, one class, epsilon SVR, nu SVR
kernel : {‘linear’, ‘rbf’, ‘poly’, ‘sigmoid’, ‘precomputed’}
Type of kernel.
degree : int
Degree of the polynomial kernel.
gamma : ﬂoat
Gamma parameter in RBF kernel.
coef0 : ﬂoat
Independent parameter in poly/sigmoid kernel.
Returnsdec_values : array
predicted values.
sklearn.svm.libsvm.predict_proba
sklearn.svm.libsvm.predict_proba()
Predict probabilities
svm_model stores all parameters needed to predict a given value.
1888
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
For speed, all real work is done at the C level in function copy_predict (libsvm_helper.c).
We have to reconstruct model and parameters to make sure we stay in sync with the python object.
See sklearn.svm.predict for a complete list of parameters.
ParametersX: array-like, dtype=ﬂoat :
kernel : {‘linear’, ‘rbf’, ‘poly’, ‘sigmoid’, ‘precomputed’}
Returnsdec_values : array
predicted values.
sklearn.svm.libsvm.cross_validation
sklearn.svm.libsvm.cross_validation()
Binding of the cross-validation routine (low-level routine)
ParametersX: array-like, dtype=ﬂoat, size=[n_samples, n_features] :
Y: array, dtype=ﬂoat, size=[n_samples] :
target vector
svm_type : {0, 1, 2, 3, 4}
Type of SVM: C SVC, nu SVC, one class, epsilon SVR, nu SVR
kernel : {‘linear’, ‘rbf’, ‘poly’, ‘sigmoid’, ‘precomputed’}
Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed.
degree : int
Degree of the polynomial kernel (only relevant if kernel is set to polynomial)
gamma : ﬂoat
Gamma parameter in RBF kernel (only relevant if kernel is set to RBF)
coef0 : ﬂoat
Independent parameter in poly/sigmoid kernel.
tol : ﬂoat
Stopping criteria.
C : ﬂoat
C parameter in C-Support Vector Classiﬁcation
nu : ﬂoat
cache_size : ﬂoat
random_seed : int, optional
Seed for the random number generator used for probability estimates. 0 by default.
Returnstarget : array, ﬂoat
29.33. sklearn.svm: Support Vector Machines
1889
scikit-learn user guide, Release 0.18.2
29.34 sklearn.tree: Decision Trees
The sklearn.tree module includes decision tree-based models for classiﬁcation and regression.
User guide: See the Decision Trees section for further details.
tree.DecisionTreeClassifier([criterion, ...])
A decision tree classiﬁer.
tree.DecisionTreeRegressor([criterion, ...])
A decision tree regressor.
tree.ExtraTreeClassifier([criterion, ...])
An extremely randomized tree classiﬁer.
tree.ExtraTreeRegressor([criterion, ...])
An extremely randomized tree regressor.
29.34.1 sklearn.tree.DecisionTreeClassiﬁer
class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=None,
random_state=None,
max_leaf_nodes=None, min_impurity_split=1e-07,
class_weight=None, presort=False)
A decision tree classiﬁer.
Read more in the User Guide.
Parameterscriterion : string, optional (default=”gini”)
The function to measure the quality of a split. Supported criteria are “gini” for the Gini
impurity and “entropy” for the information gain.
splitter : string, optional (default=”best”)
The strategy used to choose the split at each node. Supported strategies are “best” to
choose the best split and “random” to choose the best random split.
max_features : int, ﬂoat, string or None, optional (default=None)
The number of features to consider when looking for the best split:
•If int, then consider max_features features at each split.
•If ﬂoat, then max_features is a percentage and int(max_features * n_features) features
are considered at each split.
•If “auto”, then max_features=sqrt(n_features).
•If “sqrt”, then max_features=sqrt(n_features).
•If “log2”, then max_features=log2(n_features).
•If None, then max_features=n_features.
Note: the search for a split does not stop until at least one valid partition of the node
samples is found, even if it requires to effectively inspect more than max_features
features.
max_depth : int or None, optional (default=None)
The maximum depth of the tree. If None, then nodes are expanded until all leaves are
pure or until all leaves contain less than min_samples_split samples.
min_samples_split : int, ﬂoat, optional (default=2)
The minimum number of samples required to split an internal node:
1890
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
•If int, then consider min_samples_split as the minimum number.
•If ﬂoat, then min_samples_split is a percentage and ceil(min_samples_split *
n_samples) are the minimum number of samples for each split.
Changed in version 0.18: Added ﬂoat values for percentages.
min_samples_leaf : int, ﬂoat, optional (default=1)
The minimum number of samples required to be at a leaf node:
•If int, then consider min_samples_leaf as the minimum number.
•If ﬂoat, then min_samples_leaf is a percentage and ceil(min_samples_leaf *
n_samples) are the minimum number of samples for each node.
Changed in version 0.18: Added ﬂoat values for percentages.
min_weight_fraction_leaf : ﬂoat, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples)
required to be at a leaf node. Samples have equal weight when sample_weight is not
provided.
max_leaf_nodes : int or None, optional (default=None)
Grow a tree with max_leaf_nodes in best-ﬁrst fashion. Best nodes are deﬁned as
relative reduction in impurity. If None then unlimited number of leaf nodes.
class_weight : dict, list of dicts, “balanced” or None, optional (default=None)
Weights associated with classes in the form {class_label:
weight}. If not
given, all classes are supposed to have weight one. For multi-output problems, a list of
dicts can be provided in the same order as the columns of y.
The “balanced” mode uses the values of y to automatically adjust weights inversely
proportional to class frequencies in the input data as n_samples / (n_classes
* np.bincount(y))
For multi-output, the weights of each column of y will be multiplied.
Note that these weights will be multiplied with sample_weight (passed through the ﬁt
method) if sample_weight is speciﬁed.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
min_impurity_split : ﬂoat, optional (default=1e-7)
Threshold for early stopping in tree growth. A node will split if its impurity is above
the threshold, otherwise it is a leaf.
New in version 0.18.
presort : bool, optional (default=False)
Whether to presort the data to speed up the ﬁnding of best splits in ﬁtting. For the
default settings of a decision tree on large datasets, setting this to true may slow down
the training process. When using either a smaller dataset or a restricted depth, this may
speed up the training.
Attributesclasses_ : array of shape = [n_classes] or a list of such arrays
29.34. sklearn.tree: Decision Trees
1891
scikit-learn user guide, Release 0.18.2
The classes labels (single output problem), or a list of arrays of class labels (multi-output
problem).
feature_importances_ : array of shape = [n_features]
The feature importances. The higher, the more important the feature. The importance
of a feature is computed as the (normalized) total reduction of the criterion brought by
that feature. It is also known as the Gini importance [R78].
max_features_ : int,
The inferred value of max_features.
n_classes_ : int or list
The number of classes (for single output problems), or a list containing the number of
classes for each output (for multi-output problems).
n_features_ : int
The number of features when fit is performed.
n_outputs_ : int
The number of outputs when fit is performed.
tree_ : Tree object
The underlying Tree object.
See also:
DecisionTreeRegressor
References
[R75], [R76], [R77], [R78]
Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.tree import DecisionTreeClassifier
>>> clf = DecisionTreeClassifier(random_state=0)
>>> iris = load_iris()
>>> cross_val_score(clf, iris.data, iris.target, cv=10)
...
...
array([ 1.
,
0.93...,
0.86...,
0.93...,
0.93...,
0.93...,
0.93...,
1.
,
0.93...,
1.
])
Methods
apply(X[, check_input])
Returns the index of the leaf that each sample is pre-
dicted as.
decision_path(X[, check_input])
Return the decision path in the tree
Continued on next page
1892
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Table 29.253 – continued from previous page
fit(X, y[, sample_weight, check_input, ...])
Build a decision tree classiﬁer from the training set (X,
y).
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X[, check_input])
Predict class or regression value for X.
predict_log_proba(X)
Predict class log-probabilities of the input samples X.
predict_proba(X[, check_input])
Predict class probabilities of the input samples X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(criterion=’gini’,
splitter=’best’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=None,
ran-
dom_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None,
presort=False)
apply(X, check_input=True)
Returns the index of the leaf that each sample is predicted as.
New in version 0.17.
ParametersX : array_like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
ReturnsX_leaves : array_like, shape = [n_samples,]
For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered
within [0; self.tree_.node_count), possibly with gaps in the numbering.
decision_path(X, check_input=True)
Return the decision path in the tree
New in version 0.18.
ParametersX : array_like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Returnsindicator : sparse csr array, shape = [n_samples, n_nodes]
Return a node indicator matrix where non zero elements indicates that the samples goes
through the nodes.
feature_importances_
Return the feature importances.
29.34. sklearn.tree: Decision Trees
1893
scikit-learn user guide, Release 0.18.2
The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that
feature. It is also known as the Gini importance.
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)
Build a decision tree classiﬁer from the training set (X, y).
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The training input samples. Internally, it will be converted to dtype=np.float32
and if a sparse matrix is provided to a sparse csc_matrix.
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
The target values (class labels) as integers or strings.
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node. Splits are also ignored if they would result in any single class carrying a
negative weight in either child node.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
X_idx_sorted : array-like, shape = [n_samples, n_features], optional
The indexes of the sorted training input samples. If many tree are grown on the same
dataset, this allows the ordering to be cached between trees. If None, the data will be
sorted here. Don’t use this parameter unless you know what to do.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
1894
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
predict(X, check_input=True)
Predict class or regression value for X.
For a classiﬁcation model, the predicted class for each sample in X is returned. For a regression model,
the predicted value based on X is returned.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Returnsy : array of shape = [n_samples] or [n_samples, n_outputs]
The predicted classes, or the predict values.
predict_log_proba(X)
Predict class log-probabilities of the input samples X.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
Returnsp : array of shape = [n_samples, n_classes], or a list of n_outputs
such arrays if n_outputs > 1. The class log-probabilities of the input samples. The order
of the classes corresponds to that in the attribute classes_.
predict_proba(X, check_input=True)
Predict class probabilities of the input samples X.
The predicted class probability is the fraction of samples of the same class in a leaf.
check_input[boolean, (default=True)] Allow to bypass several input checking. Don’t use this parameter
unless you know what you do.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
Returnsp : array of shape = [n_samples, n_classes], or a list of n_outputs
such arrays if n_outputs > 1. The class probabilities of the input samples. The order of
the classes corresponds to that in the attribute classes_.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
29.34. sklearn.tree: Decision Trees
1895
scikit-learn user guide, Release 0.18.2
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.tree.DecisionTreeClassifier
• Classiﬁer comparison
• Discrete versus Real AdaBoost
• Multi-class AdaBoosted Decision Trees
• Two-class AdaBoost
• Plot the decision surfaces of ensembles of trees on the iris dataset
• Plot the decision boundaries of a VotingClassiﬁer
• Plot the decision surface of a decision tree on the iris dataset
• Understanding the decision tree structure
1896
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.34.2 sklearn.tree.DecisionTreeRegressor
class sklearn.tree.DecisionTreeRegressor(criterion=’mse’, splitter=’best’, max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0, max_features=None,
random_state=None,
max_leaf_nodes=None,
min_impurity_split=1e-07, presort=False)
A decision tree regressor.
Read more in the User Guide.
Parameterscriterion : string, optional (default=”mse”)
The function to measure the quality of a split. Supported criteria are “mse” for the
mean squared error, which is equal to variance reduction as feature selection criterion,
and “mae” for the mean absolute error.
New in version 0.18: Mean Absolute Error (MAE) criterion.
splitter : string, optional (default=”best”)
The strategy used to choose the split at each node. Supported strategies are “best” to
choose the best split and “random” to choose the best random split.
max_features : int, ﬂoat, string or None, optional (default=None)
The number of features to consider when looking for the best split:
•If int, then consider max_features features at each split.
•If ﬂoat, then max_features is a percentage and int(max_features * n_features) features
are considered at each split.
•If “auto”, then max_features=n_features.
•If “sqrt”, then max_features=sqrt(n_features).
•If “log2”, then max_features=log2(n_features).
•If None, then max_features=n_features.
Note: the search for a split does not stop until at least one valid partition of the node
samples is found, even if it requires to effectively inspect more than max_features
features.
max_depth : int or None, optional (default=None)
The maximum depth of the tree. If None, then nodes are expanded until all leaves are
pure or until all leaves contain less than min_samples_split samples.
min_samples_split : int, ﬂoat, optional (default=2)
The minimum number of samples required to split an internal node:
•If int, then consider min_samples_split as the minimum number.
•If ﬂoat, then min_samples_split is a percentage and ceil(min_samples_split *
n_samples) are the minimum number of samples for each split.
Changed in version 0.18: Added ﬂoat values for percentages.
min_samples_leaf : int, ﬂoat, optional (default=1)
The minimum number of samples required to be at a leaf node:
•If int, then consider min_samples_leaf as the minimum number.
29.34. sklearn.tree: Decision Trees
1897
scikit-learn user guide, Release 0.18.2
•If ﬂoat, then min_samples_leaf is a percentage and ceil(min_samples_leaf *
n_samples) are the minimum number of samples for each node.
Changed in version 0.18: Added ﬂoat values for percentages.
min_weight_fraction_leaf : ﬂoat, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples)
required to be at a leaf node. Samples have equal weight when sample_weight is not
provided.
max_leaf_nodes : int or None, optional (default=None)
Grow a tree with max_leaf_nodes in best-ﬁrst fashion. Best nodes are deﬁned as
relative reduction in impurity. If None then unlimited number of leaf nodes.
random_state : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState
instance, random_state is the random number generator; If None, the random number
generator is the RandomState instance used by np.random.
min_impurity_split : ﬂoat, optional (default=1e-7)
Threshold for early stopping in tree growth. If the impurity of a node is below the
threshold, the node is a leaf.
New in version 0.18.
presort : bool, optional (default=False)
Whether to presort the data to speed up the ﬁnding of best splits in ﬁtting. For the
default settings of a decision tree on large datasets, setting this to true may slow down
the training process. When using either a smaller dataset or a restricted depth, this may
speed up the training.
Attributesfeature_importances_ : array of shape = [n_features]
The feature importances. The higher, the more important the feature. The importance
of a feature is computed as the (normalized) total reduction of the criterion brought by
that feature. It is also known as the Gini importance [R82].
max_features_ : int,
The inferred value of max_features.
n_features_ : int
The number of features when fit is performed.
n_outputs_ : int
The number of outputs when fit is performed.
tree_ : Tree object
The underlying Tree object.
See also:
DecisionTreeClassifier
References
[R79], [R80], [R81], [R82]
1898
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn.datasets import load_boston
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.tree import DecisionTreeRegressor
>>> boston = load_boston()
>>> regressor = DecisionTreeRegressor(random_state=0)
>>> cross_val_score(regressor, boston.data, boston.target, cv=10)
...
...
array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,
0.07..., 0.29..., 0.33..., -1.42..., -1.77...])
Methods
apply(X[, check_input])
Returns the index of the leaf that each sample is pre-
dicted as.
decision_path(X[, check_input])
Return the decision path in the tree
fit(X, y[, sample_weight, check_input, ...])
Build a decision tree regressor from the training set (X,
y).
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X[, check_input])
Predict class or regression value for X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(criterion=’mse’,
splitter=’best’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=None,
ran-
dom_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, presort=False)
apply(X, check_input=True)
Returns the index of the leaf that each sample is predicted as.
New in version 0.17.
ParametersX : array_like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
ReturnsX_leaves : array_like, shape = [n_samples,]
For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered
within [0; self.tree_.node_count), possibly with gaps in the numbering.
decision_path(X, check_input=True)
Return the decision path in the tree
29.34. sklearn.tree: Decision Trees
1899
scikit-learn user guide, Release 0.18.2
New in version 0.18.
ParametersX : array_like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Returnsindicator : sparse csr array, shape = [n_samples, n_nodes]
Return a node indicator matrix where non zero elements indicates that the samples goes
through the nodes.
feature_importances_
Return the feature importances.
The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that
feature. It is also known as the Gini importance.
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)
Build a decision tree regressor from the training set (X, y).
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The training input samples. Internally, it will be converted to dtype=np.float32
and if a sparse matrix is provided to a sparse csc_matrix.
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
The target values (real numbers). Use dtype=np.float64 and order='C' for
maximum efﬁciency.
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
X_idx_sorted : array-like, shape = [n_samples, n_features], optional
The indexes of the sorted training input samples. If many tree are grown on the same
dataset, this allows the ordering to be cached between trees. If None, the data will be
sorted here. Don’t use this parameter unless you know what to do.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
1900
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X, check_input=True)
Predict class or regression value for X.
For a classiﬁcation model, the predicted class for each sample in X is returned. For a regression model,
the predicted value based on X is returned.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Returnsy : array of shape = [n_samples] or [n_samples, n_outputs]
The predicted classes, or the predict values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
29.34. sklearn.tree: Decision Trees
1901
scikit-learn user guide, Release 0.18.2
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
Examples using sklearn.tree.DecisionTreeRegressor
• Decision Tree Regression with AdaBoost
• Single estimator versus bagging: bias-variance decomposition
• Decision Tree Regression
• Multi-output Decision Tree Regression
29.34.3 sklearn.tree.ExtraTreeClassiﬁer
class sklearn.tree.ExtraTreeClassifier(criterion=’gini’, splitter=’random’, max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
random_state=None,
max_leaf_nodes=None,
min_impurity_split=1e-07, class_weight=None)
An extremely randomized tree classiﬁer.
Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate
the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected
features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally
random decision tree.
Warning: Extra-trees should only be used within ensemble methods.
Read more in the User Guide.
See also:
1902
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor
References
[R250]
Methods
apply(X[, check_input])
Returns the index of the leaf that each sample is pre-
dicted as.
decision_path(X[, check_input])
Return the decision path in the tree
fit(X, y[, sample_weight, check_input, ...])
Build a decision tree classiﬁer from the training set (X,
y).
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X[, check_input])
Predict class or regression value for X.
predict_log_proba(X)
Predict class log-probabilities of the input samples X.
predict_proba(X[, check_input])
Predict class probabilities of the input samples X.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(criterion=’gini’,
splitter=’random’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
ran-
dom_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None)
apply(X, check_input=True)
Returns the index of the leaf that each sample is predicted as.
New in version 0.17.
ParametersX : array_like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
ReturnsX_leaves : array_like, shape = [n_samples,]
For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered
within [0; self.tree_.node_count), possibly with gaps in the numbering.
decision_path(X, check_input=True)
Return the decision path in the tree
New in version 0.18.
ParametersX : array_like or sparse matrix, shape = [n_samples, n_features]
29.34. sklearn.tree: Decision Trees
1903
scikit-learn user guide, Release 0.18.2
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Returnsindicator : sparse csr array, shape = [n_samples, n_nodes]
Return a node indicator matrix where non zero elements indicates that the samples goes
through the nodes.
feature_importances_
Return the feature importances.
The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that
feature. It is also known as the Gini importance.
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)
Build a decision tree classiﬁer from the training set (X, y).
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The training input samples. Internally, it will be converted to dtype=np.float32
and if a sparse matrix is provided to a sparse csc_matrix.
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
The target values (class labels) as integers or strings.
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node. Splits are also ignored if they would result in any single class carrying a
negative weight in either child node.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
X_idx_sorted : array-like, shape = [n_samples, n_features], optional
The indexes of the sorted training input samples. If many tree are grown on the same
dataset, this allows the ordering to be cached between trees. If None, the data will be
sorted here. Don’t use this parameter unless you know what to do.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
1904
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X, check_input=True)
Predict class or regression value for X.
For a classiﬁcation model, the predicted class for each sample in X is returned. For a regression model,
the predicted value based on X is returned.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Returnsy : array of shape = [n_samples] or [n_samples, n_outputs]
The predicted classes, or the predict values.
predict_log_proba(X)
Predict class log-probabilities of the input samples X.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
Returnsp : array of shape = [n_samples, n_classes], or a list of n_outputs
such arrays if n_outputs > 1. The class log-probabilities of the input samples. The order
of the classes corresponds to that in the attribute classes_.
predict_proba(X, check_input=True)
Predict class probabilities of the input samples X.
The predicted class probability is the fraction of samples of the same class in a leaf.
check_input[boolean, (default=True)] Allow to bypass several input checking. Don’t use this parameter
unless you know what you do.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
Returnsp : array of shape = [n_samples, n_classes], or a list of n_outputs
such arrays if n_outputs > 1. The class probabilities of the input samples. The order of
the classes corresponds to that in the attribute classes_.
29.34. sklearn.tree: Decision Trees
1905
scikit-learn user guide, Release 0.18.2
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
1906
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.34.4 sklearn.tree.ExtraTreeRegressor
class sklearn.tree.ExtraTreeRegressor(criterion=’mse’, splitter=’random’, max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
random_state=None,
min_impurity_split=1e-07,
max_leaf_nodes=None)
An extremely randomized tree regressor.
Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate
the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected
features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally
random decision tree.
Warning: Extra-trees should only be used within ensemble methods.
Read more in the User Guide.
See also:
ExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor
References
[R251]
Methods
apply(X[, check_input])
Returns the index of the leaf that each sample is pre-
dicted as.
decision_path(X[, check_input])
Return the decision path in the tree
fit(X, y[, sample_weight, check_input, ...])
Build a decision tree regressor from the training set (X,
y).
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X[, check_input])
Predict class or regression value for X.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
DEPRECATED: Support to use estimators as feature se-
lectors will be removed in version 0.19.
__init__(criterion=’mse’,
splitter=’random’,
max_depth=None,
min_samples_split=2,
min_samples_leaf=1,
min_weight_fraction_leaf=0.0,
max_features=’auto’,
ran-
dom_state=None, min_impurity_split=1e-07, max_leaf_nodes=None)
apply(X, check_input=True)
Returns the index of the leaf that each sample is predicted as.
New in version 0.17.
ParametersX : array_like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
29.34. sklearn.tree: Decision Trees
1907
scikit-learn user guide, Release 0.18.2
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
ReturnsX_leaves : array_like, shape = [n_samples,]
For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered
within [0; self.tree_.node_count), possibly with gaps in the numbering.
decision_path(X, check_input=True)
Return the decision path in the tree
New in version 0.18.
ParametersX : array_like or sparse matrix, shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Returnsindicator : sparse csr array, shape = [n_samples, n_nodes]
Return a node indicator matrix where non zero elements indicates that the samples goes
through the nodes.
feature_importances_
Return the feature importances.
The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that
feature. It is also known as the Gini importance.
Returnsfeature_importances_ : array, shape = [n_features]
fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)
Build a decision tree regressor from the training set (X, y).
ParametersX : array-like or sparse matrix, shape = [n_samples, n_features]
The training input samples. Internally, it will be converted to dtype=np.float32
and if a sparse matrix is provided to a sparse csc_matrix.
y : array-like, shape = [n_samples] or [n_samples, n_outputs]
The target values (real numbers). Use dtype=np.float64 and order='C' for
maximum efﬁciency.
sample_weight : array-like, shape = [n_samples] or None
Sample weights. If None, then samples are equally weighted. Splits that would create
child nodes with net zero or negative weight are ignored while searching for a split in
each node.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
X_idx_sorted : array-like, shape = [n_samples, n_features], optional
1908
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The indexes of the sorted training input samples. If many tree are grown on the same
dataset, this allows the ordering to be cached between trees. If None, the data will be
sorted here. Don’t use this parameter unless you know what to do.
Returnsself : object
Returns self.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X, check_input=True)
Predict class or regression value for X.
For a classiﬁcation model, the predicted class for each sample in X is returned. For a regression model,
the predicted value based on X is returned.
ParametersX : array-like or sparse matrix of shape = [n_samples, n_features]
The input samples. Internally, it will be converted to dtype=np.float32 and if a
sparse matrix is provided to a sparse csr_matrix.
check_input : boolean, (default=True)
Allow to bypass several input checking. Don’t use this parameter unless you know what
you do.
Returnsy : array of shape = [n_samples] or [n_samples, n_outputs]
The predicted classes, or the predict values.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
29.34. sklearn.tree: Decision Trees
1909
scikit-learn user guide, Release 0.18.2
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use
SelectFromModel instead.
Reduce X to its most important features.
Uses coef_ or feature_importances_ to determine the most important features. For
models with a coef_ for each class, the absolute sum over the classes is used.
ParametersX : array or scipy sparse matrix of shape [n_samples, n_features]
The input samples.
threshold[string, ﬂoat or None, optional (default=None)] The threshold value to use for
feature selection. Features whose importance is greater or equal are kept while the
others are discarded. If “median” (resp. “mean”), then the threshold value is the me-
dian (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”)
may also be used. If None and if available, the object attribute threshold is used.
Otherwise, “mean” is used by default.
ReturnsX_r : array of shape [n_samples, n_selected_features]
The input samples with only the selected features.
tree.export_graphviz
Export a decision tree in DOT format.
29.34.5 sklearn.tree.export_graphviz
sklearn.tree.export_graphviz()
Export a decision tree in DOT format.
This function generates a GraphViz representation of the decision tree, which is then written into out_ﬁle. Once
exported, graphical renderings can be generated using, for example:
$ dot -Tps tree.dot -o tree.ps
(PostScript format)
$ dot -Tpng tree.dot -o tree.png
(PNG format)
1910
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
The sample counts that are shown are weighted with any sample_weights that might be present.
Read more in the User Guide.
Parametersdecision_tree : decision tree classiﬁer
The decision tree to be exported to GraphViz.
out_ﬁle : ﬁle object or string, optional (default=’tree.dot’)
Handle or name of the output ﬁle. If None, the result is returned as a string. This will
the default from version 0.20.
max_depth : int, optional (default=None)
The maximum depth of the representation. If None, the tree is fully generated.
feature_names : list of strings, optional (default=None)
Names of each of the features.
class_names : list of strings, bool or None, optional (default=None)
Names of each of the target classes in ascending numerical order. Only relevant for clas-
siﬁcation and not supported for multi-output. If True, shows a symbolic representation
of the class name.
label : {‘all’, ‘root’, ‘none’}, optional (default=’all’)
Whether to show informative labels for impurity, etc. Options include ‘all’ to show at
every node, ‘root’ to show only at the top root node, or ‘none’ to not show at any node.
ﬁlled : bool, optional (default=False)
When set to True, paint nodes to indicate majority class for classiﬁcation, extremity of
values for regression, or purity of node for multi-output.
leaves_parallel : bool, optional (default=False)
When set to True, draw all leaf nodes at the bottom of the tree.
impurity : bool, optional (default=True)
When set to True, show the impurity at each node.
node_ids : bool, optional (default=False)
When set to True, show the ID number on each node.
proportion : bool, optional (default=False)
When set to True, change the display of ‘values’ and/or ‘samples’ to be proportions
and percentages respectively.
rotate : bool, optional (default=False)
When set to True, orient tree left to right rather than top-down.
rounded : bool, optional (default=False)
When set to True, draw node boxes with rounded corners and use Helvetica fonts
instead of Times-Roman.
special_characters : bool, optional (default=False)
When set to False, ignore special characters for PostScript compatibility.
Returnsdot_data : string
29.34. sklearn.tree: Decision Trees
1911
scikit-learn user guide, Release 0.18.2
String representation of the input tree in GraphViz dot format.
Only returned if
out_file is None.
New in version 0.18.
Examples
>>> from sklearn.datasets import load_iris
>>> from sklearn import tree
>>> clf = tree.DecisionTreeClassifier()
>>> iris = load_iris()
>>> clf = clf.fit(iris.data, iris.target)
>>> tree.export_graphviz(clf,
...
out_file='tree.dot')
29.35 sklearn.utils: Utilities
The sklearn.utils module includes various utilities.
Developer guide: See the Utilities for Developers page for further details.
utils.check_random_state(seed)
Turn seed into a np.random.RandomState instance
utils.estimator_checks.check_estimator(Estimator)
Check if estimator adheres to scikit-learn conventions.
utils.resample(\*arrays, \*\*options)
Resample arrays or sparse matrices in a consistent way
utils.shuffle(\*arrays, \*\*options)
Shufﬂe arrays or sparse matrices in a consistent way
29.35.1 sklearn.utils.check_random_state
sklearn.utils.check_random_state(seed)
Turn seed into a np.random.RandomState instance
If seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new Ran-
domState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise
ValueError.
Examples using sklearn.utils.check_random_state
• Isotonic Regression
• Face completion with a multi-output estimators
• Empirical evaluation of the impact of k-means initialization
• Manifold Learning methods on a severed sphere
• Scaling the regularization parameter for SVCs
1912
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
29.35.2 sklearn.utils.estimator_checks.check_estimator
sklearn.utils.estimator_checks.check_estimator(Estimator)
Check if estimator adheres to scikit-learn conventions.
This estimator will run an extensive test-suite for input validation, shapes, etc. Additional tests for classiﬁers,
regressors, clustering or transformers will be run if the Estimator class inherits from the corresponding mixin
from sklearn.base.
ParametersEstimator : class
Class to check. Estimator is a class object (not an instance).
29.35.3 sklearn.utils.resample
sklearn.utils.resample(*arrays, **options)
Resample arrays or sparse matrices in a consistent way
The default strategy implements one step of the bootstrapping procedure.
Parameters*arrays : sequence of indexable data-structures
Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with
consistent ﬁrst dimension.
replace : boolean, True by default
Implements resampling with replacement. If False, this will implement (sliced) random
permutations.
n_samples : int, None by default
Number of samples to generate. If left to None this is automatically set to the ﬁrst
dimension of the arrays. If replace is False it should not be larger than the length of
arrays.
random_state : int or RandomState instance
Control the shufﬂing for reproducible behavior.
Returnsresampled_arrays : sequence of indexable data-structures
Sequence of resampled views of the collections. The original arrays are not impacted.
See also:
sklearn.utils.shuffle
Examples
It is possible to mix sparse and dense arrays in the same run:
>>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])
>>> y = np.array([0, 1, 2])
>>> from scipy.sparse import coo_matrix
>>> X_sparse = coo_matrix(X)
>>> from sklearn.utils import resample
>>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)
>>> X
29.35. sklearn.utils: Utilities
1913
scikit-learn user guide, Release 0.18.2
array([[ 1.,
0.],
[ 2.,
1.],
[ 1.,
0.]])
>>> X_sparse
<3x2 sparse matrix of type '<... 'numpy.float64'>'
with 4 stored elements in Compressed Sparse Row format>
>>> X_sparse.toarray()
array([[ 1.,
0.],
[ 2.,
1.],
[ 1.,
0.]])
>>> y
array([0, 1, 0])
>>> resample(y, n_samples=2, random_state=0)
array([0, 1])
29.35.4 sklearn.utils.shufﬂe
sklearn.utils.shuffle(*arrays, **options)
Shufﬂe arrays or sparse matrices in a consistent way
This is a convenience alias to resample(*arrays,replace=False) to do random permutations of the
collections.
Parameters*arrays : sequence of indexable data-structures
Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with
consistent ﬁrst dimension.
random_state : int or RandomState instance
Control the shufﬂing for reproducible behavior.
n_samples : int, None by default
Number of samples to generate. If left to None this is automatically set to the ﬁrst
dimension of the arrays.
Returnsshufﬂed_arrays : sequence of indexable data-structures
Sequence of shufﬂed views of the collections. The original arrays are not impacted.
See also:
sklearn.utils.resample
Examples
It is possible to mix sparse and dense arrays in the same run:
>>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])
>>> y = np.array([0, 1, 2])
>>> from scipy.sparse import coo_matrix
>>> X_sparse = coo_matrix(X)
1914
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
>>> from sklearn.utils import shuffle
>>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)
>>> X
array([[ 0.,
0.],
[ 2.,
1.],
[ 1.,
0.]])
>>> X_sparse
<3x2 sparse matrix of type '<... 'numpy.float64'>'
with 3 stored elements in Compressed Sparse Row format>
>>> X_sparse.toarray()
array([[ 0.,
0.],
[ 2.,
1.],
[ 1.,
0.]])
>>> y
array([2, 1, 0])
>>> shuffle(y, n_samples=2, random_state=0)
array([0, 1])
Examples using sklearn.utils.shuffle
• Model Complexity Inﬂuence
• Prediction Latency
• Color Quantization using K-Means
• Empirical evaluation of the impact of k-means initialization
• Gradient Boosting regression
29.36 Recently deprecated
29.36.1 To be removed in 0.19
lda.LDA([solver, shrinkage, priors, ...])
Alias for sklearn.discriminant_analysis.LinearDiscrim
qda.QDA([priors, reg_param, ...])
Alias for sklearn.discriminant_analysis.QuadraticDis
sklearn.lda.LDA
Warning: DEPRECATED
class sklearn.lda.LDA(solver=’svd’,
shrinkage=None,
priors=None,
n_components=None,
store_covariance=False, tol=0.0001)
Alias for sklearn.discriminant_analysis.LinearDiscriminantAnalysis.
Deprecated
since
version
0.17:
This
class
will
be
removed
in
0.19.
Use
sklearn.discriminant_analysis.LinearDiscriminantAnalysis instead.
29.36. Recently deprecated
1915
scikit-learn user guide, Release 0.18.2
Methods
decision_function(X)
Predict conﬁdence scores for samples.
fit(X, y[, store_covariance, tol])
Fit LinearDiscriminantAnalysis model according to the
given training data and parameters.
fit_transform(X[, y])
Fit to data, then transform it.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict class labels for samples in X.
predict_log_proba(X)
Estimate log probability.
predict_proba(X)
Estimate probability.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X)
Project data to maximize class separation.
__init__(solver=’svd’,
shrinkage=None,
priors=None,
n_components=None,
store_covariance=False, tol=0.0001)
decision_function(X)
Predict conﬁdence scores for samples.
The conﬁdence score for a sample is the signed distance of that sample to the hyperplane.
ParametersX : {array-like, sparse matrix}, shape = (n_samples, n_features)
Samples.
Returnsarray, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :
Conﬁdence scores per (sample, class) combination. In the binary case, conﬁdence score
for self.classes_[1] where >0 means this class would be predicted.
fit(X, y, store_covariance=None, tol=None)
Fit LinearDiscriminantAnalysis model according to the giventraining data and parameters.
Changed in version 0.17: Deprecated store_covariance have been moved to main constructor.
Changed in version 0.17: Deprecated tol have been moved to main constructor.
ParametersX : array-like, shape (n_samples, n_features)
Training data.
y : array, shape (n_samples,)
Target values.
fit_transform(X, y=None, **ﬁt_params)
Fit to data, then transform it.
Fits transformer to X and y with optional parameters ﬁt_params and returns a transformed version of X.
ParametersX : numpy array of shape [n_samples, n_features]
Training set.
y : numpy array of shape [n_samples]
Target values.
ReturnsX_new : numpy array of shape [n_samples, n_features_new]
1916
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Transformed array.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict class labels for samples in X.
ParametersX : {array-like, sparse matrix}, shape = [n_samples, n_features]
Samples.
ReturnsC : array, shape = [n_samples]
Predicted class label per sample.
predict_log_proba(X)
Estimate log probability.
ParametersX : array-like, shape (n_samples, n_features)
Input data.
ReturnsC : array, shape (n_samples, n_classes)
Estimated log probabilities.
predict_proba(X)
Estimate probability.
ParametersX : array-like, shape (n_samples, n_features)
Input data.
ReturnsC : array, shape (n_samples, n_classes)
Estimated probabilities.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
29.36. Recently deprecated
1917
scikit-learn user guide, Release 0.18.2
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X)
Project data to maximize class separation.
ParametersX : array-like, shape (n_samples, n_features)
Input data.
ReturnsX_new : array, shape (n_samples, n_components)
Transformed data.
sklearn.qda.QDA
Warning: DEPRECATED
class sklearn.qda.QDA(priors=None, reg_param=0.0, store_covariances=False, tol=0.0001)
Alias for sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.
Deprecated
since
version
0.17:
This
class
will
be
removed
in
0.19.
Use
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis instead.
Methods
decision_function(X)
Apply decision function to an array of samples.
fit(X, y[, store_covariances, tol])
Fit the model according to the given training data and
parameters.
get_params([deep])
Get parameters for this estimator.
predict(X)
Perform classiﬁcation on an array of test vectors X.
predict_log_proba(X)
Return posterior probabilities of classiﬁcation.
predict_proba(X)
Return posterior probabilities of classiﬁcation.
score(X, y[, sample_weight])
Returns the mean accuracy on the given test data and
labels.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(priors=None, reg_param=0.0, store_covariances=False, tol=0.0001)
decision_function(X)
Apply decision function to an array of samples.
ParametersX : array-like, shape = [n_samples, n_features]
Array of samples (test vectors).
ReturnsC : array, shape = [n_samples, n_classes] or [n_samples,]
Decision function values related to each class, per sample. In the two-class case, the
1918
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
shape is [n_samples,], giving the log likelihood ratio of the positive class.
fit(X, y, store_covariances=None, tol=None)
Fit the model according to the given training data and parameters.
Changed in version 0.17: Deprecated store_covariance have been moved to main constructor.
Changed in version 0.17: Deprecated tol have been moved to main constructor.
ParametersX : array-like, shape = [n_samples, n_features]
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
y : array, shape = [n_samples]
Target values (integers)
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Perform classiﬁcation on an array of test vectors X.
The predicted class C for each sample in X is returned.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = [n_samples]
predict_log_proba(X)
Return posterior probabilities of classiﬁcation.
ParametersX : array-like, shape = [n_samples, n_features]
Array of samples/test vectors.
ReturnsC : array, shape = [n_samples, n_classes]
Posterior log-probabilities of classiﬁcation per class.
predict_proba(X)
Return posterior probabilities of classiﬁcation.
ParametersX : array-like, shape = [n_samples, n_features]
Array of samples/test vectors.
ReturnsC : array, shape = [n_samples, n_classes]
Posterior probabilities of classiﬁcation per class.
score(X, y, sample_weight=None)
Returns the mean accuracy on the given test data and labels.
In multi-label classiﬁcation, this is the subset accuracy which is a harsh metric since you require for each
sample that each label set be correctly predicted.
29.36. Recently deprecated
1919
scikit-learn user guide, Release 0.18.2
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True labels for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
Mean accuracy of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
datasets.load_lfw_pairs(\*args, \*\*kwargs)
DEPRECATED: Function ‘load_lfw_pairs’ has been dep-
recated in 0.17 and will be removed in 0.19.Use
fetch_lfw_pairs(download_if_missing=False) instead.
datasets.load_lfw_people(\*args, \*\*kwargs)
DEPRECATED: Function ‘load_lfw_people’ has been
deprecated in 0.17 and will be removed in 0.19.Use
fetch_lfw_people(download_if_missing=False) instead.
sklearn.datasets.load_lfw_pairs
Warning: DEPRECATED
sklearn.datasets.load_lfw_pairs(*args, **kwargs)
DEPRECATED: Function ‘load_lfw_pairs’ has been deprecated in 0.17 and will be removed in 0.19.Use
fetch_lfw_pairs(download_if_missing=False) instead.
Alias for fetch_lfw_pairs(download_if_missing=False)
Deprecated
since
version
0.17:
This
function
will
be
removed
in
0.19.
Use
sklearn.datasets.fetch_lfw_pairs
with
parameter
download_if_missing=False
in-
stead.
Check fetch_lfw_pairs.__doc__ for the documentation and parameter list.
sklearn.datasets.load_lfw_people
Warning: DEPRECATED
sklearn.datasets.load_lfw_people(*args, **kwargs)
DEPRECATED: Function ‘load_lfw_people’ has been deprecated in 0.17 and will be removed in 0.19.Use
fetch_lfw_people(download_if_missing=False) instead.
1920
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Alias for fetch_lfw_people(download_if_missing=False)
Deprecated
since
version
0.17:
This
function
will
be
removed
in
0.19.
Use
sklearn.datasets.fetch_lfw_people
with
parameter
download_if_missing=False
instead.
Check fetch_lfw_people.__doc__ for the documentation and parameter list.
29.36.2 To be removed in 0.20
grid_search.ParameterGrid(param_grid)
Grid of parameters with a discrete number of values for
each.
grid_search.ParameterSampler(...[,
ran-
dom_state])
Generator on parameters sampled from given distributions.
grid_search.GridSearchCV(estimator,
param_grid)
Exhaustive search over speciﬁed parameter values for an
estimator.
grid_search.RandomizedSearchCV(estimator,
...)
Randomized search on hyper parameters.
cross_validation.LeaveOneOut(n)
Leave-One-Out cross validation iterator.
cross_validation.LeavePOut(n, p)
Leave-P-Out cross validation iterator
cross_validation.KFold(n[, n_folds, ...])
K-Folds cross validation iterator.
cross_validation.LabelKFold(labels[, n_folds])
K-fold iterator variant with non-overlapping labels.
cross_validation.LeaveOneLabelOut(labels)
Leave-One-Label_Out cross-validation iterator
cross_validation.LeavePLabelOut(labels, p)
Leave-P-Label_Out cross-validation iterator
cross_validation.LabelShuffleSplit(labels[,
...])
Shufﬂe-Labels-Out cross-validation iterator
cross_validation.StratifiedKFold(y[, ...])
Stratiﬁed K-Folds cross validation iterator
cross_validation.ShuffleSplit(n[, n_iter, ...])
Random permutation cross-validation iterator.
cross_validation.StratifiedShuffleSplit(y[,
...])
Stratiﬁed ShufﬂeSplit cross validation iterator
cross_validation.PredefinedSplit(test_fold)
Predeﬁned split cross validation iterator
decomposition.RandomizedPCA(\*args,
\*\*kwargs)
Principal component analysis (PCA) using randomized
SVD
gaussian_process.GaussianProcess(\*args,
\*\*kwargs)
The legacy Gaussian Process model class.
mixture.GMM(\*args, \*\*kwargs)
Legacy Gaussian Mixture Model
mixture.DPGMM(\*args, \*\*kwargs)
Dirichlet Process Gaussian Mixture Models
mixture.VBGMM(\*args, \*\*kwargs)
Variational Inference for the Gaussian Mixture Model
sklearn.grid_search.ParameterGrid
Warning: DEPRECATED
class sklearn.grid_search.ParameterGrid(param_grid)
Grid of parameters with a discrete number of values for each.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.ParameterGrid instead.
Can be used to iterate over parameter value combinations with the Python built-in function iter.
Read more in the User Guide.
29.36. Recently deprecated
1921
scikit-learn user guide, Release 0.18.2
Parametersparam_grid : dict of string to sequence, or sequence of such
The parameter grid to explore, as a dictionary mapping estimator parameters to se-
quences of allowed values.
An empty dict signiﬁes default parameters.
A sequence of dicts signiﬁes a sequence of grids to search, and is useful to avoid ex-
ploring parameter combinations that make no sense or have no effect. See the examples
below.
See also:
GridSearchCVuses ParameterGrid to perform a full parallelized parameter search.
Examples
>>> from sklearn.grid_search import ParameterGrid
>>> param_grid = {'a': [1, 2], 'b': [True, False]}
>>> list(ParameterGrid(param_grid)) == (
...
[{'a': 1, 'b': True}, {'a': 1, 'b': False},
...
{'a': 2, 'b': True}, {'a': 2, 'b': False}])
True
>>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]
>>> list(ParameterGrid(grid)) == [{'kernel': 'linear'},
...
{'kernel': 'rbf', 'gamma': 1},
...
{'kernel': 'rbf', 'gamma': 10}]
True
>>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}
True
.. automethod:: __init__
sklearn.grid_search.ParameterSampler
Warning: DEPRECATED
class sklearn.grid_search.ParameterSampler(param_distributions,
n_iter,
ran-
dom_state=None)
Generator on parameters sampled from given distributions.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.ParameterSampler instead.
Non-deterministic iterable over random candidate combinations for hyper- parameter search. If all parameters
are presented as a list, sampling without replacement is performed. If at least one parameter is given as a
distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for
continuous parameters.
Note that as of SciPy 0.12, the scipy.stats.distributions do not accept a custom RNG instance and
always use the singleton RNG from numpy.random. Hence setting random_state will not guarantee a
deterministic iteration whenever scipy.stats distributions are used to deﬁne the parameter search space.
Read more in the User Guide.
1922
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Parametersparam_distributions : dict
Dictionary where the keys are parameters and values are distributions from which a
parameter is to be sampled. Distributions either have to provide a rvs function to
sample from them, or can be given as a list of values, where a uniform distribution is
assumed.
n_iter : integer
Number of parameter settings that are produced.
random_state : int or RandomState
Pseudo random number generator state used for random uniform sampling from lists of
possible values instead of scipy.stats distributions.
Returnsparams : dict of string to any
Yields dictionaries mapping each estimator parameter to as sampled value.
Examples
>>> from sklearn.grid_search import ParameterSampler
>>> from scipy.stats.distributions import expon
>>> import numpy as np
>>> np.random.seed(0)
>>> param_grid = {'a':[1, 2], 'b': expon()}
>>> param_list = list(ParameterSampler(param_grid, n_iter=4))
>>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())
...
for d in param_list]
>>> rounded_list == [{'b': 0.89856, 'a': 1},
...
{'b': 0.923223, 'a': 1},
...
{'b': 1.878964, 'a': 2},
...
{'b': 1.038159, 'a': 2}]
True
.. automethod:: __init__
sklearn.grid_search.GridSearchCV
Warning: DEPRECATED
class sklearn.grid_search.GridSearchCV(estimator,
param_grid,
scoring=None,
ﬁt_params=None,
n_jobs=1,
iid=True,
reﬁt=True,
cv=None,
verbose=0,
pre_dispatch=‘2*n_jobs’,
error_score=’raise’)
Exhaustive search over speciﬁed parameter values for an estimator.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.GridSearchCV instead.
Important members are ﬁt, predict.
GridSearchCV implements a “ﬁt” and a “score” method. It also implements “predict”, “predict_proba”, “deci-
sion_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.
The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over
a parameter grid.
29.36. Recently deprecated
1923
scikit-learn user guide, Release 0.18.2
Read more in the User Guide.
Parametersestimator : estimator object.
A object of that type is instantiated for each grid point. This is assumed to implement
the scikit-learn estimator interface. Either estimator needs to provide a score function,
or scoring must be passed.
param_grid : dict or list of dictionaries
Dictionary with parameters names (string) as keys and lists of parameter settings to try
as values, or a list of such dictionaries, in which case the grids spanned by each dic-
tionary in the list are explored. This enables searching over any sequence of parameter
settings.
scoring : string, callable or None, default=None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y). If None, the score method of the estimator
is used.
ﬁt_params : dict, optional
Parameters to pass to the ﬁt method.
n_jobs : int, default=1
Number of jobs to run in parallel.
Changed in version 0.17: Upgraded to joblib 0.9.3.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
iid : boolean, default=True
If True, the data is assumed to be identically distributed across the folds, and the loss
minimized is the total loss per sample, and not the mean loss across the folds.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or mul-
ticlass, sklearn.model_selection.StratifiedKFold is used. In all other
cases, sklearn.model_selection.KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
1924
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
reﬁt : boolean, default=True
Reﬁt the best estimator with the entire dataset. If “False”, it is impossible to make
predictions using this GridSearchCV instance after ﬁtting.
verbose : integer
Controls the verbosity: the higher, the more messages.
error_score : ‘raise’ (default) or numeric
Value to assign to the score if an error occurs in estimator ﬁtting. If set to ‘raise’, the
error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter
does not affect the reﬁt step, which will always raise the error.
Attributesgrid_scores_ : list of named tuples
Contains scores for all parameter combinations in param_grid. Each entry corresponds
to one parameter setting. Each named tuple has the attributes:
•parameters, a dict of parameter settings
•mean_validation_score, the mean score over the cross-validation folds
•cv_validation_scores, the list of scores for each fold
best_estimator_ : estimator
Estimator that was chosen by the search, i.e. estimator which gave highest score (or
smallest loss if speciﬁed) on the left out data. Not available if reﬁt=False.
best_score_ : ﬂoat
Score of best_estimator on the left out data.
best_params_ : dict
Parameter setting that gave the best results on the hold out data.
scorer_ : function
Scorer function used on the held out data to choose the best parameters for the model.
See also:
ParameterGridgenerates all the combinations of a hyperparameter grid.
sklearn.cross_validation.train_test_splitutility function to split the data into a development
set usable for ﬁtting a GridSearchCV instance and an evaluation set for its ﬁnal evaluation.
sklearn.metrics.make_scorerMake a scorer from a performance metric or loss function.
Notes
The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed
in which case it is used instead.
If n_jobs was set to a value higher than one, the data is copied for each point in the grid (and not n_jobs times).
This is done for efﬁciency reasons if individual jobs take very little time, but may raise errors if the dataset is
large and not enough memory is available. A workaround in this case is to set pre_dispatch. Then, the memory
is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs.
29.36. Recently deprecated
1925
scikit-learn user guide, Release 0.18.2
Examples
>>> from sklearn import svm, grid_search, datasets
>>> iris = datasets.load_iris()
>>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
>>> svr = svm.SVC()
>>> clf = grid_search.GridSearchCV(svr, parameters)
>>> clf.fit(iris.data, iris.target)
...
GridSearchCV(cv=None, error_score=...,
estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
decision_function_shape=None, degree=..., gamma=...,
kernel='rbf', max_iter=-1, probability=False,
random_state=None, shrinking=True, tol=...,
verbose=False),
fit_params={}, iid=..., n_jobs=1,
param_grid=..., pre_dispatch=..., refit=...,
scoring=..., verbose=...)
Methods
decision_function(\*args, \*\*kwargs)
Call decision_function on the estimator with the best
found parameters.
fit(X[, y])
Run ﬁt with all sets of parameters.
get_params([deep])
Get parameters for this estimator.
inverse_transform(\*args, \*\*kwargs)
Call inverse_transform on the estimator with the best
found parameters.
predict(\*args, \*\*kwargs)
Call predict on the estimator with the best found param-
eters.
predict_log_proba(\*args, \*\*kwargs)
Call predict_log_proba on the estimator with the best
found parameters.
predict_proba(\*args, \*\*kwargs)
Call predict_proba on the estimator with the best found
parameters.
score(X[, y])
Returns the score on the given data, if the estimator has
been reﬁt.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
Call transform on the estimator with the best found pa-
rameters.
__init__(estimator, param_grid, scoring=None, ﬁt_params=None, n_jobs=1, iid=True, reﬁt=True,
cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’)
decision_function(*args, **kwargs)
Call decision_function on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports decision_function.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
fit(X, y=None)
Run ﬁt with all sets of parameters.
ParametersX : array-like, shape = [n_samples, n_features]
1926
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Training vector, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape = [n_samples] or [n_samples, n_output], optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(*args, **kwargs)
Call inverse_transform on the estimator with the best found parameters.
Only available if the underlying estimator implements inverse_transform and refit=True.
ParametersXt : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict(*args, **kwargs)
Call predict on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict_log_proba(*args, **kwargs)
Call predict_log_proba on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict_log_proba.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict_proba(*args, **kwargs)
Call predict_proba on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict_proba.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
score(X, y=None)
Returns the score on the given data, if the estimator has been reﬁt.
This uses the score deﬁned by scoring where provided, and the best_estimator_.score method
otherwise.
ParametersX : array-like, shape = [n_samples, n_features]
Input data, where n_samples is the number of samples and n_features is the number of
features.
y : array-like, shape = [n_samples] or [n_samples, n_output], optional
29.36. Recently deprecated
1927
scikit-learn user guide, Release 0.18.2
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
Returnsscore : ﬂoat
Notes
•The long-standing behavior of this method changed in version 0.16.
•It no longer uses the metric provided by estimator.score if the scoring parameter was set
when ﬁtting.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(*args, **kwargs)
Call transform on the estimator with the best found parameters.
Only available if the underlying estimator supports transform and refit=True.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
sklearn.grid_search.RandomizedSearchCV
Warning: DEPRECATED
class sklearn.grid_search.RandomizedSearchCV(estimator,
param_distributions,
n_iter=10,
scoring=None, ﬁt_params=None, n_jobs=1,
iid=True,
reﬁt=True,
cv=None,
ver-
bose=0,
pre_dispatch=‘2*n_jobs’,
ran-
dom_state=None, error_score=’raise’)
Randomized search on hyper parameters.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.RandomizedSearchCV instead.
RandomizedSearchCV implements a “ﬁt” and a “score” method. It also implements “predict”, “predict_proba”,
“decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.
The parameters of the estimator used to apply these methods are optimized by cross-validated search over
parameter settings.
In contrast to GridSearchCV, not all parameter values are tried out, but rather a ﬁxed number of parameter
settings is sampled from the speciﬁed distributions. The number of parameter settings that are tried is given by
n_iter.
If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter
is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous
distributions for continuous parameters.
Read more in the User Guide.
1928
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Parametersestimator : estimator object.
A object of that type is instantiated for each grid point. This is assumed to implement
the scikit-learn estimator interface. Either estimator needs to provide a score function,
or scoring must be passed.
param_distributions : dict
Dictionary with parameters names (string) as keys and distributions or lists of parame-
ters to try. Distributions must provide a rvs method for sampling (such as those from
scipy.stats.distributions). If a list is given, it is sampled uniformly.
n_iter : int, default=10
Number of parameter settings that are sampled. n_iter trades off runtime vs quality of
the solution.
scoring : string, callable or None, default=None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y). If None, the score method of the estimator
is used.
ﬁt_params : dict, optional
Parameters to pass to the ﬁt method.
n_jobs : int, default=1
Number of jobs to run in parallel.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
iid : boolean, default=True
If True, the data is assumed to be identically distributed across the folds, and the loss
minimized is the total loss per sample, and not the mean loss across the folds.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or mul-
ticlass, sklearn.model_selection.StratifiedKFold is used. In all other
cases, sklearn.model_selection.KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
29.36. Recently deprecated
1929
scikit-learn user guide, Release 0.18.2
reﬁt : boolean, default=True
Reﬁt the best estimator with the entire dataset. If “False”, it is impossible to make
predictions using this RandomizedSearchCV instance after ﬁtting.
verbose : integer
Controls the verbosity: the higher, the more messages.
random_state : int or RandomState
Pseudo random number generator state used for random uniform sampling from lists of
possible values instead of scipy.stats distributions.
error_score : ‘raise’ (default) or numeric
Value to assign to the score if an error occurs in estimator ﬁtting. If set to ‘raise’, the
error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter
does not affect the reﬁt step, which will always raise the error.
Attributesgrid_scores_ : list of named tuples
Contains scores for all parameter combinations in param_grid. Each entry corresponds
to one parameter setting. Each named tuple has the attributes:
•parameters, a dict of parameter settings
•mean_validation_score, the mean score over the cross-validation folds
•cv_validation_scores, the list of scores for each fold
best_estimator_ : estimator
Estimator that was chosen by the search, i.e. estimator which gave highest score (or
smallest loss if speciﬁed) on the left out data. Not available if reﬁt=False.
best_score_ : ﬂoat
Score of best_estimator on the left out data.
best_params_ : dict
Parameter setting that gave the best results on the hold out data.
See also:
GridSearchCVDoes exhaustive search over a grid of parameters.
ParameterSamplerA generator over parameter settings, constructed from param_distributions.
Notes
The parameters selected are those that maximize the score of the held-out data, according to the scoring param-
eter.
If n_jobs was set to a value higher than one, the data is copied for each parameter setting(and not n_jobs times).
This is done for efﬁciency reasons if individual jobs take very little time, but may raise errors if the dataset is
large and not enough memory is available. A workaround in this case is to set pre_dispatch. Then, the memory
is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs.
Methods
1930
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
decision_function(\*args, \*\*kwargs)
Call decision_function on the estimator with the best
found parameters.
fit(X[, y])
Run ﬁt on the estimator with randomly drawn parame-
ters.
get_params([deep])
Get parameters for this estimator.
inverse_transform(\*args, \*\*kwargs)
Call inverse_transform on the estimator with the best
found parameters.
predict(\*args, \*\*kwargs)
Call predict on the estimator with the best found param-
eters.
predict_log_proba(\*args, \*\*kwargs)
Call predict_log_proba on the estimator with the best
found parameters.
predict_proba(\*args, \*\*kwargs)
Call predict_proba on the estimator with the best found
parameters.
score(X[, y])
Returns the score on the given data, if the estimator has
been reﬁt.
set_params(\*\*params)
Set the parameters of this estimator.
transform(\*args, \*\*kwargs)
Call transform on the estimator with the best found pa-
rameters.
__init__(estimator, param_distributions, n_iter=10, scoring=None, ﬁt_params=None, n_jobs=1,
iid=True, reﬁt=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, random_state=None,
error_score=’raise’)
decision_function(*args, **kwargs)
Call decision_function on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports decision_function.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
fit(X, y=None)
Run ﬁt on the estimator with randomly drawn parameters.
ParametersX : array-like, shape = [n_samples, n_features]
Training vector, where n_samples in the number of samples and n_features is the num-
ber of features.
y : array-like, shape = [n_samples] or [n_samples, n_output], optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(*args, **kwargs)
Call inverse_transform on the estimator with the best found parameters.
Only available if the underlying estimator implements inverse_transform and refit=True.
29.36. Recently deprecated
1931
scikit-learn user guide, Release 0.18.2
ParametersXt : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict(*args, **kwargs)
Call predict on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict_log_proba(*args, **kwargs)
Call predict_log_proba on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict_log_proba.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
predict_proba(*args, **kwargs)
Call predict_proba on the estimator with the best found parameters.
Only available if refit=True and the underlying estimator supports predict_proba.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
score(X, y=None)
Returns the score on the given data, if the estimator has been reﬁt.
This uses the score deﬁned by scoring where provided, and the best_estimator_.score method
otherwise.
ParametersX : array-like, shape = [n_samples, n_features]
Input data, where n_samples is the number of samples and n_features is the number of
features.
y : array-like, shape = [n_samples] or [n_samples, n_output], optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
Returnsscore : ﬂoat
Notes
•The long-standing behavior of this method changed in version 0.16.
•It no longer uses the metric provided by estimator.score if the scoring parameter was set
when ﬁtting.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
1932
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
transform(*args, **kwargs)
Call transform on the estimator with the best found parameters.
Only available if the underlying estimator supports transform and refit=True.
ParametersX : indexable, length n_samples
Must fulﬁll the input assumptions of the underlying estimator.
sklearn.cross_validation.LeaveOneOut
Warning: DEPRECATED
class sklearn.cross_validation.LeaveOneOut(n)
Leave-One-Out cross validation iterator.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.LeaveOneOut instead.
Provides train/test indices to split data in train test sets. Each sample is used once as a test set (singleton) while
the remaining samples form the training set.
Note: LeaveOneOut(n) is equivalent to KFold(n,n_folds=n) and LeavePOut(n,p=1).
Due to the high number of test sets (which is the same as the number of samples) this cross validation method
can be very costly. For large datasets one should favor KFold, StratiﬁedKFold or ShufﬂeSplit.
Read more in the User Guide.
Parametersn : int
Total number of elements in dataset.
See also:
LeaveOneLabelOut, domain-specific
Examples
>>> from sklearn import cross_validation
>>> X = np.array([[1, 2], [3, 4]])
>>> y = np.array([1, 2])
>>> loo = cross_validation.LeaveOneOut(2)
>>> len(loo)
2
>>> print(loo)
sklearn.cross_validation.LeaveOneOut(n=2)
>>> for train_index, test_index in loo:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
...
print(X_train, X_test, y_train, y_test)
TRAIN: [1] TEST: [0]
[[3 4]] [[1 2]] [2] [1]
TRAIN: [0] TEST: [1]
[[1 2]] [[3 4]] [1] [2]
.. automethod:: __init__
29.36. Recently deprecated
1933
scikit-learn user guide, Release 0.18.2
sklearn.cross_validation.LeavePOut
Warning: DEPRECATED
class sklearn.cross_validation.LeavePOut(n, p)
Leave-P-Out cross validation iterator
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.LeavePOut instead.
Provides train/test indices to split data in train test sets. This results in testing on all distinct samples of size p,
while the remaining n - p samples form the training set in each iteration.
Note:
LeavePOut(n,p) is NOT equivalent to KFold(n,n_folds=n // p) which creates non-
overlapping test sets.
Due to the high number of iterations which grows combinatorically with the number of samples this cross
validation method can be very costly. For large datasets one should favor KFold, StratiﬁedKFold or ShufﬂeSplit.
Read more in the User Guide.
Parametersn : int
Total number of elements in dataset.
p : int
Size of the test sets.
Examples
>>> from sklearn import cross_validation
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
>>> y = np.array([1, 2, 3, 4])
>>> lpo = cross_validation.LeavePOut(4, 2)
>>> len(lpo)
6
>>> print(lpo)
sklearn.cross_validation.LeavePOut(n=4, p=2)
>>> for train_index, test_index in lpo:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [2 3] TEST: [0 1]
TRAIN: [1 3] TEST: [0 2]
TRAIN: [1 2] TEST: [0 3]
TRAIN: [0 3] TEST: [1 2]
TRAIN: [0 2] TEST: [1 3]
TRAIN: [0 1] TEST: [2 3]
.. automethod:: __init__
1934
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.cross_validation.KFold
Warning: DEPRECATED
class sklearn.cross_validation.KFold(n, n_folds=3, shufﬂe=False, random_state=None)
K-Folds cross validation iterator.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.KFold instead.
Provides train/test indices to split data in train test sets. Split dataset into k consecutive folds (without shufﬂing
by default).
Each fold is then used as a validation set once while the k - 1 remaining fold(s) form the training set.
Read more in the User Guide.
Parametersn : int
Total number of elements.
n_folds : int, default=3
Number of folds. Must be at least 2.
shufﬂe : boolean, optional
Whether to shufﬂe the data before splitting into batches.
random_state : None, int or RandomState
When shufﬂe=True, pseudo-random number generator state used for shufﬂing. If None,
use default numpy RNG for shufﬂing.
See also:
StratifiedKFold, folds, classification
LabelKFoldK-fold iterator variant with non-overlapping labels.
Notes
The ﬁrst n % n_folds folds have size n // n_folds + 1, other folds have size n // n_folds.
Examples
>>> from sklearn.cross_validation import KFold
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([1, 2, 3, 4])
>>> kf = KFold(4, n_folds=2)
>>> len(kf)
2
>>> print(kf)
sklearn.cross_validation.KFold(n=4, n_folds=2, shuffle=False,
random_state=None)
>>> for train_index, test_index in kf:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
29.36. Recently deprecated
1935
scikit-learn user guide, Release 0.18.2
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [2 3] TEST: [0 1]
TRAIN: [0 1] TEST: [2 3]
.. automethod:: __init__
sklearn.cross_validation.LabelKFold
Warning: DEPRECATED
class sklearn.cross_validation.LabelKFold(labels, n_folds=3)
K-fold iterator variant with non-overlapping labels.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.GroupKFold instead.
The same label will not appear in two different folds (the number of distinct labels has to be at least equal to the
number of folds).
The folds are approximately balanced in the sense that the number of distinct labels is approximately the same
in each fold.
New in version 0.17.
Parameterslabels : array-like with shape (n_samples, )
Contains a label for each sample. The folds are built so that the same label does not
appear in two different folds.
n_folds : int, default=3
Number of folds. Must be at least 2.
See also:
LeaveOneLabelOut, domain-specific
Examples
>>> from sklearn.cross_validation import LabelKFold
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
>>> y = np.array([1, 2, 3, 4])
>>> labels = np.array([0, 0, 2, 2])
>>> label_kfold = LabelKFold(labels, n_folds=2)
>>> len(label_kfold)
2
>>> print(label_kfold)
sklearn.cross_validation.LabelKFold(n_labels=4, n_folds=2)
>>> for train_index, test_index in label_kfold:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
...
print(X_train, X_test, y_train, y_test)
...
TRAIN: [0 1] TEST: [2 3]
[[1 2]
[3 4]] [[5 6]
1936
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
[7 8]] [1 2] [3 4]
TRAIN: [2 3] TEST: [0 1]
[[5 6]
[7 8]] [[1 2]
[3 4]] [3 4] [1 2]
.. automethod:: __init__
sklearn.cross_validation.LeaveOneLabelOut
Warning: DEPRECATED
class sklearn.cross_validation.LeaveOneLabelOut(labels)
Leave-One-Label_Out cross-validation iterator
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.LeaveOneGroupOut instead.
Provides train/test indices to split data according to a third-party provided label. This label information can be
used to encode arbitrary domain speciﬁc stratiﬁcations of the samples as integers.
For instance the labels could be the year of collection of the samples and thus allow for cross-validation against
time-based splits.
Read more in the User Guide.
Parameterslabels : array-like of int with shape (n_samples,)
Arbitrary domain-speciﬁc stratiﬁcation of the data to be used to draw the splits.
See also:
LabelKFoldK-fold iterator variant with non-overlapping labels.
Examples
>>> from sklearn import cross_validation
>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
>>> y = np.array([1, 2, 1, 2])
>>> labels = np.array([1, 1, 2, 2])
>>> lol = cross_validation.LeaveOneLabelOut(labels)
>>> len(lol)
2
>>> print(lol)
sklearn.cross_validation.LeaveOneLabelOut(labels=[1 1 2 2])
>>> for train_index, test_index in lol:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
...
print(X_train, X_test, y_train, y_test)
TRAIN: [2 3] TEST: [0 1]
[[5 6]
[7 8]] [[1 2]
[3 4]] [1 2] [1 2]
TRAIN: [0 1] TEST: [2 3]
29.36. Recently deprecated
1937
scikit-learn user guide, Release 0.18.2
[[1 2]
[3 4]] [[5 6]
[7 8]] [1 2] [1 2]
.. automethod:: __init__
sklearn.cross_validation.LeavePLabelOut
Warning: DEPRECATED
class sklearn.cross_validation.LeavePLabelOut(labels, p)
Leave-P-Label_Out cross-validation iterator
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.LeavePGroupsOut instead.
Provides train/test indices to split data according to a third-party provided label. This label information can be
used to encode arbitrary domain speciﬁc stratiﬁcations of the samples as integers.
For instance the labels could be the year of collection of the samples and thus allow for cross-validation against
time-based splits.
The difference between LeavePLabelOut and LeaveOneLabelOut is that the former builds the test sets with all
the samples assigned to p different values of the labels while the latter uses samples all assigned the same labels.
Read more in the User Guide.
Parameterslabels : array-like of int with shape (n_samples,)
Arbitrary domain-speciﬁc stratiﬁcation of the data to be used to draw the splits.
p : int
Number of samples to leave out in the test split.
See also:
LabelKFoldK-fold iterator variant with non-overlapping labels.
Examples
>>> from sklearn import cross_validation
>>> X = np.array([[1, 2], [3, 4], [5, 6]])
>>> y = np.array([1, 2, 1])
>>> labels = np.array([1, 2, 3])
>>> lpl = cross_validation.LeavePLabelOut(labels, p=2)
>>> len(lpl)
3
>>> print(lpl)
sklearn.cross_validation.LeavePLabelOut(labels=[1 2 3], p=2)
>>> for train_index, test_index in lpl:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
...
print(X_train, X_test, y_train, y_test)
TRAIN: [2] TEST: [0 1]
1938
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
[[5 6]] [[1 2]
[3 4]] [1] [1 2]
TRAIN: [1] TEST: [0 2]
[[3 4]] [[1 2]
[5 6]] [2] [1 1]
TRAIN: [0] TEST: [1 2]
[[1 2]] [[3 4]
[5 6]] [1] [2 1]
.. automethod:: __init__
sklearn.cross_validation.LabelShufﬂeSplit
Warning: DEPRECATED
class sklearn.cross_validation.LabelShuffleSplit(labels,
n_iter=5,
test_size=0.2,
train_size=None, random_state=None)
Shufﬂe-Labels-Out cross-validation iterator
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.GroupShuffleSplit instead.
Provides randomized train/test indices to split data according to a third-party provided label. This label infor-
mation can be used to encode arbitrary domain speciﬁc stratiﬁcations of the samples as integers.
For instance the labels could be the year of collection of the samples and thus allow for cross-validation against
time-based splits.
The difference between LeavePLabelOut and LabelShufﬂeSplit is that the former generates splits using all
subsets of size p unique labels, whereas LabelShufﬂeSplit generates a user-determined number of random test
splits, each with a user-determined fraction of unique labels.
For example, a less computationally intensive alternative to LeavePLabelOut(labels,p=10) would be
LabelShuffleSplit(labels,test_size=10,n_iter=100).
Note: The parameters test_size and train_size refer to labels, and not to samples, as in ShufﬂeSplit.
New in version 0.17.
Parameterslabels : array, [n_samples]
Labels of samples
n_iter : int (default 5)
Number of re-shufﬂing and splitting iterations.
test_size : ﬂoat (default 0.2), int, or None
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the labels to
include in the test split. If int, represents the absolute number of test labels. If None,
the value is automatically set to the complement of the train size.
train_size : ﬂoat, int, or None (default is None)
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the labels to
include in the train split. If int, represents the absolute number of train labels. If None,
the value is automatically set to the complement of the test size.
random_state : int or RandomState
29.36. Recently deprecated
1939
scikit-learn user guide, Release 0.18.2
Pseudo-random number generator state used for random sampling.
__init__(labels, n_iter=5, test_size=0.2, train_size=None, random_state=None)
sklearn.cross_validation.StratiﬁedKFold
Warning: DEPRECATED
class sklearn.cross_validation.StratifiedKFold(y,
n_folds=3,
shufﬂe=False,
ran-
dom_state=None)
Stratiﬁed K-Folds cross validation iterator
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.StratifiedKFold instead.
Provides train/test indices to split data in train test sets.
This cross-validation object is a variation of KFold that returns stratiﬁed folds. The folds are made by preserving
the percentage of samples for each class.
Read more in the User Guide.
Parametersy : array-like, [n_samples]
Samples to split in K folds.
n_folds : int, default=3
Number of folds. Must be at least 2.
shufﬂe : boolean, optional
Whether to shufﬂe each stratiﬁcation of the data before splitting into batches.
random_state : None, int or RandomState
When shufﬂe=True, pseudo-random number generator state used for shufﬂing. If None,
use default numpy RNG for shufﬂing.
See also:
LabelKFoldK-fold iterator variant with non-overlapping labels.
Notes
All the folds have size trunc(n_samples / n_folds), the last one has the complementary.
Examples
>>> from sklearn.cross_validation import StratifiedKFold
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
>>> skf = StratifiedKFold(y, n_folds=2)
>>> len(skf)
2
>>> print(skf)
sklearn.cross_validation.StratifiedKFold(labels=[0 0 1 1], n_folds=2,
1940
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
shuffle=False, random_state=None)
>>> for train_index, test_index in skf:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [1 3] TEST: [0 2]
TRAIN: [0 2] TEST: [1 3]
.. automethod:: __init__
sklearn.cross_validation.ShufﬂeSplit
Warning: DEPRECATED
class sklearn.cross_validation.ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None,
random_state=None)
Random permutation cross-validation iterator.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.ShuffleSplit instead.
Yields indices to split data into training and test sets.
Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different,
although this is still very likely for sizeable datasets.
Read more in the User Guide.
Parametersn : int
Total number of elements in the dataset.
n_iter : int (default 10)
Number of re-shufﬂing & splitting iterations.
test_size : ﬂoat (default 0.1), int, or None
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the test split. If int, represents the absolute number of test samples. If None,
the value is automatically set to the complement of the train size.
train_size : ﬂoat, int, or None (default is None)
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the train split. If int, represents the absolute number of train samples. If
None, the value is automatically set to the complement of the test size.
random_state : int or RandomState
Pseudo-random number generator state used for random sampling.
Examples
>>> from sklearn import cross_validation
>>> rs = cross_validation.ShuffleSplit(4, n_iter=3,
...
test_size=.25, random_state=0)
>>> len(rs)
29.36. Recently deprecated
1941
scikit-learn user guide, Release 0.18.2
3
>>> print(rs)
...
ShuffleSplit(4, n_iter=3, test_size=0.25, ...)
>>> for train_index, test_index in rs:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
TRAIN: [3 1 0] TEST: [2]
TRAIN: [2 1 3] TEST: [0]
TRAIN: [0 2 1] TEST: [3]
>>> rs = cross_validation.ShuffleSplit(4, n_iter=3,
...
train_size=0.5, test_size=.25, random_state=0)
>>> for train_index, test_index in rs:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
TRAIN: [3 1] TEST: [2]
TRAIN: [2 1] TEST: [0]
TRAIN: [0 2] TEST: [3]
.. automethod:: __init__
sklearn.cross_validation.StratiﬁedShufﬂeSplit
Warning: DEPRECATED
class sklearn.cross_validation.StratifiedShuffleSplit(y,
n_iter=10,
test_size=0.1,
train_size=None,
ran-
dom_state=None)
Stratiﬁed ShufﬂeSplit cross validation iterator
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.StratifiedShuffleSplit instead.
Provides train/test indices to split data in train test sets.
This cross-validation object is a merge of StratiﬁedKFold and ShufﬂeSplit, which returns stratiﬁed randomized
folds. The folds are made by preserving the percentage of samples for each class.
Note: like the ShufﬂeSplit strategy, stratiﬁed random splits do not guarantee that all folds will be different,
although this is still very likely for sizeable datasets.
Read more in the User Guide.
Parametersy : array, [n_samples]
Labels of samples.
n_iter : int (default 10)
Number of re-shufﬂing & splitting iterations.
test_size : ﬂoat (default 0.1), int, or None
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the test split. If int, represents the absolute number of test samples. If None,
the value is automatically set to the complement of the train size.
train_size : ﬂoat, int, or None (default is None)
1942
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the train split. If int, represents the absolute number of train samples. If
None, the value is automatically set to the complement of the test size.
random_state : int or RandomState
Pseudo-random number generator state used for random sampling.
Examples
>>> from sklearn.cross_validation import StratifiedShuffleSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
>>> sss = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0)
>>> len(sss)
3
>>> print(sss)
StratifiedShuffleSplit(labels=[0 0 1 1], n_iter=3, ...)
>>> for train_index, test_index in sss:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [1 2] TEST: [3 0]
TRAIN: [0 2] TEST: [1 3]
TRAIN: [0 2] TEST: [3 1]
.. automethod:: __init__
sklearn.cross_validation.PredeﬁnedSplit
Warning: DEPRECATED
class sklearn.cross_validation.PredefinedSplit(test_fold)
Predeﬁned split cross validation iterator
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.PredefinedSplit instead.
Splits the data into training/test set folds according to a predeﬁned scheme. Each sample can be assigned to at
most one test set fold, as speciﬁed by the user through the test_fold parameter.
Read more in the User Guide.
Parameterstest_fold : “array-like, shape (n_samples,)
test_fold[i] gives the test set fold of sample i. A value of -1 indicates that the corre-
sponding sample is not part of any test set folds, but will instead always be put into the
training fold.
Examples
>>> from sklearn.cross_validation import PredefinedSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([0, 0, 1, 1])
29.36. Recently deprecated
1943
scikit-learn user guide, Release 0.18.2
>>> ps = PredefinedSplit(test_fold=[0, 1, -1, 1])
>>> len(ps)
2
>>> print(ps)
sklearn.cross_validation.PredefinedSplit(test_fold=[ 0
1 -1
1])
>>> for train_index, test_index in ps:
...
print("TRAIN:", train_index, "TEST:", test_index)
...
X_train, X_test = X[train_index], X[test_index]
...
y_train, y_test = y[train_index], y[test_index]
TRAIN: [1 2 3] TEST: [0]
TRAIN: [0 2] TEST: [1 3]
.. automethod:: __init__
sklearn.decomposition.RandomizedPCA
Warning: DEPRECATED
class sklearn.decomposition.RandomizedPCA(*args, **kwargs)
Principal component analysis (PCA) using randomized SVD
Deprecated since version 0.18: This class will be removed in 0.20. Use PCA with parameter svd_solver ‘ran-
domized’ instead. The new implementation DOES NOT store whiten components_. Apply transform to get
them.
Linear dimensionality reduction using approximated Singular Value Decomposition of the data and keeping
only the most signiﬁcant singular vectors to project the data to a lower dimensional space.
Read more in the User Guide.
Parametersn_components : int, optional
Maximum number of components to keep. When not given or None, this is set to
n_features (the second dimension of the training data).
copy : bool
If False, data passed to ﬁt are overwritten and running ﬁt(X).transform(X) will not yield
the expected results, use ﬁt_transform(X) instead.
iterated_power : int, default=2
Number of iterations for the power method.
Changed in version 0.18.
whiten : bool, optional
When True (False by default) the components_ vectors are multiplied by the square root
of (n_samples) and divided by the singular values to ensure uncorrelated outputs with
unit component-wise variances.
Whitening will remove some information from the transformed signal (the relative vari-
ance scales of the components) but can sometime improve the predictive accuracy of
the downstream estimators by making their data respect some hard-wired assumptions.
random_state : int or RandomState instance or None (default)
Pseudo Random Number generator seed control. If None, use the numpy.random sin-
gleton.
1944
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Attributescomponents_ : array, [n_components, n_features]
Components with maximum variance.
explained_variance_ratio_ : array, [n_components]
Percentage of variance explained by each of the selected components. k is not set then
all components are stored and the sum of explained variances is equal to 1.0
mean_ : array, [n_features]
Per-feature empirical mean, estimated from the training set.
See also:
PCA, TruncatedSVD
References
[Halko2009], [MRT]
Examples
>>> import numpy as np
>>> from sklearn.decomposition import RandomizedPCA
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> pca = RandomizedPCA(n_components=2)
>>> pca.fit(X)
RandomizedPCA(copy=True, iterated_power=2, n_components=2,
random_state=None, whiten=False)
>>> print(pca.explained_variance_ratio_)
[ 0.99244...
0.00755...]
Methods
fit(X[, y])
Fit the model with X by extracting the ﬁrst principal
components.
fit_transform(X[, y])
Fit the model with X and apply the dimensionality re-
duction on X.
get_params([deep])
Get parameters for this estimator.
inverse_transform(X[, y])
Transform data back to its original space.
set_params(\*\*params)
Set the parameters of this estimator.
transform(X[, y])
Apply dimensionality reduction on X.
__init__(*args, **kwargs)
DEPRECATED: RandomizedPCA was deprecated in 0.18 and will be removed in 0.20.
Use
PCA(svd_solver=’randomized’)
instead.
The
new
implementation
DOES
NOT
store
whiten
components_. Apply transform to get them.
fit(X, y=None)
Fit the model with X by extracting the ﬁrst principal components.
ParametersX: array-like, shape (n_samples, n_features) :
29.36. Recently deprecated
1945
scikit-learn user guide, Release 0.18.2
Training data, where n_samples in the number of samples and n_features is the number
of features.
Returnsself : object
Returns the instance itself.
fit_transform(X, y=None)
Fit the model with X and apply the dimensionality reduction on X.
ParametersX : array-like, shape (n_samples, n_features)
New data, where n_samples in the number of samples and n_features is the number of
features.
ReturnsX_new : array-like, shape (n_samples, n_components)
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
inverse_transform(X, y=None)
Transform data back to its original space.
Returns an array X_original whose transform would be X.
ParametersX : array-like, shape (n_samples, n_components)
New data, where n_samples in the number of samples and n_components is the number
of components.
ReturnsX_original array-like, shape (n_samples, n_features) :
Notes
If whitening is enabled, inverse_transform does not compute the exact inverse operation of transform.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
transform(X, y=None)
Apply dimensionality reduction on X.
X is projected on the ﬁrst principal components previous extracted from a training set.
ParametersX : array-like, shape (n_samples, n_features)
New data, where n_samples in the number of samples and n_features is the number of
features.
ReturnsX_new : array-like, shape (n_samples, n_components)
1946
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.gaussian_process.GaussianProcess
Warning: DEPRECATED
class sklearn.gaussian_process.GaussianProcess(*args, **kwargs)
The legacy Gaussian Process model class.
Deprecated since version 0.18: This class will be removed in 0.20. Use the GaussianProcessRegressor
instead.
Read more in the User Guide.
Parametersregr : string or callable, optional
A regression function returning an array of outputs of the linear regression functional
basis. The number of observations n_samples should be greater than the size p of this
basis. Default assumes a simple constant regression trend. Available built-in regression
models are:
'constant', 'linear', 'quadratic'
corr : string or callable, optional
A stationary autocorrelation function returning the autocorrelation between two points
x and x’. Default assumes a squared-exponential autocorrelation model. Built-in corre-
lation models are:
'absolute_exponential', 'squared_exponential',
'generalized_exponential', 'cubic', 'linear'
beta0 : double array_like, optional
The regression weight vector to perform Ordinary Kriging (OK). Default assumes Uni-
versal Kriging (UK) so that the vector beta of regression weights is estimated using the
maximum likelihood principle.
storage_mode : string, optional
A string specifying whether the Cholesky decomposition of the correlation matrix
should be stored in the class (storage_mode = ‘full’) or not (storage_mode = ‘light’).
Default assumes storage_mode = ‘full’, so that the Cholesky decomposition of the cor-
relation matrix is stored. This might be a useful parameter when one is not interested
in the MSE and only plan to estimate the BLUP, for which the correlation matrix is not
required.
verbose : boolean, optional
A boolean specifying the verbose level. Default is verbose = False.
theta0 : double array_like, optional
An array with shape (n_features, ) or (1, ). The parameters in the autocorrelation model.
If thetaL and thetaU are also speciﬁed, theta0 is considered as the starting point for the
maximum likelihood estimation of the best set of parameters. Default assumes isotropic
autocorrelation model with theta0 = 1e-1.
thetaL : double array_like, optional
29.36. Recently deprecated
1947
scikit-learn user guide, Release 0.18.2
An array with shape matching theta0’s. Lower bound on the autocorrelation parame-
ters for maximum likelihood estimation. Default is None, so that it skips maximum
likelihood estimation and it uses theta0.
thetaU : double array_like, optional
An array with shape matching theta0’s. Upper bound on the autocorrelation parame-
ters for maximum likelihood estimation. Default is None, so that it skips maximum
likelihood estimation and it uses theta0.
normalize : boolean, optional
Input X and observations y are centered and reduced wrt means and standard deviations
estimated from the n_samples observations provided. Default is normalize = True so
that data is normalized to ease maximum likelihood estimation.
nugget : double or ndarray, optional
Introduce a nugget effect to allow smooth predictions from noisy data. If nugget is
an ndarray, it must be the same length as the number of data points used for the ﬁt.
The nugget is added to the diagonal of the assumed training covariance; in this way
it acts as a Tikhonov regularization in the problem. In the special case of the squared
exponential correlation function, the nugget mathematically represents the variance of
the input values. Default assumes a nugget close to machine precision for the sake of
robustness (nugget = 10. * MACHINE_EPSILON).
optimizer : string, optional
A string specifying the optimization algorithm to be used. Default uses ‘fmin_cobyla’
algorithm from scipy.optimize. Available optimizers are:
'fmin_cobyla', 'Welch'
‘Welch’ optimizer is dued to Welch et al., see reference [WBSWM1992]. It consists
in iterating over several one-dimensional optimizations instead of running one single
multi-dimensional optimization.
random_start : int, optional
The number of times the Maximum Likelihood Estimation should be performed from a
random starting point. The ﬁrst MLE always uses the speciﬁed starting point (theta0),
the next starting points are picked at random according to an exponential distribution
(log-uniform on [thetaL, thetaU]). Default does not use random starting point (ran-
dom_start = 1).
random_state: integer or numpy.RandomState, optional :
The generator used to shufﬂe the sequence of coordinates of theta in the Welch opti-
mizer. If an integer is given, it ﬁxes the seed. Defaults to the global numpy random
number generator.
Attributestheta_ : array
Speciﬁed theta OR the best set of autocorrelation parameters (the sought maximizer of
the reduced likelihood function).
reduced_likelihood_function_value_ : array
The optimal reduced likelihood function value.
1948
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Notes
The presentation implementation is based on a translation of the DACE Matlab toolbox, see reference
[NLNS2002].
References
[NLNS2002], [WBSWM1992]
Examples
>>> import numpy as np
>>> from sklearn.gaussian_process import GaussianProcess
>>> X = np.array([[1., 3., 5., 6., 7., 8.]]).T
>>> y = (X * np.sin(X)).ravel()
>>> gp = GaussianProcess(theta0=0.1, thetaL=.001, thetaU=1.)
>>> gp.fit(X, y)
GaussianProcess(beta0=None...
...
Methods
fit(X, y)
The Gaussian Process model ﬁtting method.
get_params([deep])
Get parameters for this estimator.
predict(X[, eval_MSE, batch_size])
This function evaluates the Gaussian Process model at
x.
reduced_likelihood_function([theta])
This function determines the BLUP parameters and
evaluates the reduced likelihood function for the given
autocorrelation parameters theta.
score(X, y[, sample_weight])
Returns the coefﬁcient of determination R^2 of the pre-
diction.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(*args, **kwargs)
DEPRECATED: GaussianProcess was deprecated in version 0.18 and will be removed in 0.20. Use the
GaussianProcessRegressor instead.
fit(X, y)
The Gaussian Process model ﬁtting method.
ParametersX : double array_like
An array with shape (n_samples, n_features) with the input at which observations were
made.
y : double array_like
An array with shape (n_samples, ) or shape (n_samples, n_targets) with the observations
of the output to be predicted.
Returnsgp : self
29.36. Recently deprecated
1949
scikit-learn user guide, Release 0.18.2
A ﬁtted Gaussian Process model object awaiting data to perform predictions.
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X, eval_MSE=False, batch_size=None)
This function evaluates the Gaussian Process model at x.
ParametersX : array_like
An array with shape (n_eval, n_features) giving the point(s) at which the prediction(s)
should be made.
eval_MSE : boolean, optional
A boolean specifying whether the Mean Squared Error should be evaluated or not. De-
fault assumes evalMSE = False and evaluates only the BLUP (mean prediction).
batch_size : integer, optional
An integer giving the maximum number of points that can be evaluated simultaneously
(depending on the available memory). Default is None so that all given points are eval-
uated at the same time.
Returnsy : array_like, shape (n_samples, ) or (n_samples, n_targets)
An array with shape (n_eval, ) if the Gaussian Process was trained on an array of shape
(n_samples, ) or an array with shape (n_eval, n_targets) if the Gaussian Process was
trained on an array of shape (n_samples, n_targets) with the Best Linear Unbiased Pre-
diction at x.
MSE : array_like, optional (if eval_MSE == True)
An array with shape (n_eval, ) or (n_eval, n_targets) as with y, with the Mean Squared
Error at x.
reduced_likelihood_function(theta=None)
This function determines the BLUP parameters and evaluates the reduced likelihood function for the given
autocorrelation parameters theta.
Maximizing this function wrt the autocorrelation parameters theta is equivalent to maximizing the likeli-
hood of the assumed joint Gaussian distribution of the observations y evaluated onto the design of experi-
ments X.
Parameterstheta : array_like, optional
An array containing the autocorrelation parameters at which the Gaussian Process
model parameters should be determined. Default uses the built-in autocorrelation pa-
rameters (ie theta = self.theta_).
Returnsreduced_likelihood_function_value : double
The value of the reduced likelihood function associated to the given autocorrelation
parameters theta.
par : dict
1950
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
A dictionary containing the requested Gaussian Process model parameters:
sigma2Gaussian Process variance.
betaGeneralized least-squares regression weights for Universal Kriging or given
beta0 for Ordinary Kriging.
gammaGaussian Process weights.
CCholesky decomposition of the correlation matrix [R].
FtSolution of the linear equation system : [R] x Ft = F
GQR decomposition of the matrix Ft.
score(X, y, sample_weight=None)
Returns the coefﬁcient of determination R^2 of the prediction.
The coefﬁcient R^2 is deﬁned as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) **
2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score
is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
ParametersX : array-like, shape = (n_samples, n_features)
Test samples.
y : array-like, shape = (n_samples) or (n_samples, n_outputs)
True values for X.
sample_weight : array-like, shape = [n_samples], optional
Sample weights.
Returnsscore : ﬂoat
R^2 of self.predict(X) wrt. y.
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sklearn.mixture.GMM
Warning: DEPRECATED
class sklearn.mixture.GMM(*args, **kwargs)
Legacy Gaussian Mixture Model
Deprecated
since
version
0.18:
This
class
will
be
removed
in
0.20.
Use
sklearn.mixture.GaussianMixture instead.
Methods
29.36. Recently deprecated
1951
scikit-learn user guide, Release 0.18.2
aic(X)
Akaike information criterion for the current model ﬁt
and the proposed data.
bic(X)
Bayesian information criterion for the current model ﬁt
and the proposed data.
fit(X[, y])
Estimate model parameters with the EM algorithm.
fit_predict(X[, y])
Fit and then predict labels for data.
get_params([deep])
Get parameters for this estimator.
predict(X)
Predict label for data.
predict_proba(X)
Predict posterior probability of data under each Gaus-
sian in the model.
sample([n_samples, random_state])
Generate random samples from the model.
score(X[, y])
Compute the log probability under the model.
score_samples(X)
Return the per-sample likelihood of the data under the
model.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(*args, **kwargs)
DEPRECATED: The class GMM is deprecated in 0.18 and will be removed in 0.20. Use class Gaussian-
Mixture instead.
aic(X)
Akaike information criterion for the current model ﬁt and the proposed data.
ParametersX : array of shape(n_samples, n_dimensions)
Returnsaic: ﬂoat (the lower the better) :
bic(X)
Bayesian information criterion for the current model ﬁt and the proposed data.
ParametersX : array of shape(n_samples, n_dimensions)
Returnsbic: ﬂoat (the lower the better) :
fit(X, y=None)
Estimate model parameters with the EM algorithm.
A initialization step is performed before entering the expectation-maximization (EM) algorithm. If you
want to avoid this step, set the keyword argument init_params to the empty string ‘’ when creating the
GMM object. Likewise, if you would like just to do an initialization, set n_iter=0.
ParametersX : array_like, shape (n, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnsself :
fit_predict(X, y=None)
Fit and then predict labels for data.
Warning: Due to the ﬁnal maximization step in the EM algorithm, with low iterations the prediction may
not be 100% accurate.
New in version 0.17: ﬁt_predict method in Gaussian Mixture Model.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = (n_samples,) component memberships
get_params(deep=True)
Get parameters for this estimator.
1952
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
predict(X)
Predict label for data.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = (n_samples,) component memberships
predict_proba(X)
Predict posterior probability of data under each Gaussian in the model.
ParametersX : array-like, shape = [n_samples, n_features]
Returnsresponsibilities : array-like, shape = (n_samples, n_components)
Returns the probability of the sample for each Gaussian (state) in the model.
sample(n_samples=1, random_state=None)
Generate random samples from the model.
Parametersn_samples : int, optional
Number of samples to generate. Defaults to 1.
ReturnsX : array_like, shape (n_samples, n_features)
List of samples
score(X, y=None)
Compute the log probability under the model.
ParametersX : array_like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslogprob : array_like, shape (n_samples,)
Log probabilities of each data point in X
score_samples(X)
Return the per-sample likelihood of the data under the model.
Compute the log probability of X under the model and return the posterior distribution (responsibilities)
of each mixture component for each element of X.
ParametersX: array_like, shape (n_samples, n_features) :
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslogprob : array_like, shape (n_samples,)
Log probabilities of each data point in X.
responsibilities : array_like, shape (n_samples, n_components)
Posterior probabilities of each mixture component for each observation
set_params(**params)
Set the parameters of this estimator.
29.36. Recently deprecated
1953
scikit-learn user guide, Release 0.18.2
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sklearn.mixture.DPGMM
Warning: DEPRECATED
class sklearn.mixture.DPGMM(*args, **kwargs)
Dirichlet Process Gaussian Mixture Models
Deprecated
since
version
0.18:
This
class
will
be
removed
in
0.20.
Use
sklearn.mixture.BayesianGaussianMixture
with
parameter
weight_concentration_prior_type='dirichlet_process' instead.
Methods
aic(X)
Akaike information criterion for the current model ﬁt
and the proposed data.
bic(X)
Bayesian information criterion for the current model ﬁt
and the proposed data.
fit(X[, y])
Estimate model parameters with the EM algorithm.
fit_predict(X[, y])
Fit and then predict labels for data.
get_params([deep])
Get parameters for this estimator.
lower_bound(X, z)
returns a lower bound on model evidence based on X
and membership
predict(X)
Predict label for data.
predict_proba(X)
Predict posterior probability of data under each Gaus-
sian in the model.
sample([n_samples, random_state])
Generate random samples from the model.
score(X[, y])
Compute the log probability under the model.
score_samples(X)
Return the likelihood of the data under the model.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(*args, **kwargs)
DEPRECATED:
The
DPGMM
class
is
not
working
correctly
and
it’s
bet-
ter
to
use
sklearn.mixture.BayesianGaussianMixture
class
with
parameter
weight_concentration_prior_type=’dirichlet_process’ instead.
DPGMM is deprecated in 0.18 and
will be removed in 0.20.
aic(X)
Akaike information criterion for the current model ﬁt and the proposed data.
ParametersX : array of shape(n_samples, n_dimensions)
Returnsaic: ﬂoat (the lower the better) :
bic(X)
Bayesian information criterion for the current model ﬁt and the proposed data.
1954
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
ParametersX : array of shape(n_samples, n_dimensions)
Returnsbic: ﬂoat (the lower the better) :
fit(X, y=None)
Estimate model parameters with the EM algorithm.
A initialization step is performed before entering the expectation-maximization (EM) algorithm. If you
want to avoid this step, set the keyword argument init_params to the empty string ‘’ when creating the
GMM object. Likewise, if you would like just to do an initialization, set n_iter=0.
ParametersX : array_like, shape (n, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnsself :
fit_predict(X, y=None)
Fit and then predict labels for data.
Warning: Due to the ﬁnal maximization step in the EM algorithm, with low iterations the prediction may
not be 100% accurate.
New in version 0.17: ﬁt_predict method in Gaussian Mixture Model.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = (n_samples,) component memberships
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
lower_bound(X, z)
returns a lower bound on model evidence based on X and membership
predict(X)
Predict label for data.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = (n_samples,) component memberships
predict_proba(X)
Predict posterior probability of data under each Gaussian in the model.
ParametersX : array-like, shape = [n_samples, n_features]
Returnsresponsibilities : array-like, shape = (n_samples, n_components)
Returns the probability of the sample for each Gaussian (state) in the model.
sample(n_samples=1, random_state=None)
Generate random samples from the model.
Parametersn_samples : int, optional
Number of samples to generate. Defaults to 1.
ReturnsX : array_like, shape (n_samples, n_features)
29.36. Recently deprecated
1955
scikit-learn user guide, Release 0.18.2
List of samples
score(X, y=None)
Compute the log probability under the model.
ParametersX : array_like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslogprob : array_like, shape (n_samples,)
Log probabilities of each data point in X
score_samples(X)
Return the likelihood of the data under the model.
Compute the bound on log probability of X under the model and return the posterior distribution (respon-
sibilities) of each mixture component for each element of X.
This is done by computing the parameters for the mean-ﬁeld of z for each observation.
ParametersX : array_like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslogprob : array_like, shape (n_samples,)
Log probabilities of each data point in X
responsibilities : array_like, shape (n_samples, n_components)
Posterior probabilities of each mixture component for each observation
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
sklearn.mixture.VBGMM
Warning: DEPRECATED
class sklearn.mixture.VBGMM(*args, **kwargs)
Variational Inference for the Gaussian Mixture Model
Deprecated
since
version
0.18:
This
class
will
be
removed
in
0.20.
Use
sklearn.mixture.BayesianGaussianMixture
with
parameter
weight_concentration_prior_type='dirichlet_distribution' instead.
Variational inference for a Gaussian mixture model probability distribution. This class allows for easy and
efﬁcient inference of an approximate posterior distribution over the parameters of a Gaussian mixture model
with a ﬁxed number of components.
Initialization is with normally-distributed means and identity covariance, for proper convergence.
Read more in the User Guide.
Parametersn_components: int, default 1 :
1956
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Number of mixture components.
covariance_type: string, default ‘diag’ :
String describing the type of covariance parameters to use. Must be one of ‘spherical’,
‘tied’, ‘diag’, ‘full’.
alpha: ﬂoat, default 1 :
Real number representing the concentration parameter of the dirichlet distribution. Intu-
itively, the higher the value of alpha the more likely the variational mixture of Gaussians
model will use all components it can.
tol : ﬂoat, default 1e-3
Convergence threshold.
n_iter : int, default 10
Maximum number of iterations to perform before convergence.
params : string, default ‘wmc’
Controls which parameters are updated in the training process. Can contain any combi-
nation of ‘w’ for weights, ‘m’ for means, and ‘c’ for covars.
init_params : string, default ‘wmc’
Controls which parameters are updated in the initialization process. Can contain any
combination of ‘w’ for weights, ‘m’ for means, and ‘c’ for covars. Defaults to ‘wmc’.
verbose : int, default 0
Controls output verbosity.
Attributescovariance_type : string
String describing the type of covariance parameters used by the DP-GMM. Must be one
of ‘spherical’, ‘tied’, ‘diag’, ‘full’.
n_features : int
Dimensionality of the Gaussians.
n_components : int (read-only)
Number of mixture components.
weights_ : array, shape (n_components,)
Mixing weights for each mixture component.
means_ : array, shape (n_components, n_features)
Mean parameters for each mixture component.
precs_ : array
Precision (inverse covariance) parameters for each mixture component. The shape de-
pends on covariance_type:
(`n_components`, 'n_features')
if 'spherical',
(`n_features`, `n_features`)
if 'tied',
(`n_components`, `n_features`)
if 'diag',
(`n_components`, `n_features`, `n_features`)
if 'full'
converged_ : bool
29.36. Recently deprecated
1957
scikit-learn user guide, Release 0.18.2
True when convergence was reached in ﬁt(), False otherwise.
See also:
GMMFinite Gaussian mixture model ﬁt with EM
DPGMMInﬁnite Gaussian mixture model, using the dirichlet process, ﬁt with a variational algorithm
Methods
aic(X)
Akaike information criterion for the current model ﬁt
and the proposed data.
bic(X)
Bayesian information criterion for the current model ﬁt
and the proposed data.
fit(X[, y])
Estimate model parameters with the EM algorithm.
fit_predict(X[, y])
Fit and then predict labels for data.
get_params([deep])
Get parameters for this estimator.
lower_bound(X, z)
returns a lower bound on model evidence based on X
and membership
predict(X)
Predict label for data.
predict_proba(X)
Predict posterior probability of data under each Gaus-
sian in the model.
sample([n_samples, random_state])
Generate random samples from the model.
score(X[, y])
Compute the log probability under the model.
score_samples(X)
Return the likelihood of the data under the model.
set_params(\*\*params)
Set the parameters of this estimator.
__init__(*args, **kwargs)
DEPRECATED:
The
VBGMM
class
is
not
working
correctly
and
it’s
bet-
ter
to
use
sklearn.mixture.BayesianGaussianMixture
class
with
parameter
weight_concentration_prior_type=’dirichlet_distribution’ instead.
VBGMM is deprecated in 0.18
and will be removed in 0.20.
aic(X)
Akaike information criterion for the current model ﬁt and the proposed data.
ParametersX : array of shape(n_samples, n_dimensions)
Returnsaic: ﬂoat (the lower the better) :
bic(X)
Bayesian information criterion for the current model ﬁt and the proposed data.
ParametersX : array of shape(n_samples, n_dimensions)
Returnsbic: ﬂoat (the lower the better) :
fit(X, y=None)
Estimate model parameters with the EM algorithm.
A initialization step is performed before entering the expectation-maximization (EM) algorithm. If you
want to avoid this step, set the keyword argument init_params to the empty string ‘’ when creating the
GMM object. Likewise, if you would like just to do an initialization, set n_iter=0.
ParametersX : array_like, shape (n, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
1958
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Returnsself :
fit_predict(X, y=None)
Fit and then predict labels for data.
Warning: Due to the ﬁnal maximization step in the EM algorithm, with low iterations the prediction may
not be 100% accurate.
New in version 0.17: ﬁt_predict method in Gaussian Mixture Model.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = (n_samples,) component memberships
get_params(deep=True)
Get parameters for this estimator.
Parametersdeep : boolean, optional
If True, will return the parameters for this estimator and contained subobjects that are
estimators.
Returnsparams : mapping of string to any
Parameter names mapped to their values.
lower_bound(X, z)
returns a lower bound on model evidence based on X and membership
predict(X)
Predict label for data.
ParametersX : array-like, shape = [n_samples, n_features]
ReturnsC : array, shape = (n_samples,) component memberships
predict_proba(X)
Predict posterior probability of data under each Gaussian in the model.
ParametersX : array-like, shape = [n_samples, n_features]
Returnsresponsibilities : array-like, shape = (n_samples, n_components)
Returns the probability of the sample for each Gaussian (state) in the model.
sample(n_samples=1, random_state=None)
Generate random samples from the model.
Parametersn_samples : int, optional
Number of samples to generate. Defaults to 1.
ReturnsX : array_like, shape (n_samples, n_features)
List of samples
score(X, y=None)
Compute the log probability under the model.
ParametersX : array_like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslogprob : array_like, shape (n_samples,)
Log probabilities of each data point in X
29.36. Recently deprecated
1959
scikit-learn user guide, Release 0.18.2
score_samples(X)
Return the likelihood of the data under the model.
Compute the bound on log probability of X under the model and return the posterior distribution (respon-
sibilities) of each mixture component for each element of X.
This is done by computing the parameters for the mean-ﬁeld of z for each observation.
ParametersX : array_like, shape (n_samples, n_features)
List of n_features-dimensional data points. Each row corresponds to a single data point.
Returnslogprob : array_like, shape (n_samples,)
Log probabilities of each data point in X
responsibilities : array_like, shape (n_samples, n_components)
Posterior probabilities of each mixture component for each observation
set_params(**params)
Set the parameters of this estimator.
The method works on simple estimators as well as on nested objects (such as pipelines). The latter have
parameters of the form <component>__<parameter> so that it’s possible to update each component
of a nested object.
Returnsself :
grid_search.fit_grid_point(X, y, estimator, ...)
Run ﬁt on one set of parameters.
learning_curve.learning_curve(estimator,
X,
y)
Learning curve.
learning_curve.validation_curve(estimator,
...)
Validation curve.
cross_validation.cross_val_predict(estimator,
X)
Generate cross-validated estimates for each input data point
cross_validation.cross_val_score(estimator,
X)
Evaluate a score by cross-validation
cross_validation.check_cv(cv[, X, y, classiﬁer])
Input checker utility for building a CV in a user friendly
way.
cross_validation.permutation_test_score(...)Evaluate the signiﬁcance of a cross-validated score with
permutations
cross_validation.train_test_split(\*arrays,
...)
Split arrays or matrices into random train and test subsets
sklearn.grid_search.ﬁt_grid_point
Warning: DEPRECATED
sklearn.grid_search.fit_grid_point(X, y, estimator, parameters, train, test, scorer, verbose, er-
ror_score=’raise’, **ﬁt_params)
Run ﬁt on one set of parameters.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.fit_grid_point instead.
ParametersX : array-like, sparse matrix or list
1960
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Input data.
y : array-like or None
Targets for input data.
estimator : estimator object
A object of that type is instantiated for each grid point. This is assumed to implement
the scikit-learn estimator interface. Either estimator needs to provide a score function,
or scoring must be passed.
parameters : dict
Parameters to be set on estimator for this grid point.
train : ndarray, dtype int or bool
Boolean mask or indices for training set.
test : ndarray, dtype int or bool
Boolean mask or indices for test set.
scorer : callable or None.
If
provided
must
be
a
scorer
callable
object
/
function
with
signature
scorer(estimator,X,y).
verbose : int
Verbosity level.
**ﬁt_params : kwargs
Additional parameter passed to the ﬁt function of the estimator.
error_score : ‘raise’ (default) or numeric
Value to assign to the score if an error occurs in estimator ﬁtting. If set to ‘raise’, the
error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter
does not affect the reﬁt step, which will always raise the error.
Returnsscore : ﬂoat
Score of this parameter setting on given training / test split.
parameters : dict
The parameters that have been evaluated.
n_samples_test : int
Number of test samples in this split.
sklearn.learning_curve.learning_curve
Warning: DEPRECATED
sklearn.learning_curve.learning_curve(estimator,
X,
y,
train_sizes=array([
0.1,
0.33,
0.55,
0.78,
1.
]),
cv=None,
scoring=None,
exploit_incremental_learning=False,
n_jobs=1,
pre_dispatch=’all’, verbose=0, error_score=’raise’)
Learning curve.
29.36. Recently deprecated
1961
scikit-learn user guide, Release 0.18.2
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.learning_curve instead.
Determines cross-validated training and test scores for different training set sizes.
A cross-validation generator splits the whole dataset k times in training and test data. Subsets of the training set
with varying sizes will be used to train the estimator and a score for each training subset size and the test set
will be computed. Afterwards, the scores will be averaged over all k runs for each training subset size.
Read more in the User Guide.
Parametersestimator : object type that implements the “ﬁt” and “predict” methods
An object of that type which is cloned for each validation.
X : array-like, shape (n_samples, n_features)
Training vector, where n_samples is the number of samples and n_features is the number
of features.
y : array-like, shape (n_samples) or (n_samples, n_features), optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
train_sizes : array-like, shape (n_ticks,), dtype ﬂoat or int
Relative or absolute numbers of training examples that will be used to generate the
learning curve. If the dtype is ﬂoat, it is regarded as a fraction of the maximum size
of the training set (that is determined by the selected validation method), i.e. it has to
be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note
that for classiﬁcation the number of samples usually have to be big enough to contain at
least one sample from each class. (default: np.linspace(0.1, 1.0, 5))
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or mul-
ticlass, sklearn.model_selection.StratifiedKFold is used. In all other
cases, sklearn.model_selection.KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
scoring : string, callable or None, optional, default: None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
exploit_incremental_learning : boolean, optional, default: False
If the estimator supports incremental learning, this will be used to speed up ﬁtting for
different training set sizes.
n_jobs : integer, optional
Number of jobs to run in parallel (default 1).
pre_dispatch : integer or string, optional
1962
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
Number of predispatched jobs for parallel execution (default is all). The option can
reduce the allocated memory. The string can be an expression like ‘2*n_jobs’.
verbose : integer, optional
Controls the verbosity: the higher, the more messages.
error_score : ‘raise’ (default) or numeric
Value to assign to the score if an error occurs in estimator ﬁtting. If set to ‘raise’, the
error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter
does not affect the reﬁt step, which will always raise the error.
Returnstrain_sizes_abs : array, shape = (n_unique_ticks,), dtype int
Numbers of training examples that has been used to generate the learning curve. Note
that the number of ticks might be less than n_ticks because duplicate entries will be
removed.
train_scores : array, shape (n_ticks, n_cv_folds)
Scores on training sets.
test_scores : array, shape (n_ticks, n_cv_folds)
Scores on test set.
Notes
See examples/model_selection/plot_learning_curve.py
sklearn.learning_curve.validation_curve
Warning: DEPRECATED
sklearn.learning_curve.validation_curve(estimator,
X,
y,
param_name,
param_range,
cv=None,
scoring=None,
n_jobs=1,
pre_dispatch=’all’, verbose=0)
Validation curve.
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.validation_curve instead.
Determine training and test scores for varying parameter values.
Compute scores for an estimator with different values of a speciﬁed parameter. This is similar to grid search
with one parameter. However, this will also compute training scores and is merely a utility for plotting the
results.
Read more in the User Guide.
Parametersestimator : object type that implements the “ﬁt” and “predict” methods
An object of that type which is cloned for each validation.
X : array-like, shape (n_samples, n_features)
Training vector, where n_samples is the number of samples and n_features is the number
of features.
29.36. Recently deprecated
1963
scikit-learn user guide, Release 0.18.2
y : array-like, shape (n_samples) or (n_samples, n_features), optional
Target relative to X for classiﬁcation or regression; None for unsupervised learning.
param_name : string
Name of the parameter that will be varied.
param_range : array-like, shape (n_values,)
The values of the parameter that will be evaluated.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or mul-
ticlass, sklearn.model_selection.StratifiedKFold is used. In all other
cases, sklearn.model_selection.KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
scoring : string, callable or None, optional, default: None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
n_jobs : integer, optional
Number of jobs to run in parallel (default 1).
pre_dispatch : integer or string, optional
Number of predispatched jobs for parallel execution (default is all). The option can
reduce the allocated memory. The string can be an expression like ‘2*n_jobs’.
verbose : integer, optional
Controls the verbosity: the higher, the more messages.
Returnstrain_scores : array, shape (n_ticks, n_cv_folds)
Scores on training sets.
test_scores : array, shape (n_ticks, n_cv_folds)
Scores on test set.
Notes
See examples/model_selection/plot_validation_curve.py
1964
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
sklearn.cross_validation.cross_val_predict
Warning: DEPRECATED
sklearn.cross_validation.cross_val_predict(estimator,
X,
y=None,
cv=None,
n_jobs=1,
verbose=0,
ﬁt_params=None,
pre_dispatch=‘2*n_jobs’)
Generate cross-validated estimates for each input data point
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.cross_val_predict instead.
Read more in the User Guide.
Parametersestimator : estimator object implementing ‘ﬁt’ and ‘predict’
The object to use to ﬁt the data.
X : array-like
The data to ﬁt. Can be, for example a list, or an array at least 2d.
y : array-like, optional, default: None
The target variable to try to predict in the case of supervised learning.
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
n_jobs : integer, optional
The number of CPUs to use to do the computation. -1 means ‘all CPUs’.
verbose : integer, optional
The verbosity level.
ﬁt_params : dict, optional
Parameters to pass to the ﬁt method of the estimator.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
29.36. Recently deprecated
1965
scikit-learn user guide, Release 0.18.2
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
Returnspreds : ndarray
This is the result of calling ‘predict’
Examples
>>> from sklearn import datasets, linear_model
>>> from sklearn.cross_validation import cross_val_predict
>>> diabetes = datasets.load_diabetes()
>>> X = diabetes.data[:150]
>>> y = diabetes.target[:150]
>>> lasso = linear_model.Lasso()
>>> y_pred = cross_val_predict(lasso, X, y)
sklearn.cross_validation.cross_val_score
Warning: DEPRECATED
sklearn.cross_validation.cross_val_score(estimator, X, y=None, scoring=None, cv=None,
n_jobs=1,
verbose=0,
ﬁt_params=None,
pre_dispatch=‘2*n_jobs’)
Evaluate a score by cross-validation
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.cross_val_score instead.
Read more in the User Guide.
Parametersestimator : estimator object implementing ‘ﬁt’
The object to use to ﬁt the data.
X : array-like
The data to ﬁt. Can be, for example a list, or an array at least 2d.
y : array-like, optional, default: None
The target variable to try to predict in the case of supervised learning.
scoring : string, callable or None, optional, default: None
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
1966
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
n_jobs : integer, optional
The number of CPUs to use to do the computation. -1 means ‘all CPUs’.
verbose : integer, optional
The verbosity level.
ﬁt_params : dict, optional
Parameters to pass to the ﬁt method of the estimator.
pre_dispatch : int, or string, optional
Controls the number of jobs that get dispatched during parallel execution. Reducing
this number can be useful to avoid an explosion of memory consumption when more
jobs get dispatched than CPUs can process. This parameter can be:
•None, in which case all the jobs are immediately created and spawned. Use this for
lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the
jobs
•An int, giving the exact number of total jobs that are spawned
•A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’
Returnsscores : array of ﬂoat, shape=(len(list(cv)),)
Array of scores of the estimator for each run of the cross validation.
See also:
sklearn.metrics.make_scorerMake a scorer from a performance metric or loss function.
Examples
>>> from sklearn import datasets, linear_model
>>> from sklearn.cross_validation import cross_val_score
>>> diabetes = datasets.load_diabetes()
>>> X = diabetes.data[:150]
>>> y = diabetes.target[:150]
>>> lasso = linear_model.Lasso()
>>> print(cross_val_score(lasso, X, y))
[ 0.33150734
0.08022311
0.03531764]
sklearn.cross_validation.check_cv
Warning: DEPRECATED
sklearn.cross_validation.check_cv(cv, X=None, y=None, classiﬁer=False)
Input checker utility for building a CV in a user friendly way.
29.36. Recently deprecated
1967
scikit-learn user guide, Release 0.18.2
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.check_cv instead.
Parameterscv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs,
if classiﬁer is True and y is binary or multiclass,
StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
X : array-like
The data the cross-val object will be applied on.
y : array-like
The target variable for a supervised learning problem.
classiﬁer : boolean optional
Whether the task is a classiﬁcation task, in which case stratiﬁed KFold will be used.
Returnschecked_cv: a cross-validation generator instance. :
The return value is guaranteed to be a cv generator instance, whatever the input type.
sklearn.cross_validation.permutation_test_score
Warning: DEPRECATED
sklearn.cross_validation.permutation_test_score(estimator,
X,
y,
cv=None,
n_permutations=100,
n_jobs=1,
labels=None,
random_state=0,
ver-
bose=0, scoring=None)
Evaluate the signiﬁcance of a cross-validated score with permutations
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.permutation_test_score instead.
Read more in the User Guide.
Parametersestimator : estimator object implementing ‘ﬁt’
The object to use to ﬁt the data.
X : array-like of shape at least 2D
The data to ﬁt.
y : array-like
The target variable to try to predict in the case of supervised learning.
scoring : string, callable or None, optional, default: None
1968
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
A string (see model evaluation documentation) or a scorer callable object / function with
signature scorer(estimator,X,y).
cv : int, cross-validation generator or an iterable, optional
Determines the cross-validation splitting strategy. Possible inputs for cv are:
•None, to use the default 3-fold cross-validation,
•integer, to specify the number of folds.
•An object to be used as a cross-validation generator.
•An iterable yielding train/test splits.
For integer/None inputs, if the estimator is a classiﬁer and y is either binary or multi-
class, StratifiedKFold is used. In all other cases, KFold is used.
Refer User Guide for the various cross-validation strategies that can be used here.
n_permutations : integer, optional
Number of times to permute y.
n_jobs : integer, optional
The number of CPUs to use to do the computation. -1 means ‘all CPUs’.
labels : array-like of shape [n_samples] (optional)
Labels constrain the permutation among groups of samples with a same label.
random_state : RandomState or an int seed (0 by default)
A random number generator instance to deﬁne the state of the random permutations
generator.
verbose : integer, optional
The verbosity level.
Returnsscore : ﬂoat
The true score without permuting targets.
permutation_scores : array, shape (n_permutations,)
The scores obtained for each permutations.
pvalue : ﬂoat
The returned value equals p-value if scoring returns bigger numbers for better scores
(e.g., accuracy_score). If scoring is rather a loss function (i.e. when lower is better such
as with mean_squared_error) then this is actually the complement of the p-value: 1 -
p-value.
Notes
This function implements Test 1 in:
Ojala and Garriga. Permutation Tests for Studying Classiﬁer Performance. The Journal of Machine
Learning Research (2010) vol. 11
29.36. Recently deprecated
1969
scikit-learn user guide, Release 0.18.2
sklearn.cross_validation.train_test_split
Warning: DEPRECATED
sklearn.cross_validation.train_test_split(*arrays, **options)
Split arrays or matrices into random train and test subsets
Deprecated
since
version
0.18:
This
module
will
be
removed
in
0.20.
Use
sklearn.model_selection.train_test_split instead.
Quick utility that wraps input validation and next(iter(ShuffleSplit(n_samples))) and applica-
tion to input data into a single call for splitting (and optionally subsampling) data in a oneliner.
Read more in the User Guide.
Parameters*arrays : sequence of indexables with same length / shape[0]
Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.
test_size : ﬂoat, int, or None (default is None)
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the test split. If int, represents the absolute number of test samples. If None,
the value is automatically set to the complement of the train size. If train size is also
None, test size is set to 0.25.
train_size : ﬂoat, int, or None (default is None)
If ﬂoat, should be between 0.0 and 1.0 and represent the proportion of the dataset to
include in the train split. If int, represents the absolute number of train samples. If
None, the value is automatically set to the complement of the test size.
random_state : int or RandomState
Pseudo-random number generator state used for random sampling.
stratify : array-like or None (default is None)
If not None, data is split in a stratiﬁed fashion, using this as the labels array.
New in version 0.17: stratify splitting
Returnssplitting : list, length = 2 * len(arrays),
List containing train-test split of inputs.
New
in
version
0.16:
If
the
input
is
sparse,
the
output
will
be
a
scipy.sparse.csr_matrix.
Else, output type is the same as the input
type.
Examples
>>> import numpy as np
>>> from sklearn.cross_validation import train_test_split
>>> X, y = np.arange(10).reshape((5, 2)), range(5)
>>> X
array([[0, 1],
[2, 3],
[4, 5],
[6, 7],
1970
Chapter 29. API Reference
scikit-learn user guide, Release 0.18.2
[8, 9]])
>>> list(y)
[0, 1, 2, 3, 4]
>>> X_train, X_test, y_train, y_test = train_test_split(
...
X, y, test_size=0.33, random_state=42)
...
>>> X_train
array([[4, 5],
[0, 1],
[6, 7]])
>>> y_train
[2, 0, 3]
>>> X_test
array([[2, 3],
[8, 9]])
>>> y_test
[1, 4]
29.36. Recently deprecated
1971
scikit-learn user guide, Release 0.18.2
1972
Chapter 29. API Reference
CHAPTER
THIRTY
DEVELOPER’S GUIDE
30.1 Contributing
This project is a community effort, and everyone is welcome to contribute.
The project is hosted on https://github.com/scikit-learn/scikit-learn
Scikit-learn is somewhat selective when it comes to adding new algorithms, and the best way to contribute and to help
the project is to start working on known issues. See Issues for New Contributors to get started.
Our community, our values
We are a community based on openness and friendly, didactic, discussions.
We aspire to treat everybody equally, and value their contributions.
Decisions are made based on technical merit and consensus.
Code is not the only way to help the project. Reviewing pull requests, answering questions to help others on
mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are
all priceless contributions.
We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation:
https://www.python.org/psf/codeofconduct/
30.1.1 Submitting a bug report
In case you experience issues using this package, do not hesitate to submit a ticket to the Bug Tracker. You are also
welcome to post feature requests or pull requests.
30.1.2 Retrieving the latest code
We use Git for version control and GitHub for hosting our main repository.
You can check out the latest sources with the command:
git clone git://github.com/scikit-learn/scikit-learn.git
1973
scikit-learn user guide, Release 0.18.2
or if you have write privileges:
git clone git@github.com:scikit-learn/scikit-learn.git
If you run the development version, it is cumbersome to reinstall the package each time you update the sources. It is
thus preferred that you add the scikit-learn directory to your PYTHONPATH and build the extension in place:
python setup.py build_ext --inplace
Another option is to use the develop option if you change your code a lot and do not want to have to reinstall every
time. This basically builds the extension in place and creates a link to the development directory (see the setuptool
docs):
python setup.py develop
Note: if you decide to do that you have to rerun:
python setup.py build_ext --inplace
every time the source code of a compiled extension is changed (for instance when switching branches or pulling
changes from upstream).
On Unix-like systems, you can simply type make in the top-level folder to build in-place and launch all the tests. Have
a look at the Makefile for additional utilities.
30.1.3 Contributing code
Note: To avoid duplicating work, it is highly advised that you contact the developers on the mailing list before starting
work on a non-trivial feature.
https://mail.python.org/mailman/listinfo/scikit-learn
How to contribute
The preferred way to contribute to scikit-learn is to fork the main repository on GitHub, then submit a “pull request”
(PR):
1. Create an account on GitHub if you do not already have one.
2. Fork the project repository: click on the ‘Fork’ button near the top of the page. This creates a copy of the code
under your account on the GitHub server.
3. Clone this copy to your local disk:
$ git clone git@github.com:YourLogin/scikit-learn.git
4. Create a branch to hold your changes:
$ git checkout -b my-feature
and start making changes. Never work in the master branch!
5. Work on this copy, on your computer, using Git to do the version control. When you’re done editing, do:
1974
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
$ git add modified_files
$ git commit
to record your changes in Git, then push them to GitHub with:
$ git push -u origin my-feature
Finally, go to the web page of the your fork of the scikit-learn repo, and click ‘Pull request’ to send your changes to
the maintainers for review. You may want to consider sending an email to the mailing list for more visibility.
Note:
In the above setup, your origin remote repository points to YourLogin/scikit-learn.git. If you wish to
fetch/merge from the main repository instead of your forked one, you will need to add another remote to use instead
of origin. If we choose the name upstream for it, the command will be:
$ git remote add upstream https://github.com/scikit-learn/scikit-learn.git
If any of the above seems like magic to you, then look up the Git documentation and the Git development workﬂow
on the web.
In particular, if some conﬂicts arise between your branch and the master branch, you will need to rebase your branch
on master. Please avoid merging master branch into yours. If you did it anyway, you can ﬁx it following this example.
Contributing pull requests
It is recommended to check that your contribution complies with the following rules before submitting a pull request:
• Follow the coding-guidelines (see below). To make sure that your PR does not add PEP8 violations you can run
./build_tools/travis/ﬂake8_diff.sh or make ﬂake8-diff on a Unix-like system.
• When applicable, use the validation tools and other code in the sklearn.utils submodule. A list of utility
routines available for developers can be found in the Utilities for Developers page.
• If your pull request addresses an issue, please use the title to describe the issue and mention the issue number in
the pull request description to ensure a link is created to the original issue.
• All public methods should have informative docstrings with sample usage presented as doctests when appropri-
ate.
• Please preﬁx the title of your pull request with [MRG] if the contribution is complete and should be subjected
to a detailed review. Two core developers will review your code and change the preﬁx of the pull request to
[MRG + 1] and [MRG + 2] on approval, making it eligible for merging. Incomplete contributions should
be preﬁxed [WIP] to indicate a work in progress (and changed to [MRG] when it matures). WIPs may be useful
to: indicate you are working on something to avoid duplicated work, request broad review of functionality or
API, or seek collaborators. WIPs often beneﬁt from the inclusion of a task list in the PR description.
• All other tests pass when everything is rebuilt from scratch. On Unix-like systems, check with (from the toplevel
source folder):
$ make
• When adding additional functionality, provide at least one example script in the examples/ folder. Have
a look at other examples for reference. Examples should demonstrate why the new functionality is useful in
practice and, if possible, compare it to other methods available in scikit-learn.
• Documentation and high-coverage tests are necessary for enhancements to be accepted.
30.1. Contributing
1975
scikit-learn user guide, Release 0.18.2
• At least one paragraph of narrative documentation with links to references in the literature (with PDF links when
possible) and the example. For more details on writing and building the documentation, see the Documentation
section.
You can also check for common programming errors with the following tools:
• Code with a good unittest coverage (at least 90%, better 100%), check with:
$ pip install nose coverage
$ nosetests --with-coverage path/to/tests_for_package
see also Testing and improving test coverage
• No pyﬂakes warnings, check with:
$ pip install pyflakes
$ pyflakes path/to/module.py
• No PEP8 warnings, check with:
$ pip install pep8
$ pep8 path/to/module.py
• AutoPEP8 can help you ﬁx some of the easy redundant errors:
$ pip install autopep8
$ autopep8 path/to/pep8.py
Bonus points for contributions that include a performance analysis with a benchmark script and proﬁling output (please
report on the mailing list or on the GitHub wiki).
Also check out the How to optimize for speed guide for more details on proﬁling and Cython optimizations.
Note: The current state of the scikit-learn code base is not compliant with all of those guidelines, but we expect that
enforcing those constraints on all new contributions will get the overall code base quality in the right direction.
Note:
For two very well documented and more detailed guides on development workﬂow, please pay a visit to the
Scipy Development Workﬂow - and the Astropy Workﬂow for Developers sections.
Filing Bugs
We use Github issues to track all bugs and feature requests; feel free to open an issue if you have found a bug or wish
to see a feature implemented.
It is recommended to check that your issue complies with the following rules before submitting:
• Verify that your issue is not being currently addressed by other issues or pull requests.
• If you are submitting an algorithm or feature request, please verify that the algorithm fulﬁlls our new algorithm
requirements.
• Please ensure all code snippets and error messages are formatted in appropriate code blocks. See Creating and
highlighting code blocks.
• Please include your operating system type and version number, as well as your Python, scikit-learn, numpy, and
scipy versions. This information can be found by runnning the following code snippet:
1976
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
import platform; print(platform.platform())
import sys; print("Python", sys.version)
import numpy; print("NumPy", numpy.__version__)
import scipy; print("SciPy", scipy.__version__)
import sklearn; print("Scikit-Learn", sklearn.__version__)
• Please be speciﬁc about what estimators and/or functions are involved and the shape of the data, as appropriate;
please include a reproducible code snippet or link to a gist. If an exception is raised, please provide the traceback.
Issues for New Contributors
New contributors should look for the following tags when looking for issues. We strongly recommend that new
contributors tackle “easy” issues ﬁrst: this helps the contributor become familiar with the contribution workﬂow, and
for the core devs to become acquainted with the contributor; besides which, we frequently underestimate how easy an
issue is to solve!
Easy Tags
A great way to start contributing to scikit-learn is to pick an item from the list of Easy issues in the issue tracker.
Resolving these issues allow you to start contributing to the project without much prior knowledge. Your assis-
tance in this area will be greatly appreciated by the more experienced developers as it helps free up their time to
concentrate on other issues.
Need Contributor Tags
We often use the Need Contributor tag to mark issues regardless of difﬁculty. Additionally, we use the Need
Contributor tag to mark Pull Requests which have been abandoned by their original contributor and are available
for someone to pick up where the original contributor left off. The list of issues with the Need Contributor tag can
be found here .
Note that not all issues which need contributors will have this tag.
Documentation
We are glad to accept any sort of documentation: function docstrings, reStructuredText documents (like this one),
tutorials, etc. reStructuredText documents live in the source code repository under the doc/ directory.
You can edit the documentation using any text editor, and then generate the HTML output by typing make html from
the doc/ directory. Alternatively, make html-noplot can be used to quickly generate the documentation without
the example gallery. The resulting HTML ﬁles will be placed in _build/html/ and are viewable in a web browser. See
the README ﬁle in the doc/ directory for more information.
For building the documentation, you will need sphinx, matplotlib and pillow.
When you are writing documentation, it is important to keep a good compromise between mathematical and algo-
rithmic details, and give intuition to the reader on what the algorithm does.
Basically, to elaborate on the above, it is best to always start with a small paragraph with a hand-waving explanation of
what the method does to the data. Then, it is very helpful to point out why the feature is useful and when it should be
used - the latter also including “big O” (𝑂(𝑔(𝑛))) complexities of the algorithm, as opposed to just rules of thumb, as
the latter can be very machine-dependent. If those complexities are not available, then rules of thumb may be provided
instead.
30.1. Contributing
1977
scikit-learn user guide, Release 0.18.2
Secondly, a generated ﬁgure from an example (as mentioned in the previous paragraph) should then be included to
further provide some intuition.
Next, one or two small code examples to show its use can be added.
Next, any math and equations, followed by references, can be added to further the documentation. Not starting the
documentation with the maths makes it more friendly towards users that are just interested in what the feature will do,
as opposed to how it works “under the hood”.
Finally, follow the formatting rules below to make it consistently good:
• Add “See also” in docstrings for related classes/functions.
• “See also” in docstrings should be one line per reference, with a colon and an explanation, for example:
See also
--------
SelectKBest : Select features based on the k highest scores.
SelectFpr : Select features based on a false positive rate test.
• For unwritten formatting rules, try to follow existing good works:
– For
“References”
in
docstrings,
see
the
Silhouette
Coefﬁcient
(sklearn.metrics.silhouette_score).
Warning: Sphinx version
While we do our best to have the documentation build under as many version of Sphinx as possible, the different
versions tend to behave slightly differently. To get the best results, you should use version 1.0.
Testing and improving test coverage
High-quality unit testing is a corner-stone of the scikit-learn development process. For this purpose, we use the nose
package. The tests are functions appropriately named, located in tests subdirectories, that check the validity of the
algorithms and the different options of the code.
The full scikit-learn tests can be run using ‘make’ in the root folder. Alternatively, running ‘nosetests’ in a folder will
run all the tests of the corresponding subpackages.
We expect code coverage of new features to be at least around 90%.
Note: Workﬂow to improve test coverage
To test code coverage, you need to install the coverage package in addition to nose.
1. Run ‘make test-coverage’. The output lists for each ﬁle the line numbers that are not tested.
2. Find a low hanging fruit, looking at which lines are not tested, write or adapt a test speciﬁcally for these lines.
3. Loop.
Developers web site
More information can be found on the developer’s wiki.
1978
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
Issue Tracker Tags
All issues and pull requests on the Github issue tracker should have (at least) one of the following tags:
Bug / Crash Something is happening that clearly shouldn’t happen. Wrong results as well as unexpected
errors from estimators go here.
Cleanup / Enhancement Improving performance, usability, consistency.
Documentation Missing, incorrect or sub-standard documentations and examples.
New Feature Feature requests and pull requests implementing a new feature.
There are three other tags to help new contributors:
Easy This issue can be tackled by anyone, no experience needed. Ask for help if the formulation is
unclear.
Moderate Might need some knowledge of machine learning or the package, but is still approachable for
someone new to the project.
Needs Contributor This tag marks an issue which currently lacks a contributor or a PR that needs an-
other contributor to take over the work. These issues can range in difﬁculty, and may not be ap-
proachable for new contributors. Note that not all issues which need contributors will have this
tag.
30.1.4 Other ways to contribute
Code is not the only way to contribute to scikit-learn. For instance, documentation is also a very important part of the
project and often doesn’t get as much attention as it deserves. If you ﬁnd a typo in the documentation, or have made
improvements, do not hesitate to send an email to the mailing list or submit a GitHub pull request. Full documentation
can be found under the doc/ directory.
It also helps us if you spread the word: reference the project from your blog and articles, link to it from your website,
or simply say “I use it”:
30.1.5 Coding guidelines
The following are some guidelines on how new code should be written. Of course, there are special cases and there
will be exceptions to these rules. However, following these rules when submitting new code makes the review easier
so new code can be integrated in less time.
Uniformly formatted code makes it easier to share code ownership. The scikit-learn project tries to closely follow the
ofﬁcial Python guidelines detailed in PEP8 that detail how code should be formatted and indented. Please read it and
follow it.
In addition, we add the following guidelines:
• Use underscores to separate words in non class names: n_samples rather than nsamples.
• Avoid multiple statements on one line. Prefer a line return after a control ﬂow statement (if/for).
• Use relative imports for references inside scikit-learn.
• Unit tests are an exception to the previous rule; they should use absolute imports, exactly as client
code would.
A corollary is that, if sklearn.foo exports a class or function that is implemented in
sklearn.foo.bar.baz, the test should import it from sklearn.foo.
30.1. Contributing
1979
scikit-learn user guide, Release 0.18.2
• Please don’t use import * in any case. It is considered harmful by the ofﬁcial Python recommendations. It
makes the code harder to read as the origin of symbols is no longer explicitly referenced, but most important, it
prevents using a static analysis tool like pyﬂakes to automatically ﬁnd bugs in scikit-learn.
• Use the numpy docstring standard in all your docstrings.
A good example of code that we like can be found here.
Input validation
The module sklearn.utils contains various functions for doing input validation and conversion. Sometimes,
np.asarray sufﬁces for validation; do not use np.asanyarray or np.atleast_2d, since those let NumPy’s
np.matrix through, which has a different API (e.g., * means dot product on np.matrix, but Hadamard product
on np.ndarray).
In other cases, be sure to call check_array on any array-like argument passed to a scikit-learn API function. The
exact parameters to use depends mainly on whether and which scipy.sparse matrices must be accepted.
For more information, refer to the Utilities for Developers page.
Random Numbers
If your code depends on a random number generator, do not use numpy.random.random() or similar routines.
To ensure repeatability in error checking, the routine should accept a keyword random_state and use this to con-
struct a numpy.random.RandomState object. See sklearn.utils.check_random_state in Utilities
for Developers.
Here’s a simple example of code using some of the above guidelines:
from sklearn.utils import check_array, check_random_state
def choose_random_sample(X, random_state=0):
"""
Choose a random point from X
Parameters
----------
X : array-like, shape = (n_samples, n_features)
array representing the data
random_state : RandomState or an int seed (0 by default)
A random number generator instance to define the state of the
random permutations generator.
Returns
-------
x : numpy array, shape = (n_features,)
A random point selected from X
"""
X = check_array(X)
random_state = check_random_state(random_state)
i = random_state.randint(X.shape[0])
return X[i]
If you use randomness in an estimator instead of a freestanding function, some additional guidelines apply.
First off, the estimator should take a random_state argument to its __init__ with a default value of
None.
It should store that argument’s value, unmodiﬁed, in an attribute random_state.
fit can call
1980
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
check_random_state on that attribute to get an actual random number generator. If, for some reason, ran-
domness is needed after fit, the RNG should be stored in an attribute random_state_. The following example
should make this clear:
class GaussianNoise(BaseEstimator, TransformerMixin):
"""This estimator ignores its input and returns random Gaussian noise.
It also does not adhere to all scikit-learn conventions,
but showcases how to handle randomness.
"""
def __init__(self, n_components=100, random_state=None):
self.random_state = random_state
# the arguments are ignored anyway, so we make them optional
def fit(self, X=None, y=None):
self.random_state_ = check_random_state(self.random_state)
def transform(self, X):
n_samples = X.shape[0]
return self.random_state_.randn(n_samples, n_components)
The reason for this setup is reproducibility: when an estimator is fit twice to the same data, it should produce an
identical model both times, hence the validation in fit, not __init__.
Deprecation
If any publicly accessible method, function, attribute or parameter is renamed, we still support the old one for two
releases and issue a deprecation warning when it is called/passed/accessed. E.g., if the function zero_one is re-
named to zero_one_loss, we add the decorator deprecated (from sklearn.utils) to zero_one and call
zero_one_loss from that function:
from ..utils import deprecated
def zero_one_loss(y_true, y_pred, normalize=True):
# actual implementation
pass
@deprecated("Function 'zero_one' was renamed to 'zero_one_loss' "
"in version 0.13 and will be removed in release 0.15. "
"Default behavior is changed from 'normalize=False' to "
"'normalize=True'")
def zero_one(y_true, y_pred, normalize=False):
return zero_one_loss(y_true, y_pred, normalize)
If an attribute is to be deprecated, use the decorator deprecated on a property. E.g., renaming an attribute labels_
to classes_ can be done as:
@property
@deprecated("Attribute labels_ was deprecated in version 0.13 and "
"will be removed in 0.15. Use 'classes_' instead")
def labels_(self):
return self.classes_
If a parameter has to be deprecated, use DeprecationWarning appropriately. In the following example, k is
deprecated and renamed to n_clusters:
30.1. Contributing
1981
scikit-learn user guide, Release 0.18.2
import warnings
def example_function(n_clusters=8, k=None):
if k is not None:
warnings.warn("'k' was renamed to n_clusters in version 0.13 and "
"will be removed in 0.15.", DeprecationWarning)
n_clusters = k
As in these examples, the warning message should always give both the version in which the deprecation happened
and the version in which the old behavior will be removed. If the deprecation happened in version 0.x-dev, the message
should say deprecation occurred in version 0.x and the removal will be in 0.(x+2). For example, if the deprecation
happened in version 0.18-dev, the message should say it happened in version 0.18 and the old behavior will be removed
in version 0.20.
Python 3.x support
All scikit-learn code should work unchanged in both Python 2.[67] and 3.2 or newer. Since Python 3.x is not backwards
compatible, that may require changes to code and it certainly requires testing on both 2.6 or 2.7, and 3.2 or newer.
For most numerical algorithms, Python 3.x support is easy: just remember that print is a function and integer
division is written //. String handling has been overhauled, though, as have parts of the Python standard library. The
six package helps with cross-compatibility and is included in scikit-learn as sklearn.externals.six.
30.1.6 APIs of scikit-learn objects
To have a uniform API, we try to have a common basic API for all the objects. In addition, to avoid the proliferation
of framework code, we try to adopt simple conventions and limit to a minimum the number of methods an object must
implement.
Different objects
The main objects in scikit-learn are (one class can implement multiple interfaces):
Estimator The base object, implements a fit method to learn from data, either:
estimator = obj.fit(data, targets)
or:
estimator = obj.fit(data)
Predictor For supervised learning, or some unsupervised problems, implements:
prediction = obj.predict(data)
Classiﬁcation algorithms usually also offer a way to quantify certainty of a prediction, either using
decision_function or predict_proba:
probability = obj.predict_proba(data)
Transformer For ﬁltering or modifying the data, in a supervised or unsupervised way, implements:
new_data = obj.transform(data)
1982
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
When ﬁtting and transforming can be performed much more efﬁciently together than separately,
implements:
new_data = obj.fit_transform(data)
Model A model that can give a goodness of ﬁt measure or a likelihood of unseen data, implements (higher
is better):
score = obj.score(data)
Estimators
The API has one predominant object: the estimator. A estimator is an object that ﬁts a model based on some training
data and is capable of inferring some properties on new data. It can be, for instance, a classiﬁer or a regressor. All
estimators implement the ﬁt method:
estimator.fit(X, y)
All built-in estimators also have a set_params method, which sets data-independent parameters (overriding previ-
ous parameter values passed to __init__).
All estimators in the main scikit-learn codebase should inherit from sklearn.base.BaseEstimator.
Instantiation
This concerns the creation of an object. The object’s __init__ method might accept constants as arguments that
determine the estimator’s behavior (like the C constant in SVMs). It should not, however, take the actual training data
as an argument, as this is left to the fit() method:
clf2 = SVC(C=2.3)
clf3 = SVC([[1, 2], [2, 3]], [-1, 1]) # WRONG!
The arguments accepted by __init__ should all be keyword arguments with a default value. In other words, a user
should be able to instantiate an estimator without passing any arguments to it. The arguments should all correspond to
hyperparameters describing the model or the optimisation problem the estimator tries to solve. These initial arguments
(or parameters) are always remembered by the estimator. Also note that they should not be documented under the
“Attributes” section, but rather under the “Parameters” section for that estimator.
In addition, every keyword argument accepted by __init__ should correspond to an attribute on the instance.
Scikit-learn relies on this to ﬁnd the relevant attributes to set on an estimator when doing model selection.
To summarize, an __init__ should look like:
def __init__(self, param1=1, param2=2):
self.param1 = param1
self.param2 = param2
There should be no logic, not even input validation, and the parameters should not be changed. The corresponding
logic should be put where the parameters are used, typically in fit. The following is wrong:
def __init__(self, param1=1, param2=2, param3=3):
# WRONG: parameters should not be modified
if param1 > 1:
param2 += 1
self.param1 = param1
30.1. Contributing
1983
scikit-learn user guide, Release 0.18.2
# WRONG: the object's attributes should have exactly the name of
# the argument in the constructor
self.param3 = param2
The reason for postponing the validation is that the same validation would have to be performed in set_params,
which is used in algorithms like GridSearchCV.
Fitting
The next thing you will probably want to do is to estimate some parameters in the model. This is implemented in the
fit() method.
The fit() method takes the training data as arguments, which can be one array in the case of unsupervised learning,
or two arrays in the case of supervised learning.
Note that the model is ﬁtted using X and y, but the object holds no reference to X and y. There are, however, some
exceptions to this, as in the case of precomputed kernels where this data must be stored for use by the predict method.
Parameters
X
array-like, with shape = [N, D], where N is the number of samples and D is the number of features.
y
array, with shape = [N], where N is the number of samples.
kwargs
optional data-dependent parameters.
X.shape[0] should be the same as y.shape[0]. If this requisite is not met, an exception of type ValueError
should be raised.
y might be ignored in the case of unsupervised learning. However, to make it possible to use the estimator as part of
a pipeline that can mix both supervised and unsupervised transformers, even unsupervised estimators need to accept
a y=None keyword argument in the second position that is just ignored by the estimator. For the same reason,
fit_predict, fit_transform, score and partial_fit methods need to accept a y argument in the second
place if they are implemented.
The method should return the object (self). This pattern is useful to be able to implement quick one liners in an
IPython session such as:
y_predicted = SVC(C=100).fit(X_train, y_train).predict(X_test)
Depending on the nature of the algorithm, fit can sometimes also accept additional keywords arguments. However,
any parameter that can have a value assigned prior to having access to the data should be an __init__ keyword
argument. ﬁt parameters should be restricted to directly data dependent variables. For instance a Gram matrix
or an afﬁnity matrix which are precomputed from the data matrix X are data dependent. A tolerance stopping criterion
tol is not directly data dependent (although the optimal value according to some scoring function probably is).
Estimated Attributes
Attributes that have been estimated from the data must always have a name ending with trailing underscore, for
example the coefﬁcients of some regression estimator would be stored in a coef_ attribute after fit has been called.
The last-mentioned attributes are expected to be overridden when you call fit a second time without taking any
previous value into account: ﬁt should be idempotent.
Optional Arguments
In iterative algorithms, the number of iterations should be speciﬁed by an integer called n_iter.
1984
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
30.1.7 Rolling your own estimator
If you want to implement a new estimator that is scikit-learn-compatible, whether it is just for you or for contributing it
to scikit-learn, there are several internals of scikit-learn that you should be aware of in addition to the scikit-learn API
outlined above. You can check whether your estimator adheres to the scikit-learn interface and standards by running
utils.estimator_checks.check_estimator on the class:
>>> from sklearn.utils.estimator_checks import check_estimator
>>> from sklearn.svm import LinearSVC
>>> check_estimator(LinearSVC)
# passes
The main motivation to make a class compatible to the scikit-learn estimator interface might be that you want to use it
together with model assessment and selection tools such as model_selection.GridSearchCV.
For this to work, you need to implement the following interface. If a dependency on scikit-learn is okay for your code,
you can prevent a lot of boilerplate code by deriving a class from BaseEstimator and optionally the mixin classes
in sklearn.base. E.g., below is a custom classiﬁer. For more information on this example, see scikit-learn-contrib:
>>> import numpy as np
>>> from sklearn.base import BaseEstimator, ClassifierMixin
>>> from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
>>> from sklearn.utils.multiclass import unique_labels
>>> from sklearn.metrics import euclidean_distances
>>> class TemplateClassifier(BaseEstimator, ClassifierMixin):
...
...
def __init__(self, demo_param='demo'):
...
self.demo_param = demo_param
...
...
def fit(self, X, y):
...
...
# Check that X and y have correct shape
...
X, y = check_X_y(X, y)
...
# Store the classes seen during fit
...
self.classes_ = unique_labels(y)
...
...
self.X_ = X
...
self.y_ = y
...
# Return the classifier
...
return self
...
...
def predict(self, X):
...
...
# Check is fit had been called
...
check_is_fitted(self, ['X_', 'y_'])
...
...
# Input validation
...
X = check_array(X)
...
...
closest = np.argmin(euclidean_distances(X, self.X_), axis=1)
...
return self.y_[closest]
get_params and set_params
All scikit-learn estimators have get_params and set_params functions. The get_params function takes no
arguments and returns a dict of the __init__ parameters of the estimator, together with their values. It must take
one keyword argument, deep, which receives a boolean value that determines whether the method should return the
parameters of sub-estimators (for most estimators, this can be ignored). The default value for deep should be true.
30.1. Contributing
1985
scikit-learn user guide, Release 0.18.2
The set_params on the other hand takes as input a dict of the form 'parameter':
value and sets the
parameter of the estimator using this dict. Return value must be estimator itself.
While the get_params mechanism is not essential (see Cloning below), the set_params function is necessary as
it is used to set parameters during grid searches.
The easiest way to implement these functions, and to get a sensible __repr__ method, is to inherit from
sklearn.base.BaseEstimator. If you do not want to make your code dependent on scikit-learn, the easi-
est way to implement the interface is:
def get_params(self, deep=True):
# suppose this estimator has parameters "alpha" and "recursive"
return {"alpha": self.alpha, "recursive": self.recursive}
def set_params(self, **parameters):
for parameter, value in parameters.items():
self.setattr(parameter, value)
return self
Parameters and init
As model_selection.GridSearchCV uses set_params to apply parameter setting to estimators, it is essen-
tial that calling set_params has the same effect as setting parameters using the __init__ method. The easiest
and recommended way to accomplish this is to not do any parameter validation in __init__. All logic behind
estimator parameters, like translating string arguments into functions, should be done in fit.
Also it is expected that parameters with trailing _ are not to be set inside the __init__ method. All and only the
public attributes set by ﬁt have a trailing _. As a result the existence of parameters with trailing _ is used to check if
the estimator has been ﬁtted.
Cloning
For use with the model_selection module, an estimator must support the base.clone function to repli-
cate an estimator.
This can be done by providing a get_params method.
If get_params is present, then
clone(estimator) will be an instance of type(estimator) on which set_params has been called with
clones of the result of estimator.get_params().
Objects that do not provide this method will be deep-copied (using the Python standard function copy.deepcopy)
if safe=False is passed to clone.
Pipeline compatibility
For an estimator to be usable together with pipeline.Pipeline in any but the last step, it needs to provide a fit
or fit_transform function. To be able to evaluate the pipeline on any data but the training set, it also needs to
provide a transform function. There are no special requirements for the last step in a pipeline, except that it has a
fit function. All fit and fit_transform functions must take arguments X,y, even if y is not used. Similarly,
for score to be usable, the last step of the pipeline needs to have a score function that accepts an optional y.
Estimator types
Some common functionality depends on the kind of estimator passed.
For example, cross-validation in
model_selection.GridSearchCV and model_selection.cross_val_score defaults to being strat-
iﬁed when used on a classiﬁer, but not otherwise. Similarly, scorers for average precision that take a continuous
1986
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
prediction need to call decision_function for classiﬁers, but predict for regressors. This distinction be-
tween classiﬁers and regressors is implemented using the _estimator_type attribute, which takes a string value.
It should be "classifier" for classiﬁers and "regressor" for regressors and "clusterer" for clustering
methods, to work as expected. Inheriting from ClassifierMixin, RegressorMixin or ClusterMixin will
set the attribute automatically.
Working notes
For unresolved issues, TODOs, and remarks on ongoing work, developers are advised to maintain notes on the GitHub
wiki.
Speciﬁc models
Classiﬁers should accept y (target) arguments to fit that are sequences (lists, arrays) of either strings or integers.
They should not assume that the class labels are a contiguous range of integers; instead, they should store a list of
classes in a classes_ attribute or property. The order of class labels in this attribute should match the order in which
predict_proba, predict_log_proba and decision_function return their values. The easiest way to
achieve this is to put:
self.classes_, y = np.unique(y, return_inverse=True)
in fit. This returns a new y that contains class indexes, rather than labels, in the range [0, n_classes).
A classiﬁer’s predict method should return arrays containing class labels from classes_. In a classiﬁer that
implements decision_function, this can be achieved with:
def predict(self, X):
D = self.decision_function(X)
return self.classes_[np.argmax(D, axis=1)]
In linear models, coefﬁcients are stored in an array called coef_, and the independent term is stored in intercept_.
sklearn.linear_model.base contains a few base classes and mixins that implement common linear model
patterns.
The sklearn.utils.multiclass module contains useful functions for working with multiclass and multilabel
problems.
30.2 Developers’ Tips for Debugging
30.2.1 Memory errors: debugging Cython with valgrind
While python/numpy’s built-in memory management is relatively robust, it can lead to performance penalties for some
routines. For this reason, much of the high-performance code in scikit-learn in written in cython. This performance
gain comes with a tradeoff, however: it is very easy for memory bugs to crop up in cython code, especially in situations
where that code relies heavily on pointer arithmetic.
Memory errors can manifest themselves a number of ways. The easiest ones to debug are often segmentation faults
and related glibc errors. Uninitialized variables can lead to unexpected behavior that is difﬁcult to track down. A very
useful tool when debugging these sorts of errors is valgrind.
Valgrind is a command-line tool that can trace memory errors in a variety of code. Follow these steps:
1. Install valgrind on your system.
30.2. Developers’ Tips for Debugging
1987
scikit-learn user guide, Release 0.18.2
2. Download the python valgrind suppression ﬁle: valgrind-python.supp.
3. Follow the directions in the README.valgrind ﬁle to customize your python suppressions. If you don’t, you
will have spurious output coming related to the python interpreter instead of your own code.
4. Run valgrind as follows:
$> valgrind -v --suppressions=valgrind-python.supp python my_test_script.py
The result will be a list of all the memory-related errors, which reference lines in the C-code generated by cython
from your .pyx ﬁle. If you examine the referenced lines in the .c ﬁle, you will see comments which indicate the
corresponding location in your .pyx source ﬁle. Hopefully the output will give you clues as to the source of your
memory error.
For more information on valgrind and the array of options it has, see the tutorials and documentation on the valgrind
web site.
30.3 Utilities for Developers
Scikit-learn contains a number of utilities to help with development. These are located in sklearn.utils, and
include tools in a number of categories. All the following functions and classes are in the module sklearn.utils.
Warning:
These utilities are meant to be used internally within the scikit-learn package. They are not guar-
anteed to be stable between versions of scikit-learn. Backports, in particular, will be removed as the scikit-learn
dependencies evolve.
30.3.1 Validation Tools
These are tools used to check and validate input. When you write a function which accepts arrays, matrices, or sparse
matrices as arguments, the following should be used when applicable.
• assert_all_finite: Throw an error if array contains NaNs or Infs.
• as_float_array: convert input to an array of ﬂoats. If a sparse matrix is passed, a sparse matrix will be
returned.
• check_array: convert input to 2d array, raise error on sparse matrices. Allowed sparse matrix formats can
be given optionally, as well as allowing 1d or nd arrays. Calls assert_all_finite by default.
• check_X_y: check that X and y have consistent length, calls check_array on X, and column_or_1d on y. For
multilabel classiﬁcation or multitarget regression, specify multi_output=True, in which case check_array will
be called on y.
• indexable: check that all input arrays have consistent length and can be sliced or indexed using safe_index.
This is used to validate input for cross-validation.
If your code relies on a random number generator, it should never use functions like numpy.random.random
or numpy.random.normal.
This approach can lead to repeatability issues in unit tests.
Instead, a
numpy.random.RandomState object should be used, which is built from a random_state argument passed
to the class or function. The function check_random_state, below, can then be used to create a random number
generator object.
• check_random_state: create a np.random.RandomState object from a parameter random_state.
– If random_state is None or np.random, then a randomly-initialized RandomState object is re-
turned.
1988
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
– If random_state is an integer, then it is used to seed a new RandomState object.
– If random_state is a RandomState object, then it is passed through.
For example:
>>> from sklearn.utils import check_random_state
>>> random_state = 0
>>> random_state = check_random_state(random_state)
>>> random_state.rand(4)
array([ 0.5488135 ,
0.71518937,
0.60276338,
0.54488318])
30.3.2 Efﬁcient Linear Algebra & Array Operations
• extmath.randomized_range_finder: construct an orthonormal matrix whose range approximates the
range of the input. This is used in extmath.randomized_svd, below.
• extmath.randomized_svd: compute the k-truncated randomized SVD. This algorithm ﬁnds the exact
truncated singular values decomposition using randomization to speed up the computations. It is particularly
fast on large matrices on which you wish to extract only a small number of components.
• arrayfuncs.cholesky_delete: (used in sklearn.linear_model.least_angle.lars_path)
Remove an item from a cholesky factorization.
• arrayfuncs.min_pos: (used in sklearn.linear_model.least_angle) Find the minimum of the
positive values within an array.
• extmath.norm: computes Euclidean (L2) vector norm by directly calling the BLAS nrm2 function. This is
more stable than scipy.linalg.norm. See Fabian’s blog post for a discussion.
• extmath.fast_logdet: efﬁciently compute the log of the determinant of a matrix.
• extmath.density: efﬁciently compute the density of a sparse vector
• extmath.safe_sparse_dot: dot product which will correctly handle scipy.sparse inputs. If the
inputs are dense, it is equivalent to numpy.dot.
• extmath.logsumexp: compute the sum of X assuming X is in the log domain. This is equivalent to calling
np.log(np.sum(np.exp(X))), but is robust to overﬂow/underﬂow errors. Note that there is similar
functionality in np.logaddexp.reduce, but because of the pairwise nature of this routine, it is slower
for large arrays. Scipy has a similar routine in scipy.misc.logsumexp (In scipy versions < 0.10, this is
found in scipy.maxentropy.logsumexp), but the scipy version does not accept an axis keyword.
• extmath.weighted_mode: an extension of scipy.stats.mode which allows each item to have a real-
valued weight.
• resample: Resample arrays or sparse matrices in a consistent way. used in shuffle, below.
• shuffle: Shufﬂe arrays or sparse matrices in a consistent way. Used in sklearn.cluster.k_means.
30.3.3 Efﬁcient Random Sampling
• random.sample_without_replacement: implements efﬁcient algorithms for sampling n_samples
integers from a population of size n_population without replacement.
30.3. Utilities for Developers
1989
scikit-learn user guide, Release 0.18.2
30.3.4 Efﬁcient Routines for Sparse Matrices
The sklearn.utils.sparsefuncs cython module hosts compiled extensions to efﬁciently process
scipy.sparse data.
• sparsefuncs.mean_variance_axis: compute the means and variances along a speciﬁed axis of a CSR
matrix. Used for normalizing the tolerance stopping criterion in sklearn.cluster.k_means_.KMeans.
• sparsefuncs.inplace_csr_row_normalize_l1 and sparsefuncs.inplace_csr_row_normalize_l2:
can
be
used
to
normalize
individual
sparse
samples
to
unit
L1
or
L2
norm
as
done
in
sklearn.preprocessing.Normalizer.
• sparsefuncs.inplace_csr_column_scale: can be used to multiply the columns of a CSR ma-
trix by a constant scale (one scale per column).
Used for scaling features to unit standard deviation in
sklearn.preprocessing.StandardScaler.
30.3.5 Graph Routines
• graph.single_source_shortest_path_length: (not currently used in scikit-learn) Return the
shortest path from a single source to all connected nodes on a graph.
Code is adapted from networkx.
If this is ever needed again, it would be far faster to use a single iteration of Dijkstra’s algorithm from
graph_shortest_path.
• graph.graph_laplacian: (used in sklearn.cluster.spectral.spectral_embedding) Re-
turn the Laplacian of a given graph. There is specialized code for both dense and sparse connectivity matrices.
• graph_shortest_path.graph_shortest_path: (used in sklearn.manifold.Isomap) Return
the shortest path between all pairs of connected points on a directed or undirected graph. Both the Floyd-
Warshall algorithm and Dijkstra’s algorithm are available. The algorithm is most efﬁcient when the connectivity
matrix is a scipy.sparse.csr_matrix.
30.3.6 Backports
• fixes.expit: Logistic sigmoid function. Replacement for SciPy 0.10’s scipy.special.expit.
• sparsetools.connected_components (backported from scipy.sparse.connected_components
in
scipy
0.12).
Used
in
sklearn.cluster.hierarchical,
as
well
as
in
tests
for
sklearn.feature_extraction.
ARPACK
• arpack.eigs (backported from scipy.sparse.linalg.eigs in scipy 0.10) Sparse non-symmetric
eigenvalue decomposition using the Arnoldi method. A limited version of eigs is available in earlier scipy
versions.
• arpack.eigsh (backported from scipy.sparse.linalg.eigsh in scipy 0.10) Sparse non-symmetric
eigenvalue decomposition using the Arnoldi method. A limited version of eigsh is available in earlier scipy
versions.
• arpack.svds (backported from scipy.sparse.linalg.svds in scipy 0.10) Sparse non-symmetric
eigenvalue decomposition using the Arnoldi method. A limited version of svds is available in earlier scipy
versions.
1990
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
Benchmarking
• bench.total_seconds (back-ported from timedelta.total_seconds in Python 2.7).
Used in
benchmarks/bench_glm.py.
30.3.7 Testing Functions
• testing.assert_in, testing.assert_not_in: Assertions for container membership. Designed for
forward compatibility with Nose 1.0.
• testing.assert_raise_message: Assertions for checking the error raise message.
• testing.mock_mldata_urlopen: Mocks the urlopen function to fake requests to mldata.org. Used in
tests of sklearn.datasets.
• testing.all_estimators : returns a list of all estimators in scikit-learn to test for consistent behavior
and interfaces.
30.3.8 Multiclass and multilabel utility function
• multiclass.is_multilabel: Helper function to check if the task is a multi-label classiﬁcation one.
• multiclass.is_label_indicator_matrix: Helper function to check if a classiﬁcation output is in
label indicator matrix format.
• multiclass.unique_labels: Helper function to extract an ordered array of unique labels from different
formats of target.
30.3.9 Helper Functions
• gen_even_slices:
generator
to
create
n-packs
of
slices
going
up
to
n.
Used
in
sklearn.decomposition.dict_learning and sklearn.cluster.k_means.
• safe_mask: Helper function to convert a mask to the format expected by the numpy array or scipy sparse
matrix on which to use it (sparse matrices support integer indices only while numpy arrays support both boolean
masks and integer indices).
• safe_sqr: Helper function for uniﬁed squaring (**2) of array-likes, matrices and sparse matrices.
30.3.10 Hash Functions
• murmurhash3_32 provides a python wrapper for the MurmurHash3_x86_32 C++ non cryptographic hash
function. This hash function is suitable for implementing lookup tables, Bloom ﬁlters, Count Min Sketch, feature
hashing and implicitly deﬁned sparse random projections:
>>> from sklearn.utils import murmurhash3_32
>>> murmurhash3_32("some feature", seed=0) == -384616559
True
>>> murmurhash3_32("some feature", seed=0, positive=True) == 3910350737
True
The sklearn.utils.murmurhash module can also be “cimported” from other cython modules so as to
beneﬁt from the high performance of MurmurHash while skipping the overhead of the Python interpreter.
30.3. Utilities for Developers
1991
scikit-learn user guide, Release 0.18.2
30.3.11 Warnings and Exceptions
• deprecated: Decorator to mark a function or class as deprecated.
• sklearn.exceptions.ConvergenceWarning: Custom warning to catch convergence problems. Used
in sklearn.covariance.graph_lasso.
30.4 How to optimize for speed
The following gives some practical guidelines to help you write efﬁcient code for the scikit-learn project.
Note:
While it is always useful to proﬁle your code so as to check performance assumptions, it is also highly
recommended to review the literature to ensure that the implemented algorithm is the state of the art for the task
before investing into costly implementation optimization.
Times and times, hours of efforts invested in optimizing complicated implementation details have been rendered
irrelevant by the subsequent discovery of simple algorithmic tricks, or by using another algorithm altogether that is
better suited to the problem.
The section A sample algorithmic trick: warm restarts for cross validation gives an example of such a trick.
30.4.1 Python, Cython or C/C++?
In general, the scikit-learn project emphasizes the readability of the source code to make it easy for the project
users to dive into the source code so as to understand how the algorithm behaves on their data but also for ease of
maintainability (by the developers).
When implementing a new algorithm is thus recommended to start implementing it in Python using Numpy and
Scipy by taking care of avoiding looping code using the vectorized idioms of those libraries. In practice this means
trying to replace any nested for loops by calls to equivalent Numpy array methods. The goal is to avoid the CPU
wasting time in the Python interpreter rather than crunching numbers to ﬁt your statistical model. It’s generally a good
idea to consider NumPy and SciPy performance tips: http://scipy.github.io/old-wiki/pages/PerformanceTips
Sometimes however an algorithm cannot be expressed efﬁciently in simple vectorized Numpy code. In this case, the
recommended strategy is the following:
1. Proﬁle the Python implementation to ﬁnd the main bottleneck and isolate it in a dedicated module level func-
tion. This function will be reimplemented as a compiled extension module.
2. If there exists a well maintained BSD or MIT C/C++ implementation of the same algorithm that is not
too big, you can write a Cython wrapper for it and include a copy of the source code of the library
in the scikit-learn source tree:
this strategy is used for the classes svm.LinearSVC, svm.SVC and
linear_model.LogisticRegression (wrappers for liblinear and libsvm).
3. Otherwise, write an optimized version of your Python function using Cython directly. This strategy is used for
the linear_model.ElasticNet and linear_model.SGDClassifier classes for instance.
4. Move the Python version of the function in the tests and use it to check that the results of the compiled
extension are consistent with the gold standard, easy to debug Python version.
5. Once the code is optimized (not simple bottleneck spottable by proﬁling), check whether it is possible to have
coarse grained parallelism that is amenable to multi-processing by using the joblib.Parallel class.
When using Cython, use either
$ python setup.py build_ext -i $ python setup.py install
1992
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
to generate C ﬁles. You are responsible for adding .c/.cpp extensions along with build parameters in each submodule
setup.py.
C/C++ generated ﬁles are embedded in distributed stable packages. The goal is to make it possible to install scikit-learn
stable version on any machine with Python, Numpy, Scipy and C/C++ compiler.
30.4.2 Fast matrix multiplications
Matrix multiplications (matrix-matrix and matrix-vector) are usually handled using the NumPy function np.dot,
but in versions of NumPy before 1.7.2 this function is suboptimal when the inputs are not both in the C (row-major)
layout; in that case, the inputs may be implicitly copied to obtain the right layout. This obviously consumes memory
and takes time.
The function fast_dot in sklearn.utils.extmath offers a fast replacement for np.dot that prevents copies
from being made in some cases. In all other cases, it dispatches to np.dot and when the NumPy version is new
enough, it is in fact an alias for that function, making it a drop-in replacement. Example usage of fast_dot:
>>> import numpy as np
>>> from sklearn.utils.extmath import fast_dot
>>> X = np.random.random_sample([2, 10])
>>> np.allclose(np.dot(X, X.T), fast_dot(X, X.T))
True
This function operates optimally on 2-dimensional arrays, both of the same dtype, which should be either single or
double precision ﬂoat. If these requirements aren’t met or the BLAS package is not available, the call is silently
dispatched to numpy.dot. If you want to be sure when the original numpy.dot has been invoked in a situation
where it is suboptimal, you can activate the related warning:
>>> import warnings
>>> from sklearn.exceptions import NonBLASDotWarning
>>> warnings.simplefilter('always', NonBLASDotWarning)
30.4.3 Proﬁling Python code
In order to proﬁle Python code we recommend to write a script that loads and prepare you data and then use the
IPython integrated proﬁler for interactively exploring the relevant part for the code.
Suppose we want to proﬁle the Non Negative Matrix Factorization module of the scikit. Let us setup a new IPython
session and load the digits dataset and as in the Recognizing hand-written digits example:
In [1]: from sklearn.decomposition import NMF
In [2]: from sklearn.datasets import load_digits
In [3]: X = load_digits().data
Before starting the proﬁling session and engaging in tentative optimization iterations, it is important to measure the
total execution time of the function we want to optimize without any kind of proﬁler overhead and save it somewhere
for later reference:
In [4]: %timeit NMF(n_components=16, tol=1e-2).fit(X)
1 loops, best of 3: 1.7 s per loop
To have a look at the overall performance proﬁle using the %prun magic command:
30.4. How to optimize for speed
1993
scikit-learn user guide, Release 0.18.2
In [5]: %prun -l nmf.py NMF(n_components=16, tol=1e-2).fit(X)
14496 function calls in 1.682 CPU seconds
Ordered by: internal time
List reduced from 90 to 9 due to restriction <'nmf.py'>
ncalls
tottime
percall
cumtime
percall filename:lineno(function)
36
0.609
0.017
1.499
0.042 nmf.py:151(_nls_subproblem)
1263
0.157
0.000
0.157
0.000 nmf.py:18(_pos)
1
0.053
0.053
1.681
1.681 nmf.py:352(fit_transform)
673
0.008
0.000
0.057
0.000 nmf.py:28(norm)
1
0.006
0.006
0.047
0.047 nmf.py:42(_initialize_nmf)
36
0.001
0.000
0.010
0.000 nmf.py:36(_sparseness)
30
0.001
0.000
0.001
0.000 nmf.py:23(_neg)
1
0.000
0.000
0.000
0.000 nmf.py:337(__init__)
1
0.000
0.000
1.681
1.681 nmf.py:461(fit)
The tottime column is the most interesting: it gives to total time spent executing the code of a given function
ignoring the time spent in executing the sub-functions. The real total time (local code + sub-function calls) is given by
the cumtime column.
Note the use of the -l nmf.py that restricts the output to lines that contains the “nmf.py” string. This is useful to
have a quick look at the hotspot of the nmf Python module it-self ignoring anything else.
Here is the beginning of the output of the same command without the -l nmf.py ﬁlter:
In [5] %prun NMF(n_components=16, tol=1e-2).fit(X)
16159 function calls in 1.840 CPU seconds
Ordered by: internal time
ncalls
tottime
percall
cumtime
percall filename:lineno(function)
2833
0.653
0.000
0.653
0.000 {numpy.core._dotblas.dot}
46
0.651
0.014
1.636
0.036 nmf.py:151(_nls_subproblem)
1397
0.171
0.000
0.171
0.000 nmf.py:18(_pos)
2780
0.167
0.000
0.167
0.000 {method 'sum' of 'numpy.ndarray'
˓→objects}
1
0.064
0.064
1.840
1.840 nmf.py:352(fit_transform)
1542
0.043
0.000
0.043
0.000 {method 'flatten' of 'numpy.ndarray'
˓→objects}
337
0.019
0.000
0.019
0.000 {method 'all' of 'numpy.ndarray'
˓→objects}
2734
0.011
0.000
0.181
0.000 fromnumeric.py:1185(sum)
2
0.010
0.005
0.010
0.005 {numpy.linalg.lapack_lite.dgesdd}
748
0.009
0.000
0.065
0.000 nmf.py:28(norm)
...
The above results show that the execution is largely dominated by dot products operations (delegated to blas). Hence
there is probably no huge gain to expect by rewriting this code in Cython or C/C++: in this case out of the 1.7s total
execution time, almost 0.7s are spent in compiled code we can consider optimal. By rewriting the rest of the Python
code and assuming we could achieve a 1000% boost on this portion (which is highly unlikely given the shallowness
of the Python loops), we would not gain more than a 2.4x speed-up globally.
Hence major improvements can only be achieved by algorithmic improvements in this particular example (e.g.
trying to ﬁnd operation that are both costly and useless to avoid computing then rather than trying to optimize their
implementation).
It is however still interesting to check what’s happening inside the _nls_subproblem function which is the hotspot
if we only consider Python code: it takes around 100% of the accumulated time of the module. In order to better
1994
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
understand the proﬁle of this speciﬁc function, let us install line-prof and wire it to IPython:
$ pip install line-profiler
• Under IPython <= 0.10, edit ~/.ipython/ipy_user_conf.py and ensure the following lines are
present:
import IPython.ipapi
ip = IPython.ipapi.get()
Towards the end of the ﬁle, deﬁne the %lprun magic:
import line_profiler
ip.expose_magic('lprun', line_profiler.magic_lprun)
• Under IPython 0.11+, ﬁrst create a conﬁguration proﬁle:
$ ipython profile create
Then create a ﬁle named ~/.ipython/extensions/line_profiler_ext.py with the following con-
tent:
import line_profiler
def load_ipython_extension(ip):
ip.define_magic('lprun', line_profiler.magic_lprun)
Then register it in ~/.ipython/profile_default/ipython_config.py:
c.TerminalIPythonApp.extensions = [
'line_profiler_ext',
]
c.InteractiveShellApp.extensions = [
'line_profiler_ext',
]
This will register the %lprun magic command in the IPython terminal application and the other frontends such
as qtconsole and notebook.
Now restart IPython and let us use this new toy:
In [1]: from sklearn.datasets import load_digits
In [2]: from sklearn.decomposition.nmf import _nls_subproblem, NMF
In [3]: X = load_digits().data
In [4]: %lprun -f _nls_subproblem NMF(n_components=16, tol=1e-2).fit(X)
Timer unit: 1e-06 s
File: sklearn/decomposition/nmf.py
Function: _nls_subproblem at line 137
Total time: 1.73153 s
Line #
Hits
Time
Per Hit
% Time
Line Contents
==============================================================
137
def _nls_subproblem(V, W, H_init,
˓→tol, max_iter):
138
"""Non-negative least square
˓→solver
30.4. How to optimize for speed
1995
scikit-learn user guide, Release 0.18.2
...
170
"""
171
48
5863
122.1
0.3
if (H_init < 0).any():
172
raise ValueError("Negative
˓→values in H_init passed to NLS solver.")
173
174
48
139
2.9
0.0
H = H_init
175
48
112141
2336.3
5.8
WtV = np.dot(W.T, V)
176
48
16144
336.3
0.8
WtW = np.dot(W.T, W)
177
178
# values justified in the paper
179
48
144
3.0
0.0
alpha = 1
180
48
113
2.4
0.0
beta = 0.1
181
638
1880
2.9
0.1
for n_iter in xrange(1, max_iter
˓→+ 1):
182
638
195133
305.9
10.2
grad = np.dot(WtW, H) - WtV
183
638
495761
777.1
25.9
proj_gradient = norm(grad[np.
˓→logical_or(grad < 0, H > 0)])
184
638
2449
3.8
0.1
if proj_gradient < tol:
185
48
130
2.7
0.0
break
186
187
1474
4474
3.0
0.2
for inner_iter in xrange(1,
˓→20):
188
1474
83833
56.9
4.4
Hn = H - alpha * grad
189
# Hn = np.where(Hn > 0,
˓→Hn, 0)
190
1474
194239
131.8
10.1
Hn = _pos(Hn)
191
1474
48858
33.1
2.5
d = Hn - H
192
1474
150407
102.0
7.8
gradd = np.sum(grad * d)
193
1474
515390
349.7
26.9
dQd = np.sum(np.dot(WtW,
˓→d) * d)
...
By looking at the top values of the % Time column it is really easy to pin-point the most expensive expressions that
would deserve additional care.
30.4.4 Memory usage proﬁling
You can analyze in detail the memory usage of any Python code with the help of memory_proﬁler. First, install the
latest version:
$ pip install -U memory_profiler
Then, setup the magics in a manner similar to line_profiler.
• Under IPython <= 0.10, edit ~/.ipython/ipy_user_conf.py and ensure the following lines are
present:
import IPython.ipapi
ip = IPython.ipapi.get()
Towards the end of the ﬁle, deﬁne the %memit and %mprun magics:
import memory_profiler
ip.expose_magic('memit', memory_profiler.magic_memit)
ip.expose_magic('mprun', memory_profiler.magic_mprun)
1996
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
• Under IPython 0.11+, ﬁrst create a conﬁguration proﬁle:
$ ipython profile create
Then create a ﬁle named ~/.ipython/extensions/memory_profiler_ext.py with the following
content:
import memory_profiler
def load_ipython_extension(ip):
ip.define_magic('memit', memory_profiler.magic_memit)
ip.define_magic('mprun', memory_profiler.magic_mprun)
Then register it in ~/.ipython/profile_default/ipython_config.py:
c.TerminalIPythonApp.extensions = [
'memory_profiler_ext',
]
c.InteractiveShellApp.extensions = [
'memory_profiler_ext',
]
This will register the %memit and %mprun magic commands in the IPython terminal application and the other
frontends such as qtconsole and notebook.
%mprun is useful to examine, line-by-line, the memory usage of key functions in your program. It is very similar to
%lprun, discussed in the previous section. For example, from the memory_profiler examples directory:
In [1] from example import my_func
In [2] %mprun -f my_func my_func()
Filename: example.py
Line #
Mem usage
Increment
Line Contents
==============================================
3
@profile
4
5.97 MB
0.00 MB
def my_func():
5
13.61 MB
7.64 MB
a = [1] * (10 ** 6)
6
166.20 MB
152.59 MB
b = [2] * (2 * 10 ** 7)
7
13.61 MB -152.59 MB
del b
8
13.61 MB
0.00 MB
return a
Another useful magic that memory_profiler deﬁnes is %memit, which is analogous to %timeit. It can be used
as follows:
In [1]: import numpy as np
In [2]: %memit np.zeros(1e7)
maximum of 3: 76.402344 MB per loop
For more details, see the docstrings of the magics, using %memit? and %mprun?.
30.4.5 Performance tips for the Cython developer
If proﬁling of the Python code reveals that the Python interpreter overhead is larger by one order of magnitude or
more than the cost of the actual numerical computation (e.g. for loops over vector components, nested evaluation
of conditional expression, scalar arithmetic...), it is probably adequate to extract the hotspot portion of the code as a
30.4. How to optimize for speed
1997
scikit-learn user guide, Release 0.18.2
standalone function in a .pyx ﬁle, add static type declarations and then use Cython to generate a C program suitable
to be compiled as a Python extension module.
The ofﬁcial documentation available at http://docs.cython.org/ contains a tutorial and reference guide for developing
such a module. In the following we will just highlight a couple of tricks that we found important in practice on the
existing cython codebase in the scikit-learn project.
TODO: html report, type declarations, bound checks, division by zero checks, memory alignment, direct blas calls...
• https://www.youtube.com/watch?v=gMvkiQ-gOW8
• http://conference.scipy.org/proceedings/SciPy2009/paper_1/
• http://conference.scipy.org/proceedings/SciPy2009/paper_2/
30.4.6 Proﬁling compiled extensions
When working with compiled extensions (written in C/C++ with a wrapper or directly as Cython extension), the default
Python proﬁler is useless: we need a dedicated tool to introspect what’s happening inside the compiled extension it-
self.
Using yep and google-perftools
Easy proﬁling without special compilation options use yep:
• https://pypi.python.org/pypi/yep
• http://fa.bianp.net/blog/2011/a-proﬁler-for-python-extensions
Note:
google-perftools provides a nice ‘line by line’ report mode that can be triggered with the --lines option.
However this does not seem to work correctly at the time of writing. This issue can be tracked on the project issue
tracker.
Using gprof
In order to proﬁle compiled Python extensions one could use gprof after having recompiled the project with gcc
-pg and using the python-dbg variant of the interpreter on debian / ubuntu: however this approach requires to also
have numpy and scipy recompiled with -pg which is rather complicated to get working.
Fortunately there exist two alternative proﬁlers that don’t require you to recompile everything.
Using valgrind / callgrind / kcachegrind
TODO
30.4.7 Multi-core parallelism using joblib.Parallel
TODO: give a simple teaser example here.
Checkout the ofﬁcial joblib documentation:
• https://pythonhosted.org/joblib
1998
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
30.4.8 A sample algorithmic trick: warm restarts for cross validation
TODO: demonstrate the warm restart tricks for cross validation of linear regression with Coordinate Descent.
30.5 Advanced installation instructions
There are different ways to get scikit-learn installed:
• Install the version of scikit-learn provided by your operating system or Python distribution. This is the quickest
option for those who have operating systems that distribute scikit-learn.
• Install an ofﬁcial release. This is the best approach for users who want a stable version number and aren’t
concerned about running a slightly older version of scikit-learn.
• Install the latest development version. This is best for users who want the latest-and-greatest features and aren’t
afraid of running brand-new code.
Note: If you wish to contribute to the project, you need to install the latest development version.
30.5.1 Installing an ofﬁcial release
Scikit-learn requires:
• Python (>= 2.6 or >= 3.3),
• NumPy (>= 1.6.1),
• SciPy (>= 0.9).
Mac OSX
Scikit-learn and its dependencies are all available as wheel packages for OSX:
pip install -U numpy scipy scikit-learn
Linux
At this time scikit-learn does not provide ofﬁcial binary packages for Linux so you have to build from source if
you want the latest version. If you don’t need the newest version, consider using your package manager to install
scikit-learn. It is usually the easiest way, but might not provide the newest version.
Installing build dependencies
Installing from source requires you to have installed the scikit-learn runtime dependencies, Python development head-
ers and a working C/C++ compiler. Under Debian-based operating systems, which include Ubuntu, if you have Python
2 you can install all these requirements by issuing:
sudo apt-get install build-essential python-dev python-setuptools \
python-numpy python-scipy \
libatlas-dev libatlas3gf-base
30.5. Advanced installation instructions
1999
scikit-learn user guide, Release 0.18.2
If you have Python 3:
sudo apt-get install build-essential python3-dev python3-setuptools \
python3-numpy python3-scipy \
libatlas-dev libatlas3gf-base
On recent Debian and Ubuntu (e.g. Ubuntu 13.04 or later) make sure that ATLAS is used to provide the implementation
of the BLAS and LAPACK linear algebra routines:
sudo update-alternatives --set libblas.so.3 \
/usr/lib/atlas-base/atlas/libblas.so.3
sudo update-alternatives --set liblapack.so.3 \
/usr/lib/atlas-base/atlas/liblapack.so.3
Note:
In order to build the documentation and run the example code contains in this documentation you will need
matplotlib:
sudo apt-get install python-matplotlib
Note:
The above installs the ATLAS implementation of BLAS (the Basic Linear Algebra Subprograms library).
Ubuntu 11.10 and later, and recent (testing) versions of Debian, offer an alternative implementation called OpenBLAS.
Using OpenBLAS can give speedups in some scikit-learn modules, but can freeze joblib/multiprocessing prior to
OpenBLAS version 0.2.8-4, so using it is not recommended unless you know what you’re doing.
If you do want to use OpenBLAS, then replacing ATLAS only requires a couple of commands. ATLAS has to be
removed, otherwise NumPy may not work:
sudo apt-get remove libatlas3gf-base libatlas-dev
sudo apt-get install libopenblas-dev
sudo update-alternatives
--set libblas.so.3 \
/usr/lib/openblas-base/libopenblas.so.0
sudo update-alternatives --set liblapack.so.3 \
/usr/lib/lapack/liblapack.so.3
On Red Hat and clones (e.g. CentOS), install the dependencies using:
sudo yum -y install gcc gcc-c++ numpy python-devel scipy
Building scikit-learn with pip
This is usually the fastest way to install or upgrade to the latest stable release:
pip install --user --install-option="--prefix=" -U scikit-learn
The --user ﬂag asks pip to install scikit-learn in the $HOME/.local folder therefore not requiring root permission.
This ﬂag should make pip ignore any old version of scikit-learn previously installed on the system while beneﬁting
from system packages for numpy and scipy. Those dependencies can be long and complex to build correctly from
source.
The --install-option="--prefix=" ﬂag is only required if Python has a distutils.cfg conﬁguration
with a predeﬁned prefix= entry.
2000
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
From source package
download the source package from pypi, unpack the sources and cd into the source directory.
This packages uses distutils, which is the default way of installing python modules. The install command is:
python setup.py install
or alternatively (also from within the scikit-learn source folder):
pip install .
Warning:
Packages installed with the python setup.py install command cannot be uninstalled nor
upgraded by pip later. To properly uninstall scikit-learn in that case it is necessary to delete the sklearn folder
from your Python site-packages directory.
Windows
First, you need to install numpy and scipy from their own ofﬁcial installers.
Wheel packages (.whl ﬁles) for scikit-learn from pypi can be installed with the pip utility. Open a console and type
the following to install or upgrade scikit-learn to the latest stable release:
pip install -U scikit-learn
If there are no binary packages matching your python, version you might to try to install scikit-learn and its dependen-
cies from christoph gohlke unofﬁcial windows installers or from a python distribution instead.
30.5.2 Third party distributions of scikit-learn
Some third-party distributions are now providing versions of scikit-learn integrated with their package-management
systems.
These can make installation and upgrading much easier for users since the integration includes the ability to automat-
ically install dependencies (numpy, scipy) that scikit-learn requires.
The following is an incomplete list of python and os distributions that provide their own version of scikit-learn.
MacPorts for Mac OSX
The MacPorts package is named py<XY>-scikits-learn, where XY denotes the Python version. It can be
installed by typing the following command:
sudo port install py26-scikit-learn
or:
sudo port install py27-scikit-learn
30.5. Advanced installation instructions
2001
scikit-learn user guide, Release 0.18.2
Arch Linux
Arch Linux’s package is provided through the ofﬁcial repositories as python-scikit-learn for Python 3 and
python2-scikit-learn for Python 2. It can be installed by typing the following command:
# pacman -S python-scikit-learn
or:
# pacman -S python2-scikit-learn
depending on the version of Python you use.
NetBSD
scikit-learn is available via pkgsrc-wip:
http://pkgsrc.se/wip/py-scikit_learn
Fedora
The Fedora package is called python-scikit-learn for the Python 2 version and python3-scikit-learn
for the Python 3 version. Both versions can be installed using yum:
$ sudo yum install python-scikit-learn
or:
$ sudo yum install python3-scikit-learn
Building on windows
To build scikit-learn on Windows you need a working C/C++ compiler in addition to numpy, scipy and setuptools.
Picking the right compiler depends on the version of Python (2 or 3) and the architecture of the Python interpreter,
32-bit or 64-bit. You can check the Python version by running the following in cmd or powershell console:
python --version
and the architecture with:
python -c "import struct; print(struct.calcsize('P') * 8)"
The above commands assume that you have the Python installation folder in your PATH environment variable.
32-bit Python
For 32-bit python it is possible use the standalone installers for microsoft visual c++ express 2008 for Python 2 or
Microsoft Visual C++ Express 2010 for Python 3.
Once installed you should be able to build scikit-learn without any particular conﬁguration by running the following
command in the scikit-learn folder:
2002
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
python setup.py install
64-bit Python
For the 64-bit architecture, you either need the full Visual Studio or the free Windows SDKs that can be downloaded
from the links below.
The Windows SDKs include the MSVC compilers both for 32 and 64-bit architectures.
They come as a
GRMSDKX_EN_DVD.iso ﬁle that can be mounted as a new drive with a setup.exe installer in it.
• For Python 2 you need SDK v7.0: MS Windows SDK for Windows 7 and .NET Framework 3.5 SP1
• For Python 3 you need SDK v7.1: MS Windows SDK for Windows 7 and .NET Framework 4
Both SDKs can be installed in parallel on the same host. To use the Windows SDKs, you need to setup the environment
of a cmd console launched with the following ﬂags (at least for SDK v7.0):
cmd /E:ON /V:ON /K
Then conﬁgure the build environment with:
SET DISTUTILS_USE_SDK=1
SET MSSdk=1
"C:\Program Files\Microsoft SDKs\Windows\v7.0\Setup\WindowsSdkVer.exe" -q -version:v7.
˓→0
"C:\Program Files\Microsoft SDKs\Windows\v7.0\Bin\SetEnv.cmd" /x64 /release
Finally you can build scikit-learn in the same cmd console:
python setup.py install
Replace v7.0 by the v7.1 in the above commands to do the same for Python 3 instead of Python 2.
Replace /x64 by /x86 to build for 32-bit Python instead of 64-bit Python.
Building binary packages and installers
The .whl package and .exe installers can be built with:
pip install wheel
python setup.py bdist_wheel bdist_wininst -b doc/logos/scikit-learn-logo.bmp
The resulting packages are generated in the dist/ folder.
Using an alternative compiler
It is possible to use MinGW (a port of GCC to Windows OS) as an alternative to MSVC for 32-bit Python. Not that
extensions built with mingw32 can be redistributed as reusable packages as they depend on GCC runtime libraries
typically not installed on end-users environment.
To force the use of a particular compiler, pass the --compiler ﬂag to the build step:
python setup.py build --compiler=my_compiler install
where my_compiler should be one of mingw32 or msvc.
30.5. Advanced installation instructions
2003
scikit-learn user guide, Release 0.18.2
30.5.3 Bleeding Edge
See section Retrieving the latest code on how to get the development version. Then follow the previous instructions to
build from source depending on your platform. You will also require Cython >=0.23 in order to build the development
version.
30.5.4 Testing
Testing scikit-learn once installed
Testing requires having the nose library. After installation, the package can be tested by executing from outside the
source directory:
$ nosetests -v sklearn
Under Windows, it is recommended to use the following command (adjust the path to the python.exe program) as
using the nosetests.exe program can badly interact with tests that use multiprocessing:
C:\Python34\python.exe -c "import nose; nose.main()" -v sklearn
This should give you a lot of output (and some warnings) but eventually should ﬁnish with a message similar to:
Ran 3246 tests in 260.618s
OK (SKIP=20)
Otherwise, please consider posting an issue into the bug tracker or to the Mailing List including the traceback of the
individual failures and errors. Please include your operating system, your version of NumPy, SciPy and scikit-learn,
and how you installed scikit-learn.
Testing scikit-learn from within the source folder
Scikit-learn can also be tested without having the package installed. For this you must compile the sources inplace
from the source directory:
python setup.py build_ext --inplace
Test can now be run using nosetests:
nosetests -v sklearn/
This is automated by the commands:
make in
and:
make test
You can also install a symlink named site-packages/scikit-learn.egg-link to the development folder
of scikit-learn with:
pip install --editable .
2004
Chapter 30. Developer’s Guide
scikit-learn user guide, Release 0.18.2
30.6 Maintainer / core-developer information
For more information see https://github.com/scikit-learn/scikit-learn/wiki/How-to-make-a-release
30.6.1 Making a release
1. Update docs:
• edit the doc/whats_new.rst ﬁle to add release title and commit statistics. You can retrieve commit statistics
with:
$ git shortlog -ns 0.998..
• edit the doc/conf.py to increase the version number
• edit the doc/themes/scikit-learn/layout.html to change the ‘News’ entry of the front page.
2. Update the version number in sklearn/__init__.py, the __version__ variable
3. Create the tag and push it:
$ git tag 0.999
$ git push origin --tags
4. create tarballs:
• Wipe clean your repo:
$ git clean -xfd
• Register and upload on PyPI:
$ python setup.py sdist register upload
• Upload manually the tarball on SourceForge: https://sourceforge.net/projects/scikit-learn/ﬁles/
5. Push the documentation to the website (see README in doc folder)
6. Build binaries for windows and push them to PyPI:
$ python setup.py bdist_wininst upload
And upload them also to sourceforge
30.6. Maintainer / core-developer information
2005
scikit-learn user guide, Release 0.18.2
2006
Chapter 30. Developer’s Guide
BIBLIOGRAPHY
[M2012] “Machine Learning: A Probabilistic Perspective” Murphy, K. P. - chapter 14.4.3, pp. 492-493, The MIT
Press, 2012
[B1999] L. Breiman, “Pasting small votes for classiﬁcation in large databases and on-line”, Machine Learning, 36(1),
85-103, 1999.
[B1996] L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140, 1996.
[H1998] T. Ho, “The random subspace method for constructing decision forests”, Pattern Analysis and Machine
Intelligence, 20(8), 832-844, 1998.
[LG2012] G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine Learning and Knowledge Discovery
in Databases, 346-361, 2012.
[B2001]
12. Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.
[B1998]
12. Breiman, “Arcing Classiﬁers”, Annals of Statistics 1998.
[GEW2006] P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42,
2006.
[FS1995] Y. Freund, and R. Schapire, “A Decision-Theoretic Generalization of On-Line Learning and an Application
to Boosting”, 1997.
[ZZRH2009] J. Zhu, H. Zou, S. Rosset, T. Hastie. “Multi-class AdaBoost”, 2009.
[D1997]
8. Drucker. “Improving Regressors using Boosting Techniques”, 1997.
[HTF] T. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, Springer, 2009.
[F2001] J. Friedman, “Greedy Function Approximation: A Gradient Boosting Machine”, The Annals of Statistics,
Vol. 29, No. 5, 2001.
[F1999]
10. Friedman, “Stochastic Gradient Boosting”, 1999
[HTF2009]
20. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, Springer, 2009.
[R2007]
7. Ridgeway, “Generalized Boosted Models: A guide to the gbm package”, 2007
[B2009] F. Bach, “Model-Consistent Sparse Estimation through the Bootstrap.” https://hal.inria.fr/hal-00354771/
[M2010] N. Meinshausen, P. Buhlmann, “Stability selection”, Journal of the Royal Statistical Society, 72 (2010)
http://arxiv.org/pdf/0809.2932.pdf
[RH2007] V-Measure: A conditional entropy-based external cluster evaluation measure Andrew Rosenberg and Julia
Hirschberg, 2007
[B2011] Identication and Characterization of Events in Social Media, Hila Becker, PhD Thesis.
[Mrl09] “Online Dictionary Learning for Sparse Coding” J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009
2007
scikit-learn user guide, Release 0.18.2
[Jen09] “Structured Sparse Principal Component Analysis” R. Jenatton, G. Obozinski, F. Bach, 2009
[RD1999] Rousseeuw, P.J., Van Driessen, K. “A fast algorithm for the minimum covariance determinant estimator”
Technometrics 41(3), 212 (1999)
[LTZ2008] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.” Data Mining, 2008. ICDM‘08.
Eighth IEEE International Conference on.
[R23]
12. Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.
[R24]
12. Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.
[R19] P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42, 2006.
[R20] P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42, 2006.
[RR2007] “Random features for large-scale kernel machines” Rahimi, A. and Recht, B. - Advances in neural infor-
mation processing 2007,
[LS2010] “Random Fourier approximations for skewed multiplicative histogram kernels” Random Fourier approxi-
mations for skewed multiplicative histogram kernels - Lecture Notes for Computer Sciencd (DAGM)
[VZ2010] “Efﬁcient additive kernels via explicit feature maps” Vedaldi, A. and Zisserman, A. - Computer Vision and
Pattern Recognition 2010
[VVZ2010] “Generalized RBF feature maps for Efﬁcient Detection” Vempati, S. and Vedaldi, A. and Zisserman, A.
and Jawahar, CV - 2010
[Rouseeuw1984] P. J. Rousseeuw. Least median of squares regression. J. Am Stat Ass, 79:871, 1984.
[Rouseeuw1999] A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical
Association and the American Society for Quality, TECHNOMETRICS
[Butler1993] R. W. Butler, P. L. Davies and M. Jhun, Asymptotics For The Minimum Covariance Determinant Esti-
mator, The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400
[R132] Analysis and Results of the 1999 DARPA Off-Line Intrusion Detection Evaluation Richard Lippmann, Joshua
W. Haines, David J. Fried, Jonathan Korba, Kumar Das
[R133] A Geometric Framework for Unsupervised Anomaly Detection: Detecting Intrusions in Unlabeled Data
(2002) by Eleazar Eskin, Andrew Arnold, Michael Prerau, Leonid Portnoy, Sal Stolfo
[R7] I. Guyon, “Design of experiments for the NIPS 2003 variable selection benchmark”, 2003.
[R137] J. Friedman, “Multivariate adaptive regression splines”, The Annals of Statistics 19 (1), pages 1-67, 1991.
[R138] L. Breiman, “Bagging predictors”, Machine Learning 24, pages 123-140, 1996.
[R139] J. Friedman, “Multivariate adaptive regression splines”, The Annals of Statistics 19 (1), pages 1-67, 1991.
[R140] L. Breiman, “Bagging predictors”, Machine Learning 24, pages 123-140, 1996.
[R141] J. Friedman, “Multivariate adaptive regression splines”, The Annals of Statistics 19 (1), pages 1-67, 1991.
[R142] L. Breiman, “Bagging predictors”, Machine Learning 24, pages 123-140, 1996.
[R8]
10. Zhu, H. Zou, S. Rosset, T. Hastie, “Multi-class AdaBoost”, 2009.
[R9] T. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, Springer, 2009.
[R145] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert, “Regularization in regression: comparing Bayesian and
frequentist methods in a poorly informative situation”, 2009.
[R10] S. Marsland, “Machine Learning: An Algorithmic Perspective”, Chapter 10, 2009. http://seat.massey.ac.nz/
personal/s.r.marsland/Code/10/lle.py
2008
Bibliography
scikit-learn user guide, Release 0.18.2
[R5] Dhillon, I. S. (2001, August). Co-clustering documents and words using bipartite spectral graph partitioning.
In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining
(pp. 269-274). ACM.
[R6] Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003). Spectral biclustering of microarray data: coclustering
genes and conditions. Genome research, 13(4), 703-716.
[R11] Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boost-
ing”, 1995.
[R12]
10. Zhu, H. Zou, S. Rosset, T. Hastie, “Multi-class AdaBoost”, 2009.
[R13] Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boost-
ing”, 1995.
[R14]
8. Drucker, “Improving Regressors using Boosting Techniques”, 1997.
[R151] L. Breiman, “Pasting small votes for classiﬁcation in large databases and on-line”, Machine Learning, 36(1),
85-103, 1999.
[R152] L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140, 1996.
[R153] T. Ho, “The random subspace method for constructing decision forests”, Pattern Analysis and Machine Intel-
ligence, 20(8), 832-844, 1998.
[R154] G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine Learning and Knowledge Discovery in
Databases, 346-361, 2012.
[R15] L. Breiman, “Pasting small votes for classiﬁcation in large databases and on-line”, Machine Learning, 36(1),
85-103, 1999.
[R16] L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140, 1996.
[R17] T. Ho, “The random subspace method for constructing decision forests”, Pattern Analysis and Machine Intelli-
gence, 20(8), 832-844, 1998.
[R18] G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine Learning and Knowledge Discovery in
Databases, 346-361, 2012.
[R21] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.” Data Mining, 2008. ICDM‘08. Eighth
IEEE International Conference on.
[R22] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based anomaly detection.” ACM Transactions on
Knowledge Discovery from Data (TKDD) 6.1 (2012): 3.
[R25] P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42, 2006.
[R26] Moosmann, F. and Triggs, B. and Jurie, F. “Fast discriminative visual codebooks using randomized clustering
forests” NIPS 2007
[Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern Information Retrieval. Addison Wesley, pp. 68-74.
[MRS2008] C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge
University Press, pp. 118-120.
[R27] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., “Gene selection for cancer classiﬁcation using support vector
machines”, Mach. Learn., 46(1-3), 389–422, 2002.
[R28] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., “Gene selection for cancer classiﬁcation using support vector
machines”, Mach. Learn., 46(1-3), 389–422, 2002.
[R169] Mutual Information on Wikipedia.
[R170] A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E 69, 2004.
[R171] B. C. Ross “Mutual Information between Discrete and Continuous Data Sets”. PLoS ONE 9(2), 2014.
Bibliography
2009
scikit-learn user guide, Release 0.18.2
[R172] L. F. Kozachenko, N. N. Leonenko, “Sample Estimate of the Entropy of a Random Vector:, Probl. Peredachi
Inf., 23:2 (1987), 9-16
[R29] Mutual Information on Wikipedia.
[R30] A. Kraskov, H. Stogbauer and P. Grassberger, “Estimating mutual information”. Phys. Rev. E 69, 2004.
[R31] B. C. Ross “Mutual Information between Discrete and Continuous Data Sets”. PLoS ONE 9(2), 2014.
[R32] L. F. Kozachenko, N. N. Leonenko, “Sample Estimate of the Entropy of a Random Vector”, Probl. Peredachi
Inf., 23:2 (1987), 9-16
[R33] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics Concomitant scale estimates, pg 172
[R34] Art B. Owen (2006), A robust hybrid of lasso and ridge regression. http://statweb.stanford.edu/~owen/reports/
hhu.pdf
[R35] https://en.wikipedia.org/wiki/RANSAC
[R36] http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf
[R37] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf
[R38] “Least Angle Regression”, Effron et al. http://statweb.stanford.edu/~tibs/ftp/lars.pdf
[R39] Wikipedia entry on the Least-angle regression
[R40] Wikipedia entry on the Lasso
[R42] Roweis, S. & Saul, L. Nonlinear dimensionality reduction by locally linear embedding. Science 290:2323
(2000).
[R43] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data.
Proc Natl Acad Sci U S A. 100:5591 (2003).
[R44] Zhang, Z. & Wang, J. MLLE: Modiﬁed Locally Linear Embedding Using Multiple Weights. http://citeseerx.ist.
psu.edu/viewdoc/summary?doi=10.1.1.70.382
[R45] Zhang, Z. & Zha, H. Principal manifolds and nonlinear dimensionality reduction via tangent space alignment.
Journal of Shanghai Univ. 8:406 (2004)
[R41] Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. A global geometric framework for nonlinear dimensionality
reduction. Science 290 (5500)
[R46] Roweis, S. & Saul, L. Nonlinear dimensionality reduction by locally linear embedding. Science 290:2323
(2000).
[R47] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data.
Proc Natl Acad Sci U S A. 100:5591 (2003).
[R48] Zhang, Z. & Wang, J. MLLE: Modiﬁed Locally Linear Embedding Using Multiple Weights. http://citeseerx.ist.
psu.edu/viewdoc/summary?doi=10.1.1.70.382
[R49] Zhang, Z. & Zha, H. Principal manifolds and nonlinear dimensionality reduction via tangent space alignment.
Journal of Shanghai Univ. 8:406 (2004)
[R52] Wikipedia entry for the Average precision
[R53] Wikipedia entry for the Brier score.
[R199] J. Cohen (1960). “A coefﬁcient of agreement for nominal scales”. Educational and Psychological Measure-
ment 20(1):37-46. doi:10.1177/001316446002000104.
[R200] R. Artstein and M. Poesio (2008). “Inter-coder agreement for computational linguistics”. Computational Lin-
guistics 34(4):555-596.
[R201] Wikipedia entry for the Cohen’s kappa.
2010
Bibliography
scikit-learn user guide, Release 0.18.2
[R55] Wikipedia entry for the Confusion matrix
[R56] Wikipedia entry for the F1-score
[R206] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern Information Retrieval. Addison Wesley, pp. 327-328.
[R207] Wikipedia entry for the F1-score
[R57] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classiﬁcation: An Overview. International Journal of Data
Warehousing & Mining, 3(3), 1-13, July-September 2007.
[R58] Wikipedia entry on the Hamming distance
[R212] Wikipedia entry on the Hinge loss
[R213] Koby Crammer, Yoram Singer. On the Algorithmic Implementation of Multiclass Kernel-based Vector Ma-
chines. Journal of Machine Learning Research 2, (2001), 265-292
[R214] L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C. Moore, John DeNero.
[R216] Wikipedia entry for the Jaccard index
[R218] Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the accuracy of prediction algorithms for
classiﬁcation: an overview
[R219] Wikipedia entry for the Matthews Correlation Coefﬁcient
[R220] Wikipedia entry for the Precision and recall
[R221] Wikipedia entry for the F1-score
[R222] Discriminative Methods for Multi-labeled Classiﬁcation Advances in Knowledge Discovery and Data Mining
(2004), pp. 22-30 by Shantanu Godbole, Sunita Sarawagi <http://www.godbole.net/shantanu/pubs/multilabelsvm-
pakdd04.pdf>
[R224] Wikipedia entry for the Receiver operating characteristic
[R61] Wikipedia entry for the Receiver operating characteristic
[R60] Wikipedia entry on the Coefﬁcient of determination
[R204] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge
discovery handbook (pp. 667-685). Springer US.
[R217] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge
discovery handbook (pp. 667-685). Springer US.
[R50] Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for Clusterings Comparison: Variants, Prop-
erties, Normalization and Correction for Chance, JMLR
[R51] Wikipedia entry for the Adjusted Mutual Information
[Hubert1985] L. Hubert and P. Arabie, Comparing Partitions, Journal of Classiﬁcation 1985 http://link.springer.com/
article/10.1007%2FBF01908075
[wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index
[R198] T. Calinski and J. Harabasz, 1974. “A dendrite method for cluster analysis”. Communications in Statistics
[R54] Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster eval-
uation measure
[R208] E. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two hierarchical clusterings”. Journal of
the American Statistical Association
[R209] Wikipedia entry for the Fowlkes-Mallows Index
Bibliography
2011
scikit-learn user guide, Release 0.18.2
[R59] Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster eval-
uation measure
[R64] Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Anal-
ysis”. Computational and Applied Mathematics 20: 53-65.
[R65] Wikipedia entry on the Silhouette Coefﬁcient
[R62] Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Anal-
ysis”. Computational and Applied Mathematics 20: 53-65.
[R63] Wikipedia entry on the Silhouette Coefﬁcient
[R66] Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster eval-
uation measure
[R67] Bishop, Christopher M. (2006). “Pattern recognition and machine learning”. Vol. 4 No. 4. New York: Springer.
[R68] Hagai Attias. (2000). “A Variational Bayesian Framework for Graphical Models”. In Advances in Neural In-
formation Processing Systems 12.
[R69] Blei, David M. and Michael I. Jordan. (2006). “Variational inference for Dirichlet process mixtures”. Bayesian
analysis 1.1
[R234] “Solving multiclass learning problems via error-correcting output codes”, Dietterich T., Bakiri G., Journal of
Artiﬁcial Intelligence Research 2, 1995.
[R235] “The error coding method and PICTs”, James G., Hastie T., Journal of Computational and Graphical statistics
7, 1998.
[R236] “The Elements of Statistical Learning”, Hastie T., Tibshirani R., Friedman J., page 606 (second-edition) 2008.
[R70] M. Bawa, T. Condie and P. Ganesan, “LSH Forest: Self-Tuning Indexes for Similarity Search”, WWW ‘05
Proceedings of the 14th international conference on World Wide Web, 651-660, 2005.
[R1] Obtaining calibrated probability estimates from decision trees and naive Bayesian classiﬁers, B. Zadrozny & C.
Elkan, ICML 2001
[R2] Transforming Classiﬁer Scores into Accurate Multiclass Probability Estimates, B. Zadrozny & C. Elkan, (KDD
2002)
[R3] Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods, J.
Platt, (1999)
[R4] Predicting Good Probabilities with Supervised Learning, A. Niculescu-Mizil & R. Caruana, ICML 2005
[R71] Ping Li, T. Hastie and K. W. Church, 2006, “Very Sparse Random Projections”. http://web.stanford.edu/~hastie/
Papers/Ping/KDD06_rp.pdf
[R72] D. Achlioptas, 2001, “Database-friendly random projections”, https://users.soe.ucsc.edu/~optas/papers/jl.pdf
[R73] https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma
[R74] Sanjoy Dasgupta and Anupam Gupta, 1999, “An elementary proof of the Johnson-Lindenstrauss Lemma.”
http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.3654
[R75] https://en.wikipedia.org/wiki/Decision_tree_learning
[R76] L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classiﬁcation and Regression Trees”, Wadsworth, Belmont,
CA, 1984.
[R77] T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical Learning”, Springer, 2009.
[R78] L. Breiman, and A. Cutler, “Random Forests”, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_
home.htm
2012
Bibliography
scikit-learn user guide, Release 0.18.2
[R79] https://en.wikipedia.org/wiki/Decision_tree_learning
[R80] L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classiﬁcation and Regression Trees”, Wadsworth, Belmont,
CA, 1984.
[R81] T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical Learning”, Springer, 2009.
[R82] L. Breiman, and A. Cutler, “Random Forests”, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_
home.htm
[R250] P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42, 2006.
[R251] P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42, 2006.
[Halko2009] Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decom-
positions Halko, et al., 2009 (arXiv:909)
[MRT] A randomized algorithm for the decomposition of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and
Mark Tygert
[NLNS2002] H.B. Nielsen, S.N. Lophaven, H. B. Nielsen and J. Sondergaard. DACE - A MATLAB Kriging
Toolbox. (2002) http://imedea.uib-csic.es/master/cambioglobal/Modulo_V_cod101615/Lab/lab_maps/krigging/
DACE-krigingsoft/dace/dace.pdf
[WBSWM1992] W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell, and M.D. Morris (1992). Screening, pre-
dicting, and computer experiments. Technometrics, 34(1) 15–25. http://www.jstor.org/stable/1269548
Bibliography
2013
scikit-learn user guide, Release 0.18.2
2014
Bibliography
INDEX
Symbols
__init__() (sklearn.base.BaseEstimator method), 1129
__init__() (sklearn.base.ClassiﬁerMixin method), 1130
__init__() (sklearn.base.ClusterMixin method), 1131
__init__() (sklearn.base.RegressorMixin method), 1131
__init__()
(sklearn.base.TransformerMixin
method),
1132
__init__()
(sklearn.calibration.CalibratedClassiﬁerCV
method), 1784
__init__() (sklearn.cluster.AfﬁnityPropagation method),
1134
__init__()
(sklearn.cluster.AgglomerativeClustering
method), 1137
__init__() (sklearn.cluster.Birch method), 1139
__init__() (sklearn.cluster.DBSCAN method), 1142
__init__()
(sklearn.cluster.FeatureAgglomeration
method), 1145
__init__() (sklearn.cluster.KMeans method), 1150
__init__() (sklearn.cluster.MeanShift method), 1156
__init__() (sklearn.cluster.MiniBatchKMeans method),
1153
__init__()
(sklearn.cluster.SpectralClustering
method),
1159
__init__()
(sklearn.cluster.bicluster.SpectralBiclustering
method), 1171
__init__() (sklearn.cluster.bicluster.SpectralCoclustering
method), 1173
__init__() (sklearn.covariance.EllipticEnvelope method),
1179
__init__()
(sklearn.covariance.EmpiricalCovariance
method), 1176
__init__()
(sklearn.covariance.GraphLasso
method),
1182
__init__() (sklearn.covariance.GraphLassoCV method),
1186
__init__() (sklearn.covariance.LedoitWolf method), 1188
__init__() (sklearn.covariance.MinCovDet method), 1192
__init__() (sklearn.covariance.OAS method), 1195
__init__()
(sklearn.covariance.ShrunkCovariance
method), 1198
__init__() (sklearn.cross_decomposition.CCA method),
1797
__init__()
(sklearn.cross_decomposition.PLSCanonical
method), 1794
__init__() (sklearn.cross_decomposition.PLSRegression
method), 1789
__init__()
(sklearn.cross_decomposition.PLSSVD
method), 1800
__init__()
(sklearn.cross_validation.LabelShufﬂeSplit
method), 1940
__init__()
(sklearn.decomposition.DictionaryLearning
method), 1334
__init__()
(sklearn.decomposition.FactorAnalysis
method), 1314
__init__()
(sklearn.decomposition.FastICA
method),
1317
__init__()
(sklearn.decomposition.IncrementalPCA
method), 1303
__init__() (sklearn.decomposition.KernelPCA method),
1311
__init__() (sklearn.decomposition.LatentDirichletAllocation
method), 1341
__init__() (sklearn.decomposition.MiniBatchDictionaryLearning
method), 1337
__init__() (sklearn.decomposition.MiniBatchSparsePCA
method), 1329
__init__() (sklearn.decomposition.NMF method), 1324
__init__() (sklearn.decomposition.PCA method), 1298
__init__() (sklearn.decomposition.ProjectedGradientNMF
method), 1308
__init__()
(sklearn.decomposition.RandomizedPCA
method), 1945
__init__() (sklearn.decomposition.SparseCoder method),
1331
__init__() (sklearn.decomposition.SparsePCA method),
1326
__init__()
(sklearn.decomposition.TruncatedSVD
method), 1319
__init__() (sklearn.discriminant_analysis.LinearDiscriminantAnalysis
method), 1495
__init__() (sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
method), 1499
__init__()
(sklearn.dummy.DummyClassiﬁer
method),
1350
2015
scikit-learn user guide, Release 0.18.2
__init__() (sklearn.dummy.DummyRegressor method),
1353
__init__()
(sklearn.ensemble.AdaBoostClassiﬁer
method), 1356
__init__()
(sklearn.ensemble.AdaBoostRegressor
method), 1361
__init__() (sklearn.ensemble.BaggingClassiﬁer method),
1365
__init__() (sklearn.ensemble.BaggingRegressor method),
1369
__init__()
(sklearn.ensemble.ExtraTreesClassiﬁer
method), 430
__init__()
(sklearn.ensemble.ExtraTreesRegressor
method), 436
__init__() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 442
__init__() (sklearn.ensemble.GradientBoostingRegressor
method), 449
__init__()
(sklearn.ensemble.IsolationForest
method),
1372
__init__()
(sklearn.ensemble.RandomForestClassiﬁer
method), 418
__init__()
(sklearn.ensemble.RandomForestRegressor
method), 424
__init__()
(sklearn.ensemble.RandomTreesEmbedding
method), 1375
__init__() (sklearn.ensemble.VotingClassiﬁer method),
1378
__init__()
(sklearn.feature_extraction.DictVectorizer
method), 1387
__init__()
(sklearn.feature_extraction.FeatureHasher
method), 1391
__init__() (sklearn.feature_extraction.image.PatchExtractor
method), 1395
__init__() (sklearn.feature_extraction.text.CountVectorizer
method), 1399
__init__() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
__init__() (sklearn.feature_extraction.text.TﬁdfTransformer
method), 1406
__init__() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1411
__init__() (sklearn.feature_selection.GenericUnivariateSelect
method), 1414
__init__() (sklearn.feature_selection.RFE method), 1432
__init__()
(sklearn.feature_selection.RFECV
method),
1436
__init__() (sklearn.feature_selection.SelectFdr method),
1424
__init__() (sklearn.feature_selection.SelectFpr method),
1422
__init__()
(sklearn.feature_selection.SelectFromModel
method), 1426
__init__() (sklearn.feature_selection.SelectFwe method),
1429
__init__()
(sklearn.feature_selection.SelectKBest
method), 1419
__init__()
(sklearn.feature_selection.SelectPercentile
method), 1416
__init__()
(sklearn.feature_selection.VarianceThreshold
method), 1438
__init__()
(sklearn.gaussian_process.GaussianProcess
method), 1949
__init__() (sklearn.gaussian_process.GaussianProcessClassiﬁer
method), 1451
__init__() (sklearn.gaussian_process.GaussianProcessRegressor
method), 1447
__init__() (sklearn.gaussian_process.kernels.CompoundKernel
method), 1474
__init__() (sklearn.gaussian_process.kernels.ConstantKernel
method), 1460
__init__() (sklearn.gaussian_process.kernels.DotProduct
method), 1471
__init__() (sklearn.gaussian_process.kernels.ExpSineSquared
method), 1469
__init__() (sklearn.gaussian_process.kernels.Exponentiation
method), 1458
__init__() (sklearn.gaussian_process.kernels.Hyperparameter
method), 1476
__init__()
(sklearn.gaussian_process.kernels.Kernel
method), 1454
__init__()
(sklearn.gaussian_process.kernels.Matern
method), 1465
__init__() (sklearn.gaussian_process.kernels.PairwiseKernel
method), 1473
__init__()
(sklearn.gaussian_process.kernels.Product
method), 1457
__init__()
(sklearn.gaussian_process.kernels.RBF
method), 1463
__init__() (sklearn.gaussian_process.kernels.RationalQuadratic
method), 1467
__init__()
(sklearn.gaussian_process.kernels.Sum
method), 1455
__init__() (sklearn.gaussian_process.kernels.WhiteKernel
method), 1462
__init__() (sklearn.grid_search.GridSearchCV method),
1926
__init__()
(sklearn.grid_search.RandomizedSearchCV
method), 1931
__init__() (sklearn.isotonic.IsotonicRegression method),
1478
__init__() (sklearn.kernel_approximation.AdditiveChi2Sampler
method), 1482
__init__()
(sklearn.kernel_approximation.Nystroem
method), 1485
__init__()
(sklearn.kernel_approximation.RBFSampler
method), 1486
2016
Index
scikit-learn user guide, Release 0.18.2
__init__() (sklearn.kernel_approximation.SkewedChi2Sampler
method), 1488
__init__() (sklearn.kernel_ridge.KernelRidge method),
1491
__init__() (sklearn.lda.LDA method), 1916
__init__()
(sklearn.linear_model.ARDRegression
method), 1504
__init__()
(sklearn.linear_model.BayesianRidge
method), 1507
__init__()
(sklearn.linear_model.ElasticNet
method),
1511
__init__() (sklearn.linear_model.ElasticNetCV method),
367
__init__()
(sklearn.linear_model.HuberRegressor
method), 1515
__init__() (sklearn.linear_model.Lars method), 1519
__init__() (sklearn.linear_model.LarsCV method), 372
__init__() (sklearn.linear_model.Lasso method), 1522
__init__() (sklearn.linear_model.LassoCV method), 376
__init__()
(sklearn.linear_model.LassoLars
method),
1528
__init__() (sklearn.linear_model.LassoLarsCV method),
382
__init__() (sklearn.linear_model.LassoLarsIC method),
413
__init__()
(sklearn.linear_model.LinearRegression
method), 1530
__init__()
(sklearn.linear_model.LogisticRegression
method), 1535
__init__()
(sklearn.linear_model.LogisticRegressionCV
method), 386
__init__()
(sklearn.linear_model.MultiTaskElasticNet
method), 1546
__init__() (sklearn.linear_model.MultiTaskElasticNetCV
method), 392
__init__()
(sklearn.linear_model.MultiTaskLasso
method), 1540
__init__()
(sklearn.linear_model.MultiTaskLassoCV
method), 398
__init__() (sklearn.linear_model.OrthogonalMatchingPursuit
method), 1550
__init__() (sklearn.linear_model.OrthogonalMatchingPursuitCV
method), 403
__init__() (sklearn.linear_model.PassiveAggressiveClassiﬁer
method), 1553
__init__() (sklearn.linear_model.PassiveAggressiveRegressor
method), 1557
__init__()
(sklearn.linear_model.Perceptron
method),
1560
__init__()
(sklearn.linear_model.RANSACRegressor
method), 1572
__init__()
(sklearn.linear_model.RandomizedLasso
method), 1566
__init__() (sklearn.linear_model.RandomizedLogisticRegression
method), 1569
__init__() (sklearn.linear_model.Ridge method), 1576
__init__() (sklearn.linear_model.RidgeCV method), 406
__init__()
(sklearn.linear_model.RidgeClassiﬁer
method), 1579
__init__()
(sklearn.linear_model.RidgeClassiﬁerCV
method), 409
__init__() (sklearn.linear_model.SGDClassiﬁer method),
1584
__init__() (sklearn.linear_model.SGDRegressor method),
1590
__init__()
(sklearn.linear_model.TheilSenRegressor
method), 1594
__init__() (sklearn.manifold.Isomap method), 1610
__init__()
(sklearn.manifold.LocallyLinearEmbedding
method), 1608
__init__() (sklearn.manifold.MDS method), 1613
__init__()
(sklearn.manifold.SpectralEmbedding
method), 1615
__init__() (sklearn.manifold.TSNE method), 1619
__init__()
(sklearn.mixture.BayesianGaussianMixture
method), 1695
__init__() (sklearn.mixture.DPGMM method), 1954
__init__() (sklearn.mixture.GMM method), 1952
__init__()
(sklearn.mixture.GaussianMixture
method),
1689
__init__() (sklearn.mixture.VBGMM method), 1958
__init__()
(sklearn.model_selection.GridSearchCV
method), 1231
__init__()
(sklearn.model_selection.GroupKFold
method), 1207
__init__()
(sklearn.model_selection.GroupShufﬂeSplit
method), 1218
__init__() (sklearn.model_selection.KFold method), 1205
__init__() (sklearn.model_selection.LeaveOneGroupOut
method), 1210
__init__()
(sklearn.model_selection.LeaveOneOut
method), 1213
__init__()
(sklearn.model_selection.LeavePGroupsOut
method), 1212
__init__() (sklearn.model_selection.LeavePOut method),
1215
__init__()
(sklearn.model_selection.PredeﬁnedSplit
method), 1222
__init__() (sklearn.model_selection.RandomizedSearchCV
method), 1236
__init__()
(sklearn.model_selection.ShufﬂeSplit
method), 1217
__init__()
(sklearn.model_selection.StratiﬁedKFold
method), 1208
__init__() (sklearn.model_selection.StratiﬁedShufﬂeSplit
method), 1220
Index
2017
scikit-learn user guide, Release 0.18.2
__init__()
(sklearn.model_selection.TimeSeriesSplit
method), 1223
__init__()
(sklearn.multiclass.OneVsOneClassiﬁer
method), 1701
__init__()
(sklearn.multiclass.OneVsRestClassiﬁer
method), 1699
__init__()
(sklearn.multiclass.OutputCodeClassiﬁer
method), 1704
__init__()
(sklearn.multioutput.MultiOutputClassiﬁer
method), 1708
__init__()
(sklearn.multioutput.MultiOutputRegressor
method), 1706
__init__()
(sklearn.naive_bayes.BernoulliNB
method),
1717
__init__()
(sklearn.naive_bayes.GaussianNB
method),
1710
__init__() (sklearn.naive_bayes.MultinomialNB method),
1714
__init__() (sklearn.neighbors.BallTree method), 1749
__init__() (sklearn.neighbors.DistanceMetric method),
1764
__init__() (sklearn.neighbors.KDTree method), 1754
__init__()
(sklearn.neighbors.KNeighborsClassiﬁer
method), 1728
__init__()
(sklearn.neighbors.KNeighborsRegressor
method), 1737
__init__()
(sklearn.neighbors.KernelDensity
method),
1766
__init__() (sklearn.neighbors.LSHForest method), 1759
__init__() (sklearn.neighbors.NearestCentroid method),
1745
__init__() (sklearn.neighbors.NearestNeighbors method),
1722
__init__() (sklearn.neighbors.RadiusNeighborsClassiﬁer
method), 1732
__init__() (sklearn.neighbors.RadiusNeighborsRegressor
method), 1741
__init__()
(sklearn.neural_network.BernoulliRBM
method), 1771
__init__()
(sklearn.neural_network.MLPClassiﬁer
method), 1776
__init__()
(sklearn.neural_network.MLPRegressor
method), 1782
__init__() (sklearn.pipeline.FeatureUnion method), 1806
__init__() (sklearn.pipeline.Pipeline method), 1802
__init__()
(sklearn.preprocessing.Binarizer
method),
1809
__init__()
(sklearn.preprocessing.FunctionTransformer
method), 1811
__init__() (sklearn.preprocessing.Imputer method), 1813
__init__()
(sklearn.preprocessing.KernelCenterer
method), 1814
__init__()
(sklearn.preprocessing.LabelBinarizer
method), 1817
__init__() (sklearn.preprocessing.LabelEncoder method),
1820
__init__()
(sklearn.preprocessing.MaxAbsScaler
method), 1823
__init__()
(sklearn.preprocessing.MinMaxScaler
method), 1826
__init__()
(sklearn.preprocessing.MultiLabelBinarizer
method), 1821
__init__() (sklearn.preprocessing.Normalizer method),
1828
__init__()
(sklearn.preprocessing.OneHotEncoder
method), 1830
__init__()
(sklearn.preprocessing.PolynomialFeatures
method), 1833
__init__() (sklearn.preprocessing.RobustScaler method),
1835
__init__()
(sklearn.preprocessing.StandardScaler
method), 1837
__init__() (sklearn.qda.QDA method), 1918
__init__() (sklearn.random_projection.GaussianRandomProjection
method), 1846
__init__() (sklearn.random_projection.SparseRandomProjection
method), 1848
__init__()
(sklearn.semi_supervised.LabelPropagation
method), 1852
__init__()
(sklearn.semi_supervised.LabelSpreading
method), 1855
__init__() (sklearn.svm.LinearSVC method), 1865
__init__() (sklearn.svm.LinearSVR method), 1878
__init__() (sklearn.svm.NuSVC method), 1870
__init__() (sklearn.svm.NuSVR method), 1881
__init__() (sklearn.svm.OneClassSVM method), 1884
__init__() (sklearn.svm.SVC method), 1859
__init__() (sklearn.svm.SVR method), 1874
__init__() (sklearn.tree.DecisionTreeClassiﬁer method),
1893
__init__() (sklearn.tree.DecisionTreeRegressor method),
1899
__init__()
(sklearn.tree.ExtraTreeClassiﬁer
method),
1903
__init__()
(sklearn.tree.ExtraTreeRegressor
method),
1907
A
accuracy_score() (in module sklearn.metrics), 1624
AdaBoostClassiﬁer (class in sklearn.ensemble), 1355
AdaBoostRegressor (class in sklearn.ensemble), 1359
add_dummy_feature() (in module sklearn.preprocessing),
1839
additive_chi2_kernel()
(in
module
sklearn.metrics.pairwise), 1672
AdditiveChi2Sampler
(class
in
sklearn.kernel_approximation), 1482
2018
Index
scikit-learn user guide, Release 0.18.2
adjusted_mutual_info_score()
(in
module
sklearn.metrics), 1658
adjusted_rand_score() (in module sklearn.metrics), 1659
afﬁnity_propagation() (in module sklearn.cluster), 1164
AfﬁnityPropagation (class in sklearn.cluster), 1133
AgglomerativeClustering (class in sklearn.cluster), 1135
aic() (sklearn.mixture.DPGMM method), 1954
aic() (sklearn.mixture.GaussianMixture method), 1689
aic() (sklearn.mixture.GMM method), 1952
aic() (sklearn.mixture.VBGMM method), 1958
apply() (sklearn.ensemble.ExtraTreesClassiﬁer method),
430
apply() (sklearn.ensemble.ExtraTreesRegressor method),
436
apply()
(sklearn.ensemble.GradientBoostingClassiﬁer
method), 442
apply()
(sklearn.ensemble.GradientBoostingRegressor
method), 449
apply()
(sklearn.ensemble.RandomForestClassiﬁer
method), 418
apply()
(sklearn.ensemble.RandomForestRegressor
method), 424
apply()
(sklearn.ensemble.RandomTreesEmbedding
method), 1375
apply()
(sklearn.tree.DecisionTreeClassiﬁer
method),
1893
apply()
(sklearn.tree.DecisionTreeRegressor
method),
1899
apply() (sklearn.tree.ExtraTreeClassiﬁer method), 1903
apply() (sklearn.tree.ExtraTreeRegressor method), 1907
ARDRegression (class in sklearn.linear_model), 1502
auc() (in module sklearn.metrics), 1626
average_precision_score() (in module sklearn.metrics),
1626
B
BaggingClassiﬁer (class in sklearn.ensemble), 1363
BaggingRegressor (class in sklearn.ensemble), 1367
BallTree (class in sklearn.neighbors), 1747
BaseEstimator (class in sklearn.base), 1129
BayesianGaussianMixture
(class
in
sklearn.mixture),
1691
BayesianRidge (class in sklearn.linear_model), 1505
BernoulliNB (class in sklearn.naive_bayes), 1716
BernoulliRBM (class in sklearn.neural_network), 1770
bic() (sklearn.mixture.DPGMM method), 1954
bic() (sklearn.mixture.GaussianMixture method), 1689
bic() (sklearn.mixture.GMM method), 1952
bic() (sklearn.mixture.VBGMM method), 1958
biclusters_ (sklearn.cluster.bicluster.SpectralBiclustering
attribute), 1171
biclusters_ (sklearn.cluster.bicluster.SpectralCoclustering
attribute), 1173
binarize() (in module sklearn.preprocessing), 1840
Binarizer (class in sklearn.preprocessing), 1809
Birch (class in sklearn.cluster), 1138
bounds (sklearn.gaussian_process.kernels.CompoundKernel
attribute), 1474
bounds (sklearn.gaussian_process.kernels.ConstantKernel
attribute), 1460
bounds (sklearn.gaussian_process.kernels.DotProduct at-
tribute), 1471
bounds (sklearn.gaussian_process.kernels.Exponentiation
attribute), 1458
bounds (sklearn.gaussian_process.kernels.ExpSineSquared
attribute), 1469
bounds (sklearn.gaussian_process.kernels.Hyperparameter
attribute), 1476
bounds
(sklearn.gaussian_process.kernels.Kernel
at-
tribute), 1454
bounds
(sklearn.gaussian_process.kernels.Matern
at-
tribute), 1465
bounds (sklearn.gaussian_process.kernels.PairwiseKernel
attribute), 1473
bounds
(sklearn.gaussian_process.kernels.Product
at-
tribute), 1457
bounds (sklearn.gaussian_process.kernels.RationalQuadratic
attribute), 1467
bounds (sklearn.gaussian_process.kernels.RBF attribute),
1463
bounds (sklearn.gaussian_process.kernels.Sum attribute),
1455
bounds
(sklearn.gaussian_process.kernels.WhiteKernel
attribute), 1462
brier_score_loss() (in module sklearn.metrics), 1628
build_analyzer() (sklearn.feature_extraction.text.CountVectorizer
method), 1399
build_analyzer() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
build_analyzer() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1411
build_preprocessor() (sklearn.feature_extraction.text.CountVectorizer
method), 1399
build_preprocessor() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
build_preprocessor() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1411
build_tokenizer() (sklearn.feature_extraction.text.CountVectorizer
method), 1400
build_tokenizer() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
build_tokenizer() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1411
C
CalibratedClassiﬁerCV
(class
in
sklearn.calibration),
1783
calibration_curve() (in module sklearn.calibration), 1786
Index
2019
scikit-learn user guide, Release 0.18.2
calinski_harabaz_score() (in module sklearn.metrics),
1661
CCA (class in sklearn.cross_decomposition), 1796
ChangedBehaviorWarning (class in sklearn.exceptions),
1384
check_cv() (in module sklearn.cross_validation), 1967
check_cv() (in module sklearn.model_selection), 1226
check_estimator()
(in
module
sklearn.utils.estimator_checks), 1913
check_increasing() (in module sklearn.isotonic), 1481
check_random_state() (in module sklearn.utils), 1912
chi2() (in module sklearn.feature_selection), 1440
chi2_kernel() (in module sklearn.metrics.pairwise), 1673
classiﬁcation_report() (in module sklearn.metrics), 1629
ClassiﬁerMixin (class in sklearn.base), 1130
clear_data_home() (in module sklearn.datasets), 1250
clone() (in module sklearn.base), 1132
clone_with_theta() (sklearn.gaussian_process.kernels.CompoundKernel
method), 1474
clone_with_theta() (sklearn.gaussian_process.kernels.ConstantKernel
method), 1460
clone_with_theta() (sklearn.gaussian_process.kernels.DotProduct
method), 1471
clone_with_theta() (sklearn.gaussian_process.kernels.Exponentiation
method), 1458
clone_with_theta() (sklearn.gaussian_process.kernels.ExpSineSquared
method), 1469
clone_with_theta() (sklearn.gaussian_process.kernels.Kernel
method), 1454
clone_with_theta() (sklearn.gaussian_process.kernels.Matern
method), 1466
clone_with_theta() (sklearn.gaussian_process.kernels.PairwiseKernel
method), 1473
clone_with_theta() (sklearn.gaussian_process.kernels.Product
method), 1457
clone_with_theta() (sklearn.gaussian_process.kernels.RationalQuadratic
method), 1467
clone_with_theta() (sklearn.gaussian_process.kernels.RBF
method), 1463
clone_with_theta() (sklearn.gaussian_process.kernels.Sum
method), 1455
clone_with_theta() (sklearn.gaussian_process.kernels.WhiteKernel
method), 1462
ClusterMixin (class in sklearn.base), 1131
cohen_kappa_score() (in module sklearn.metrics), 1630
completeness_score() (in module sklearn.metrics), 1661
CompoundKernel
(class
in
sklearn.gaussian_process.kernels), 1474
confusion_matrix() (in module sklearn.metrics), 1631
consensus_score() (in module sklearn.metrics), 1671
ConstantKernel
(class
in
sklearn.gaussian_process.kernels), 1459
ConvergenceWarning (class in sklearn.exceptions), 1384
correct_covariance() (sklearn.covariance.EllipticEnvelope
method), 1179
correct_covariance()
(sklearn.covariance.MinCovDet
method), 1192
cosine_distances() (in module sklearn.metrics.pairwise),
1680
cosine_similarity() (in module sklearn.metrics.pairwise),
1680
count() (sklearn.gaussian_process.kernels.Hyperparameter
method), 1476
CountVectorizer (class in sklearn.feature_extraction.text),
1397
coverage_error() (in module sklearn.metrics), 1656
cross_val_predict() (in module sklearn.cross_validation),
1965
cross_val_predict() (in module sklearn.model_selection),
1243
cross_val_score() (in module sklearn.cross_validation),
1966
cross_val_score() (in module sklearn.model_selection),
1241
cross_validation() (in module sklearn.svm.libsvm), 1889
D
data_min
(sklearn.preprocessing.MinMaxScaler
at-
tribute), 1826
data_range
(sklearn.preprocessing.MinMaxScaler
at-
tribute), 1826
DataConversionWarning (class in sklearn.exceptions),
1384
DataDimensionalityWarning
(class
in
sklearn.exceptions), 1384
DBSCAN (class in sklearn.cluster), 1141
dbscan() (in module sklearn.cluster), 1165
decision_function()
(in
module
sklearn.svm.libsvm),
1888
decision_function() (sklearn.covariance.EllipticEnvelope
method), 1179
decision_function() (sklearn.discriminant_analysis.LinearDiscriminantAnal
method), 1495
decision_function() (sklearn.discriminant_analysis.QuadraticDiscriminantA
method), 1499
decision_function() (sklearn.ensemble.AdaBoostClassiﬁer
method), 1356
decision_function() (sklearn.ensemble.BaggingClassiﬁer
method), 1365
decision_function() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 442
decision_function() (sklearn.ensemble.GradientBoostingRegressor
method), 450
decision_function()
(sklearn.ensemble.IsolationForest
method), 1372
decision_function()
(sklearn.grid_search.GridSearchCV
method), 1926
2020
Index
scikit-learn user guide, Release 0.18.2
decision_function() (sklearn.grid_search.RandomizedSearchCV
method), 1931
decision_function() (sklearn.lda.LDA method), 1916
decision_function() (sklearn.linear_model.ARDRegression
method), 1504
decision_function() (sklearn.linear_model.BayesianRidge
method), 1507
decision_function()
(sklearn.linear_model.ElasticNet
method), 1511
decision_function() (sklearn.linear_model.ElasticNetCV
method), 367
decision_function() (sklearn.linear_model.HuberRegressor
method), 1515
decision_function() (sklearn.linear_model.Lars method),
1519
decision_function()
(sklearn.linear_model.LarsCV
method), 372
decision_function()
(sklearn.linear_model.Lasso
method), 1522
decision_function()
(sklearn.linear_model.LassoCV
method), 376
decision_function()
(sklearn.linear_model.LassoLars
method), 1528
decision_function() (sklearn.linear_model.LassoLarsCV
method), 382
decision_function()
(sklearn.linear_model.LassoLarsIC
method), 413
decision_function() (sklearn.linear_model.LinearRegression
method), 1530
decision_function() (sklearn.linear_model.LogisticRegression
method), 1535
decision_function() (sklearn.linear_model.LogisticRegressionCV
method), 386
decision_function() (sklearn.linear_model.MultiTaskElasticNet
method), 1546
decision_function() (sklearn.linear_model.MultiTaskElasticNetCV
method), 392
decision_function() (sklearn.linear_model.MultiTaskLasso
method), 1540
decision_function() (sklearn.linear_model.MultiTaskLassoCV
method), 398
decision_function() (sklearn.linear_model.OrthogonalMatchingPursuit
method), 1550
decision_function() (sklearn.linear_model.OrthogonalMatchingPursuitCV
method), 403
decision_function() (sklearn.linear_model.PassiveAggressiveClassiﬁer
method), 1553
decision_function() (sklearn.linear_model.PassiveAggressiveRegressor
method), 1557
decision_function()
(sklearn.linear_model.Perceptron
method), 1560
decision_function()
(sklearn.linear_model.Ridge
method), 1576
decision_function() (sklearn.linear_model.RidgeClassiﬁer
method), 1579
decision_function() (sklearn.linear_model.RidgeClassiﬁerCV
method), 409
decision_function()
(sklearn.linear_model.RidgeCV
method), 406
decision_function() (sklearn.linear_model.SGDClassiﬁer
method), 1584
decision_function() (sklearn.linear_model.SGDRegressor
method), 1590
decision_function() (sklearn.linear_model.TheilSenRegressor
method), 1594
decision_function() (sklearn.model_selection.GridSearchCV
method), 1231
decision_function() (sklearn.model_selection.RandomizedSearchCV
method), 1236
decision_function() (sklearn.multiclass.OneVsOneClassiﬁer
method), 1701
decision_function() (sklearn.multiclass.OneVsRestClassiﬁer
method), 1699
decision_function() (sklearn.pipeline.Pipeline method),
1802
decision_function() (sklearn.qda.QDA method), 1918
decision_function()
(sklearn.svm.LinearSVC
method),
1865
decision_function()
(sklearn.svm.LinearSVR
method),
1878
decision_function() (sklearn.svm.NuSVC method), 1870
decision_function() (sklearn.svm.NuSVR method), 1881
decision_function()
(sklearn.svm.OneClassSVM
method), 1884
decision_function() (sklearn.svm.SVC method), 1859
decision_function() (sklearn.svm.SVR method), 1874
decision_path()
(sklearn.ensemble.ExtraTreesClassiﬁer
method), 430
decision_path()
(sklearn.ensemble.ExtraTreesRegressor
method), 436
decision_path() (sklearn.ensemble.RandomForestClassiﬁer
method), 418
decision_path() (sklearn.ensemble.RandomForestRegressor
method), 424
decision_path() (sklearn.ensemble.RandomTreesEmbedding
method), 1375
decision_path()
(sklearn.tree.DecisionTreeClassiﬁer
method), 1893
decision_path()
(sklearn.tree.DecisionTreeRegressor
method), 1899
decision_path()
(sklearn.tree.ExtraTreeClassiﬁer
method), 1903
decision_path()
(sklearn.tree.ExtraTreeRegressor
method), 1908
DecisionTreeClassiﬁer (class in sklearn.tree), 1890
DecisionTreeRegressor (class in sklearn.tree), 1897
Index
2021
scikit-learn user guide, Release 0.18.2
decode() (sklearn.feature_extraction.text.CountVectorizer
method), 1400
decode() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
decode() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1411
densify()
(sklearn.linear_model.LogisticRegression
method), 1535
densify()
(sklearn.linear_model.LogisticRegressionCV
method), 387
densify() (sklearn.linear_model.PassiveAggressiveClassiﬁer
method), 1554
densify() (sklearn.linear_model.PassiveAggressiveRegressor
method), 1557
densify()
(sklearn.linear_model.Perceptron
method),
1561
densify() (sklearn.linear_model.SGDClassiﬁer method),
1584
densify() (sklearn.linear_model.SGDRegressor method),
1590
densify() (sklearn.svm.LinearSVC method), 1865
diag() (sklearn.gaussian_process.kernels.CompoundKernel
method), 1474
diag() (sklearn.gaussian_process.kernels.ConstantKernel
method), 1460
diag()
(sklearn.gaussian_process.kernels.DotProduct
method), 1471
diag() (sklearn.gaussian_process.kernels.Exponentiation
method), 1458
diag() (sklearn.gaussian_process.kernels.ExpSineSquared
method), 1469
diag() (sklearn.gaussian_process.kernels.Kernel method),
1454
diag()
(sklearn.gaussian_process.kernels.Matern
method), 1466
diag() (sklearn.gaussian_process.kernels.PairwiseKernel
method), 1473
diag()
(sklearn.gaussian_process.kernels.Product
method), 1457
diag() (sklearn.gaussian_process.kernels.RationalQuadratic
method), 1468
diag() (sklearn.gaussian_process.kernels.RBF method),
1464
diag() (sklearn.gaussian_process.kernels.Sum method),
1455
diag()
(sklearn.gaussian_process.kernels.WhiteKernel
method), 1462
dict_learning() (in module sklearn.decomposition), 1345
dict_learning_online()
(in
module
sklearn.decomposition), 1346
DictionaryLearning
(class
in
sklearn.decomposition),
1332
DictVectorizer (class in sklearn.feature_extraction), 1386
dist_to_rdist()
(sklearn.neighbors.DistanceMetric
method), 1764
distance_metrics() (in module sklearn.metrics.pairwise),
1674
DistanceMetric (class in sklearn.neighbors), 1762
DotProduct (class in sklearn.gaussian_process.kernels),
1470
DPGMM (class in sklearn.mixture), 1954
DummyClassiﬁer (class in sklearn.dummy), 1349
DummyRegressor (class in sklearn.dummy), 1352
dump_svmlight_ﬁle() (in module sklearn.datasets), 1274
E
EfﬁciencyWarning (class in sklearn.exceptions), 1385
ElasticNet (class in sklearn.linear_model), 1508
ElasticNetCV (class in sklearn.linear_model), 365
EllipticEnvelope (class in sklearn.covariance), 1177
empirical_covariance() (in module sklearn.covariance),
1200
EmpiricalCovariance (class in sklearn.covariance), 1175
error_norm()
(sklearn.covariance.EllipticEnvelope
method), 1179
error_norm()
(sklearn.covariance.EmpiricalCovariance
method), 1176
error_norm() (sklearn.covariance.GraphLasso method),
1182
error_norm()
(sklearn.covariance.GraphLassoCV
method), 1186
error_norm() (sklearn.covariance.LedoitWolf method),
1188
error_norm() (sklearn.covariance.MinCovDet method),
1192
error_norm() (sklearn.covariance.OAS method), 1195
error_norm()
(sklearn.covariance.ShrunkCovariance
method), 1198
estimate_bandwidth() (in module sklearn.cluster), 1160
estimators_samples_ (sklearn.ensemble.BaggingClassiﬁer
attribute), 1365
estimators_samples_ (sklearn.ensemble.BaggingRegressor
attribute), 1369
estimators_samples_
(sklearn.ensemble.IsolationForest
attribute), 1372
euclidean_distances()
(in
module
sklearn.metrics.pairwise), 1674
explained_variance_score() (in module sklearn.metrics),
1651
Exponentiation
(class
in
sklearn.gaussian_process.kernels), 1458
export_graphviz() (in module sklearn.tree), 1910
ExpSineSquared
(class
in
sklearn.gaussian_process.kernels), 1469
extract_patches_2d()
(in
module
sklearn.feature_extraction.image), 1393
ExtraTreeClassiﬁer (class in sklearn.tree), 1902
2022
Index
scikit-learn user guide, Release 0.18.2
ExtraTreeRegressor (class in sklearn.tree), 1907
ExtraTreesClassiﬁer (class in sklearn.ensemble), 427
ExtraTreesRegressor (class in sklearn.ensemble), 433
F
f1_score() (in module sklearn.metrics), 1632
f_classif() (in module sklearn.feature_selection), 1441
f_regression()
(in
module
sklearn.feature_selection),
1441
FactorAnalysis (class in sklearn.decomposition), 1312
FastICA (class in sklearn.decomposition), 1315
fastica() (in module sklearn.decomposition), 1343
fbeta_score() (in module sklearn.metrics), 1633
feature_importances_ (sklearn.ensemble.AdaBoostClassiﬁer
attribute), 1356
feature_importances_ (sklearn.ensemble.AdaBoostRegressor
attribute), 1361
feature_importances_ (sklearn.ensemble.ExtraTreesClassiﬁer
attribute), 430
feature_importances_ (sklearn.ensemble.ExtraTreesRegressor
attribute), 436
feature_importances_ (sklearn.ensemble.GradientBoostingClassiﬁer
attribute), 443
feature_importances_ (sklearn.ensemble.GradientBoostingRegressor
attribute), 450
feature_importances_ (sklearn.ensemble.RandomForestClassiﬁer
attribute), 418
feature_importances_ (sklearn.ensemble.RandomForestRegressor
attribute), 425
feature_importances_ (sklearn.ensemble.RandomTreesEmbedding
attribute), 1376
feature_importances_ (sklearn.tree.DecisionTreeClassiﬁer
attribute), 1893
feature_importances_ (sklearn.tree.DecisionTreeRegressor
attribute), 1900
feature_importances_
(sklearn.tree.ExtraTreeClassiﬁer
attribute), 1904
feature_importances_
(sklearn.tree.ExtraTreeRegressor
attribute), 1908
FeatureAgglomeration (class in sklearn.cluster), 1143
FeatureHasher (class in sklearn.feature_extraction), 1389
FeatureUnion (class in sklearn.pipeline), 1805
fetch_20newsgroups() (in module sklearn.datasets), 1250
fetch_20newsgroups_vectorized()
(in
module
sklearn.datasets), 1251
fetch_california_housing() (in module sklearn.datasets),
1264
fetch_covtype() (in module sklearn.datasets), 1265
fetch_kddcup99() (in module sklearn.datasets), 1266
fetch_lfw_pairs() (in module sklearn.datasets), 1259
fetch_lfw_people() (in module sklearn.datasets), 1260
fetch_mldata() (in module sklearn.datasets), 1262
fetch_olivetti_faces() (in module sklearn.datasets), 1263
fetch_rcv1() (in module sklearn.datasets), 1267
fetch_species_distributions()
(in
module
sklearn.datasets), 1270
ﬁt() (in module sklearn.svm.libsvm), 1886
ﬁt() (sklearn.calibration.CalibratedClassiﬁerCV method),
1784
ﬁt() (sklearn.cluster.AfﬁnityPropagation method), 1134
ﬁt() (sklearn.cluster.AgglomerativeClustering method),
1137
ﬁt()
(sklearn.cluster.bicluster.SpectralBiclustering
method), 1171
ﬁt()
(sklearn.cluster.bicluster.SpectralCoclustering
method), 1174
ﬁt() (sklearn.cluster.Birch method), 1139
ﬁt() (sklearn.cluster.DBSCAN method), 1142
ﬁt()
(sklearn.cluster.FeatureAgglomeration
method),
1145
ﬁt() (sklearn.cluster.KMeans method), 1150
ﬁt() (sklearn.cluster.MeanShift method), 1156
ﬁt() (sklearn.cluster.MiniBatchKMeans method), 1153
ﬁt() (sklearn.cluster.SpectralClustering method), 1159
ﬁt() (sklearn.covariance.EmpiricalCovariance method),
1176
ﬁt() (sklearn.covariance.GraphLassoCV method), 1186
ﬁt() (sklearn.covariance.LedoitWolf method), 1189
ﬁt() (sklearn.covariance.MinCovDet method), 1192
ﬁt() (sklearn.covariance.OAS method), 1196
ﬁt()
(sklearn.covariance.ShrunkCovariance
method),
1198
ﬁt() (sklearn.cross_decomposition.CCA method), 1797
ﬁt()
(sklearn.cross_decomposition.PLSCanonical
method), 1794
ﬁt()
(sklearn.cross_decomposition.PLSRegression
method), 1789
ﬁt() (sklearn.decomposition.DictionaryLearning method),
1334
ﬁt()
(sklearn.decomposition.FactorAnalysis
method),
1314
ﬁt() (sklearn.decomposition.FastICA method), 1317
ﬁt() (sklearn.decomposition.IncrementalPCA method),
1303
ﬁt() (sklearn.decomposition.KernelPCA method), 1311
ﬁt()
(sklearn.decomposition.LatentDirichletAllocation
method), 1341
ﬁt() (sklearn.decomposition.MiniBatchDictionaryLearning
method), 1337
ﬁt()
(sklearn.decomposition.MiniBatchSparsePCA
method), 1329
ﬁt() (sklearn.decomposition.NMF method), 1324
ﬁt() (sklearn.decomposition.PCA method), 1298
ﬁt()
(sklearn.decomposition.ProjectedGradientNMF
method), 1308
ﬁt() (sklearn.decomposition.RandomizedPCA method),
1945
ﬁt() (sklearn.decomposition.SparseCoder method), 1331
Index
2023
scikit-learn user guide, Release 0.18.2
ﬁt() (sklearn.decomposition.SparsePCA method), 1326
ﬁt()
(sklearn.decomposition.TruncatedSVD
method),
1319
ﬁt() (sklearn.discriminant_analysis.LinearDiscriminantAnalysis
method), 1495
ﬁt() (sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
method), 1499
ﬁt() (sklearn.dummy.DummyClassiﬁer method), 1350
ﬁt() (sklearn.dummy.DummyRegressor method), 1353
ﬁt() (sklearn.ensemble.AdaBoostClassiﬁer method), 1356
ﬁt()
(sklearn.ensemble.AdaBoostRegressor
method),
1361
ﬁt() (sklearn.ensemble.BaggingClassiﬁer method), 1365
ﬁt() (sklearn.ensemble.BaggingRegressor method), 1369
ﬁt() (sklearn.ensemble.ExtraTreesClassiﬁer method), 431
ﬁt()
(sklearn.ensemble.ExtraTreesRegressor
method),
437
ﬁt()
(sklearn.ensemble.GradientBoostingClassiﬁer
method), 443
ﬁt()
(sklearn.ensemble.GradientBoostingRegressor
method), 450
ﬁt() (sklearn.ensemble.IsolationForest method), 1372
ﬁt() (sklearn.ensemble.RandomForestClassiﬁer method),
418
ﬁt() (sklearn.ensemble.RandomForestRegressor method),
425
ﬁt()
(sklearn.ensemble.RandomTreesEmbedding
method), 1376
ﬁt() (sklearn.ensemble.VotingClassiﬁer method), 1378
ﬁt() (sklearn.feature_extraction.DictVectorizer method),
1387
ﬁt() (sklearn.feature_extraction.FeatureHasher method),
1391
ﬁt()
(sklearn.feature_extraction.image.PatchExtractor
method), 1395
ﬁt()
(sklearn.feature_extraction.text.CountVectorizer
method), 1400
ﬁt()
(sklearn.feature_extraction.text.HashingVectorizer
method), 1404
ﬁt()
(sklearn.feature_extraction.text.TﬁdfTransformer
method), 1406
ﬁt()
(sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1411
ﬁt()
(sklearn.feature_selection.GenericUnivariateSelect
method), 1414
ﬁt() (sklearn.feature_selection.RFE method), 1432
ﬁt() (sklearn.feature_selection.RFECV method), 1436
ﬁt() (sklearn.feature_selection.SelectFdr method), 1424
ﬁt() (sklearn.feature_selection.SelectFpr method), 1422
ﬁt()
(sklearn.feature_selection.SelectFromModel
method), 1426
ﬁt() (sklearn.feature_selection.SelectFwe method), 1429
ﬁt()
(sklearn.feature_selection.SelectKBest
method),
1419
ﬁt() (sklearn.feature_selection.SelectPercentile method),
1416
ﬁt()
(sklearn.feature_selection.VarianceThreshold
method), 1438
ﬁt() (sklearn.gaussian_process.GaussianProcess method),
1949
ﬁt() (sklearn.gaussian_process.GaussianProcessClassiﬁer
method), 1451
ﬁt() (sklearn.gaussian_process.GaussianProcessRegressor
method), 1447
ﬁt() (sklearn.grid_search.GridSearchCV method), 1926
ﬁt()
(sklearn.grid_search.RandomizedSearchCV
method), 1931
ﬁt() (sklearn.isotonic.IsotonicRegression method), 1478
ﬁt() (sklearn.kernel_approximation.AdditiveChi2Sampler
method), 1482
ﬁt() (sklearn.kernel_approximation.Nystroem method),
1485
ﬁt()
(sklearn.kernel_approximation.RBFSampler
method), 1486
ﬁt() (sklearn.kernel_approximation.SkewedChi2Sampler
method), 1488
ﬁt() (sklearn.kernel_ridge.KernelRidge method), 1491
ﬁt() (sklearn.lda.LDA method), 1916
ﬁt()
(sklearn.linear_model.ARDRegression
method),
1504
ﬁt() (sklearn.linear_model.BayesianRidge method), 1507
ﬁt() (sklearn.linear_model.ElasticNet method), 1511
ﬁt() (sklearn.linear_model.ElasticNetCV method), 367
ﬁt()
(sklearn.linear_model.HuberRegressor
method),
1516
ﬁt() (sklearn.linear_model.Lars method), 1519
ﬁt() (sklearn.linear_model.LarsCV method), 372
ﬁt() (sklearn.linear_model.Lasso method), 1522
ﬁt() (sklearn.linear_model.LassoCV method), 376
ﬁt() (sklearn.linear_model.LassoLars method), 1528
ﬁt() (sklearn.linear_model.LassoLarsCV method), 382
ﬁt() (sklearn.linear_model.LassoLarsIC method), 413
ﬁt()
(sklearn.linear_model.LinearRegression
method),
1530
ﬁt() (sklearn.linear_model.LogisticRegression method),
1535
ﬁt()
(sklearn.linear_model.LogisticRegressionCV
method), 387
ﬁt() (sklearn.linear_model.MultiTaskElasticNet method),
1546
ﬁt()
(sklearn.linear_model.MultiTaskElasticNetCV
method), 393
ﬁt()
(sklearn.linear_model.MultiTaskLasso
method),
1541
ﬁt() (sklearn.linear_model.MultiTaskLassoCV method),
398
ﬁt()
(sklearn.linear_model.OrthogonalMatchingPursuit
method), 1551
2024
Index
scikit-learn user guide, Release 0.18.2
ﬁt() (sklearn.linear_model.OrthogonalMatchingPursuitCV
method), 403
ﬁt()
(sklearn.linear_model.PassiveAggressiveClassiﬁer
method), 1554
ﬁt()
(sklearn.linear_model.PassiveAggressiveRegressor
method), 1557
ﬁt() (sklearn.linear_model.Perceptron method), 1561
ﬁt() (sklearn.linear_model.RandomizedLasso method),
1566
ﬁt() (sklearn.linear_model.RandomizedLogisticRegression
method), 1569
ﬁt() (sklearn.linear_model.RANSACRegressor method),
1572
ﬁt() (sklearn.linear_model.Ridge method), 1576
ﬁt() (sklearn.linear_model.RidgeClassiﬁer method), 1579
ﬁt() (sklearn.linear_model.RidgeClassiﬁerCV method),
409
ﬁt() (sklearn.linear_model.RidgeCV method), 406
ﬁt() (sklearn.linear_model.SGDClassiﬁer method), 1584
ﬁt() (sklearn.linear_model.SGDRegressor method), 1590
ﬁt() (sklearn.linear_model.TheilSenRegressor method),
1595
ﬁt() (sklearn.manifold.Isomap method), 1610
ﬁt()
(sklearn.manifold.LocallyLinearEmbedding
method), 1608
ﬁt() (sklearn.manifold.MDS method), 1613
ﬁt() (sklearn.manifold.SpectralEmbedding method), 1615
ﬁt() (sklearn.manifold.TSNE method), 1619
ﬁt() (sklearn.mixture.BayesianGaussianMixture method),
1695
ﬁt() (sklearn.mixture.DPGMM method), 1955
ﬁt() (sklearn.mixture.GaussianMixture method), 1689
ﬁt() (sklearn.mixture.GMM method), 1952
ﬁt() (sklearn.mixture.VBGMM method), 1958
ﬁt()
(sklearn.model_selection.GridSearchCV
method),
1231
ﬁt()
(sklearn.model_selection.RandomizedSearchCV
method), 1237
ﬁt()
(sklearn.multiclass.OneVsOneClassiﬁer
method),
1702
ﬁt()
(sklearn.multiclass.OneVsRestClassiﬁer
method),
1699
ﬁt() (sklearn.multiclass.OutputCodeClassiﬁer method),
1704
ﬁt() (sklearn.multioutput.MultiOutputClassiﬁer method),
1708
ﬁt() (sklearn.multioutput.MultiOutputRegressor method),
1706
ﬁt() (sklearn.naive_bayes.BernoulliNB method), 1717
ﬁt() (sklearn.naive_bayes.GaussianNB method), 1710
ﬁt() (sklearn.naive_bayes.MultinomialNB method), 1714
ﬁt() (sklearn.neighbors.KernelDensity method), 1766
ﬁt() (sklearn.neighbors.KNeighborsClassiﬁer method),
1728
ﬁt() (sklearn.neighbors.KNeighborsRegressor method),
1737
ﬁt() (sklearn.neighbors.LSHForest method), 1759
ﬁt() (sklearn.neighbors.NearestCentroid method), 1745
ﬁt() (sklearn.neighbors.NearestNeighbors method), 1722
ﬁt()
(sklearn.neighbors.RadiusNeighborsClassiﬁer
method), 1732
ﬁt()
(sklearn.neighbors.RadiusNeighborsRegressor
method), 1741
ﬁt()
(sklearn.neural_network.BernoulliRBM
method),
1771
ﬁt()
(sklearn.neural_network.MLPClassiﬁer
method),
1776
ﬁt()
(sklearn.neural_network.MLPRegressor
method),
1782
ﬁt() (sklearn.pipeline.FeatureUnion method), 1806
ﬁt() (sklearn.pipeline.Pipeline method), 1802
ﬁt() (sklearn.preprocessing.Binarizer method), 1809
ﬁt() (sklearn.preprocessing.Imputer method), 1813
ﬁt()
(sklearn.preprocessing.KernelCenterer
method),
1814
ﬁt() (sklearn.preprocessing.LabelBinarizer method), 1817
ﬁt() (sklearn.preprocessing.LabelEncoder method), 1820
ﬁt() (sklearn.preprocessing.MaxAbsScaler method), 1823
ﬁt() (sklearn.preprocessing.MinMaxScaler method), 1826
ﬁt() (sklearn.preprocessing.MultiLabelBinarizer method),
1821
ﬁt() (sklearn.preprocessing.Normalizer method), 1828
ﬁt()
(sklearn.preprocessing.OneHotEncoder
method),
1830
ﬁt() (sklearn.preprocessing.PolynomialFeatures method),
1833
ﬁt() (sklearn.preprocessing.RobustScaler method), 1835
ﬁt()
(sklearn.preprocessing.StandardScaler
method),
1837
ﬁt() (sklearn.qda.QDA method), 1919
ﬁt() (sklearn.random_projection.GaussianRandomProjection
method), 1846
ﬁt() (sklearn.random_projection.SparseRandomProjection
method), 1848
ﬁt() (sklearn.semi_supervised.LabelPropagation method),
1852
ﬁt() (sklearn.semi_supervised.LabelSpreading method),
1855
ﬁt() (sklearn.svm.LinearSVC method), 1865
ﬁt() (sklearn.svm.LinearSVR method), 1878
ﬁt() (sklearn.svm.NuSVC method), 1870
ﬁt() (sklearn.svm.NuSVR method), 1881
ﬁt() (sklearn.svm.OneClassSVM method), 1884
ﬁt() (sklearn.svm.SVC method), 1859
ﬁt() (sklearn.svm.SVR method), 1875
ﬁt() (sklearn.tree.DecisionTreeClassiﬁer method), 1894
ﬁt() (sklearn.tree.DecisionTreeRegressor method), 1900
ﬁt() (sklearn.tree.ExtraTreeClassiﬁer method), 1904
Index
2025
scikit-learn user guide, Release 0.18.2
ﬁt() (sklearn.tree.ExtraTreeRegressor method), 1908
ﬁt_grid_point() (in module sklearn.grid_search), 1960
ﬁt_grid_point()
(in
module
sklearn.model_selection),
1240
ﬁt_predict() (sklearn.base.ClusterMixin method), 1131
ﬁt_predict()
(sklearn.cluster.AfﬁnityPropagation
method), 1134
ﬁt_predict()
(sklearn.cluster.AgglomerativeClustering
method), 1137
ﬁt_predict() (sklearn.cluster.Birch method), 1139
ﬁt_predict() (sklearn.cluster.DBSCAN method), 1142
ﬁt_predict() (sklearn.cluster.KMeans method), 1150
ﬁt_predict() (sklearn.cluster.MeanShift method), 1156
ﬁt_predict() (sklearn.cluster.MiniBatchKMeans method),
1153
ﬁt_predict() (sklearn.cluster.SpectralClustering method),
1160
ﬁt_predict() (sklearn.mixture.DPGMM method), 1955
ﬁt_predict() (sklearn.mixture.GMM method), 1952
ﬁt_predict() (sklearn.mixture.VBGMM method), 1959
ﬁt_predict() (sklearn.pipeline.Pipeline method), 1803
ﬁt_transform() (sklearn.base.TransformerMixin method),
1132
ﬁt_transform() (sklearn.cluster.Birch method), 1139
ﬁt_transform()
(sklearn.cluster.FeatureAgglomeration
method), 1145
ﬁt_transform() (sklearn.cluster.KMeans method), 1150
ﬁt_transform()
(sklearn.cluster.MiniBatchKMeans
method), 1153
ﬁt_transform()
(sklearn.cross_decomposition.CCA
method), 1798
ﬁt_transform() (sklearn.cross_decomposition.PLSCanonical
method), 1794
ﬁt_transform() (sklearn.cross_decomposition.PLSRegression
method), 1789
ﬁt_transform()
(sklearn.cross_decomposition.PLSSVD
method), 1800
ﬁt_transform() (sklearn.decomposition.DictionaryLearning
method), 1334
ﬁt_transform()
(sklearn.decomposition.FactorAnalysis
method), 1314
ﬁt_transform() (sklearn.decomposition.FastICA method),
1317
ﬁt_transform() (sklearn.decomposition.IncrementalPCA
method), 1303
ﬁt_transform()
(sklearn.decomposition.KernelPCA
method), 1311
ﬁt_transform() (sklearn.decomposition.LatentDirichletAllocation
method), 1341
ﬁt_transform() (sklearn.decomposition.MiniBatchDictionaryLearning
method), 1338
ﬁt_transform() (sklearn.decomposition.MiniBatchSparsePCA
method), 1329
ﬁt_transform()
(sklearn.decomposition.NMF
method),
1324
ﬁt_transform()
(sklearn.decomposition.PCA
method),
1298
ﬁt_transform() (sklearn.decomposition.ProjectedGradientNMF
method), 1308
ﬁt_transform() (sklearn.decomposition.RandomizedPCA
method), 1946
ﬁt_transform()
(sklearn.decomposition.SparseCoder
method), 1331
ﬁt_transform()
(sklearn.decomposition.SparsePCA
method), 1326
ﬁt_transform()
(sklearn.decomposition.TruncatedSVD
method), 1320
ﬁt_transform() (sklearn.discriminant_analysis.LinearDiscriminantAnalysis
method), 1495
ﬁt_transform()
(sklearn.ensemble.ExtraTreesClassiﬁer
method), 431
ﬁt_transform()
(sklearn.ensemble.ExtraTreesRegressor
method), 437
ﬁt_transform() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 443
ﬁt_transform() (sklearn.ensemble.GradientBoostingRegressor
method), 450
ﬁt_transform() (sklearn.ensemble.RandomForestClassiﬁer
method), 419
ﬁt_transform() (sklearn.ensemble.RandomForestRegressor
method), 425
ﬁt_transform() (sklearn.ensemble.RandomTreesEmbedding
method), 1376
ﬁt_transform()
(sklearn.ensemble.VotingClassiﬁer
method), 1378
ﬁt_transform() (sklearn.feature_extraction.DictVectorizer
method), 1387
ﬁt_transform() (sklearn.feature_extraction.FeatureHasher
method), 1391
ﬁt_transform() (sklearn.feature_extraction.text.CountVectorizer
method), 1400
ﬁt_transform() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
ﬁt_transform() (sklearn.feature_extraction.text.TﬁdfTransformer
method), 1406
ﬁt_transform() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1411
ﬁt_transform() (sklearn.feature_selection.GenericUnivariateSelect
method), 1414
ﬁt_transform() (sklearn.feature_selection.RFE method),
1432
ﬁt_transform()
(sklearn.feature_selection.RFECV
method), 1436
ﬁt_transform()
(sklearn.feature_selection.SelectFdr
method), 1424
ﬁt_transform()
(sklearn.feature_selection.SelectFpr
method), 1422
2026
Index
scikit-learn user guide, Release 0.18.2
ﬁt_transform() (sklearn.feature_selection.SelectFromModel
method), 1426
ﬁt_transform()
(sklearn.feature_selection.SelectFwe
method), 1429
ﬁt_transform()
(sklearn.feature_selection.SelectKBest
method), 1419
ﬁt_transform() (sklearn.feature_selection.SelectPercentile
method), 1417
ﬁt_transform() (sklearn.feature_selection.VarianceThreshold
method), 1438
ﬁt_transform()
(sklearn.isotonic.IsotonicRegression
method), 1478
ﬁt_transform() (sklearn.kernel_approximation.AdditiveChi2Sampler
method), 1482
ﬁt_transform() (sklearn.kernel_approximation.Nystroem
method), 1485
ﬁt_transform() (sklearn.kernel_approximation.RBFSampler
method), 1487
ﬁt_transform() (sklearn.kernel_approximation.SkewedChi2Sampler
method), 1488
ﬁt_transform() (sklearn.lda.LDA method), 1916
ﬁt_transform() (sklearn.linear_model.LogisticRegression
method), 1536
ﬁt_transform() (sklearn.linear_model.LogisticRegressionCV
method), 387
ﬁt_transform()
(sklearn.linear_model.Perceptron
method), 1561
ﬁt_transform() (sklearn.linear_model.RandomizedLasso
method), 1566
ﬁt_transform() (sklearn.linear_model.RandomizedLogisticRegression
method), 1569
ﬁt_transform()
(sklearn.linear_model.SGDClassiﬁer
method), 1584
ﬁt_transform()
(sklearn.linear_model.SGDRegressor
method), 1591
ﬁt_transform() (sklearn.manifold.Isomap method), 1611
ﬁt_transform() (sklearn.manifold.LocallyLinearEmbedding
method), 1608
ﬁt_transform() (sklearn.manifold.MDS method), 1613
ﬁt_transform()
(sklearn.manifold.SpectralEmbedding
method), 1615
ﬁt_transform() (sklearn.manifold.TSNE method), 1619
ﬁt_transform()
(sklearn.neural_network.BernoulliRBM
method), 1772
ﬁt_transform() (sklearn.pipeline.FeatureUnion method),
1806
ﬁt_transform() (sklearn.pipeline.Pipeline method), 1803
ﬁt_transform() (sklearn.preprocessing.Binarizer method),
1810
ﬁt_transform() (sklearn.preprocessing.FunctionTransformer
method), 1811
ﬁt_transform() (sklearn.preprocessing.Imputer method),
1813
ﬁt_transform()
(sklearn.preprocessing.KernelCenterer
method), 1815
ﬁt_transform()
(sklearn.preprocessing.LabelBinarizer
method), 1817
ﬁt_transform()
(sklearn.preprocessing.LabelEncoder
method), 1820
ﬁt_transform()
(sklearn.preprocessing.MaxAbsScaler
method), 1823
ﬁt_transform()
(sklearn.preprocessing.MinMaxScaler
method), 1826
ﬁt_transform() (sklearn.preprocessing.MultiLabelBinarizer
method), 1822
ﬁt_transform()
(sklearn.preprocessing.Normalizer
method), 1828
ﬁt_transform()
(sklearn.preprocessing.OneHotEncoder
method), 1831
ﬁt_transform() (sklearn.preprocessing.PolynomialFeatures
method), 1833
ﬁt_transform()
(sklearn.preprocessing.RobustScaler
method), 1835
ﬁt_transform()
(sklearn.preprocessing.StandardScaler
method), 1837
ﬁt_transform() (sklearn.random_projection.GaussianRandomProjection
method), 1846
ﬁt_transform() (sklearn.random_projection.SparseRandomProjection
method), 1849
ﬁt_transform() (sklearn.svm.LinearSVC method), 1866
ﬁt_transform()
(sklearn.tree.DecisionTreeClassiﬁer
method), 1894
ﬁt_transform()
(sklearn.tree.DecisionTreeRegressor
method), 1900
ﬁt_transform() (sklearn.tree.ExtraTreeClassiﬁer method),
1904
ﬁt_transform()
(sklearn.tree.ExtraTreeRegressor
method), 1909
FitFailedWarning (class in sklearn.exceptions), 1385
ﬁxed (sklearn.gaussian_process.kernels.Hyperparameter
attribute), 1476
fowlkes_mallows_score() (in module sklearn.metrics),
1662
FunctionTransformer (class in sklearn.preprocessing),
1810
G
GaussianMixture (class in sklearn.mixture), 1686
GaussianNB (class in sklearn.naive_bayes), 1709
GaussianProcess
(class
in
sklearn.gaussian_process),
1947
GaussianProcessClassiﬁer
(class
in
sklearn.gaussian_process), 1449
GaussianProcessRegressor
(class
in
sklearn.gaussian_process), 1445
GaussianRandomProjection
(class
in
sklearn.random_projection), 1845
Index
2027
scikit-learn user guide, Release 0.18.2
GenericUnivariateSelect
(class
in
sklearn.feature_selection), 1413
get_covariance() (sklearn.decomposition.FactorAnalysis
method), 1314
get_covariance() (sklearn.decomposition.IncrementalPCA
method), 1303
get_covariance() (sklearn.decomposition.PCA method),
1298
get_data_home() (in module sklearn.datasets), 1250
get_feature_names() (sklearn.feature_extraction.DictVectorizer
method), 1388
get_feature_names() (sklearn.feature_extraction.text.CountVectorizer
method), 1400
get_feature_names() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1411
get_feature_names()
(sklearn.pipeline.FeatureUnion
method), 1806
get_feature_names() (sklearn.preprocessing.PolynomialFeatures
method), 1833
get_indices() (sklearn.cluster.bicluster.SpectralBiclustering
method), 1171
get_indices() (sklearn.cluster.bicluster.SpectralCoclustering
method), 1174
get_metric() (sklearn.neighbors.DistanceMetric method),
1764
get_n_splits()
(sklearn.model_selection.GroupKFold
method), 1207
get_n_splits() (sklearn.model_selection.GroupShufﬂeSplit
method), 1218
get_n_splits() (sklearn.model_selection.KFold method),
1205
get_n_splits() (sklearn.model_selection.LeaveOneGroupOut
method), 1210
get_n_splits()
(sklearn.model_selection.LeaveOneOut
method), 1213
get_n_splits() (sklearn.model_selection.LeavePGroupsOut
method), 1212
get_n_splits()
(sklearn.model_selection.LeavePOut
method), 1215
get_n_splits()
(sklearn.model_selection.PredeﬁnedSplit
method), 1222
get_n_splits()
(sklearn.model_selection.ShufﬂeSplit
method), 1217
get_n_splits()
(sklearn.model_selection.StratiﬁedKFold
method), 1208
get_n_splits() (sklearn.model_selection.StratiﬁedShufﬂeSplit
method), 1220
get_n_splits()
(sklearn.model_selection.TimeSeriesSplit
method), 1223
get_params() (sklearn.base.BaseEstimator method), 1129
get_params() (sklearn.calibration.CalibratedClassiﬁerCV
method), 1785
get_params()
(sklearn.cluster.AfﬁnityPropagation
method), 1135
get_params()
(sklearn.cluster.AgglomerativeClustering
method), 1137
get_params() (sklearn.cluster.bicluster.SpectralBiclustering
method), 1171
get_params() (sklearn.cluster.bicluster.SpectralCoclustering
method), 1174
get_params() (sklearn.cluster.Birch method), 1140
get_params() (sklearn.cluster.DBSCAN method), 1143
get_params()
(sklearn.cluster.FeatureAgglomeration
method), 1145
get_params() (sklearn.cluster.KMeans method), 1150
get_params() (sklearn.cluster.MeanShift method), 1157
get_params()
(sklearn.cluster.MiniBatchKMeans
method), 1153
get_params()
(sklearn.cluster.SpectralClustering
method), 1160
get_params()
(sklearn.covariance.EllipticEnvelope
method), 1180
get_params()
(sklearn.covariance.EmpiricalCovariance
method), 1176
get_params() (sklearn.covariance.GraphLasso method),
1183
get_params()
(sklearn.covariance.GraphLassoCV
method), 1186
get_params() (sklearn.covariance.LedoitWolf method),
1189
get_params() (sklearn.covariance.MinCovDet method),
1193
get_params() (sklearn.covariance.OAS method), 1196
get_params()
(sklearn.covariance.ShrunkCovariance
method), 1198
get_params()
(sklearn.cross_decomposition.CCA
method), 1798
get_params() (sklearn.cross_decomposition.PLSCanonical
method), 1794
get_params() (sklearn.cross_decomposition.PLSRegression
method), 1790
get_params()
(sklearn.cross_decomposition.PLSSVD
method), 1800
get_params() (sklearn.decomposition.DictionaryLearning
method), 1335
get_params()
(sklearn.decomposition.FactorAnalysis
method), 1314
get_params() (sklearn.decomposition.FastICA method),
1317
get_params()
(sklearn.decomposition.IncrementalPCA
method), 1303
get_params()
(sklearn.decomposition.KernelPCA
method), 1311
get_params() (sklearn.decomposition.LatentDirichletAllocation
method), 1342
get_params() (sklearn.decomposition.MiniBatchDictionaryLearning
method), 1338
2028
Index
scikit-learn user guide, Release 0.18.2
get_params() (sklearn.decomposition.MiniBatchSparsePCA
method), 1329
get_params()
(sklearn.decomposition.NMF
method),
1324
get_params()
(sklearn.decomposition.PCA
method),
1299
get_params() (sklearn.decomposition.ProjectedGradientNMF
method), 1308
get_params()
(sklearn.decomposition.RandomizedPCA
method), 1946
get_params()
(sklearn.decomposition.SparseCoder
method), 1331
get_params()
(sklearn.decomposition.SparsePCA
method), 1327
get_params()
(sklearn.decomposition.TruncatedSVD
method), 1320
get_params() (sklearn.discriminant_analysis.LinearDiscriminantAnalysis
method), 1496
get_params() (sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
method), 1499
get_params()
(sklearn.dummy.DummyClassiﬁer
method), 1351
get_params()
(sklearn.dummy.DummyRegressor
method), 1353
get_params()
(sklearn.ensemble.AdaBoostClassiﬁer
method), 1357
get_params()
(sklearn.ensemble.AdaBoostRegressor
method), 1361
get_params()
(sklearn.ensemble.BaggingClassiﬁer
method), 1366
get_params()
(sklearn.ensemble.BaggingRegressor
method), 1369
get_params()
(sklearn.ensemble.ExtraTreesClassiﬁer
method), 431
get_params()
(sklearn.ensemble.ExtraTreesRegressor
method), 437
get_params() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 443
get_params() (sklearn.ensemble.GradientBoostingRegressor
method), 451
get_params() (sklearn.ensemble.IsolationForest method),
1373
get_params() (sklearn.ensemble.RandomForestClassiﬁer
method), 419
get_params() (sklearn.ensemble.RandomForestRegressor
method), 425
get_params() (sklearn.ensemble.RandomTreesEmbedding
method), 1376
get_params()
(sklearn.ensemble.VotingClassiﬁer
method), 1379
get_params()
(sklearn.feature_extraction.DictVectorizer
method), 1388
get_params()
(sklearn.feature_extraction.FeatureHasher
method), 1391
get_params() (sklearn.feature_extraction.image.PatchExtractor
method), 1395
get_params() (sklearn.feature_extraction.text.CountVectorizer
method), 1400
get_params() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
get_params() (sklearn.feature_extraction.text.TﬁdfTransformer
method), 1406
get_params() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1411
get_params() (sklearn.feature_selection.GenericUnivariateSelect
method), 1414
get_params()
(sklearn.feature_selection.RFE
method),
1432
get_params()
(sklearn.feature_selection.RFECV
method), 1436
get_params()
(sklearn.feature_selection.SelectFdr
method), 1424
get_params()
(sklearn.feature_selection.SelectFpr
method), 1422
get_params() (sklearn.feature_selection.SelectFromModel
method), 1427
get_params()
(sklearn.feature_selection.SelectFwe
method), 1429
get_params()
(sklearn.feature_selection.SelectKBest
method), 1419
get_params() (sklearn.feature_selection.SelectPercentile
method), 1417
get_params() (sklearn.feature_selection.VarianceThreshold
method), 1439
get_params() (sklearn.gaussian_process.GaussianProcess
method), 1950
get_params() (sklearn.gaussian_process.GaussianProcessClassiﬁer
method), 1451
get_params() (sklearn.gaussian_process.GaussianProcessRegressor
method), 1447
get_params() (sklearn.gaussian_process.kernels.CompoundKernel
method), 1475
get_params() (sklearn.gaussian_process.kernels.ConstantKernel
method), 1460
get_params() (sklearn.gaussian_process.kernels.DotProduct
method), 1471
get_params() (sklearn.gaussian_process.kernels.Exponentiation
method), 1459
get_params() (sklearn.gaussian_process.kernels.ExpSineSquared
method), 1470
get_params()
(sklearn.gaussian_process.kernels.Kernel
method), 1454
get_params()
(sklearn.gaussian_process.kernels.Matern
method), 1466
get_params() (sklearn.gaussian_process.kernels.PairwiseKernel
method), 1473
get_params()
(sklearn.gaussian_process.kernels.Product
method), 1457
Index
2029
scikit-learn user guide, Release 0.18.2
get_params() (sklearn.gaussian_process.kernels.RationalQuadratic
method), 1468
get_params()
(sklearn.gaussian_process.kernels.RBF
method), 1464
get_params()
(sklearn.gaussian_process.kernels.Sum
method), 1456
get_params() (sklearn.gaussian_process.kernels.WhiteKernel
method), 1462
get_params()
(sklearn.grid_search.GridSearchCV
method), 1927
get_params() (sklearn.grid_search.RandomizedSearchCV
method), 1931
get_params()
(sklearn.isotonic.IsotonicRegression
method), 1479
get_params() (sklearn.kernel_approximation.AdditiveChi2Sampler
method), 1483
get_params()
(sklearn.kernel_approximation.Nystroem
method), 1485
get_params() (sklearn.kernel_approximation.RBFSampler
method), 1487
get_params() (sklearn.kernel_approximation.SkewedChi2Sampler
method), 1489
get_params()
(sklearn.kernel_ridge.KernelRidge
method), 1491
get_params() (sklearn.lda.LDA method), 1917
get_params()
(sklearn.linear_model.ARDRegression
method), 1504
get_params()
(sklearn.linear_model.BayesianRidge
method), 1507
get_params() (sklearn.linear_model.ElasticNet method),
1511
get_params()
(sklearn.linear_model.ElasticNetCV
method), 368
get_params()
(sklearn.linear_model.HuberRegressor
method), 1516
get_params() (sklearn.linear_model.Lars method), 1519
get_params()
(sklearn.linear_model.LarsCV
method),
373
get_params() (sklearn.linear_model.Lasso method), 1523
get_params() (sklearn.linear_model.LassoCV method),
376
get_params() (sklearn.linear_model.LassoLars method),
1528
get_params()
(sklearn.linear_model.LassoLarsCV
method), 382
get_params()
(sklearn.linear_model.LassoLarsIC
method), 413
get_params()
(sklearn.linear_model.LinearRegression
method), 1531
get_params()
(sklearn.linear_model.LogisticRegression
method), 1536
get_params() (sklearn.linear_model.LogisticRegressionCV
method), 387
get_params() (sklearn.linear_model.MultiTaskElasticNet
method), 1546
get_params() (sklearn.linear_model.MultiTaskElasticNetCV
method), 393
get_params()
(sklearn.linear_model.MultiTaskLasso
method), 1541
get_params()
(sklearn.linear_model.MultiTaskLassoCV
method), 398
get_params() (sklearn.linear_model.OrthogonalMatchingPursuit
method), 1551
get_params() (sklearn.linear_model.OrthogonalMatchingPursuitCV
method), 403
get_params() (sklearn.linear_model.PassiveAggressiveClassiﬁer
method), 1554
get_params() (sklearn.linear_model.PassiveAggressiveRegressor
method), 1558
get_params() (sklearn.linear_model.Perceptron method),
1561
get_params()
(sklearn.linear_model.RandomizedLasso
method), 1566
get_params() (sklearn.linear_model.RandomizedLogisticRegression
method), 1570
get_params() (sklearn.linear_model.RANSACRegressor
method), 1573
get_params() (sklearn.linear_model.Ridge method), 1576
get_params()
(sklearn.linear_model.RidgeClassiﬁer
method), 1580
get_params()
(sklearn.linear_model.RidgeClassiﬁerCV
method), 410
get_params() (sklearn.linear_model.RidgeCV method),
406
get_params()
(sklearn.linear_model.SGDClassiﬁer
method), 1585
get_params()
(sklearn.linear_model.SGDRegressor
method), 1591
get_params()
(sklearn.linear_model.TheilSenRegressor
method), 1595
get_params() (sklearn.manifold.Isomap method), 1611
get_params() (sklearn.manifold.LocallyLinearEmbedding
method), 1608
get_params() (sklearn.manifold.MDS method), 1613
get_params()
(sklearn.manifold.SpectralEmbedding
method), 1616
get_params() (sklearn.manifold.TSNE method), 1619
get_params() (sklearn.mixture.BayesianGaussianMixture
method), 1696
get_params() (sklearn.mixture.DPGMM method), 1955
get_params() (sklearn.mixture.GaussianMixture method),
1690
get_params() (sklearn.mixture.GMM method), 1952
get_params() (sklearn.mixture.VBGMM method), 1959
get_params()
(sklearn.model_selection.GridSearchCV
method), 1231
2030
Index
scikit-learn user guide, Release 0.18.2
get_params() (sklearn.model_selection.RandomizedSearchCV
method), 1237
get_params()
(sklearn.multiclass.OneVsOneClassiﬁer
method), 1702
get_params()
(sklearn.multiclass.OneVsRestClassiﬁer
method), 1699
get_params()
(sklearn.multiclass.OutputCodeClassiﬁer
method), 1704
get_params() (sklearn.multioutput.MultiOutputClassiﬁer
method), 1708
get_params() (sklearn.multioutput.MultiOutputRegressor
method), 1706
get_params()
(sklearn.naive_bayes.BernoulliNB
method), 1718
get_params() (sklearn.naive_bayes.GaussianNB method),
1711
get_params()
(sklearn.naive_bayes.MultinomialNB
method), 1714
get_params() (sklearn.neighbors.KernelDensity method),
1766
get_params()
(sklearn.neighbors.KNeighborsClassiﬁer
method), 1728
get_params()
(sklearn.neighbors.KNeighborsRegressor
method), 1737
get_params()
(sklearn.neighbors.LSHForest
method),
1759
get_params()
(sklearn.neighbors.NearestCentroid
method), 1746
get_params()
(sklearn.neighbors.NearestNeighbors
method), 1722
get_params() (sklearn.neighbors.RadiusNeighborsClassiﬁer
method), 1733
get_params() (sklearn.neighbors.RadiusNeighborsRegressor
method), 1742
get_params()
(sklearn.neural_network.BernoulliRBM
method), 1772
get_params()
(sklearn.neural_network.MLPClassiﬁer
method), 1777
get_params()
(sklearn.neural_network.MLPRegressor
method), 1782
get_params()
(sklearn.pipeline.FeatureUnion
method),
1806
get_params() (sklearn.pipeline.Pipeline method), 1803
get_params() (sklearn.preprocessing.Binarizer method),
1810
get_params() (sklearn.preprocessing.FunctionTransformer
method), 1812
get_params()
(sklearn.preprocessing.Imputer
method),
1814
get_params()
(sklearn.preprocessing.KernelCenterer
method), 1815
get_params()
(sklearn.preprocessing.LabelBinarizer
method), 1817
get_params()
(sklearn.preprocessing.LabelEncoder
method), 1820
get_params()
(sklearn.preprocessing.MaxAbsScaler
method), 1824
get_params()
(sklearn.preprocessing.MinMaxScaler
method), 1826
get_params() (sklearn.preprocessing.MultiLabelBinarizer
method), 1822
get_params()
(sklearn.preprocessing.Normalizer
method), 1828
get_params()
(sklearn.preprocessing.OneHotEncoder
method), 1831
get_params() (sklearn.preprocessing.PolynomialFeatures
method), 1833
get_params()
(sklearn.preprocessing.RobustScaler
method), 1835
get_params()
(sklearn.preprocessing.StandardScaler
method), 1838
get_params() (sklearn.qda.QDA method), 1919
get_params() (sklearn.random_projection.GaussianRandomProjection
method), 1846
get_params() (sklearn.random_projection.SparseRandomProjection
method), 1849
get_params() (sklearn.semi_supervised.LabelPropagation
method), 1852
get_params()
(sklearn.semi_supervised.LabelSpreading
method), 1855
get_params() (sklearn.svm.LinearSVC method), 1866
get_params() (sklearn.svm.LinearSVR method), 1878
get_params() (sklearn.svm.NuSVC method), 1871
get_params() (sklearn.svm.NuSVR method), 1882
get_params() (sklearn.svm.OneClassSVM method), 1885
get_params() (sklearn.svm.SVC method), 1860
get_params() (sklearn.svm.SVR method), 1875
get_params()
(sklearn.tree.DecisionTreeClassiﬁer
method), 1894
get_params()
(sklearn.tree.DecisionTreeRegressor
method), 1901
get_params() (sklearn.tree.ExtraTreeClassiﬁer method),
1905
get_params() (sklearn.tree.ExtraTreeRegressor method),
1909
get_precision()
(sklearn.covariance.EllipticEnvelope
method), 1180
get_precision() (sklearn.covariance.EmpiricalCovariance
method), 1176
get_precision() (sklearn.covariance.GraphLasso method),
1183
get_precision()
(sklearn.covariance.GraphLassoCV
method), 1186
get_precision() (sklearn.covariance.LedoitWolf method),
1189
get_precision() (sklearn.covariance.MinCovDet method),
1193
Index
2031
scikit-learn user guide, Release 0.18.2
get_precision() (sklearn.covariance.OAS method), 1196
get_precision()
(sklearn.covariance.ShrunkCovariance
method), 1199
get_precision()
(sklearn.decomposition.FactorAnalysis
method), 1314
get_precision() (sklearn.decomposition.IncrementalPCA
method), 1304
get_precision()
(sklearn.decomposition.PCA
method),
1299
get_scorer() (in module sklearn.metrics), 1624
get_shape() (sklearn.cluster.bicluster.SpectralBiclustering
method), 1172
get_shape() (sklearn.cluster.bicluster.SpectralCoclustering
method), 1174
get_stop_words() (sklearn.feature_extraction.text.CountVectorizer
method), 1400
get_stop_words() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
get_stop_words() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1412
get_submatrix() (sklearn.cluster.bicluster.SpectralBiclustering
method), 1172
get_submatrix() (sklearn.cluster.bicluster.SpectralCoclustering
method), 1174
get_support() (sklearn.feature_selection.GenericUnivariateSelect
method), 1415
get_support()
(sklearn.feature_selection.RFE
method),
1432
get_support()
(sklearn.feature_selection.RFECV
method), 1436
get_support()
(sklearn.feature_selection.SelectFdr
method), 1425
get_support()
(sklearn.feature_selection.SelectFpr
method), 1422
get_support() (sklearn.feature_selection.SelectFromModel
method), 1427
get_support()
(sklearn.feature_selection.SelectFwe
method), 1429
get_support()
(sklearn.feature_selection.SelectKBest
method), 1420
get_support() (sklearn.feature_selection.SelectPercentile
method), 1417
get_support() (sklearn.feature_selection.VarianceThreshold
method), 1439
get_support()
(sklearn.linear_model.RandomizedLasso
method), 1567
get_support() (sklearn.linear_model.RandomizedLogisticRegression
method), 1570
gibbs() (sklearn.neural_network.BernoulliRBM method),
1772
GMM (class in sklearn.mixture), 1951
GradientBoostingClassiﬁer (class in sklearn.ensemble),
439
GradientBoostingRegressor (class in sklearn.ensemble),
446
graph_lasso() (in module sklearn.covariance), 1202
GraphLasso (class in sklearn.covariance), 1181
GraphLassoCV (class in sklearn.covariance), 1184
grid_to_graph()
(in
module
sklearn.feature_extraction.image), 1393
GridSearchCV (class in sklearn.grid_search), 1923
GridSearchCV (class in sklearn.model_selection), 1227
GroupKFold (class in sklearn.model_selection), 1206
GroupShufﬂeSplit (class in sklearn.model_selection),
1218
H
hamming_loss() (in module sklearn.metrics), 1635
HashingVectorizer
(class
in
sklearn.feature_extraction.text), 1401
hinge_loss() (in module sklearn.metrics), 1636
homogeneity_completeness_v_measure()
(in
module
sklearn.metrics), 1663
homogeneity_score() (in module sklearn.metrics), 1664
HuberRegressor (class in sklearn.linear_model), 1514
Hyperparameter
(class
in
sklearn.gaussian_process.kernels), 1475
hyperparameters (sklearn.gaussian_process.kernels.CompoundKernel
attribute), 1475
hyperparameters (sklearn.gaussian_process.kernels.ConstantKernel
attribute), 1460
hyperparameters (sklearn.gaussian_process.kernels.DotProduct
attribute), 1472
hyperparameters (sklearn.gaussian_process.kernels.Exponentiation
attribute), 1459
hyperparameters (sklearn.gaussian_process.kernels.ExpSineSquared
attribute), 1470
hyperparameters (sklearn.gaussian_process.kernels.Kernel
attribute), 1454
hyperparameters (sklearn.gaussian_process.kernels.Matern
attribute), 1466
hyperparameters (sklearn.gaussian_process.kernels.PairwiseKernel
attribute), 1474
hyperparameters (sklearn.gaussian_process.kernels.Product
attribute), 1457
hyperparameters (sklearn.gaussian_process.kernels.RationalQuadratic
attribute), 1468
hyperparameters (sklearn.gaussian_process.kernels.RBF
attribute), 1464
hyperparameters (sklearn.gaussian_process.kernels.Sum
attribute), 1456
hyperparameters (sklearn.gaussian_process.kernels.WhiteKernel
attribute), 1462
I
img_to_graph()
(in
module
sklearn.feature_extraction.image), 1392
2032
Index
scikit-learn user guide, Release 0.18.2
Imputer (class in sklearn.preprocessing), 1812
IncrementalPCA (class in sklearn.decomposition), 1301
index() (sklearn.gaussian_process.kernels.Hyperparameter
method), 1476
inverse_transform
(sklearn.pipeline.Pipeline
attribute),
1803
inverse_transform() (sklearn.cluster.FeatureAgglomeration
method), 1145
inverse_transform()
(sklearn.decomposition.FastICA
method), 1317
inverse_transform() (sklearn.decomposition.IncrementalPCA
method), 1304
inverse_transform()
(sklearn.decomposition.KernelPCA
method), 1311
inverse_transform()
(sklearn.decomposition.NMF
method), 1324
inverse_transform()
(sklearn.decomposition.PCA
method), 1299
inverse_transform() (sklearn.decomposition.ProjectedGradientNMF
method), 1308
inverse_transform() (sklearn.decomposition.RandomizedPCA
method), 1946
inverse_transform() (sklearn.decomposition.TruncatedSVD
method), 1320
inverse_transform() (sklearn.feature_extraction.DictVectorizer
method), 1388
inverse_transform() (sklearn.feature_extraction.text.CountVectorizer
method), 1400
inverse_transform() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1412
inverse_transform() (sklearn.feature_selection.GenericUnivariateSelect
method), 1415
inverse_transform()
(sklearn.feature_selection.RFE
method), 1433
inverse_transform()
(sklearn.feature_selection.RFECV
method), 1437
inverse_transform() (sklearn.feature_selection.SelectFdr
method), 1425
inverse_transform() (sklearn.feature_selection.SelectFpr
method), 1422
inverse_transform() (sklearn.feature_selection.SelectFromModel
method), 1427
inverse_transform() (sklearn.feature_selection.SelectFwe
method), 1430
inverse_transform() (sklearn.feature_selection.SelectKBest
method), 1420
inverse_transform() (sklearn.feature_selection.SelectPercentile
method), 1417
inverse_transform() (sklearn.feature_selection.VarianceThreshold
method), 1439
inverse_transform() (sklearn.grid_search.GridSearchCV
method), 1927
inverse_transform() (sklearn.grid_search.RandomizedSearchCV
method), 1931
inverse_transform() (sklearn.linear_model.RandomizedLasso
method), 1567
inverse_transform() (sklearn.linear_model.RandomizedLogisticRegression
method), 1570
inverse_transform() (sklearn.model_selection.GridSearchCV
method), 1231
inverse_transform() (sklearn.model_selection.RandomizedSearchCV
method), 1237
inverse_transform() (sklearn.preprocessing.LabelBinarizer
method), 1818
inverse_transform() (sklearn.preprocessing.LabelEncoder
method), 1820
inverse_transform() (sklearn.preprocessing.MaxAbsScaler
method), 1824
inverse_transform() (sklearn.preprocessing.MinMaxScaler
method), 1826
inverse_transform() (sklearn.preprocessing.MultiLabelBinarizer
method), 1822
inverse_transform() (sklearn.preprocessing.RobustScaler
method), 1835
inverse_transform() (sklearn.preprocessing.StandardScaler
method), 1838
is_stationary() (sklearn.gaussian_process.kernels.CompoundKernel
method), 1475
is_stationary() (sklearn.gaussian_process.kernels.ConstantKernel
method), 1460
is_stationary() (sklearn.gaussian_process.kernels.DotProduct
method), 1472
is_stationary() (sklearn.gaussian_process.kernels.Exponentiation
method), 1459
is_stationary() (sklearn.gaussian_process.kernels.ExpSineSquared
method), 1470
is_stationary() (sklearn.gaussian_process.kernels.Kernel
method), 1454
is_stationary() (sklearn.gaussian_process.kernels.Matern
method), 1466
is_stationary() (sklearn.gaussian_process.kernels.PairwiseKernel
method), 1474
is_stationary() (sklearn.gaussian_process.kernels.Product
method), 1457
is_stationary() (sklearn.gaussian_process.kernels.RationalQuadratic
method), 1468
is_stationary()
(sklearn.gaussian_process.kernels.RBF
method), 1464
is_stationary()
(sklearn.gaussian_process.kernels.Sum
method), 1456
is_stationary() (sklearn.gaussian_process.kernels.WhiteKernel
method), 1462
IsolationForest (class in sklearn.ensemble), 1370
Isomap (class in sklearn.manifold), 1609
isotonic_regression() (in module sklearn.isotonic), 1480
IsotonicRegression (class in sklearn.isotonic), 1477
Index
2033
scikit-learn user guide, Release 0.18.2
J
jaccard_similarity_score() (in module sklearn.metrics),
1638
johnson_lindenstrauss_min_dim()
(in
module
sklearn.random_projection), 1850
K
k_means() (in module sklearn.cluster), 1161
KDTree (class in sklearn.neighbors), 1752
Kernel (class in sklearn.gaussian_process.kernels), 1454
kernel_density()
(sklearn.neighbors.BallTree
method),
1749
kernel_density()
(sklearn.neighbors.KDTree
method),
1754
kernel_metrics() (in module sklearn.metrics.pairwise),
1675
KernelCenterer (class in sklearn.preprocessing), 1814
KernelDensity (class in sklearn.neighbors), 1765
KernelPCA (class in sklearn.decomposition), 1309
KernelRidge (class in sklearn.kernel_ridge), 1489
KFold (class in sklearn.cross_validation), 1935
KFold (class in sklearn.model_selection), 1204
KMeans (class in sklearn.cluster), 1147
kneighbors()
(sklearn.neighbors.KNeighborsClassiﬁer
method), 1728
kneighbors()
(sklearn.neighbors.KNeighborsRegressor
method), 1737
kneighbors()
(sklearn.neighbors.LSHForest
method),
1760
kneighbors()
(sklearn.neighbors.NearestNeighbors
method), 1722
kneighbors_graph() (in module sklearn.neighbors), 1767
kneighbors_graph() (sklearn.neighbors.KNeighborsClassiﬁer
method), 1729
kneighbors_graph() (sklearn.neighbors.KNeighborsRegressor
method), 1738
kneighbors_graph()
(sklearn.neighbors.LSHForest
method), 1760
kneighbors_graph() (sklearn.neighbors.NearestNeighbors
method), 1723
KNeighborsClassiﬁer (class in sklearn.neighbors), 1726
KNeighborsRegressor (class in sklearn.neighbors), 1735
L
l1_min_c() (in module sklearn.svm), 1885
label_binarize() (in module sklearn.preprocessing), 1840
label_ranking_average_precision_score()
(in
module
sklearn.metrics), 1656
label_ranking_loss() (in module sklearn.metrics), 1657
LabelBinarizer (class in sklearn.preprocessing), 1815
LabelEncoder (class in sklearn.preprocessing), 1819
LabelKFold (class in sklearn.cross_validation), 1936
LabelPropagation
(class
in
sklearn.semi_supervised),
1851
LabelShufﬂeSplit
(class
in
sklearn.cross_validation),
1939
LabelSpreading (class in sklearn.semi_supervised), 1854
laplacian_kernel() (in module sklearn.metrics.pairwise),
1681
Lars (class in sklearn.linear_model), 1517
lars_path() (in module sklearn.linear_model), 1596
LarsCV (class in sklearn.linear_model), 370
Lasso (class in sklearn.linear_model), 1520
lasso_path() (in module sklearn.linear_model), 1598
lasso_stability_path() (in module sklearn.linear_model),
1600
LassoCV (class in sklearn.linear_model), 374
LassoLars (class in sklearn.linear_model), 1526
LassoLarsCV (class in sklearn.linear_model), 380
LassoLarsIC (class in sklearn.linear_model), 411
LatentDirichletAllocation
(class
in
sklearn.decomposition), 1339
LDA (class in sklearn.lda), 1915
learning_curve()
(in
module
sklearn.learning_curve),
1961
learning_curve() (in module sklearn.model_selection),
1246
LeaveOneGroupOut (class in sklearn.model_selection),
1209
LeaveOneLabelOut (class in sklearn.cross_validation),
1937
LeaveOneOut (class in sklearn.cross_validation), 1933
LeaveOneOut (class in sklearn.model_selection), 1213
LeavePGroupsOut (class in sklearn.model_selection),
1211
LeavePLabelOut (class in sklearn.cross_validation), 1938
LeavePOut (class in sklearn.cross_validation), 1934
LeavePOut (class in sklearn.model_selection), 1214
ledoit_wolf() (in module sklearn.covariance), 1200
LedoitWolf (class in sklearn.covariance), 1187
linear_kernel()
(in
module
sklearn.metrics.pairwise),
1675
LinearDiscriminantAnalysis
(class
in
sklearn.discriminant_analysis), 1493
LinearRegression (class in sklearn.linear_model), 1529
LinearSVC (class in sklearn.svm), 1863
LinearSVR (class in sklearn.svm), 1876
load_boston() (in module sklearn.datasets), 1252
load_breast_cancer() (in module sklearn.datasets), 1253
load_diabetes() (in module sklearn.datasets), 1254
load_digits() (in module sklearn.datasets), 1254
load_ﬁles() (in module sklearn.datasets), 1256
load_iris() (in module sklearn.datasets), 1257
load_lfw_pairs() (in module sklearn.datasets), 1920
load_lfw_people() (in module sklearn.datasets), 1920
load_linnerud() (in module sklearn.datasets), 1261
load_mlcomp() (in module sklearn.datasets), 1268
load_sample_image() (in module sklearn.datasets), 1269
2034
Index
scikit-learn user guide, Release 0.18.2
load_sample_images() (in module sklearn.datasets), 1270
load_svmlight_ﬁle() (in module sklearn.datasets), 1271
load_svmlight_ﬁles() (in module sklearn.datasets), 1273
locally_linear_embedding()
(in
module
sklearn.manifold), 1620
LocallyLinearEmbedding (class in sklearn.manifold),
1606
log_loss() (in module sklearn.metrics), 1639
log_marginal_likelihood()
(sklearn.gaussian_process.GaussianProcessClassiﬁer
method), 1452
log_marginal_likelihood()
(sklearn.gaussian_process.GaussianProcessRegressor
method), 1447
logistic_regression_path()
(in
module
sklearn.linear_model), 1601
LogisticRegression (class in sklearn.linear_model), 1532
LogisticRegressionCV (class in sklearn.linear_model),
383
lower_bound() (sklearn.mixture.DPGMM method), 1955
lower_bound() (sklearn.mixture.VBGMM method), 1959
LSHForest (class in sklearn.neighbors), 1757
M
mahalanobis()
(sklearn.covariance.EllipticEnvelope
method), 1180
mahalanobis() (sklearn.covariance.EmpiricalCovariance
method), 1176
mahalanobis() (sklearn.covariance.GraphLasso method),
1183
mahalanobis()
(sklearn.covariance.GraphLassoCV
method), 1186
mahalanobis() (sklearn.covariance.LedoitWolf method),
1189
mahalanobis() (sklearn.covariance.MinCovDet method),
1193
mahalanobis() (sklearn.covariance.OAS method), 1196
mahalanobis()
(sklearn.covariance.ShrunkCovariance
method), 1199
make_biclusters() (in module sklearn.datasets), 1293
make_blobs() (in module sklearn.datasets), 1275
make_checkerboard() (in module sklearn.datasets), 1294
make_circles() (in module sklearn.datasets), 1279
make_classiﬁcation() (in module sklearn.datasets), 1277
make_friedman1() (in module sklearn.datasets), 1280
make_friedman2() (in module sklearn.datasets), 1281
make_friedman3() (in module sklearn.datasets), 1281
make_gaussian_quantiles() (in module sklearn.datasets),
1282
make_hastie_10_2() (in module sklearn.datasets), 1283
make_low_rank_matrix() (in module sklearn.datasets),
1284
make_moons() (in module sklearn.datasets), 1285
make_multilabel_classiﬁcation()
(in
module
sklearn.datasets), 1286
make_pipeline() (in module sklearn.pipeline), 1807
make_regression() (in module sklearn.datasets), 1287
make_s_curve() (in module sklearn.datasets), 1289
make_scorer() (in module sklearn.metrics), 1623
make_sparse_coded_signal()
(in
module
sklearn.datasets), 1289
make_sparse_spd_matrix() (in module sklearn.datasets),
1290
make_sparse_uncorrelated() (in module sklearn.datasets),
1291
make_spd_matrix() (in module sklearn.datasets), 1291
make_swiss_roll() (in module sklearn.datasets), 1292
make_union() (in module sklearn.pipeline), 1808
manhattan_distances()
(in
module
sklearn.metrics.pairwise), 1676
Matern (class in sklearn.gaussian_process.kernels), 1465
matthews_corrcoef() (in module sklearn.metrics), 1640
maxabs_scale() (in module sklearn.preprocessing), 1841
MaxAbsScaler (class in sklearn.preprocessing), 1823
MDS (class in sklearn.manifold), 1612
mean_absolute_error() (in module sklearn.metrics), 1652
mean_shift() (in module sklearn.cluster), 1166
mean_squared_error() (in module sklearn.metrics), 1653
MeanShift (class in sklearn.cluster), 1155
median_absolute_error() (in module sklearn.metrics),
1654
MinCovDet (class in sklearn.covariance), 1190
MiniBatchDictionaryLearning
(class
in
sklearn.decomposition), 1335
MiniBatchKMeans (class in sklearn.cluster), 1151
MiniBatchSparsePCA (class in sklearn.decomposition),
1327
minmax_scale() (in module sklearn.preprocessing), 1842
MinMaxScaler (class in sklearn.preprocessing), 1824
mldata_ﬁlename() (in module sklearn.datasets), 1262
MLPClassiﬁer (class in sklearn.neural_network), 1773
MLPRegressor (class in sklearn.neural_network), 1779
multilabel_ (sklearn.multiclass.OneVsRestClassiﬁer at-
tribute), 1699
MultiLabelBinarizer
(class
in
sklearn.preprocessing),
1820
MultinomialNB (class in sklearn.naive_bayes), 1713
MultiOutputClassiﬁer
(class
in
sklearn.multioutput),
1707
MultiOutputRegressor
(class
in
sklearn.multioutput),
1705
MultiTaskElasticNet
(class
in
sklearn.linear_model),
1544
MultiTaskElasticNetCV (class in sklearn.linear_model),
390
MultiTaskLasso (class in sklearn.linear_model), 1539
MultiTaskLassoCV (class in sklearn.linear_model), 396
Index
2035
scikit-learn user guide, Release 0.18.2
mutual_info_classif()
(in
module
sklearn.feature_selection), 1442
mutual_info_regression()
(in
module
sklearn.feature_selection), 1443
mutual_info_score() (in module sklearn.metrics), 1665
N
n_dims (sklearn.gaussian_process.kernels.CompoundKernel
attribute), 1475
n_dims (sklearn.gaussian_process.kernels.ConstantKernel
attribute), 1460
n_dims (sklearn.gaussian_process.kernels.DotProduct at-
tribute), 1472
n_dims (sklearn.gaussian_process.kernels.Exponentiation
attribute), 1459
n_dims (sklearn.gaussian_process.kernels.ExpSineSquared
attribute), 1470
n_dims
(sklearn.gaussian_process.kernels.Kernel
at-
tribute), 1454
n_dims
(sklearn.gaussian_process.kernels.Matern
at-
tribute), 1466
n_dims (sklearn.gaussian_process.kernels.PairwiseKernel
attribute), 1474
n_dims
(sklearn.gaussian_process.kernels.Product
attribute), 1457
n_dims (sklearn.gaussian_process.kernels.RationalQuadratic
attribute), 1468
n_dims (sklearn.gaussian_process.kernels.RBF attribute),
1464
n_dims (sklearn.gaussian_process.kernels.Sum attribute),
1456
n_dims
(sklearn.gaussian_process.kernels.WhiteKernel
attribute), 1462
n_elements (sklearn.gaussian_process.kernels.Hyperparameter
attribute), 1476
name (sklearn.gaussian_process.kernels.Hyperparameter
attribute), 1476
NearestCentroid (class in sklearn.neighbors), 1744
NearestNeighbors (class in sklearn.neighbors), 1720
NMF (class in sklearn.decomposition), 1321
NonBLASDotWarning
(class
in
sklearn.exceptions),
1385
normalize() (in module sklearn.preprocessing), 1842
normalized_mutual_info_score()
(in
module
sklearn.metrics), 1666
Normalizer (class in sklearn.preprocessing), 1827
NotFittedError (class in sklearn.exceptions), 1383
NuSVC (class in sklearn.svm), 1868
NuSVR (class in sklearn.svm), 1879
Nystroem (class in sklearn.kernel_approximation), 1483
O
OAS (class in sklearn.covariance), 1194
oas() (in module sklearn.covariance), 1201
OneClassSVM (class in sklearn.svm), 1883
OneHotEncoder (class in sklearn.preprocessing), 1829
OneVsOneClassiﬁer (class in sklearn.multiclass), 1701
OneVsRestClassiﬁer (class in sklearn.multiclass), 1698
orthogonal_mp() (in module sklearn.linear_model), 1603
orthogonal_mp_gram()
(in
module
sklearn.linear_model), 1605
OrthogonalMatchingPursuit
(class
in
sklearn.linear_model), 1549
OrthogonalMatchingPursuitCV
(class
in
sklearn.linear_model), 401
OutputCodeClassiﬁer (class in sklearn.multiclass), 1703
P
paired_cosine_distances()
(in
module
sklearn.metrics.pairwise), 1685
paired_distances() (in module sklearn.metrics.pairwise),
1685
paired_euclidean_distances()
(in
module
sklearn.metrics.pairwise), 1685
paired_manhattan_distances()
(in
module
sklearn.metrics.pairwise), 1685
pairwise() (sklearn.neighbors.DistanceMetric method),
1764
pairwise_distances() (in module sklearn.metrics), 1681
pairwise_distances()
(in
module
sklearn.metrics.pairwise), 1676
pairwise_distances_argmin() (in module sklearn.metrics),
1682
pairwise_distances_argmin_min()
(in
module
sklearn.metrics), 1683
pairwise_kernels() (in module sklearn.metrics.pairwise),
1678
PairwiseKernel
(class
in
sklearn.gaussian_process.kernels), 1472
ParameterGrid (class in sklearn.grid_search), 1921
ParameterGrid (class in sklearn.model_selection), 1238
ParameterSampler (class in sklearn.grid_search), 1922
ParameterSampler (class in sklearn.model_selection),
1239
partial_dependence()
(in
module
sklearn.ensemble.partial_dependence), 1380
partial_ﬁt
(sklearn.neural_network.MLPClassiﬁer
at-
tribute), 1777
partial_ﬁt
(sklearn.neural_network.MLPRegressor
attribute), 1782
partial_ﬁt() (sklearn.cluster.Birch method), 1140
partial_ﬁt() (sklearn.cluster.MiniBatchKMeans method),
1154
partial_ﬁt()
(sklearn.decomposition.IncrementalPCA
method), 1304
partial_ﬁt() (sklearn.decomposition.LatentDirichletAllocation
method), 1342
2036
Index
scikit-learn user guide, Release 0.18.2
partial_ﬁt() (sklearn.decomposition.MiniBatchDictionaryLearning
method), 1338
partial_ﬁt() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
partial_ﬁt() (sklearn.feature_selection.SelectFromModel
method), 1427
partial_ﬁt() (sklearn.linear_model.PassiveAggressiveClassiﬁer
method), 1554
partial_ﬁt() (sklearn.linear_model.PassiveAggressiveRegressor
method), 1558
partial_ﬁt() (sklearn.linear_model.Perceptron method),
1562
partial_ﬁt()
(sklearn.linear_model.SGDClassiﬁer
method), 1585
partial_ﬁt()
(sklearn.linear_model.SGDRegressor
method), 1591
partial_ﬁt()
(sklearn.multiclass.OneVsOneClassiﬁer
method), 1702
partial_ﬁt()
(sklearn.multiclass.OneVsRestClassiﬁer
method), 1699
partial_ﬁt() (sklearn.naive_bayes.BernoulliNB method),
1718
partial_ﬁt() (sklearn.naive_bayes.GaussianNB method),
1711
partial_ﬁt()
(sklearn.naive_bayes.MultinomialNB
method), 1714
partial_ﬁt() (sklearn.neighbors.LSHForest method), 1761
partial_ﬁt()
(sklearn.neural_network.BernoulliRBM
method), 1772
partial_ﬁt()
(sklearn.preprocessing.MaxAbsScaler
method), 1824
partial_ﬁt()
(sklearn.preprocessing.MinMaxScaler
method), 1826
partial_ﬁt()
(sklearn.preprocessing.StandardScaler
method), 1838
PassiveAggressiveClassiﬁer
(class
in
sklearn.linear_model), 1552
PassiveAggressiveRegressor
(class
in
sklearn.linear_model), 1556
PatchExtractor
(class
in
sklearn.feature_extraction.image), 1395
path() (sklearn.linear_model.ElasticNet static method),
1511
path()
(sklearn.linear_model.ElasticNetCV
static
method), 368
path() (sklearn.linear_model.Lasso static method), 1523
path() (sklearn.linear_model.LassoCV static method),
376
path()
(sklearn.linear_model.MultiTaskElasticNet
method), 1547
path()
(sklearn.linear_model.MultiTaskElasticNetCV
static method), 393
path() (sklearn.linear_model.MultiTaskLasso
method),
1541
path()
(sklearn.linear_model.MultiTaskLassoCV
static
method), 398
PCA (class in sklearn.decomposition), 1295
Perceptron (class in sklearn.linear_model), 1559
permutation_test_score()
(in
module
sklearn.cross_validation), 1968
permutation_test_score()
(in
module
sklearn.model_selection), 1244
perplexity() (sklearn.decomposition.LatentDirichletAllocation
method), 1342
Pipeline (class in sklearn.pipeline), 1801
plot_partial_dependence()
(in
module
sklearn.ensemble.partial_dependence), 1381
PLSCanonical (class in sklearn.cross_decomposition),
1791
PLSRegression (class in sklearn.cross_decomposition),
1787
PLSSVD (class in sklearn.cross_decomposition), 1799
polynomial_kernel()
(in
module
sklearn.metrics.pairwise), 1679
PolynomialFeatures
(class
in
sklearn.preprocessing),
1831
pooling_func()
(sklearn.cluster.FeatureAgglomeration
method), 1145
precision_recall_curve()
(in
module
sklearn.metrics),
1641
precision_recall_fscore_support()
(in
module
sklearn.metrics), 1642
precision_score() (in module sklearn.metrics), 1644
PredeﬁnedSplit (class in sklearn.cross_validation), 1943
PredeﬁnedSplit (class in sklearn.model_selection), 1221
predict() (in module sklearn.svm.libsvm), 1888
predict()
(sklearn.calibration.CalibratedClassiﬁerCV
method), 1785
predict()
(sklearn.cluster.AfﬁnityPropagation
method),
1135
predict() (sklearn.cluster.Birch method), 1140
predict() (sklearn.cluster.KMeans method), 1150
predict() (sklearn.cluster.MeanShift method), 1157
predict()
(sklearn.cluster.MiniBatchKMeans
method),
1154
predict() (sklearn.covariance.EllipticEnvelope method),
1180
predict()
(sklearn.cross_decomposition.CCA
method),
1798
predict()
(sklearn.cross_decomposition.PLSCanonical
method), 1794
predict()
(sklearn.cross_decomposition.PLSRegression
method), 1790
predict() (sklearn.discriminant_analysis.LinearDiscriminantAnalysis
method), 1496
predict() (sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
method), 1499
Index
2037
scikit-learn user guide, Release 0.18.2
predict()
(sklearn.dummy.DummyClassiﬁer
method),
1351
predict()
(sklearn.dummy.DummyRegressor
method),
1353
predict() (sklearn.ensemble.AdaBoostClassiﬁer method),
1357
predict() (sklearn.ensemble.AdaBoostRegressor method),
1361
predict() (sklearn.ensemble.BaggingClassiﬁer method),
1366
predict() (sklearn.ensemble.BaggingRegressor method),
1370
predict()
(sklearn.ensemble.ExtraTreesClassiﬁer
method), 431
predict()
(sklearn.ensemble.ExtraTreesRegressor
method), 437
predict()
(sklearn.ensemble.GradientBoostingClassiﬁer
method), 444
predict()
(sklearn.ensemble.GradientBoostingRegressor
method), 451
predict()
(sklearn.ensemble.IsolationForest
method),
1373
predict()
(sklearn.ensemble.RandomForestClassiﬁer
method), 419
predict()
(sklearn.ensemble.RandomForestRegressor
method), 425
predict()
(sklearn.ensemble.VotingClassiﬁer
method),
1379
predict() (sklearn.feature_selection.RFE method), 1433
predict()
(sklearn.feature_selection.RFECV
method),
1437
predict()
(sklearn.gaussian_process.GaussianProcess
method), 1950
predict() (sklearn.gaussian_process.GaussianProcessClassiﬁer
method), 1452
predict() (sklearn.gaussian_process.GaussianProcessRegressor
method), 1447
predict()
(sklearn.grid_search.GridSearchCV
method),
1927
predict()
(sklearn.grid_search.RandomizedSearchCV
method), 1932
predict() (sklearn.isotonic.IsotonicRegression method),
1479
predict()
(sklearn.kernel_ridge.KernelRidge
method),
1491
predict() (sklearn.lda.LDA method), 1917
predict() (sklearn.linear_model.ARDRegression method),
1504
predict() (sklearn.linear_model.BayesianRidge method),
1508
predict() (sklearn.linear_model.ElasticNet method), 1513
predict() (sklearn.linear_model.ElasticNetCV method),
370
predict() (sklearn.linear_model.HuberRegressor method),
1516
predict() (sklearn.linear_model.Lars method), 1519
predict() (sklearn.linear_model.LarsCV method), 373
predict() (sklearn.linear_model.Lasso method), 1525
predict() (sklearn.linear_model.LassoCV method), 379
predict() (sklearn.linear_model.LassoLars method), 1528
predict() (sklearn.linear_model.LassoLarsCV method),
382
predict()
(sklearn.linear_model.LassoLarsIC
method),
413
predict()
(sklearn.linear_model.LinearRegression
method), 1531
predict()
(sklearn.linear_model.LogisticRegression
method), 1536
predict()
(sklearn.linear_model.LogisticRegressionCV
method), 388
predict()
(sklearn.linear_model.MultiTaskElasticNet
method), 1548
predict()
(sklearn.linear_model.MultiTaskElasticNetCV
method), 395
predict() (sklearn.linear_model.MultiTaskLasso method),
1543
predict()
(sklearn.linear_model.MultiTaskLassoCV
method), 400
predict() (sklearn.linear_model.OrthogonalMatchingPursuit
method), 1551
predict() (sklearn.linear_model.OrthogonalMatchingPursuitCV
method), 403
predict() (sklearn.linear_model.PassiveAggressiveClassiﬁer
method), 1554
predict() (sklearn.linear_model.PassiveAggressiveRegressor
method), 1558
predict() (sklearn.linear_model.Perceptron method), 1562
predict()
(sklearn.linear_model.RANSACRegressor
method), 1573
predict() (sklearn.linear_model.Ridge method), 1576
predict() (sklearn.linear_model.RidgeClassiﬁer method),
1580
predict()
(sklearn.linear_model.RidgeClassiﬁerCV
method), 410
predict() (sklearn.linear_model.RidgeCV method), 407
predict() (sklearn.linear_model.SGDClassiﬁer method),
1585
predict() (sklearn.linear_model.SGDRegressor method),
1591
predict()
(sklearn.linear_model.TheilSenRegressor
method), 1595
predict()
(sklearn.mixture.BayesianGaussianMixture
method), 1696
predict() (sklearn.mixture.DPGMM method), 1955
predict()
(sklearn.mixture.GaussianMixture
method),
1690
predict() (sklearn.mixture.GMM method), 1953
2038
Index
scikit-learn user guide, Release 0.18.2
predict() (sklearn.mixture.VBGMM method), 1959
predict()
(sklearn.model_selection.GridSearchCV
method), 1231
predict() (sklearn.model_selection.RandomizedSearchCV
method), 1237
predict()
(sklearn.multiclass.OneVsOneClassiﬁer
method), 1702
predict()
(sklearn.multiclass.OneVsRestClassiﬁer
method), 1700
predict()
(sklearn.multiclass.OutputCodeClassiﬁer
method), 1704
predict()
(sklearn.multioutput.MultiOutputClassiﬁer
method), 1708
predict()
(sklearn.multioutput.MultiOutputRegressor
method), 1706
predict()
(sklearn.naive_bayes.BernoulliNB
method),
1718
predict()
(sklearn.naive_bayes.GaussianNB
method),
1711
predict() (sklearn.naive_bayes.MultinomialNB method),
1715
predict()
(sklearn.neighbors.KNeighborsClassiﬁer
method), 1729
predict()
(sklearn.neighbors.KNeighborsRegressor
method), 1739
predict()
(sklearn.neighbors.NearestCentroid
method),
1746
predict()
(sklearn.neighbors.RadiusNeighborsClassiﬁer
method), 1733
predict()
(sklearn.neighbors.RadiusNeighborsRegressor
method), 1742
predict()
(sklearn.neural_network.MLPClassiﬁer
method), 1777
predict()
(sklearn.neural_network.MLPRegressor
method), 1782
predict() (sklearn.pipeline.Pipeline method), 1804
predict() (sklearn.qda.QDA method), 1919
predict()
(sklearn.semi_supervised.LabelPropagation
method), 1853
predict()
(sklearn.semi_supervised.LabelSpreading
method), 1856
predict() (sklearn.svm.LinearSVC method), 1866
predict() (sklearn.svm.LinearSVR method), 1878
predict() (sklearn.svm.NuSVC method), 1871
predict() (sklearn.svm.NuSVR method), 1882
predict() (sklearn.svm.OneClassSVM method), 1885
predict() (sklearn.svm.SVC method), 1860
predict() (sklearn.svm.SVR method), 1875
predict()
(sklearn.tree.DecisionTreeClassiﬁer
method),
1894
predict() (sklearn.tree.DecisionTreeRegressor method),
1901
predict() (sklearn.tree.ExtraTreeClassiﬁer method), 1905
predict() (sklearn.tree.ExtraTreeRegressor method), 1909
predict_log_proba
(sklearn.linear_model.SGDClassiﬁer
attribute), 1585
predict_log_proba (sklearn.svm.NuSVC attribute), 1871
predict_log_proba (sklearn.svm.SVC attribute), 1860
predict_log_proba() (sklearn.discriminant_analysis.LinearDiscriminantAnal
method), 1496
predict_log_proba() (sklearn.discriminant_analysis.QuadraticDiscriminantA
method), 1499
predict_log_proba()
(sklearn.dummy.DummyClassiﬁer
method), 1351
predict_log_proba() (sklearn.ensemble.AdaBoostClassiﬁer
method), 1357
predict_log_proba() (sklearn.ensemble.BaggingClassiﬁer
method), 1366
predict_log_proba() (sklearn.ensemble.ExtraTreesClassiﬁer
method), 432
predict_log_proba() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 444
predict_log_proba() (sklearn.ensemble.RandomForestClassiﬁer
method), 419
predict_log_proba() (sklearn.grid_search.GridSearchCV
method), 1927
predict_log_proba() (sklearn.grid_search.RandomizedSearchCV
method), 1932
predict_log_proba() (sklearn.lda.LDA method), 1917
predict_log_proba() (sklearn.linear_model.LogisticRegression
method), 1536
predict_log_proba() (sklearn.linear_model.LogisticRegressionCV
method), 388
predict_log_proba() (sklearn.model_selection.GridSearchCV
method), 1231
predict_log_proba() (sklearn.model_selection.RandomizedSearchCV
method), 1237
predict_log_proba()
(sklearn.naive_bayes.BernoulliNB
method), 1719
predict_log_proba()
(sklearn.naive_bayes.GaussianNB
method), 1711
predict_log_proba() (sklearn.naive_bayes.MultinomialNB
method), 1715
predict_log_proba() (sklearn.neural_network.MLPClassiﬁer
method), 1777
predict_log_proba() (sklearn.pipeline.Pipeline method),
1804
predict_log_proba() (sklearn.qda.QDA method), 1919
predict_log_proba() (sklearn.tree.DecisionTreeClassiﬁer
method), 1895
predict_log_proba()
(sklearn.tree.ExtraTreeClassiﬁer
method), 1905
predict_proba
(sklearn.ensemble.VotingClassiﬁer
at-
tribute), 1379
predict_proba
(sklearn.linear_model.SGDClassiﬁer
at-
tribute), 1585
predict_proba (sklearn.svm.NuSVC attribute), 1871
predict_proba (sklearn.svm.SVC attribute), 1861
Index
2039
scikit-learn user guide, Release 0.18.2
predict_proba() (in module sklearn.svm.libsvm), 1888
predict_proba() (sklearn.calibration.CalibratedClassiﬁerCV
method), 1785
predict_proba() (sklearn.discriminant_analysis.LinearDiscriminantAnalysis
method), 1496
predict_proba() (sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
method), 1500
predict_proba()
(sklearn.dummy.DummyClassiﬁer
method), 1351
predict_proba()
(sklearn.ensemble.AdaBoostClassiﬁer
method), 1357
predict_proba()
(sklearn.ensemble.BaggingClassiﬁer
method), 1366
predict_proba()
(sklearn.ensemble.ExtraTreesClassiﬁer
method), 432
predict_proba() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 444
predict_proba() (sklearn.ensemble.RandomForestClassiﬁer
method), 420
predict_proba() (sklearn.gaussian_process.GaussianProcessClassiﬁer
method), 1452
predict_proba()
(sklearn.grid_search.GridSearchCV
method), 1927
predict_proba() (sklearn.grid_search.RandomizedSearchCV
method), 1932
predict_proba() (sklearn.lda.LDA method), 1917
predict_proba() (sklearn.linear_model.LogisticRegression
method), 1537
predict_proba() (sklearn.linear_model.LogisticRegressionCV
method), 388
predict_proba() (sklearn.mixture.BayesianGaussianMixture
method), 1696
predict_proba()
(sklearn.mixture.DPGMM
method),
1955
predict_proba()
(sklearn.mixture.GaussianMixture
method), 1690
predict_proba() (sklearn.mixture.GMM method), 1953
predict_proba()
(sklearn.mixture.VBGMM
method),
1959
predict_proba() (sklearn.model_selection.GridSearchCV
method), 1232
predict_proba() (sklearn.model_selection.RandomizedSearchCV
method), 1237
predict_proba() (sklearn.multiclass.OneVsRestClassiﬁer
method), 1700
predict_proba() (sklearn.multioutput.MultiOutputClassiﬁer
method), 1708
predict_proba()
(sklearn.naive_bayes.BernoulliNB
method), 1719
predict_proba()
(sklearn.naive_bayes.GaussianNB
method), 1712
predict_proba()
(sklearn.naive_bayes.MultinomialNB
method), 1715
predict_proba() (sklearn.neighbors.KNeighborsClassiﬁer
method), 1730
predict_proba()
(sklearn.neural_network.MLPClassiﬁer
method), 1777
predict_proba() (sklearn.pipeline.Pipeline method), 1804
predict_proba() (sklearn.qda.QDA method), 1919
predict_proba() (sklearn.semi_supervised.LabelPropagation
method), 1853
predict_proba() (sklearn.semi_supervised.LabelSpreading
method), 1856
predict_proba()
(sklearn.tree.DecisionTreeClassiﬁer
method), 1895
predict_proba()
(sklearn.tree.ExtraTreeClassiﬁer
method), 1905
Product (class in sklearn.gaussian_process.kernels), 1456
ProjectedGradientNMF (class in sklearn.decomposition),
1305
Q
QDA (class in sklearn.qda), 1918
QuadraticDiscriminantAnalysis
(class
in
sklearn.discriminant_analysis), 1497
query() (sklearn.neighbors.BallTree method), 1749
query() (sklearn.neighbors.KDTree method), 1755
query_radius()
(sklearn.neighbors.BallTree
method),
1750
query_radius() (sklearn.neighbors.KDTree method), 1756
R
r2_score() (in module sklearn.metrics), 1654
radius_neighbors()
(sklearn.neighbors.LSHForest
method), 1761
radius_neighbors() (sklearn.neighbors.NearestNeighbors
method), 1724
radius_neighbors() (sklearn.neighbors.RadiusNeighborsClassiﬁer
method), 1733
radius_neighbors() (sklearn.neighbors.RadiusNeighborsRegressor
method), 1742
radius_neighbors_graph() (in module sklearn.neighbors),
1769
radius_neighbors_graph() (sklearn.neighbors.LSHForest
method), 1761
radius_neighbors_graph()
(sklearn.neighbors.NearestNeighbors method),
1725
radius_neighbors_graph()
(sklearn.neighbors.RadiusNeighborsClassiﬁer
method), 1734
radius_neighbors_graph()
(sklearn.neighbors.RadiusNeighborsRegressor
method), 1743
RadiusNeighborsClassiﬁer (class in sklearn.neighbors),
1731
2040
Index
scikit-learn user guide, Release 0.18.2
RadiusNeighborsRegressor (class in sklearn.neighbors),
1740
RandomForestClassiﬁer (class in sklearn.ensemble), 415
RandomForestRegressor (class in sklearn.ensemble), 421
RandomizedLasso (class in sklearn.linear_model), 1564
RandomizedLogisticRegression
(class
in
sklearn.linear_model), 1567
RandomizedPCA (class in sklearn.decomposition), 1944
RandomizedSearchCV (class in sklearn.grid_search),
1928
RandomizedSearchCV
(class
in
sklearn.model_selection), 1233
RandomTreesEmbedding (class in sklearn.ensemble),
1373
RANSACRegressor (class in sklearn.linear_model), 1570
RationalQuadratic
(class
in
sklearn.gaussian_process.kernels), 1467
RBF (class in sklearn.gaussian_process.kernels), 1463
rbf_kernel() (in module sklearn.metrics.pairwise), 1679
RBFSampler (class in sklearn.kernel_approximation),
1486
rdist_to_dist()
(sklearn.neighbors.DistanceMetric
method), 1765
recall_score() (in module sklearn.metrics), 1645
reconstruct_from_patches_2d()
(in
module
sklearn.feature_extraction.image), 1394
reconstruction_error()
(sklearn.manifold.Isomap
method), 1611
reduced_likelihood_function()
(sklearn.gaussian_process.GaussianProcess
method), 1950
RegressorMixin (class in sklearn.base), 1131
resample() (in module sklearn.utils), 1913
residues_
(sklearn.linear_model.LinearRegression
attribute), 1531
restrict()
(sklearn.feature_extraction.DictVectorizer
method), 1388
reweight_covariance() (sklearn.covariance.EllipticEnvelope
method), 1180
reweight_covariance()
(sklearn.covariance.MinCovDet
method), 1193
RFE (class in sklearn.feature_selection), 1430
RFECV (class in sklearn.feature_selection), 1434
Ridge (class in sklearn.linear_model), 1574
RidgeClassiﬁer (class in sklearn.linear_model), 1577
RidgeClassiﬁerCV (class in sklearn.linear_model), 407
RidgeCV (class in sklearn.linear_model), 404
robust_scale() (in module sklearn.preprocessing), 1843
RobustScaler (class in sklearn.preprocessing), 1834
roc_auc_score() (in module sklearn.metrics), 1647
roc_curve() (in module sklearn.metrics), 1648
S
sample()
(sklearn.mixture.BayesianGaussianMixture
method), 1696
sample() (sklearn.mixture.DPGMM method), 1955
sample()
(sklearn.mixture.GaussianMixture
method),
1690
sample() (sklearn.mixture.GMM method), 1953
sample() (sklearn.mixture.VBGMM method), 1959
sample()
(sklearn.neighbors.KernelDensity
method),
1766
sample_y() (sklearn.gaussian_process.GaussianProcessRegressor
method), 1448
scale() (in module sklearn.preprocessing), 1844
score() (sklearn.base.ClassiﬁerMixin method), 1130
score() (sklearn.base.RegressorMixin method), 1131
score()
(sklearn.calibration.CalibratedClassiﬁerCV
method), 1785
score() (sklearn.cluster.KMeans method), 1150
score() (sklearn.cluster.MiniBatchKMeans method), 1154
score()
(sklearn.covariance.EllipticEnvelope
method),
1181
score()
(sklearn.covariance.EmpiricalCovariance
method), 1177
score() (sklearn.covariance.GraphLasso method), 1183
score()
(sklearn.covariance.GraphLassoCV
method),
1187
score() (sklearn.covariance.LedoitWolf method), 1189
score() (sklearn.covariance.MinCovDet method), 1193
score() (sklearn.covariance.OAS method), 1196
score() (sklearn.covariance.ShrunkCovariance method),
1199
score()
(sklearn.cross_decomposition.CCA
method),
1798
score()
(sklearn.cross_decomposition.PLSCanonical
method), 1795
score()
(sklearn.cross_decomposition.PLSRegression
method), 1790
score() (sklearn.decomposition.FactorAnalysis method),
1314
score() (sklearn.decomposition.LatentDirichletAllocation
method), 1342
score() (sklearn.decomposition.PCA method), 1299
score() (sklearn.discriminant_analysis.LinearDiscriminantAnalysis
method), 1496
score() (sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
method), 1500
score() (sklearn.dummy.DummyClassiﬁer method), 1351
score() (sklearn.dummy.DummyRegressor method), 1353
score() (sklearn.ensemble.AdaBoostClassiﬁer method),
1358
score() (sklearn.ensemble.AdaBoostRegressor method),
1361
score()
(sklearn.ensemble.BaggingClassiﬁer
method),
1366
Index
2041
scikit-learn user guide, Release 0.18.2
score()
(sklearn.ensemble.BaggingRegressor
method),
1370
score() (sklearn.ensemble.ExtraTreesClassiﬁer method),
432
score() (sklearn.ensemble.ExtraTreesRegressor method),
438
score()
(sklearn.ensemble.GradientBoostingClassiﬁer
method), 444
score()
(sklearn.ensemble.GradientBoostingRegressor
method), 451
score()
(sklearn.ensemble.RandomForestClassiﬁer
method), 420
score()
(sklearn.ensemble.RandomForestRegressor
method), 426
score() (sklearn.ensemble.VotingClassiﬁer method), 1379
score() (sklearn.feature_selection.RFE method), 1433
score() (sklearn.feature_selection.RFECV method), 1437
score()
(sklearn.gaussian_process.GaussianProcess
method), 1951
score() (sklearn.gaussian_process.GaussianProcessClassiﬁer
method), 1452
score() (sklearn.gaussian_process.GaussianProcessRegressor
method), 1448
score()
(sklearn.grid_search.GridSearchCV
method),
1927
score()
(sklearn.grid_search.RandomizedSearchCV
method), 1932
score()
(sklearn.isotonic.IsotonicRegression
method),
1479
score() (sklearn.kernel_ridge.KernelRidge method), 1492
score() (sklearn.lda.LDA method), 1917
score() (sklearn.linear_model.ARDRegression method),
1504
score() (sklearn.linear_model.BayesianRidge method),
1508
score() (sklearn.linear_model.ElasticNet method), 1513
score()
(sklearn.linear_model.ElasticNetCV
method),
370
score() (sklearn.linear_model.HuberRegressor method),
1516
score() (sklearn.linear_model.Lars method), 1519
score() (sklearn.linear_model.LarsCV method), 373
score() (sklearn.linear_model.Lasso method), 1525
score() (sklearn.linear_model.LassoCV method), 379
score() (sklearn.linear_model.LassoLars method), 1529
score() (sklearn.linear_model.LassoLarsCV method), 382
score() (sklearn.linear_model.LassoLarsIC method), 414
score() (sklearn.linear_model.LinearRegression method),
1531
score()
(sklearn.linear_model.LogisticRegression
method), 1537
score()
(sklearn.linear_model.LogisticRegressionCV
method), 388
score()
(sklearn.linear_model.MultiTaskElasticNet
method), 1549
score()
(sklearn.linear_model.MultiTaskElasticNetCV
method), 395
score() (sklearn.linear_model.MultiTaskLasso method),
1543
score()
(sklearn.linear_model.MultiTaskLassoCV
method), 401
score() (sklearn.linear_model.OrthogonalMatchingPursuit
method), 1551
score() (sklearn.linear_model.OrthogonalMatchingPursuitCV
method), 403
score() (sklearn.linear_model.PassiveAggressiveClassiﬁer
method), 1555
score() (sklearn.linear_model.PassiveAggressiveRegressor
method), 1558
score() (sklearn.linear_model.Perceptron method), 1562
score()
(sklearn.linear_model.RANSACRegressor
method), 1573
score() (sklearn.linear_model.Ridge method), 1577
score() (sklearn.linear_model.RidgeClassiﬁer method),
1580
score()
(sklearn.linear_model.RidgeClassiﬁerCV
method), 410
score() (sklearn.linear_model.RidgeCV method), 407
score()
(sklearn.linear_model.SGDClassiﬁer
method),
1586
score()
(sklearn.linear_model.SGDRegressor
method),
1592
score()
(sklearn.linear_model.TheilSenRegressor
method), 1595
score()
(sklearn.mixture.BayesianGaussianMixture
method), 1696
score() (sklearn.mixture.DPGMM method), 1956
score() (sklearn.mixture.GaussianMixture method), 1690
score() (sklearn.mixture.GMM method), 1953
score() (sklearn.mixture.VBGMM method), 1959
score() (sklearn.model_selection.GridSearchCV method),
1232
score() (sklearn.model_selection.RandomizedSearchCV
method), 1237
score() (sklearn.multiclass.OneVsOneClassiﬁer method),
1702
score() (sklearn.multiclass.OneVsRestClassiﬁer method),
1700
score()
(sklearn.multiclass.OutputCodeClassiﬁer
method), 1704
score()
(sklearn.multioutput.MultiOutputClassiﬁer
method), 1709
score()
(sklearn.multioutput.MultiOutputRegressor
method), 1706
score() (sklearn.naive_bayes.BernoulliNB method), 1719
score() (sklearn.naive_bayes.GaussianNB method), 1712
2042
Index
scikit-learn user guide, Release 0.18.2
score()
(sklearn.naive_bayes.MultinomialNB
method),
1715
score() (sklearn.neighbors.KernelDensity method), 1767
score()
(sklearn.neighbors.KNeighborsClassiﬁer
method), 1730
score()
(sklearn.neighbors.KNeighborsRegressor
method), 1739
score()
(sklearn.neighbors.NearestCentroid
method),
1746
score()
(sklearn.neighbors.RadiusNeighborsClassiﬁer
method), 1734
score()
(sklearn.neighbors.RadiusNeighborsRegressor
method), 1744
score() (sklearn.neural_network.MLPClassiﬁer method),
1778
score() (sklearn.neural_network.MLPRegressor method),
1782
score() (sklearn.pipeline.Pipeline method), 1804
score() (sklearn.qda.QDA method), 1919
score()
(sklearn.semi_supervised.LabelPropagation
method), 1853
score()
(sklearn.semi_supervised.LabelSpreading
method), 1856
score() (sklearn.svm.LinearSVC method), 1866
score() (sklearn.svm.LinearSVR method), 1879
score() (sklearn.svm.NuSVC method), 1872
score() (sklearn.svm.NuSVR method), 1882
score() (sklearn.svm.SVC method), 1861
score() (sklearn.svm.SVR method), 1875
score()
(sklearn.tree.DecisionTreeClassiﬁer
method),
1895
score()
(sklearn.tree.DecisionTreeRegressor
method),
1901
score() (sklearn.tree.ExtraTreeClassiﬁer method), 1905
score() (sklearn.tree.ExtraTreeRegressor method), 1909
score_samples()
(sklearn.decomposition.FactorAnalysis
method), 1315
score_samples() (sklearn.decomposition.PCA method),
1299
score_samples() (sklearn.mixture.BayesianGaussianMixture
method), 1696
score_samples()
(sklearn.mixture.DPGMM
method),
1956
score_samples()
(sklearn.mixture.GaussianMixture
method), 1690
score_samples() (sklearn.mixture.GMM method), 1953
score_samples()
(sklearn.mixture.VBGMM
method),
1959
score_samples()
(sklearn.neighbors.KernelDensity
method), 1767
score_samples() (sklearn.neural_network.BernoulliRBM
method), 1772
SelectFdr (class in sklearn.feature_selection), 1423
SelectFpr (class in sklearn.feature_selection), 1421
SelectFromModel (class in sklearn.feature_selection),
1425
SelectFwe (class in sklearn.feature_selection), 1428
SelectKBest (class in sklearn.feature_selection), 1418
SelectPercentile (class in sklearn.feature_selection), 1415
set_params() (sklearn.base.BaseEstimator method), 1130
set_params() (sklearn.calibration.CalibratedClassiﬁerCV
method), 1786
set_params()
(sklearn.cluster.AfﬁnityPropagation
method), 1135
set_params()
(sklearn.cluster.AgglomerativeClustering
method), 1137
set_params() (sklearn.cluster.bicluster.SpectralBiclustering
method), 1172
set_params() (sklearn.cluster.bicluster.SpectralCoclustering
method), 1174
set_params() (sklearn.cluster.Birch method), 1140
set_params() (sklearn.cluster.DBSCAN method), 1143
set_params()
(sklearn.cluster.FeatureAgglomeration
method), 1147
set_params() (sklearn.cluster.KMeans method), 1150
set_params() (sklearn.cluster.MeanShift method), 1157
set_params()
(sklearn.cluster.MiniBatchKMeans
method), 1154
set_params() (sklearn.cluster.SpectralClustering method),
1160
set_params()
(sklearn.covariance.EllipticEnvelope
method), 1181
set_params()
(sklearn.covariance.EmpiricalCovariance
method), 1177
set_params() (sklearn.covariance.GraphLasso method),
1183
set_params()
(sklearn.covariance.GraphLassoCV
method), 1187
set_params()
(sklearn.covariance.LedoitWolf
method),
1190
set_params() (sklearn.covariance.MinCovDet method),
1194
set_params() (sklearn.covariance.OAS method), 1197
set_params()
(sklearn.covariance.ShrunkCovariance
method), 1199
set_params()
(sklearn.cross_decomposition.CCA
method), 1799
set_params() (sklearn.cross_decomposition.PLSCanonical
method), 1795
set_params() (sklearn.cross_decomposition.PLSRegression
method), 1791
set_params()
(sklearn.cross_decomposition.PLSSVD
method), 1800
set_params() (sklearn.decomposition.DictionaryLearning
method), 1335
set_params()
(sklearn.decomposition.FactorAnalysis
method), 1315
Index
2043
scikit-learn user guide, Release 0.18.2
set_params() (sklearn.decomposition.FastICA method),
1317
set_params()
(sklearn.decomposition.IncrementalPCA
method), 1304
set_params()
(sklearn.decomposition.KernelPCA
method), 1312
set_params() (sklearn.decomposition.LatentDirichletAllocation
method), 1342
set_params() (sklearn.decomposition.MiniBatchDictionaryLearning
method), 1338
set_params() (sklearn.decomposition.MiniBatchSparsePCA
method), 1329
set_params()
(sklearn.decomposition.NMF
method),
1324
set_params() (sklearn.decomposition.PCA method), 1300
set_params() (sklearn.decomposition.ProjectedGradientNMF
method), 1308
set_params()
(sklearn.decomposition.RandomizedPCA
method), 1946
set_params()
(sklearn.decomposition.SparseCoder
method), 1332
set_params()
(sklearn.decomposition.SparsePCA
method), 1327
set_params()
(sklearn.decomposition.TruncatedSVD
method), 1320
set_params() (sklearn.discriminant_analysis.LinearDiscriminantAnalysis
method), 1497
set_params() (sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
method), 1500
set_params() (sklearn.dummy.DummyClassiﬁer method),
1352
set_params()
(sklearn.dummy.DummyRegressor
method), 1354
set_params()
(sklearn.ensemble.AdaBoostClassiﬁer
method), 1358
set_params()
(sklearn.ensemble.AdaBoostRegressor
method), 1362
set_params()
(sklearn.ensemble.BaggingClassiﬁer
method), 1367
set_params()
(sklearn.ensemble.BaggingRegressor
method), 1370
set_params()
(sklearn.ensemble.ExtraTreesClassiﬁer
method), 432
set_params()
(sklearn.ensemble.ExtraTreesRegressor
method), 438
set_params() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 445
set_params() (sklearn.ensemble.GradientBoostingRegressor
method), 451
set_params() (sklearn.ensemble.IsolationForest method),
1373
set_params() (sklearn.ensemble.RandomForestClassiﬁer
method), 420
set_params() (sklearn.ensemble.RandomForestRegressor
method), 426
set_params() (sklearn.ensemble.RandomTreesEmbedding
method), 1376
set_params()
(sklearn.ensemble.VotingClassiﬁer
method), 1379
set_params()
(sklearn.feature_extraction.DictVectorizer
method), 1389
set_params()
(sklearn.feature_extraction.FeatureHasher
method), 1391
set_params() (sklearn.feature_extraction.image.PatchExtractor
method), 1396
set_params() (sklearn.feature_extraction.text.CountVectorizer
method), 1400
set_params() (sklearn.feature_extraction.text.HashingVectorizer
method), 1404
set_params() (sklearn.feature_extraction.text.TﬁdfTransformer
method), 1407
set_params() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1412
set_params() (sklearn.feature_selection.GenericUnivariateSelect
method), 1415
set_params()
(sklearn.feature_selection.RFE
method),
1433
set_params() (sklearn.feature_selection.RFECV method),
1437
set_params()
(sklearn.feature_selection.SelectFdr
method), 1425
set_params()
(sklearn.feature_selection.SelectFpr
method), 1423
set_params() (sklearn.feature_selection.SelectFromModel
method), 1428
set_params()
(sklearn.feature_selection.SelectFwe
method), 1430
set_params()
(sklearn.feature_selection.SelectKBest
method), 1420
set_params()
(sklearn.feature_selection.SelectPercentile
method), 1417
set_params() (sklearn.feature_selection.VarianceThreshold
method), 1439
set_params() (sklearn.gaussian_process.GaussianProcess
method), 1951
set_params() (sklearn.gaussian_process.GaussianProcessClassiﬁer
method), 1453
set_params() (sklearn.gaussian_process.GaussianProcessRegressor
method), 1449
set_params() (sklearn.gaussian_process.kernels.CompoundKernel
method), 1475
set_params() (sklearn.gaussian_process.kernels.ConstantKernel
method), 1461
set_params() (sklearn.gaussian_process.kernels.DotProduct
method), 1472
set_params() (sklearn.gaussian_process.kernels.Exponentiation
method), 1459
2044
Index
scikit-learn user guide, Release 0.18.2
set_params() (sklearn.gaussian_process.kernels.ExpSineSquared
method), 1470
set_params()
(sklearn.gaussian_process.kernels.Kernel
method), 1455
set_params()
(sklearn.gaussian_process.kernels.Matern
method), 1466
set_params() (sklearn.gaussian_process.kernels.PairwiseKernel
method), 1474
set_params()
(sklearn.gaussian_process.kernels.Product
method), 1457
set_params() (sklearn.gaussian_process.kernels.RationalQuadratic
method), 1468
set_params()
(sklearn.gaussian_process.kernels.RBF
method), 1464
set_params()
(sklearn.gaussian_process.kernels.Sum
method), 1456
set_params() (sklearn.gaussian_process.kernels.WhiteKernel
method), 1462
set_params()
(sklearn.grid_search.GridSearchCV
method), 1928
set_params() (sklearn.grid_search.RandomizedSearchCV
method), 1932
set_params()
(sklearn.isotonic.IsotonicRegression
method), 1479
set_params() (sklearn.kernel_approximation.AdditiveChi2Sampler
method), 1483
set_params()
(sklearn.kernel_approximation.Nystroem
method), 1485
set_params() (sklearn.kernel_approximation.RBFSampler
method), 1487
set_params() (sklearn.kernel_approximation.SkewedChi2Sampler
method), 1489
set_params() (sklearn.kernel_ridge.KernelRidge method),
1492
set_params() (sklearn.lda.LDA method), 1917
set_params()
(sklearn.linear_model.ARDRegression
method), 1505
set_params()
(sklearn.linear_model.BayesianRidge
method), 1508
set_params() (sklearn.linear_model.ElasticNet method),
1514
set_params()
(sklearn.linear_model.ElasticNetCV
method), 370
set_params()
(sklearn.linear_model.HuberRegressor
method), 1517
set_params() (sklearn.linear_model.Lars method), 1520
set_params()
(sklearn.linear_model.LarsCV
method),
373
set_params() (sklearn.linear_model.Lasso method), 1525
set_params() (sklearn.linear_model.LassoCV method),
379
set_params() (sklearn.linear_model.LassoLars method),
1529
set_params()
(sklearn.linear_model.LassoLarsCV
method), 383
set_params()
(sklearn.linear_model.LassoLarsIC
method), 414
set_params()
(sklearn.linear_model.LinearRegression
method), 1531
set_params()
(sklearn.linear_model.LogisticRegression
method), 1537
set_params() (sklearn.linear_model.LogisticRegressionCV
method), 388
set_params() (sklearn.linear_model.MultiTaskElasticNet
method), 1549
set_params() (sklearn.linear_model.MultiTaskElasticNetCV
method), 395
set_params()
(sklearn.linear_model.MultiTaskLasso
method), 1543
set_params()
(sklearn.linear_model.MultiTaskLassoCV
method), 401
set_params() (sklearn.linear_model.OrthogonalMatchingPursuit
method), 1552
set_params() (sklearn.linear_model.OrthogonalMatchingPursuitCV
method), 404
set_params()
(sklearn.linear_model.RandomizedLasso
method), 1567
set_params() (sklearn.linear_model.RandomizedLogisticRegression
method), 1570
set_params()
(sklearn.linear_model.RANSACRegressor
method), 1573
set_params() (sklearn.linear_model.Ridge method), 1577
set_params()
(sklearn.linear_model.RidgeClassiﬁer
method), 1580
set_params()
(sklearn.linear_model.RidgeClassiﬁerCV
method), 410
set_params() (sklearn.linear_model.RidgeCV method),
407
set_params()
(sklearn.linear_model.TheilSenRegressor
method), 1595
set_params() (sklearn.manifold.Isomap method), 1611
set_params() (sklearn.manifold.LocallyLinearEmbedding
method), 1608
set_params() (sklearn.manifold.MDS method), 1614
set_params()
(sklearn.manifold.SpectralEmbedding
method), 1616
set_params() (sklearn.manifold.TSNE method), 1619
set_params() (sklearn.mixture.BayesianGaussianMixture
method), 1697
set_params() (sklearn.mixture.DPGMM method), 1956
set_params() (sklearn.mixture.GaussianMixture method),
1691
set_params() (sklearn.mixture.GMM method), 1953
set_params() (sklearn.mixture.VBGMM method), 1960
set_params()
(sklearn.model_selection.GridSearchCV
method), 1232
Index
2045
scikit-learn user guide, Release 0.18.2
set_params() (sklearn.model_selection.RandomizedSearchCV
method), 1238
set_params()
(sklearn.multiclass.OneVsOneClassiﬁer
method), 1703
set_params()
(sklearn.multiclass.OneVsRestClassiﬁer
method), 1700
set_params()
(sklearn.multiclass.OutputCodeClassiﬁer
method), 1705
set_params() (sklearn.multioutput.MultiOutputClassiﬁer
method), 1709
set_params() (sklearn.multioutput.MultiOutputRegressor
method), 1707
set_params() (sklearn.naive_bayes.BernoulliNB method),
1719
set_params() (sklearn.naive_bayes.GaussianNB method),
1712
set_params()
(sklearn.naive_bayes.MultinomialNB
method), 1716
set_params() (sklearn.neighbors.KernelDensity method),
1767
set_params()
(sklearn.neighbors.KNeighborsClassiﬁer
method), 1730
set_params()
(sklearn.neighbors.KNeighborsRegressor
method), 1739
set_params()
(sklearn.neighbors.LSHForest
method),
1762
set_params()
(sklearn.neighbors.NearestCentroid
method), 1746
set_params()
(sklearn.neighbors.NearestNeighbors
method), 1725
set_params() (sklearn.neighbors.RadiusNeighborsClassiﬁer
method), 1735
set_params() (sklearn.neighbors.RadiusNeighborsRegressor
method), 1744
set_params()
(sklearn.neural_network.BernoulliRBM
method), 1773
set_params()
(sklearn.neural_network.MLPClassiﬁer
method), 1778
set_params()
(sklearn.neural_network.MLPRegressor
method), 1783
set_params()
(sklearn.pipeline.FeatureUnion
method),
1806
set_params() (sklearn.pipeline.Pipeline method), 1804
set_params() (sklearn.preprocessing.Binarizer method),
1810
set_params() (sklearn.preprocessing.FunctionTransformer
method), 1812
set_params()
(sklearn.preprocessing.Imputer
method),
1814
set_params()
(sklearn.preprocessing.KernelCenterer
method), 1815
set_params()
(sklearn.preprocessing.LabelBinarizer
method), 1818
set_params()
(sklearn.preprocessing.LabelEncoder
method), 1820
set_params()
(sklearn.preprocessing.MaxAbsScaler
method), 1824
set_params()
(sklearn.preprocessing.MinMaxScaler
method), 1827
set_params() (sklearn.preprocessing.MultiLabelBinarizer
method), 1822
set_params() (sklearn.preprocessing.Normalizer method),
1828
set_params()
(sklearn.preprocessing.OneHotEncoder
method), 1831
set_params() (sklearn.preprocessing.PolynomialFeatures
method), 1833
set_params()
(sklearn.preprocessing.RobustScaler
method), 1836
set_params()
(sklearn.preprocessing.StandardScaler
method), 1838
set_params() (sklearn.qda.QDA method), 1920
set_params() (sklearn.random_projection.GaussianRandomProjection
method), 1846
set_params() (sklearn.random_projection.SparseRandomProjection
method), 1849
set_params() (sklearn.semi_supervised.LabelPropagation
method), 1853
set_params()
(sklearn.semi_supervised.LabelSpreading
method), 1856
set_params() (sklearn.svm.LinearSVC method), 1866
set_params() (sklearn.svm.LinearSVR method), 1879
set_params() (sklearn.svm.NuSVC method), 1872
set_params() (sklearn.svm.NuSVR method), 1882
set_params() (sklearn.svm.OneClassSVM method), 1885
set_params() (sklearn.svm.SVC method), 1861
set_params() (sklearn.svm.SVR method), 1876
set_params()
(sklearn.tree.DecisionTreeClassiﬁer
method), 1896
set_params()
(sklearn.tree.DecisionTreeRegressor
method), 1901
set_params() (sklearn.tree.ExtraTreeClassiﬁer method),
1906
set_params() (sklearn.tree.ExtraTreeRegressor method),
1910
SGDClassiﬁer (class in sklearn.linear_model), 1581
SGDRegressor (class in sklearn.linear_model), 1588
shrunk_covariance()
(in
module
sklearn.covariance),
1201
ShrunkCovariance (class in sklearn.covariance), 1197
shufﬂe() (in module sklearn.utils), 1914
ShufﬂeSplit (class in sklearn.cross_validation), 1941
ShufﬂeSplit (class in sklearn.model_selection), 1216
sigmoid_kernel() (in module sklearn.metrics.pairwise),
1679
silhouette_samples() (in module sklearn.metrics), 1668
silhouette_score() (in module sklearn.metrics), 1667
2046
Index
scikit-learn user guide, Release 0.18.2
SkewedChi2Sampler
(class
in
sklearn.kernel_approximation), 1487
sklearn.base (module), 1129
sklearn.calibration (module), 1783
sklearn.cluster (module), 1133
sklearn.cluster.bicluster (module), 1169
sklearn.covariance (module), 1174
sklearn.cross_decomposition (module), 1787
sklearn.datasets (module), 1249
sklearn.decomposition (module), 1295
sklearn.discriminant_analysis (module), 1492
sklearn.dummy (module), 1349
sklearn.ensemble (module), 1354
sklearn.ensemble.partial_dependence (module), 1380
sklearn.exceptions (module), 1383
sklearn.feature_extraction (module), 1386
sklearn.feature_extraction.image (module), 1392
sklearn.feature_extraction.text (module), 1396
sklearn.feature_selection (module), 1413
sklearn.gaussian_process (module), 1444
sklearn.isotonic (module), 1476
sklearn.kernel_approximation (module), 1481
sklearn.kernel_ridge (module), 1489
sklearn.linear_model (module), 1501
sklearn.manifold (module), 1606
sklearn.metrics (module), 1622
sklearn.metrics.cluster (module), 1658
sklearn.metrics.pairwise (module), 1672
sklearn.mixture (module), 1686
sklearn.model_selection (module), 1203
sklearn.multiclass (module), 1697
sklearn.multioutput (module), 1705
sklearn.naive_bayes (module), 1709
sklearn.neighbors (module), 1719
sklearn.neural_network (module), 1770
sklearn.pipeline (module), 1801
sklearn.preprocessing (module), 1808
sklearn.random_projection (module), 1845
sklearn.semi_supervised (module), 1851
sklearn.svm (module), 1857
sklearn.tree (module), 1890
sklearn.utils (module), 1912
sparse_coef_ (sklearn.linear_model.ElasticNet attribute),
1514
sparse_coef_
(sklearn.linear_model.Lasso
attribute),
1525
sparse_coef_ (sklearn.linear_model.MultiTaskElasticNet
attribute), 1549
sparse_coef_ (sklearn.linear_model.MultiTaskLasso at-
tribute), 1544
sparse_encode()
(in
module
sklearn.decomposition),
1348
SparseCoder (class in sklearn.decomposition), 1330
SparsePCA (class in sklearn.decomposition), 1325
SparseRandomProjection
(class
in
sklearn.random_projection), 1847
sparsify()
(sklearn.linear_model.LogisticRegression
method), 1537
sparsify()
(sklearn.linear_model.LogisticRegressionCV
method), 389
sparsify() (sklearn.linear_model.PassiveAggressiveClassiﬁer
method), 1555
sparsify() (sklearn.linear_model.PassiveAggressiveRegressor
method), 1558
sparsify()
(sklearn.linear_model.Perceptron
method),
1562
sparsify() (sklearn.linear_model.SGDClassiﬁer method),
1586
sparsify() (sklearn.linear_model.SGDRegressor method),
1592
sparsify() (sklearn.svm.LinearSVC method), 1867
spectral_clustering() (in module sklearn.cluster), 1168
spectral_embedding() (in module sklearn.manifold), 1621
SpectralBiclustering (class in sklearn.cluster.bicluster),
1169
SpectralClustering (class in sklearn.cluster), 1157
SpectralCoclustering (class in sklearn.cluster.bicluster),
1172
SpectralEmbedding (class in sklearn.manifold), 1614
split()
(sklearn.model_selection.GroupKFold
method),
1207
split()
(sklearn.model_selection.GroupShufﬂeSplit
method), 1219
split() (sklearn.model_selection.KFold method), 1205
split()
(sklearn.model_selection.LeaveOneGroupOut
method), 1210
split() (sklearn.model_selection.LeaveOneOut method),
1214
split()
(sklearn.model_selection.LeavePGroupsOut
method), 1212
split()
(sklearn.model_selection.LeavePOut
method),
1215
split() (sklearn.model_selection.PredeﬁnedSplit method),
1222
split()
(sklearn.model_selection.ShufﬂeSplit
method),
1217
split() (sklearn.model_selection.StratiﬁedKFold method),
1209
split()
(sklearn.model_selection.StratiﬁedShufﬂeSplit
method), 1220
split()
(sklearn.model_selection.TimeSeriesSplit
method), 1223
staged_decision_function()
(sklearn.ensemble.AdaBoostClassiﬁer
method), 1358
staged_decision_function()
(sklearn.ensemble.GradientBoostingClassiﬁer
method), 445
Index
2047
scikit-learn user guide, Release 0.18.2
staged_decision_function()
(sklearn.ensemble.GradientBoostingRegressor
method), 452
staged_predict()
(sklearn.ensemble.AdaBoostClassiﬁer
method), 1358
staged_predict()
(sklearn.ensemble.AdaBoostRegressor
method), 1362
staged_predict() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 445
staged_predict() (sklearn.ensemble.GradientBoostingRegressor
method), 452
staged_predict_proba() (sklearn.ensemble.AdaBoostClassiﬁer
method), 1358
staged_predict_proba() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 445
staged_score()
(sklearn.ensemble.AdaBoostClassiﬁer
method), 1359
staged_score()
(sklearn.ensemble.AdaBoostRegressor
method), 1362
StandardScaler (class in sklearn.preprocessing), 1836
std_
(sklearn.preprocessing.StandardScaler
attribute),
1838
StratiﬁedKFold (class in sklearn.cross_validation), 1940
StratiﬁedKFold (class in sklearn.model_selection), 1207
StratiﬁedShufﬂeSplit (class in sklearn.cross_validation),
1942
StratiﬁedShufﬂeSplit (class in sklearn.model_selection),
1219
Sum (class in sklearn.gaussian_process.kernels), 1455
SVC (class in sklearn.svm), 1857
SVR (class in sklearn.svm), 1872
T
TﬁdfTransformer
(class
in
sklearn.feature_extraction.text), 1405
TﬁdfVectorizer (class in sklearn.feature_extraction.text),
1408
TheilSenRegressor (class in sklearn.linear_model), 1593
theta (sklearn.gaussian_process.kernels.CompoundKernel
attribute), 1475
theta
(sklearn.gaussian_process.kernels.ConstantKernel
attribute), 1461
theta (sklearn.gaussian_process.kernels.DotProduct at-
tribute), 1472
theta
(sklearn.gaussian_process.kernels.Exponentiation
attribute), 1459
theta (sklearn.gaussian_process.kernels.ExpSineSquared
attribute), 1470
theta (sklearn.gaussian_process.kernels.Kernel attribute),
1455
theta (sklearn.gaussian_process.kernels.Matern attribute),
1466
theta
(sklearn.gaussian_process.kernels.PairwiseKernel
attribute), 1474
theta
(sklearn.gaussian_process.kernels.Product
at-
tribute), 1458
theta (sklearn.gaussian_process.kernels.RationalQuadratic
attribute), 1468
theta (sklearn.gaussian_process.kernels.RBF attribute),
1464
theta (sklearn.gaussian_process.kernels.Sum attribute),
1456
theta (sklearn.gaussian_process.kernels.WhiteKernel at-
tribute), 1462
TimeSeriesSplit (class in sklearn.model_selection), 1222
train_test_split() (in module sklearn.cross_validation),
1970
train_test_split() (in module sklearn.model_selection),
1224
transform (sklearn.pipeline.Pipeline attribute), 1804
transform() (sklearn.cluster.Birch method), 1140
transform()
(sklearn.cluster.FeatureAgglomeration
method), 1147
transform() (sklearn.cluster.KMeans method), 1150
transform() (sklearn.cluster.MiniBatchKMeans method),
1154
transform() (sklearn.cross_decomposition.CCA method),
1799
transform() (sklearn.cross_decomposition.PLSCanonical
method), 1795
transform() (sklearn.cross_decomposition.PLSRegression
method), 1791
transform()
(sklearn.cross_decomposition.PLSSVD
method), 1801
transform()
(sklearn.decomposition.DictionaryLearning
method), 1335
transform()
(sklearn.decomposition.FactorAnalysis
method), 1315
transform()
(sklearn.decomposition.FastICA
method),
1317
transform()
(sklearn.decomposition.IncrementalPCA
method), 1304
transform() (sklearn.decomposition.KernelPCA method),
1312
transform() (sklearn.decomposition.LatentDirichletAllocation
method), 1342
transform() (sklearn.decomposition.MiniBatchDictionaryLearning
method), 1338
transform() (sklearn.decomposition.MiniBatchSparsePCA
method), 1329
transform() (sklearn.decomposition.NMF method), 1324
transform() (sklearn.decomposition.PCA method), 1300
transform() (sklearn.decomposition.ProjectedGradientNMF
method), 1309
transform()
(sklearn.decomposition.RandomizedPCA
method), 1946
transform()
(sklearn.decomposition.SparseCoder
method), 1332
2048
Index
scikit-learn user guide, Release 0.18.2
transform() (sklearn.decomposition.SparsePCA method),
1327
transform()
(sklearn.decomposition.TruncatedSVD
method), 1320
transform() (sklearn.discriminant_analysis.LinearDiscriminantAnalysis
method), 1497
transform()
(sklearn.ensemble.ExtraTreesClassiﬁer
method), 433
transform()
(sklearn.ensemble.ExtraTreesRegressor
method), 438
transform() (sklearn.ensemble.GradientBoostingClassiﬁer
method), 445
transform() (sklearn.ensemble.GradientBoostingRegressor
method), 452
transform()
(sklearn.ensemble.RandomForestClassiﬁer
method), 420
transform()
(sklearn.ensemble.RandomForestRegressor
method), 426
transform() (sklearn.ensemble.RandomTreesEmbedding
method), 1376
transform() (sklearn.ensemble.VotingClassiﬁer method),
1380
transform()
(sklearn.feature_extraction.DictVectorizer
method), 1389
transform()
(sklearn.feature_extraction.FeatureHasher
method), 1391
transform() (sklearn.feature_extraction.image.PatchExtractor
method), 1396
transform() (sklearn.feature_extraction.text.CountVectorizer
method), 1401
transform() (sklearn.feature_extraction.text.HashingVectorizer
method), 1405
transform() (sklearn.feature_extraction.text.TﬁdfTransformer
method), 1407
transform() (sklearn.feature_extraction.text.TﬁdfVectorizer
method), 1412
transform() (sklearn.feature_selection.GenericUnivariateSelect
method), 1415
transform()
(sklearn.feature_selection.RFE
method),
1433
transform() (sklearn.feature_selection.RFECV method),
1437
transform() (sklearn.feature_selection.SelectFdr method),
1425
transform() (sklearn.feature_selection.SelectFpr method),
1423
transform() (sklearn.feature_selection.SelectFromModel
method), 1428
transform()
(sklearn.feature_selection.SelectFwe
method), 1430
transform()
(sklearn.feature_selection.SelectKBest
method), 1420
transform()
(sklearn.feature_selection.SelectPercentile
method), 1418
transform() (sklearn.feature_selection.VarianceThreshold
method), 1439
transform() (sklearn.grid_search.GridSearchCV method),
1928
transform()
(sklearn.grid_search.RandomizedSearchCV
method), 1932
transform()
(sklearn.isotonic.IsotonicRegression
method), 1479
transform() (sklearn.kernel_approximation.AdditiveChi2Sampler
method), 1483
transform()
(sklearn.kernel_approximation.Nystroem
method), 1485
transform() (sklearn.kernel_approximation.RBFSampler
method), 1487
transform() (sklearn.kernel_approximation.SkewedChi2Sampler
method), 1489
transform() (sklearn.lda.LDA method), 1918
transform()
(sklearn.linear_model.LogisticRegression
method), 1538
transform() (sklearn.linear_model.LogisticRegressionCV
method), 389
transform() (sklearn.linear_model.Perceptron method),
1563
transform()
(sklearn.linear_model.RandomizedLasso
method), 1567
transform() (sklearn.linear_model.RandomizedLogisticRegression
method), 1570
transform()
(sklearn.linear_model.SGDClassiﬁer
method), 1587
transform()
(sklearn.linear_model.SGDRegressor
method), 1592
transform() (sklearn.manifold.Isomap method), 1611
transform()
(sklearn.manifold.LocallyLinearEmbedding
method), 1608
transform()
(sklearn.model_selection.GridSearchCV
method), 1232
transform() (sklearn.model_selection.RandomizedSearchCV
method), 1238
transform()
(sklearn.neural_network.BernoulliRBM
method), 1773
transform()
(sklearn.pipeline.FeatureUnion
method),
1806
transform()
(sklearn.preprocessing.Binarizer
method),
1810
transform()
(sklearn.preprocessing.Imputer
method),
1814
transform()
(sklearn.preprocessing.KernelCenterer
method), 1815
transform()
(sklearn.preprocessing.LabelBinarizer
method), 1818
transform()
(sklearn.preprocessing.LabelEncoder
method), 1820
transform()
(sklearn.preprocessing.MaxAbsScaler
method), 1824
Index
2049
scikit-learn user guide, Release 0.18.2
transform()
(sklearn.preprocessing.MinMaxScaler
method), 1827
transform()
(sklearn.preprocessing.MultiLabelBinarizer
method), 1822
transform() (sklearn.preprocessing.Normalizer method),
1828
transform()
(sklearn.preprocessing.OneHotEncoder
method), 1831
transform()
(sklearn.preprocessing.PolynomialFeatures
method), 1833
transform()
(sklearn.preprocessing.RobustScaler
method), 1836
transform()
(sklearn.preprocessing.StandardScaler
method), 1838
transform() (sklearn.random_projection.GaussianRandomProjection
method), 1847
transform() (sklearn.random_projection.SparseRandomProjection
method), 1849
transform() (sklearn.svm.LinearSVC method), 1867
transform() (sklearn.tree.DecisionTreeClassiﬁer method),
1896
transform()
(sklearn.tree.DecisionTreeRegressor
method), 1902
transform()
(sklearn.tree.ExtraTreeClassiﬁer
method),
1906
transform()
(sklearn.tree.ExtraTreeRegressor
method),
1910
TransformerMixin (class in sklearn.base), 1132
TruncatedSVD (class in sklearn.decomposition), 1318
TSNE (class in sklearn.manifold), 1616
two_point_correlation()
(sklearn.neighbors.BallTree
method), 1751
two_point_correlation()
(sklearn.neighbors.KDTree
method), 1757
U
UndeﬁnedMetricWarning (class in sklearn.exceptions),
1385
V
v_measure_score() (in module sklearn.metrics), 1669
validation_curve() (in module sklearn.learning_curve),
1963
validation_curve() (in module sklearn.model_selection),
1247
value_type (sklearn.gaussian_process.kernels.Hyperparameter
attribute), 1476
VarianceThreshold (class in sklearn.feature_selection),
1438
VBGMM (class in sklearn.mixture), 1956
VotingClassiﬁer (class in sklearn.ensemble), 1377
W
ward_tree() (in module sklearn.cluster), 1163
WhiteKernel (class in sklearn.gaussian_process.kernels),
1461
X
X_ (sklearn.isotonic.IsotonicRegression attribute), 1478
Y
y_ (sklearn.isotonic.IsotonicRegression attribute), 1480
Z
zero_one_loss() (in module sklearn.metrics), 1649
2050
Index
